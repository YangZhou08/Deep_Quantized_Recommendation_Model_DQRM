Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 96.49 ms/it, loss 0.522147
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 97.17 ms/it, loss 0.519562
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 94.87 ms/it, loss 0.517596
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 95.25 ms/it, loss 0.517922
Finished training it 2048/76743 of epoch 0, 87.73 ms/it, loss 0.512356
Finished training it 2048/76743 of epoch 0, 87.66 ms/it, loss 0.510121
Finished training it 2048/76743 of epoch 0, 87.75 ms/it, loss 0.512447
Finished training it 2048/76743 of epoch 0, 87.73 ms/it, loss 0.513999
Finished training it 3072/76743 of epoch 0, 87.68 ms/it, loss 0.509756
Finished training it 3072/76743 of epoch 0, 87.56 ms/it, loss 0.509070
Finished training it 3072/76743 of epoch 0, 87.45 ms/it, loss 0.512294
Finished training it 3072/76743 of epoch 0, 87.46 ms/it, loss 0.509518
Finished training it 4096/76743 of epoch 0, 94.84 ms/it, loss 0.510411
Finished training it 4096/76743 of epoch 0, 94.09 ms/it, loss 0.511156
Finished training it 4096/76743 of epoch 0, 94.77 ms/it, loss 0.511390
Finished training it 4096/76743 of epoch 0, 94.28 ms/it, loss 0.511176
Finished training it 5120/76743 of epoch 0, 216.19 ms/it, loss 0.508344
Finished training it 5120/76743 of epoch 0, 216.77 ms/it, loss 0.512550
Finished training it 5120/76743 of epoch 0, 218.93 ms/it, loss 0.508690
Finished training it 5120/76743 of epoch 0, 215.44 ms/it, loss 0.509533
Finished training it 6144/76743 of epoch 0, 283.77 ms/it, loss 0.509307
Finished training it 6144/76743 of epoch 0, 283.66 ms/it, loss 0.508333
Finished training it 6144/76743 of epoch 0, 283.68 ms/it, loss 0.508340
Finished training it 6144/76743 of epoch 0, 284.01 ms/it, loss 0.507581
Finished training it 7168/76743 of epoch 0, 249.91 ms/it, loss 0.509263
Finished training it 7168/76743 of epoch 0, 249.90 ms/it, loss 0.508215
Finished training it 7168/76743 of epoch 0, 251.09 ms/it, loss 0.510399
Finished training it 7168/76743 of epoch 0, 249.70 ms/it, loss 0.510922
Finished training it 8192/76743 of epoch 0, 122.44 ms/it, loss 0.510318
Finished training it 8192/76743 of epoch 0, 121.76 ms/it, loss 0.509985
Finished training it 8192/76743 of epoch 0, 121.61 ms/it, loss 0.508201
Finished training it 8192/76743 of epoch 0, 122.07 ms/it, loss 0.508524
Finished training it 9216/76743 of epoch 0, 174.10 ms/it, loss 0.510364
Finished training it 9216/76743 of epoch 0, 175.61 ms/it, loss 0.508565
Finished training it 9216/76743 of epoch 0, 174.61 ms/it, loss 0.508907
Finished training it 9216/76743 of epoch 0, 175.90 ms/it, loss 0.508275
Finished training it 10240/76743 of epoch 0, 208.13 ms/it, loss 0.507269
Finished training it 10240/76743 of epoch 0, 207.37 ms/it, loss 0.509604
Finished training it 10240/76743 of epoch 0, 207.96 ms/it, loss 0.509018
Finished training it 10240/76743 of epoch 0, 207.55 ms/it, loss 0.506726
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2479486.0
get out
0 has test check 2479486.0 and sample count 3274240
 accuracy 75.727 %, best 75.727 %, roc auc score 0.7223, best 0.7223
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2479486.0
get out
1 has test check 2479486.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 174.90 ms/it, loss 0.506672
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2479486.0
get out
3 has test check 2479486.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 172.90 ms/it, loss 0.506277
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2479486.0
get out
2 has test check 2479486.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 173.08 ms/it, loss 0.507034
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 175.89 ms/it, loss 0.508525
Finished training it 12288/76743 of epoch 0, 174.47 ms/it, loss 0.506828
Finished training it 12288/76743 of epoch 0, 171.48 ms/it, loss 0.507893
Finished training it 12288/76743 of epoch 0, 171.43 ms/it, loss 0.504861
Finished training it 12288/76743 of epoch 0, 173.44 ms/it, loss 0.507433
Finished training it 13312/76743 of epoch 0, 177.68 ms/it, loss 0.504196
Finished training it 13312/76743 of epoch 0, 175.25 ms/it, loss 0.503471
Finished training it 13312/76743 of epoch 0, 175.50 ms/it, loss 0.502869
Finished training it 13312/76743 of epoch 0, 176.81 ms/it, loss 0.503846
Finished training it 14336/76743 of epoch 0, 157.38 ms/it, loss 0.503004
Finished training it 14336/76743 of epoch 0, 158.65 ms/it, loss 0.503169
Finished training it 14336/76743 of epoch 0, 159.56 ms/it, loss 0.500747
Finished training it 14336/76743 of epoch 0, 157.27 ms/it, loss 0.500068
Finished training it 15360/76743 of epoch 0, 179.56 ms/it, loss 0.500367
Finished training it 15360/76743 of epoch 0, 177.26 ms/it, loss 0.499440
Finished training it 15360/76743 of epoch 0, 177.60 ms/it, loss 0.499756
Finished training it 15360/76743 of epoch 0, 178.36 ms/it, loss 0.500654
Finished training it 16384/76743 of epoch 0, 173.83 ms/it, loss 0.500020
Finished training it 16384/76743 of epoch 0, 172.83 ms/it, loss 0.501441
Finished training it 16384/76743 of epoch 0, 174.91 ms/it, loss 0.497349
Finished training it 16384/76743 of epoch 0, 173.55 ms/it, loss 0.498623
Finished training it 17408/76743 of epoch 0, 165.05 ms/it, loss 0.502128
Finished training it 17408/76743 of epoch 0, 164.51 ms/it, loss 0.498487
Finished training it 17408/76743 of epoch 0, 164.56 ms/it, loss 0.500365
Finished training it 17408/76743 of epoch 0, 165.99 ms/it, loss 0.496870
Finished training it 18432/76743 of epoch 0, 147.84 ms/it, loss 0.500366
Finished training it 18432/76743 of epoch 0, 148.48 ms/it, loss 0.500582
Finished training it 18432/76743 of epoch 0, 149.39 ms/it, loss 0.500216
Finished training it 18432/76743 of epoch 0, 147.45 ms/it, loss 0.499570
Finished training it 19456/76743 of epoch 0, 88.50 ms/it, loss 0.499494
Finished training it 19456/76743 of epoch 0, 88.51 ms/it, loss 0.498656
Finished training it 19456/76743 of epoch 0, 87.59 ms/it, loss 0.500600
Finished training it 19456/76743 of epoch 0, 87.59 ms/it, loss 0.500422
Finished training it 20480/76743 of epoch 0, 87.67 ms/it, loss 0.500285
Finished training it 20480/76743 of epoch 0, 86.97 ms/it, loss 0.500797
Finished training it 20480/76743 of epoch 0, 87.62 ms/it, loss 0.500598
Finished training it 20480/76743 of epoch 0, 86.79 ms/it, loss 0.501599
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2495338.0
get out
0 has test check 2495338.0 and sample count 3274240
 accuracy 76.211 %, best 76.211 %, roc auc score 0.7376, best 0.7376
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2495338.0
get out
1 has test check 2495338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 87.38 ms/it, loss 0.498950
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2495338.0
get out
3 has test check 2495338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 87.46 ms/it, loss 0.497894
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2495338.0
get out
2 has test check 2495338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 87.72 ms/it, loss 0.496498
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 87.40 ms/it, loss 0.497611
Finished training it 22528/76743 of epoch 0, 87.03 ms/it, loss 0.497341
Finished training it 22528/76743 of epoch 0, 87.26 ms/it, loss 0.495354
Finished training it 22528/76743 of epoch 0, 87.13 ms/it, loss 0.498152
Finished training it 22528/76743 of epoch 0, 87.22 ms/it, loss 0.498259
Finished training it 23552/76743 of epoch 0, 87.86 ms/it, loss 0.494746
Finished training it 23552/76743 of epoch 0, 87.92 ms/it, loss 0.496184
Finished training it 23552/76743 of epoch 0, 87.98 ms/it, loss 0.496973
Finished training it 23552/76743 of epoch 0, 87.99 ms/it, loss 0.495671
Finished training it 24576/76743 of epoch 0, 87.95 ms/it, loss 0.495894
Finished training it 24576/76743 of epoch 0, 87.64 ms/it, loss 0.495957
Finished training it 24576/76743 of epoch 0, 87.60 ms/it, loss 0.495658
Finished training it 24576/76743 of epoch 0, 87.91 ms/it, loss 0.499474
Finished training it 25600/76743 of epoch 0, 92.53 ms/it, loss 0.496456
Finished training it 25600/76743 of epoch 0, 93.24 ms/it, loss 0.498364
Finished training it 25600/76743 of epoch 0, 93.24 ms/it, loss 0.497089
Finished training it 25600/76743 of epoch 0, 92.51 ms/it, loss 0.494967
Finished training it 26624/76743 of epoch 0, 108.37 ms/it, loss 0.495004
Finished training it 26624/76743 of epoch 0, 108.44 ms/it, loss 0.493937
Finished training it 26624/76743 of epoch 0, 107.86 ms/it, loss 0.495415
Finished training it 26624/76743 of epoch 0, 107.76 ms/it, loss 0.494678
Finished training it 27648/76743 of epoch 0, 87.58 ms/it, loss 0.496822
Finished training it 27648/76743 of epoch 0, 86.76 ms/it, loss 0.496706
Finished training it 27648/76743 of epoch 0, 87.78 ms/it, loss 0.497201
Finished training it 27648/76743 of epoch 0, 86.73 ms/it, loss 0.495581
Finished training it 28672/76743 of epoch 0, 87.61 ms/it, loss 0.494359
Finished training it 28672/76743 of epoch 0, 86.47 ms/it, loss 0.494552
Finished training it 28672/76743 of epoch 0, 86.74 ms/it, loss 0.496740
Finished training it 28672/76743 of epoch 0, 87.43 ms/it, loss 0.498380
Finished training it 29696/76743 of epoch 0, 87.35 ms/it, loss 0.494078
Finished training it 29696/76743 of epoch 0, 86.58 ms/it, loss 0.493671
Finished training it 29696/76743 of epoch 0, 86.55 ms/it, loss 0.491987
Finished training it 29696/76743 of epoch 0, 87.42 ms/it, loss 0.494719
Finished training it 30720/76743 of epoch 0, 87.95 ms/it, loss 0.493758
Finished training it 30720/76743 of epoch 0, 88.15 ms/it, loss 0.492305
Finished training it 30720/76743 of epoch 0, 88.03 ms/it, loss 0.490175
Finished training it 30720/76743 of epoch 0, 87.73 ms/it, loss 0.493636
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2504292.0
get out
0 has test check 2504292.0 and sample count 3274240
 accuracy 76.485 %, best 76.485 %, roc auc score 0.7447, best 0.7447
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2504292.0
get out
1 has test check 2504292.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 86.57 ms/it, loss 0.493506
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2504292.0
get out
2 has test check 2504292.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 86.55 ms/it, loss 0.496475
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2504292.0
get out
3 has test check 2504292.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 86.44 ms/it, loss 0.496790
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 86.65 ms/it, loss 0.494630
Finished training it 32768/76743 of epoch 0, 86.97 ms/it, loss 0.491970
Finished training it 32768/76743 of epoch 0, 86.86 ms/it, loss 0.493051
Finished training it 32768/76743 of epoch 0, 87.10 ms/it, loss 0.493966
Finished training it 32768/76743 of epoch 0, 86.95 ms/it, loss 0.492103
Finished training it 33792/76743 of epoch 0, 86.07 ms/it, loss 0.496141
Finished training it 33792/76743 of epoch 0, 85.87 ms/it, loss 0.493117
Finished training it 33792/76743 of epoch 0, 86.04 ms/it, loss 0.493607
Finished training it 33792/76743 of epoch 0, 85.81 ms/it, loss 0.492805
Finished training it 34816/76743 of epoch 0, 92.52 ms/it, loss 0.490844
Finished training it 34816/76743 of epoch 0, 92.56 ms/it, loss 0.492592
Finished training it 34816/76743 of epoch 0, 92.61 ms/it, loss 0.490760
Finished training it 34816/76743 of epoch 0, 92.57 ms/it, loss 0.494085
Finished training it 35840/76743 of epoch 0, 93.43 ms/it, loss 0.493234
Finished training it 35840/76743 of epoch 0, 93.09 ms/it, loss 0.491654
Finished training it 35840/76743 of epoch 0, 93.35 ms/it, loss 0.491129
Finished training it 35840/76743 of epoch 0, 93.13 ms/it, loss 0.492216
Finished training it 36864/76743 of epoch 0, 87.37 ms/it, loss 0.489355
Finished training it 36864/76743 of epoch 0, 87.17 ms/it, loss 0.492051
Finished training it 36864/76743 of epoch 0, 87.67 ms/it, loss 0.492995
Finished training it 36864/76743 of epoch 0, 87.65 ms/it, loss 0.492252
Finished training it 37888/76743 of epoch 0, 87.66 ms/it, loss 0.490409
Finished training it 37888/76743 of epoch 0, 87.93 ms/it, loss 0.488422
Finished training it 37888/76743 of epoch 0, 88.08 ms/it, loss 0.491750
Finished training it 37888/76743 of epoch 0, 87.91 ms/it, loss 0.491894
Finished training it 38912/76743 of epoch 0, 87.15 ms/it, loss 0.492009
Finished training it 38912/76743 of epoch 0, 87.08 ms/it, loss 0.491829
Finished training it 38912/76743 of epoch 0, 86.73 ms/it, loss 0.492856
Finished training it 38912/76743 of epoch 0, 86.54 ms/it, loss 0.491224
Finished training it 39936/76743 of epoch 0, 87.53 ms/it, loss 0.489810
Finished training it 39936/76743 of epoch 0, 87.12 ms/it, loss 0.490134
Finished training it 39936/76743 of epoch 0, 87.39 ms/it, loss 0.488203
Finished training it 39936/76743 of epoch 0, 86.89 ms/it, loss 0.493268
Finished training it 40960/76743 of epoch 0, 86.76 ms/it, loss 0.490287
Finished training it 40960/76743 of epoch 0, 86.91 ms/it, loss 0.492581
Finished training it 40960/76743 of epoch 0, 87.26 ms/it, loss 0.486983
Finished training it 40960/76743 of epoch 0, 87.51 ms/it, loss 0.488807
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2509046.0
get out
0 has test check 2509046.0 and sample count 3274240
 accuracy 76.630 %, best 76.630 %, roc auc score 0.7478, best 0.7478
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2509046.0
get out
2 has test check 2509046.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 86.33 ms/it, loss 0.489261
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2509046.0
get out
1 has test check 2509046.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 86.68 ms/it, loss 0.491267
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 86.73 ms/it, loss 0.490564
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2509046.0
get out
3 has test check 2509046.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 86.24 ms/it, loss 0.489604
Finished training it 43008/76743 of epoch 0, 85.15 ms/it, loss 0.491707
Finished training it 43008/76743 of epoch 0, 85.09 ms/it, loss 0.493268
Finished training it 43008/76743 of epoch 0, 85.62 ms/it, loss 0.489892
Finished training it 43008/76743 of epoch 0, 85.66 ms/it, loss 0.486780
Finished training it 44032/76743 of epoch 0, 86.66 ms/it, loss 0.489061
Finished training it 44032/76743 of epoch 0, 86.79 ms/it, loss 0.491633
Finished training it 44032/76743 of epoch 0, 86.32 ms/it, loss 0.490860
Finished training it 44032/76743 of epoch 0, 86.13 ms/it, loss 0.490637
Finished training it 45056/76743 of epoch 0, 86.20 ms/it, loss 0.487790
Finished training it 45056/76743 of epoch 0, 86.55 ms/it, loss 0.491322
Finished training it 45056/76743 of epoch 0, 86.02 ms/it, loss 0.492654
Finished training it 45056/76743 of epoch 0, 86.47 ms/it, loss 0.490446
Finished training it 46080/76743 of epoch 0, 86.42 ms/it, loss 0.490393
Finished training it 46080/76743 of epoch 0, 86.38 ms/it, loss 0.490876
Finished training it 46080/76743 of epoch 0, 86.15 ms/it, loss 0.488578
Finished training it 46080/76743 of epoch 0, 86.04 ms/it, loss 0.489083
Finished training it 47104/76743 of epoch 0, 87.50 ms/it, loss 0.487818
Finished training it 47104/76743 of epoch 0, 87.44 ms/it, loss 0.487854
Finished training it 47104/76743 of epoch 0, 87.23 ms/it, loss 0.485944
Finished training it 47104/76743 of epoch 0, 87.01 ms/it, loss 0.486650
Finished training it 48128/76743 of epoch 0, 87.30 ms/it, loss 0.487384
Finished training it 48128/76743 of epoch 0, 87.27 ms/it, loss 0.487303
Finished training it 48128/76743 of epoch 0, 86.70 ms/it, loss 0.487771
Finished training it 48128/76743 of epoch 0, 86.90 ms/it, loss 0.491389
Finished training it 49152/76743 of epoch 0, 86.85 ms/it, loss 0.486260
Finished training it 49152/76743 of epoch 0, 87.18 ms/it, loss 0.487942
Finished training it 49152/76743 of epoch 0, 87.15 ms/it, loss 0.489744
Finished training it 49152/76743 of epoch 0, 86.46 ms/it, loss 0.487862
Finished training it 50176/76743 of epoch 0, 86.81 ms/it, loss 0.487725
Finished training it 50176/76743 of epoch 0, 87.00 ms/it, loss 0.489128
Finished training it 50176/76743 of epoch 0, 86.70 ms/it, loss 0.486744
Finished training it 50176/76743 of epoch 0, 87.04 ms/it, loss 0.489629
Finished training it 51200/76743 of epoch 0, 87.24 ms/it, loss 0.488928
Finished training it 51200/76743 of epoch 0, 87.00 ms/it, loss 0.486365
Finished training it 51200/76743 of epoch 0, 86.57 ms/it, loss 0.490793
Finished training it 51200/76743 of epoch 0, 86.64 ms/it, loss 0.487992
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2511284.0
get out
0 has test check 2511284.0 and sample count 3274240
 accuracy 76.698 %, best 76.698 %, roc auc score 0.7502, best 0.7502
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2511284.0
get out
3 has test check 2511284.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 85.54 ms/it, loss 0.487145
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2511284.0
get out
1 has test check 2511284.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 86.22 ms/it, loss 0.488210
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 86.04 ms/it, loss 0.485992
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2511284.0
get out
2 has test check 2511284.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 85.70 ms/it, loss 0.486600
Finished training it 53248/76743 of epoch 0, 86.56 ms/it, loss 0.486475
Finished training it 53248/76743 of epoch 0, 86.30 ms/it, loss 0.488585
Finished training it 53248/76743 of epoch 0, 86.28 ms/it, loss 0.488518
Finished training it 53248/76743 of epoch 0, 86.48 ms/it, loss 0.487488
Finished training it 54272/76743 of epoch 0, 86.45 ms/it, loss 0.489624
Finished training it 54272/76743 of epoch 0, 85.85 ms/it, loss 0.487047
Finished training it 54272/76743 of epoch 0, 86.11 ms/it, loss 0.487633
Finished training it 54272/76743 of epoch 0, 85.99 ms/it, loss 0.486669
Finished training it 55296/76743 of epoch 0, 99.21 ms/it, loss 0.489648
Finished training it 55296/76743 of epoch 0, 99.41 ms/it, loss 0.484599
Finished training it 55296/76743 of epoch 0, 99.02 ms/it, loss 0.485949
Finished training it 55296/76743 of epoch 0, 98.99 ms/it, loss 0.487172
Finished training it 56320/76743 of epoch 0, 86.32 ms/it, loss 0.483980
Finished training it 56320/76743 of epoch 0, 86.01 ms/it, loss 0.486865
Finished training it 56320/76743 of epoch 0, 85.97 ms/it, loss 0.485482
Finished training it 56320/76743 of epoch 0, 86.24 ms/it, loss 0.488000
Finished training it 57344/76743 of epoch 0, 87.70 ms/it, loss 0.487562
Finished training it 57344/76743 of epoch 0, 87.99 ms/it, loss 0.490222
Finished training it 57344/76743 of epoch 0, 87.57 ms/it, loss 0.485583
Finished training it 57344/76743 of epoch 0, 87.89 ms/it, loss 0.484973
Finished training it 58368/76743 of epoch 0, 87.32 ms/it, loss 0.489002
Finished training it 58368/76743 of epoch 0, 87.33 ms/it, loss 0.485958
Finished training it 58368/76743 of epoch 0, 86.64 ms/it, loss 0.486926
Finished training it 58368/76743 of epoch 0, 86.84 ms/it, loss 0.489108
Finished training it 59392/76743 of epoch 0, 87.06 ms/it, loss 0.488042
Finished training it 59392/76743 of epoch 0, 87.11 ms/it, loss 0.486035
Finished training it 59392/76743 of epoch 0, 86.58 ms/it, loss 0.486146
Finished training it 59392/76743 of epoch 0, 86.38 ms/it, loss 0.484343
Finished training it 60416/76743 of epoch 0, 86.84 ms/it, loss 0.485645
Finished training it 60416/76743 of epoch 0, 87.08 ms/it, loss 0.484103
Finished training it 60416/76743 of epoch 0, 87.43 ms/it, loss 0.485511
Finished training it 60416/76743 of epoch 0, 87.46 ms/it, loss 0.484368
Finished training it 61440/76743 of epoch 0, 86.85 ms/it, loss 0.486260
Finished training it 61440/76743 of epoch 0, 87.67 ms/it, loss 0.485588
Finished training it 61440/76743 of epoch 0, 87.46 ms/it, loss 0.487895
Finished training it 61440/76743 of epoch 0, 87.09 ms/it, loss 0.488182
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2514085.0
get out
0 has test check 2514085.0 and sample count 3274240
 accuracy 76.784 %, best 76.784 %, roc auc score 0.7517, best 0.7517
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2514085.0
get out
1 has test check 2514085.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 87.23 ms/it, loss 0.487150
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2514085.0
get out
2 has test check 2514085.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 87.14 ms/it, loss 0.484556
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 87.49 ms/it, loss 0.486849
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2514085.0
get out
3 has test check 2514085.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 86.82 ms/it, loss 0.485323
Finished training it 63488/76743 of epoch 0, 86.24 ms/it, loss 0.487801
Finished training it 63488/76743 of epoch 0, 86.10 ms/it, loss 0.484666
Finished training it 63488/76743 of epoch 0, 85.50 ms/it, loss 0.485796
Finished training it 63488/76743 of epoch 0, 85.80 ms/it, loss 0.484794
Finished training it 64512/76743 of epoch 0, 86.39 ms/it, loss 0.484959
Finished training it 64512/76743 of epoch 0, 86.82 ms/it, loss 0.484595
Finished training it 64512/76743 of epoch 0, 86.86 ms/it, loss 0.487224
Finished training it 64512/76743 of epoch 0, 86.04 ms/it, loss 0.483905
Finished training it 65536/76743 of epoch 0, 86.96 ms/it, loss 0.487817
Finished training it 65536/76743 of epoch 0, 86.43 ms/it, loss 0.485585
Finished training it 65536/76743 of epoch 0, 86.84 ms/it, loss 0.486050
Finished training it 65536/76743 of epoch 0, 86.71 ms/it, loss 0.486608
Finished training it 66560/76743 of epoch 0, 86.16 ms/it, loss 0.486294
Finished training it 66560/76743 of epoch 0, 86.33 ms/it, loss 0.485781
Finished training it 66560/76743 of epoch 0, 86.65 ms/it, loss 0.485238
Finished training it 66560/76743 of epoch 0, 86.62 ms/it, loss 0.484304
Finished training it 67584/76743 of epoch 0, 88.09 ms/it, loss 0.486410
Finished training it 67584/76743 of epoch 0, 87.80 ms/it, loss 0.486215
Finished training it 67584/76743 of epoch 0, 87.55 ms/it, loss 0.483713
Finished training it 67584/76743 of epoch 0, 87.31 ms/it, loss 0.488200
Finished training it 68608/76743 of epoch 0, 87.54 ms/it, loss 0.485448
Finished training it 68608/76743 of epoch 0, 86.59 ms/it, loss 0.484924
Finished training it 68608/76743 of epoch 0, 87.37 ms/it, loss 0.483949
Finished training it 68608/76743 of epoch 0, 86.82 ms/it, loss 0.486188
Finished training it 69632/76743 of epoch 0, 87.51 ms/it, loss 0.485394
Finished training it 69632/76743 of epoch 0, 86.87 ms/it, loss 0.486544
Finished training it 69632/76743 of epoch 0, 86.67 ms/it, loss 0.485504
Finished training it 69632/76743 of epoch 0, 87.39 ms/it, loss 0.482464
Finished training it 70656/76743 of epoch 0, 87.44 ms/it, loss 0.485048
Finished training it 70656/76743 of epoch 0, 87.60 ms/it, loss 0.485650
Finished training it 70656/76743 of epoch 0, 86.68 ms/it, loss 0.483636
Finished training it 70656/76743 of epoch 0, 87.06 ms/it, loss 0.484959
Finished training it 71680/76743 of epoch 0, 87.15 ms/it, loss 0.481932
Finished training it 71680/76743 of epoch 0, 87.26 ms/it, loss 0.482210
Finished training it 71680/76743 of epoch 0, 86.95 ms/it, loss 0.485522
Finished training it 71680/76743 of epoch 0, 86.70 ms/it, loss 0.486349
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2517145.0
get out
0 has test check 2517145.0 and sample count 3274240
 accuracy 76.877 %, best 76.877 %, roc auc score 0.7540, best 0.7540
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 85.68 ms/it, loss 0.486591
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2517145.0
get out
2 has test check 2517145.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 85.15 ms/it, loss 0.485633
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2517145.0
get out
1 has test check 2517145.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 85.70 ms/it, loss 0.486075
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2517145.0
get out
3 has test check 2517145.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 85.08 ms/it, loss 0.483045
Finished training it 73728/76743 of epoch 0, 85.78 ms/it, loss 0.484992
Finished training it 73728/76743 of epoch 0, 85.65 ms/it, loss 0.483774
Finished training it 73728/76743 of epoch 0, 86.09 ms/it, loss 0.487510
Finished training it 73728/76743 of epoch 0, 86.20 ms/it, loss 0.483834
Finished training it 74752/76743 of epoch 0, 85.38 ms/it, loss 0.485670
Finished training it 74752/76743 of epoch 0, 85.79 ms/it, loss 0.485084
Finished training it 74752/76743 of epoch 0, 85.38 ms/it, loss 0.484743
Finished training it 74752/76743 of epoch 0, 85.78 ms/it, loss 0.484740
Finished training it 75776/76743 of epoch 0, 92.04 ms/it, loss 0.485249
Finished training it 75776/76743 of epoch 0, 91.78 ms/it, loss 0.484670
Finished training it 75776/76743 of epoch 0, 91.81 ms/it, loss 0.486457
Finished training it 75776/76743 of epoch 0, 92.17 ms/it, loss 0.486534
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 91.83 ms/it, loss 0.485998
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 91.49 ms/it, loss 0.485622
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 91.20 ms/it, loss 0.481178
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 91.00 ms/it, loss 0.484293
Finished training it 2048/76743 of epoch 1, 87.18 ms/it, loss 0.481733
Finished training it 2048/76743 of epoch 1, 86.46 ms/it, loss 0.483869
Finished training it 2048/76743 of epoch 1, 86.42 ms/it, loss 0.485863
Finished training it 2048/76743 of epoch 1, 87.20 ms/it, loss 0.483817
Finished training it 3072/76743 of epoch 1, 86.40 ms/it, loss 0.481838
Finished training it 3072/76743 of epoch 1, 87.00 ms/it, loss 0.481067
Finished training it 3072/76743 of epoch 1, 87.01 ms/it, loss 0.481671
Finished training it 3072/76743 of epoch 1, 86.57 ms/it, loss 0.483895
Finished training it 4096/76743 of epoch 1, 87.22 ms/it, loss 0.484571
Finished training it 4096/76743 of epoch 1, 86.20 ms/it, loss 0.484584
Finished training it 4096/76743 of epoch 1, 86.49 ms/it, loss 0.483774
Finished training it 4096/76743 of epoch 1, 87.08 ms/it, loss 0.482627
Finished training it 5120/76743 of epoch 1, 87.30 ms/it, loss 0.482503
Finished training it 5120/76743 of epoch 1, 87.62 ms/it, loss 0.482680
Finished training it 5120/76743 of epoch 1, 87.50 ms/it, loss 0.481240
Finished training it 5120/76743 of epoch 1, 86.96 ms/it, loss 0.486010
Finished training it 6144/76743 of epoch 1, 86.79 ms/it, loss 0.481483
Finished training it 6144/76743 of epoch 1, 86.59 ms/it, loss 0.481522
Finished training it 6144/76743 of epoch 1, 87.43 ms/it, loss 0.482051
Finished training it 6144/76743 of epoch 1, 87.34 ms/it, loss 0.484383
Finished training it 7168/76743 of epoch 1, 86.87 ms/it, loss 0.485706
Finished training it 7168/76743 of epoch 1, 86.74 ms/it, loss 0.485002
Finished training it 7168/76743 of epoch 1, 86.49 ms/it, loss 0.482777
Finished training it 7168/76743 of epoch 1, 86.28 ms/it, loss 0.483570
Finished training it 8192/76743 of epoch 1, 86.91 ms/it, loss 0.483880
Finished training it 8192/76743 of epoch 1, 87.66 ms/it, loss 0.481957
Finished training it 8192/76743 of epoch 1, 87.50 ms/it, loss 0.484242
Finished training it 8192/76743 of epoch 1, 87.20 ms/it, loss 0.482922
Finished training it 9216/76743 of epoch 1, 86.96 ms/it, loss 0.484522
Finished training it 9216/76743 of epoch 1, 87.09 ms/it, loss 0.484692
Finished training it 9216/76743 of epoch 1, 87.54 ms/it, loss 0.483246
Finished training it 9216/76743 of epoch 1, 87.68 ms/it, loss 0.482995
Finished training it 10240/76743 of epoch 1, 87.25 ms/it, loss 0.484157
Finished training it 10240/76743 of epoch 1, 87.70 ms/it, loss 0.483080
Finished training it 10240/76743 of epoch 1, 87.78 ms/it, loss 0.481184
Finished training it 10240/76743 of epoch 1, 86.97 ms/it, loss 0.483876
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2518405.0
get out
0 has test check 2518405.0 and sample count 3274240
 accuracy 76.916 %, best 76.916 %, roc auc score 0.7555, best 0.7555
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2518405.0
get out
3 has test check 2518405.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.45 ms/it, loss 0.480745
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 86.88 ms/it, loss 0.482767
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2518405.0
get out
1 has test check 2518405.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.93 ms/it, loss 0.481258
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2518405.0
get out
2 has test check 2518405.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.60 ms/it, loss 0.480986
Finished training it 12288/76743 of epoch 1, 87.01 ms/it, loss 0.484170
Finished training it 12288/76743 of epoch 1, 87.62 ms/it, loss 0.483397
Finished training it 12288/76743 of epoch 1, 87.73 ms/it, loss 0.483886
Finished training it 12288/76743 of epoch 1, 87.18 ms/it, loss 0.480053
Finished training it 13312/76743 of epoch 1, 86.86 ms/it, loss 0.483159
Finished training it 13312/76743 of epoch 1, 86.07 ms/it, loss 0.482436
Finished training it 13312/76743 of epoch 1, 86.92 ms/it, loss 0.481885
Finished training it 13312/76743 of epoch 1, 86.46 ms/it, loss 0.483541
Finished training it 14336/76743 of epoch 1, 87.71 ms/it, loss 0.481400
Finished training it 14336/76743 of epoch 1, 87.61 ms/it, loss 0.483503
Finished training it 14336/76743 of epoch 1, 87.03 ms/it, loss 0.482621
Finished training it 14336/76743 of epoch 1, 87.36 ms/it, loss 0.480261
Finished training it 15360/76743 of epoch 1, 87.22 ms/it, loss 0.483700
Finished training it 15360/76743 of epoch 1, 86.90 ms/it, loss 0.481299
Finished training it 15360/76743 of epoch 1, 86.64 ms/it, loss 0.481483
Finished training it 15360/76743 of epoch 1, 87.20 ms/it, loss 0.482347
Finished training it 16384/76743 of epoch 1, 86.61 ms/it, loss 0.480603
Finished training it 16384/76743 of epoch 1, 86.83 ms/it, loss 0.483490
Finished training it 16384/76743 of epoch 1, 87.35 ms/it, loss 0.482080
Finished training it 16384/76743 of epoch 1, 87.21 ms/it, loss 0.479708
Finished training it 17408/76743 of epoch 1, 87.32 ms/it, loss 0.479090
Finished training it 17408/76743 of epoch 1, 86.50 ms/it, loss 0.481070
Finished training it 17408/76743 of epoch 1, 86.85 ms/it, loss 0.482343
Finished training it 17408/76743 of epoch 1, 87.29 ms/it, loss 0.483701
Finished training it 18432/76743 of epoch 1, 87.64 ms/it, loss 0.483696
Finished training it 18432/76743 of epoch 1, 87.59 ms/it, loss 0.482641
Finished training it 18432/76743 of epoch 1, 86.95 ms/it, loss 0.482890
Finished training it 18432/76743 of epoch 1, 87.19 ms/it, loss 0.483135
Finished training it 19456/76743 of epoch 1, 86.51 ms/it, loss 0.483340
Finished training it 19456/76743 of epoch 1, 86.89 ms/it, loss 0.481944
Finished training it 19456/76743 of epoch 1, 86.96 ms/it, loss 0.483057
Finished training it 19456/76743 of epoch 1, 86.02 ms/it, loss 0.484235
Finished training it 20480/76743 of epoch 1, 87.46 ms/it, loss 0.482869
Finished training it 20480/76743 of epoch 1, 87.24 ms/it, loss 0.482870
Finished training it 20480/76743 of epoch 1, 86.54 ms/it, loss 0.483827
Finished training it 20480/76743 of epoch 1, 86.77 ms/it, loss 0.483448
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2522846.0
get out
0 has test check 2522846.0 and sample count 3274240
 accuracy 77.051 %, best 77.051 %, roc auc score 0.7598, best 0.7598
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 86.38 ms/it, loss 0.480238
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2522846.0
get out
1 has test check 2522846.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 86.35 ms/it, loss 0.481291
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2522846.0
get out
2 has test check 2522846.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 85.86 ms/it, loss 0.478635
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2522846.0
get out
3 has test check 2522846.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 85.81 ms/it, loss 0.479854
Finished training it 22528/76743 of epoch 1, 88.46 ms/it, loss 0.480683
Finished training it 22528/76743 of epoch 1, 88.56 ms/it, loss 0.481167
Finished training it 22528/76743 of epoch 1, 87.86 ms/it, loss 0.481602
Finished training it 22528/76743 of epoch 1, 88.12 ms/it, loss 0.477806
Finished training it 23552/76743 of epoch 1, 87.11 ms/it, loss 0.478368
Finished training it 23552/76743 of epoch 1, 86.39 ms/it, loss 0.480693
Finished training it 23552/76743 of epoch 1, 86.26 ms/it, loss 0.480764
Finished training it 23552/76743 of epoch 1, 87.06 ms/it, loss 0.479966
Finished training it 24576/76743 of epoch 1, 97.30 ms/it, loss 0.480900
Finished training it 24576/76743 of epoch 1, 98.06 ms/it, loss 0.479636
Finished training it 24576/76743 of epoch 1, 98.27 ms/it, loss 0.484949
Finished training it 24576/76743 of epoch 1, 97.19 ms/it, loss 0.480172
Finished training it 25600/76743 of epoch 1, 87.29 ms/it, loss 0.481913
Finished training it 25600/76743 of epoch 1, 86.75 ms/it, loss 0.480495
Finished training it 25600/76743 of epoch 1, 87.31 ms/it, loss 0.483354
Finished training it 25600/76743 of epoch 1, 86.59 ms/it, loss 0.480873
Finished training it 26624/76743 of epoch 1, 87.08 ms/it, loss 0.480385
Finished training it 26624/76743 of epoch 1, 86.64 ms/it, loss 0.479577
Finished training it 26624/76743 of epoch 1, 86.68 ms/it, loss 0.480704
Finished training it 26624/76743 of epoch 1, 87.17 ms/it, loss 0.479259
Finished training it 27648/76743 of epoch 1, 86.97 ms/it, loss 0.481839
Finished training it 27648/76743 of epoch 1, 87.56 ms/it, loss 0.481204
Finished training it 27648/76743 of epoch 1, 87.09 ms/it, loss 0.482204
Finished training it 27648/76743 of epoch 1, 87.66 ms/it, loss 0.482929
Finished training it 28672/76743 of epoch 1, 87.37 ms/it, loss 0.483589
Finished training it 28672/76743 of epoch 1, 86.59 ms/it, loss 0.479891
Finished training it 28672/76743 of epoch 1, 87.28 ms/it, loss 0.479270
Finished training it 28672/76743 of epoch 1, 86.97 ms/it, loss 0.482310
Finished training it 29696/76743 of epoch 1, 87.21 ms/it, loss 0.480192
Finished training it 29696/76743 of epoch 1, 87.31 ms/it, loss 0.479796
Finished training it 29696/76743 of epoch 1, 86.63 ms/it, loss 0.479798
Finished training it 29696/76743 of epoch 1, 86.34 ms/it, loss 0.478047
Finished training it 30720/76743 of epoch 1, 87.27 ms/it, loss 0.475276
Finished training it 30720/76743 of epoch 1, 87.24 ms/it, loss 0.478920
Finished training it 30720/76743 of epoch 1, 86.82 ms/it, loss 0.479858
Finished training it 30720/76743 of epoch 1, 86.71 ms/it, loss 0.480373
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2524956.0
get out
0 has test check 2524956.0 and sample count 3274240
 accuracy 77.116 %, best 77.116 %, roc auc score 0.7605, best 0.7605
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2524956.0
get out
1 has test check 2524956.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.59 ms/it, loss 0.479810
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 86.56 ms/it, loss 0.482313
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2524956.0
get out
2 has test check 2524956.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.07 ms/it, loss 0.482632
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2524956.0
get out
3 has test check 2524956.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 85.63 ms/it, loss 0.482870
Finished training it 32768/76743 of epoch 1, 86.86 ms/it, loss 0.481002
Finished training it 32768/76743 of epoch 1, 86.78 ms/it, loss 0.478998
Finished training it 32768/76743 of epoch 1, 86.21 ms/it, loss 0.479968
Finished training it 32768/76743 of epoch 1, 86.70 ms/it, loss 0.479118
Finished training it 33792/76743 of epoch 1, 86.61 ms/it, loss 0.479926
Finished training it 33792/76743 of epoch 1, 86.88 ms/it, loss 0.482409
Finished training it 33792/76743 of epoch 1, 87.07 ms/it, loss 0.481387
Finished training it 33792/76743 of epoch 1, 86.09 ms/it, loss 0.481000
Finished training it 34816/76743 of epoch 1, 86.12 ms/it, loss 0.480692
Finished training it 34816/76743 of epoch 1, 86.06 ms/it, loss 0.477933
Finished training it 34816/76743 of epoch 1, 85.30 ms/it, loss 0.478640
Finished training it 34816/76743 of epoch 1, 85.78 ms/it, loss 0.481236
Finished training it 35840/76743 of epoch 1, 86.53 ms/it, loss 0.480171
Finished training it 35840/76743 of epoch 1, 86.45 ms/it, loss 0.481458
Finished training it 35840/76743 of epoch 1, 86.04 ms/it, loss 0.478449
Finished training it 35840/76743 of epoch 1, 85.64 ms/it, loss 0.479041
Finished training it 36864/76743 of epoch 1, 86.15 ms/it, loss 0.480486
Finished training it 36864/76743 of epoch 1, 86.17 ms/it, loss 0.481564
Finished training it 36864/76743 of epoch 1, 85.57 ms/it, loss 0.480933
Finished training it 36864/76743 of epoch 1, 85.73 ms/it, loss 0.477147
Finished training it 37888/76743 of epoch 1, 87.65 ms/it, loss 0.481268
Finished training it 37888/76743 of epoch 1, 88.06 ms/it, loss 0.477178
Finished training it 37888/76743 of epoch 1, 87.18 ms/it, loss 0.479168
Finished training it 37888/76743 of epoch 1, 87.90 ms/it, loss 0.480342
Finished training it 38912/76743 of epoch 1, 87.17 ms/it, loss 0.480967
Finished training it 38912/76743 of epoch 1, 87.14 ms/it, loss 0.480234
Finished training it 38912/76743 of epoch 1, 86.49 ms/it, loss 0.479747
Finished training it 38912/76743 of epoch 1, 86.80 ms/it, loss 0.481474
Finished training it 39936/76743 of epoch 1, 86.62 ms/it, loss 0.479076
Finished training it 39936/76743 of epoch 1, 86.59 ms/it, loss 0.482088
Finished training it 39936/76743 of epoch 1, 86.77 ms/it, loss 0.479008
Finished training it 39936/76743 of epoch 1, 86.76 ms/it, loss 0.476946
Finished training it 40960/76743 of epoch 1, 86.29 ms/it, loss 0.479589
Finished training it 40960/76743 of epoch 1, 86.45 ms/it, loss 0.476306
Finished training it 40960/76743 of epoch 1, 86.56 ms/it, loss 0.477601
Finished training it 40960/76743 of epoch 1, 86.31 ms/it, loss 0.480981
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2524945.0
get out
0 has test check 2524945.0 and sample count 3274240
 accuracy 77.115 %, best 77.116 %, roc auc score 0.7605, best 0.7605
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2524945.0
get out
1 has test check 2524945.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 86.88 ms/it, loss 0.479854
Finished training it 41984/76743 of epoch 1, 86.68 ms/it, loss 0.480363
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2524945.0
get out
2 has test check 2524945.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 86.64 ms/it, loss 0.478029
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2524945.0
get out
3 has test check 2524945.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 86.64 ms/it, loss 0.478500
Finished training it 43008/76743 of epoch 1, 86.29 ms/it, loss 0.482705
Finished training it 43008/76743 of epoch 1, 86.33 ms/it, loss 0.476425
Finished training it 43008/76743 of epoch 1, 86.17 ms/it, loss 0.481635
Finished training it 43008/76743 of epoch 1, 86.31 ms/it, loss 0.478952
Finished training it 44032/76743 of epoch 1, 86.76 ms/it, loss 0.479472
Finished training it 44032/76743 of epoch 1, 86.58 ms/it, loss 0.478173
Finished training it 44032/76743 of epoch 1, 86.83 ms/it, loss 0.480600
Finished training it 44032/76743 of epoch 1, 86.77 ms/it, loss 0.479971
Finished training it 45056/76743 of epoch 1, 98.68 ms/it, loss 0.480120
Finished training it 45056/76743 of epoch 1, 98.86 ms/it, loss 0.477329
Finished training it 45056/76743 of epoch 1, 98.93 ms/it, loss 0.482157
Finished training it 45056/76743 of epoch 1, 99.06 ms/it, loss 0.479772
Finished training it 46080/76743 of epoch 1, 86.92 ms/it, loss 0.480139
Finished training it 46080/76743 of epoch 1, 86.85 ms/it, loss 0.478891
Finished training it 46080/76743 of epoch 1, 86.85 ms/it, loss 0.479141
Finished training it 46080/76743 of epoch 1, 87.02 ms/it, loss 0.480400
Finished training it 47104/76743 of epoch 1, 87.07 ms/it, loss 0.477700
Finished training it 47104/76743 of epoch 1, 86.81 ms/it, loss 0.475974
Finished training it 47104/76743 of epoch 1, 86.89 ms/it, loss 0.476835
Finished training it 47104/76743 of epoch 1, 86.95 ms/it, loss 0.477852
Finished training it 48128/76743 of epoch 1, 88.29 ms/it, loss 0.477793
Finished training it 48128/76743 of epoch 1, 88.17 ms/it, loss 0.481103
Finished training it 48128/76743 of epoch 1, 88.23 ms/it, loss 0.477604
Finished training it 48128/76743 of epoch 1, 88.27 ms/it, loss 0.477383
Finished training it 49152/76743 of epoch 1, 87.25 ms/it, loss 0.478464
Finished training it 49152/76743 of epoch 1, 87.11 ms/it, loss 0.478474
Finished training it 49152/76743 of epoch 1, 86.93 ms/it, loss 0.477010
Finished training it 49152/76743 of epoch 1, 87.31 ms/it, loss 0.479541
Finished training it 50176/76743 of epoch 1, 86.70 ms/it, loss 0.477688
Finished training it 50176/76743 of epoch 1, 86.90 ms/it, loss 0.479753
Finished training it 50176/76743 of epoch 1, 86.62 ms/it, loss 0.481093
Finished training it 50176/76743 of epoch 1, 86.64 ms/it, loss 0.478673
Finished training it 51200/76743 of epoch 1, 86.39 ms/it, loss 0.479067
Finished training it 51200/76743 of epoch 1, 86.60 ms/it, loss 0.477450
Finished training it 51200/76743 of epoch 1, 86.35 ms/it, loss 0.482111
Finished training it 51200/76743 of epoch 1, 86.55 ms/it, loss 0.479997
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2526963.0
get out
0 has test check 2526963.0 and sample count 3274240
 accuracy 77.177 %, best 77.177 %, roc auc score 0.7624, best 0.7624
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 86.78 ms/it, loss 0.476530
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2526963.0
get out
1 has test check 2526963.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.97 ms/it, loss 0.479512
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2526963.0
get out
3 has test check 2526963.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.45 ms/it, loss 0.478227
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2526963.0
get out
2 has test check 2526963.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.56 ms/it, loss 0.477519
Finished training it 53248/76743 of epoch 1, 86.94 ms/it, loss 0.477675
Finished training it 53248/76743 of epoch 1, 86.41 ms/it, loss 0.478844
Finished training it 53248/76743 of epoch 1, 86.57 ms/it, loss 0.478965
Finished training it 53248/76743 of epoch 1, 86.47 ms/it, loss 0.478966
Finished training it 54272/76743 of epoch 1, 86.44 ms/it, loss 0.477871
Finished training it 54272/76743 of epoch 1, 86.38 ms/it, loss 0.477578
Finished training it 54272/76743 of epoch 1, 86.59 ms/it, loss 0.480388
Finished training it 54272/76743 of epoch 1, 86.46 ms/it, loss 0.479028
Finished training it 55296/76743 of epoch 1, 87.53 ms/it, loss 0.478135
Finished training it 55296/76743 of epoch 1, 87.52 ms/it, loss 0.480743
Finished training it 55296/76743 of epoch 1, 87.54 ms/it, loss 0.476687
Finished training it 55296/76743 of epoch 1, 87.45 ms/it, loss 0.477730
Finished training it 56320/76743 of epoch 1, 86.77 ms/it, loss 0.476376
Finished training it 56320/76743 of epoch 1, 86.90 ms/it, loss 0.479763
Finished training it 56320/76743 of epoch 1, 86.73 ms/it, loss 0.478763
Finished training it 56320/76743 of epoch 1, 86.97 ms/it, loss 0.475359
Finished training it 57344/76743 of epoch 1, 87.33 ms/it, loss 0.476557
Finished training it 57344/76743 of epoch 1, 87.34 ms/it, loss 0.479744
Finished training it 57344/76743 of epoch 1, 87.38 ms/it, loss 0.477402
Finished training it 57344/76743 of epoch 1, 87.33 ms/it, loss 0.482436
Finished training it 58368/76743 of epoch 1, 88.07 ms/it, loss 0.478275
Finished training it 58368/76743 of epoch 1, 88.42 ms/it, loss 0.479962
Finished training it 58368/76743 of epoch 1, 88.44 ms/it, loss 0.477928
Finished training it 58368/76743 of epoch 1, 88.30 ms/it, loss 0.480154
Finished training it 59392/76743 of epoch 1, 86.37 ms/it, loss 0.476398
Finished training it 59392/76743 of epoch 1, 87.05 ms/it, loss 0.477738
Finished training it 59392/76743 of epoch 1, 87.10 ms/it, loss 0.479999
Finished training it 59392/76743 of epoch 1, 86.82 ms/it, loss 0.478348
Finished training it 60416/76743 of epoch 1, 86.65 ms/it, loss 0.477108
Finished training it 60416/76743 of epoch 1, 87.07 ms/it, loss 0.476073
Finished training it 60416/76743 of epoch 1, 87.04 ms/it, loss 0.477262
Finished training it 60416/76743 of epoch 1, 86.92 ms/it, loss 0.475687
Finished training it 61440/76743 of epoch 1, 86.31 ms/it, loss 0.479829
Finished training it 61440/76743 of epoch 1, 86.56 ms/it, loss 0.479031
Finished training it 61440/76743 of epoch 1, 86.62 ms/it, loss 0.477426
Finished training it 61440/76743 of epoch 1, 86.18 ms/it, loss 0.477691
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2527460.0
get out
0 has test check 2527460.0 and sample count 3274240
 accuracy 77.192 %, best 77.192 %, roc auc score 0.7624, best 0.7624
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2527460.0
get out
2 has test check 2527460.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 86.42 ms/it, loss 0.476225
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2527460.0
get out
1 has test check 2527460.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 86.80 ms/it, loss 0.478298
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 86.74 ms/it, loss 0.478701
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2527460.0
get out
3 has test check 2527460.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 85.72 ms/it, loss 0.476964
Finished training it 63488/76743 of epoch 1, 86.35 ms/it, loss 0.479546
Finished training it 63488/76743 of epoch 1, 86.51 ms/it, loss 0.476810
Finished training it 63488/76743 of epoch 1, 85.59 ms/it, loss 0.477161
Finished training it 63488/76743 of epoch 1, 86.19 ms/it, loss 0.476440
Finished training it 64512/76743 of epoch 1, 86.90 ms/it, loss 0.478867
Finished training it 64512/76743 of epoch 1, 87.03 ms/it, loss 0.476409
Finished training it 64512/76743 of epoch 1, 86.26 ms/it, loss 0.475225
Finished training it 64512/76743 of epoch 1, 86.72 ms/it, loss 0.476627
Finished training it 65536/76743 of epoch 1, 91.44 ms/it, loss 0.479962
Finished training it 65536/76743 of epoch 1, 91.03 ms/it, loss 0.477418
Finished training it 65536/76743 of epoch 1, 90.69 ms/it, loss 0.477699
Finished training it 65536/76743 of epoch 1, 90.35 ms/it, loss 0.476912
Finished training it 66560/76743 of epoch 1, 91.77 ms/it, loss 0.478291
Finished training it 66560/76743 of epoch 1, 92.58 ms/it, loss 0.477498
Finished training it 66560/76743 of epoch 1, 92.01 ms/it, loss 0.478123
Finished training it 66560/76743 of epoch 1, 91.79 ms/it, loss 0.476692
Finished training it 67584/76743 of epoch 1, 86.80 ms/it, loss 0.478492
Finished training it 67584/76743 of epoch 1, 86.82 ms/it, loss 0.477727
Finished training it 67584/76743 of epoch 1, 86.22 ms/it, loss 0.476156
Finished training it 67584/76743 of epoch 1, 86.41 ms/it, loss 0.480382
Finished training it 68608/76743 of epoch 1, 87.07 ms/it, loss 0.477145
Finished training it 68608/76743 of epoch 1, 87.16 ms/it, loss 0.476049
Finished training it 68608/76743 of epoch 1, 86.49 ms/it, loss 0.476550
Finished training it 68608/76743 of epoch 1, 86.83 ms/it, loss 0.478280
Finished training it 69632/76743 of epoch 1, 86.84 ms/it, loss 0.477718
Finished training it 69632/76743 of epoch 1, 86.88 ms/it, loss 0.475521
Finished training it 69632/76743 of epoch 1, 86.32 ms/it, loss 0.478212
Finished training it 69632/76743 of epoch 1, 86.61 ms/it, loss 0.478414
Finished training it 70656/76743 of epoch 1, 86.92 ms/it, loss 0.477379
Finished training it 70656/76743 of epoch 1, 87.22 ms/it, loss 0.478241
Finished training it 70656/76743 of epoch 1, 87.27 ms/it, loss 0.476928
Finished training it 70656/76743 of epoch 1, 86.68 ms/it, loss 0.476040
Finished training it 71680/76743 of epoch 1, 86.75 ms/it, loss 0.478678
Finished training it 71680/76743 of epoch 1, 87.12 ms/it, loss 0.473859
Finished training it 71680/76743 of epoch 1, 86.74 ms/it, loss 0.477149
Finished training it 71680/76743 of epoch 1, 87.14 ms/it, loss 0.474523
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2530992.0
get out
0 has test check 2530992.0 and sample count 3274240
 accuracy 77.300 %, best 77.300 %, roc auc score 0.7640, best 0.7640
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2530992.0
get out
3 has test check 2530992.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 86.54 ms/it, loss 0.474833
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2530992.0
get out
1 has test check 2530992.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 87.46 ms/it, loss 0.478378
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2530992.0
get out
2 has test check 2530992.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 87.18 ms/it, loss 0.477628
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 87.37 ms/it, loss 0.479082
Finished training it 73728/76743 of epoch 1, 86.24 ms/it, loss 0.480153
Finished training it 73728/76743 of epoch 1, 86.66 ms/it, loss 0.476102
Finished training it 73728/76743 of epoch 1, 85.45 ms/it, loss 0.478021
Finished training it 73728/76743 of epoch 1, 86.22 ms/it, loss 0.476124
Finished training it 74752/76743 of epoch 1, 86.78 ms/it, loss 0.477195
Finished training it 74752/76743 of epoch 1, 85.88 ms/it, loss 0.477901
Finished training it 74752/76743 of epoch 1, 86.78 ms/it, loss 0.477785
Finished training it 74752/76743 of epoch 1, 86.37 ms/it, loss 0.477178
Finished training it 75776/76743 of epoch 1, 87.15 ms/it, loss 0.478311
Finished training it 75776/76743 of epoch 1, 86.84 ms/it, loss 0.477743
Finished training it 75776/76743 of epoch 1, 87.19 ms/it, loss 0.478293
Finished training it 75776/76743 of epoch 1, 86.34 ms/it, loss 0.476102
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.46 ms/it, loss 0.477707
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 86.73 ms/it, loss 0.476652
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 88.06 ms/it, loss 0.478362
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.30 ms/it, loss 0.473785
Finished training it 2048/76743 of epoch 2, 86.30 ms/it, loss 0.476898
Finished training it 2048/76743 of epoch 2, 85.72 ms/it, loss 0.478281
Finished training it 2048/76743 of epoch 2, 86.65 ms/it, loss 0.473861
Finished training it 2048/76743 of epoch 2, 86.52 ms/it, loss 0.476578
Finished training it 3072/76743 of epoch 2, 86.49 ms/it, loss 0.474350
Finished training it 3072/76743 of epoch 2, 86.18 ms/it, loss 0.476738
Finished training it 3072/76743 of epoch 2, 86.46 ms/it, loss 0.474528
Finished training it 3072/76743 of epoch 2, 85.43 ms/it, loss 0.474949
Finished training it 4096/76743 of epoch 2, 85.92 ms/it, loss 0.477990
Finished training it 4096/76743 of epoch 2, 86.91 ms/it, loss 0.475508
Finished training it 4096/76743 of epoch 2, 86.46 ms/it, loss 0.476438
Finished training it 4096/76743 of epoch 2, 86.72 ms/it, loss 0.477258
Finished training it 5120/76743 of epoch 2, 86.17 ms/it, loss 0.478547
Finished training it 5120/76743 of epoch 2, 86.90 ms/it, loss 0.475691
Finished training it 5120/76743 of epoch 2, 87.05 ms/it, loss 0.476077
Finished training it 5120/76743 of epoch 2, 87.03 ms/it, loss 0.474481
Finished training it 6144/76743 of epoch 2, 86.99 ms/it, loss 0.475564
Finished training it 6144/76743 of epoch 2, 86.90 ms/it, loss 0.477903
Finished training it 6144/76743 of epoch 2, 86.71 ms/it, loss 0.475449
Finished training it 6144/76743 of epoch 2, 86.22 ms/it, loss 0.474637
Finished training it 7168/76743 of epoch 2, 86.59 ms/it, loss 0.479156
Finished training it 7168/76743 of epoch 2, 86.09 ms/it, loss 0.477040
Finished training it 7168/76743 of epoch 2, 86.42 ms/it, loss 0.476156
Finished training it 7168/76743 of epoch 2, 86.52 ms/it, loss 0.478320
Finished training it 8192/76743 of epoch 2, 86.77 ms/it, loss 0.476694
Finished training it 8192/76743 of epoch 2, 86.99 ms/it, loss 0.477502
Finished training it 8192/76743 of epoch 2, 86.80 ms/it, loss 0.476885
Finished training it 8192/76743 of epoch 2, 86.90 ms/it, loss 0.475394
Finished training it 9216/76743 of epoch 2, 86.84 ms/it, loss 0.476951
Finished training it 9216/76743 of epoch 2, 86.66 ms/it, loss 0.478019
Finished training it 9216/76743 of epoch 2, 86.79 ms/it, loss 0.475718
Finished training it 9216/76743 of epoch 2, 86.65 ms/it, loss 0.478023
Finished training it 10240/76743 of epoch 2, 86.85 ms/it, loss 0.477680
Finished training it 10240/76743 of epoch 2, 86.90 ms/it, loss 0.476191
Finished training it 10240/76743 of epoch 2, 86.81 ms/it, loss 0.476947
Finished training it 10240/76743 of epoch 2, 87.07 ms/it, loss 0.473748
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2532834.0
get out
0 has test check 2532834.0 and sample count 3274240
 accuracy 77.356 %, best 77.356 %, roc auc score 0.7657, best 0.7657
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 87.52 ms/it, loss 0.476516
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2532834.0
get out
1 has test check 2532834.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.70 ms/it, loss 0.474530
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2532834.0
get out
3 has test check 2532834.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 86.35 ms/it, loss 0.473927
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2532834.0
get out
2 has test check 2532834.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.28 ms/it, loss 0.474407
Finished training it 12288/76743 of epoch 2, 87.57 ms/it, loss 0.476577
Finished training it 12288/76743 of epoch 2, 86.65 ms/it, loss 0.478134
Finished training it 12288/76743 of epoch 2, 87.35 ms/it, loss 0.474121
Finished training it 12288/76743 of epoch 2, 87.65 ms/it, loss 0.477028
Finished training it 13312/76743 of epoch 2, 86.47 ms/it, loss 0.476795
Finished training it 13312/76743 of epoch 2, 86.42 ms/it, loss 0.476972
Finished training it 13312/76743 of epoch 2, 85.99 ms/it, loss 0.475689
Finished training it 13312/76743 of epoch 2, 86.56 ms/it, loss 0.475731
Finished training it 14336/76743 of epoch 2, 98.45 ms/it, loss 0.475031
Finished training it 14336/76743 of epoch 2, 98.35 ms/it, loss 0.476304
Finished training it 14336/76743 of epoch 2, 98.31 ms/it, loss 0.477458
Finished training it 14336/76743 of epoch 2, 98.13 ms/it, loss 0.473566
Finished training it 15360/76743 of epoch 2, 86.62 ms/it, loss 0.473944
Finished training it 15360/76743 of epoch 2, 86.84 ms/it, loss 0.475320
Finished training it 15360/76743 of epoch 2, 86.85 ms/it, loss 0.477187
Finished training it 15360/76743 of epoch 2, 86.52 ms/it, loss 0.474283
Finished training it 16384/76743 of epoch 2, 86.80 ms/it, loss 0.474570
Finished training it 16384/76743 of epoch 2, 87.04 ms/it, loss 0.475213
Finished training it 16384/76743 of epoch 2, 86.96 ms/it, loss 0.473766
Finished training it 16384/76743 of epoch 2, 86.70 ms/it, loss 0.477163
Finished training it 17408/76743 of epoch 2, 86.73 ms/it, loss 0.476494
Finished training it 17408/76743 of epoch 2, 86.70 ms/it, loss 0.474530
Finished training it 17408/76743 of epoch 2, 87.07 ms/it, loss 0.477206
Finished training it 17408/76743 of epoch 2, 86.79 ms/it, loss 0.473163
Finished training it 18432/76743 of epoch 2, 87.16 ms/it, loss 0.475759
Finished training it 18432/76743 of epoch 2, 87.26 ms/it, loss 0.476958
Finished training it 18432/76743 of epoch 2, 86.86 ms/it, loss 0.476364
Finished training it 18432/76743 of epoch 2, 86.83 ms/it, loss 0.476262
Finished training it 19456/76743 of epoch 2, 86.75 ms/it, loss 0.477893
Finished training it 19456/76743 of epoch 2, 86.82 ms/it, loss 0.476182
Finished training it 19456/76743 of epoch 2, 86.73 ms/it, loss 0.477028
Finished training it 19456/76743 of epoch 2, 86.97 ms/it, loss 0.476198
Finished training it 20480/76743 of epoch 2, 86.90 ms/it, loss 0.476436
Finished training it 20480/76743 of epoch 2, 86.63 ms/it, loss 0.477905
Finished training it 20480/76743 of epoch 2, 86.68 ms/it, loss 0.477577
Finished training it 20480/76743 of epoch 2, 86.90 ms/it, loss 0.477165
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2532317.0
get out
0 has test check 2532317.0 and sample count 3274240
 accuracy 77.341 %, best 77.356 %, roc auc score 0.7660, best 0.7660
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2532317.0
get out
3 has test check 2532317.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 86.14 ms/it, loss 0.474371
Finished training it 21504/76743 of epoch 2, 86.96 ms/it, loss 0.474417
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2532317.0
get out
1 has test check 2532317.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 87.13 ms/it, loss 0.476411
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2532317.0
get out
2 has test check 2532317.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 86.75 ms/it, loss 0.473430
Finished training it 22528/76743 of epoch 2, 88.31 ms/it, loss 0.475216
Finished training it 22528/76743 of epoch 2, 88.57 ms/it, loss 0.476160
Finished training it 22528/76743 of epoch 2, 88.26 ms/it, loss 0.471989
Finished training it 22528/76743 of epoch 2, 87.69 ms/it, loss 0.475857
Finished training it 23552/76743 of epoch 2, 87.03 ms/it, loss 0.472223
Finished training it 23552/76743 of epoch 2, 86.81 ms/it, loss 0.474123
Finished training it 23552/76743 of epoch 2, 86.87 ms/it, loss 0.474189
Finished training it 23552/76743 of epoch 2, 86.25 ms/it, loss 0.475112
Finished training it 24576/76743 of epoch 2, 86.93 ms/it, loss 0.478548
Finished training it 24576/76743 of epoch 2, 87.15 ms/it, loss 0.473336
Finished training it 24576/76743 of epoch 2, 86.66 ms/it, loss 0.475031
Finished training it 24576/76743 of epoch 2, 86.22 ms/it, loss 0.474768
Finished training it 25600/76743 of epoch 2, 86.19 ms/it, loss 0.475229
Finished training it 25600/76743 of epoch 2, 86.76 ms/it, loss 0.474972
Finished training it 25600/76743 of epoch 2, 87.30 ms/it, loss 0.476150
Finished training it 25600/76743 of epoch 2, 87.12 ms/it, loss 0.477946
Finished training it 26624/76743 of epoch 2, 87.14 ms/it, loss 0.474013
Finished training it 26624/76743 of epoch 2, 87.04 ms/it, loss 0.475131
Finished training it 26624/76743 of epoch 2, 86.41 ms/it, loss 0.474510
Finished training it 26624/76743 of epoch 2, 86.87 ms/it, loss 0.475525
Finished training it 27648/76743 of epoch 2, 87.00 ms/it, loss 0.476649
Finished training it 27648/76743 of epoch 2, 87.34 ms/it, loss 0.475675
Finished training it 27648/76743 of epoch 2, 87.23 ms/it, loss 0.476910
Finished training it 27648/76743 of epoch 2, 86.58 ms/it, loss 0.475865
Finished training it 28672/76743 of epoch 2, 86.39 ms/it, loss 0.474328
Finished training it 28672/76743 of epoch 2, 87.20 ms/it, loss 0.473476
Finished training it 28672/76743 of epoch 2, 87.14 ms/it, loss 0.478124
Finished training it 28672/76743 of epoch 2, 86.88 ms/it, loss 0.476653
Finished training it 29696/76743 of epoch 2, 87.09 ms/it, loss 0.474887
Finished training it 29696/76743 of epoch 2, 86.78 ms/it, loss 0.472977
Finished training it 29696/76743 of epoch 2, 87.19 ms/it, loss 0.474398
Finished training it 29696/76743 of epoch 2, 86.88 ms/it, loss 0.474359
Finished training it 30720/76743 of epoch 2, 87.02 ms/it, loss 0.469776
Finished training it 30720/76743 of epoch 2, 86.87 ms/it, loss 0.474167
Finished training it 30720/76743 of epoch 2, 86.87 ms/it, loss 0.474560
Finished training it 30720/76743 of epoch 2, 87.20 ms/it, loss 0.473118
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533129.0
get out
0 has test check 2533129.0 and sample count 3274240
 accuracy 77.365 %, best 77.365 %, roc auc score 0.7669, best 0.7669
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533129.0
get out
2 has test check 2533129.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.42 ms/it, loss 0.476845
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533129.0
get out
1 has test check 2533129.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.81 ms/it, loss 0.474536
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533129.0
get out
3 has test check 2533129.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 85.68 ms/it, loss 0.476991
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 86.71 ms/it, loss 0.476522
Finished training it 32768/76743 of epoch 2, 86.47 ms/it, loss 0.473625
Finished training it 32768/76743 of epoch 2, 86.66 ms/it, loss 0.473479
Finished training it 32768/76743 of epoch 2, 85.92 ms/it, loss 0.474843
Finished training it 32768/76743 of epoch 2, 86.86 ms/it, loss 0.475349
Finished training it 33792/76743 of epoch 2, 86.07 ms/it, loss 0.475626
Finished training it 33792/76743 of epoch 2, 86.47 ms/it, loss 0.477042
Finished training it 33792/76743 of epoch 2, 86.65 ms/it, loss 0.476243
Finished training it 33792/76743 of epoch 2, 86.16 ms/it, loss 0.474640
Finished training it 34816/76743 of epoch 2, 94.05 ms/it, loss 0.475292
Finished training it 34816/76743 of epoch 2, 93.59 ms/it, loss 0.475502
Finished training it 34816/76743 of epoch 2, 93.31 ms/it, loss 0.473069
Finished training it 34816/76743 of epoch 2, 93.33 ms/it, loss 0.472853
Finished training it 35840/76743 of epoch 2, 91.66 ms/it, loss 0.474717
Finished training it 35840/76743 of epoch 2, 91.52 ms/it, loss 0.476207
Finished training it 35840/76743 of epoch 2, 90.88 ms/it, loss 0.473882
Finished training it 35840/76743 of epoch 2, 91.04 ms/it, loss 0.473132
Finished training it 36864/76743 of epoch 2, 86.84 ms/it, loss 0.475838
Finished training it 36864/76743 of epoch 2, 87.04 ms/it, loss 0.475033
Finished training it 36864/76743 of epoch 2, 86.39 ms/it, loss 0.475314
Finished training it 36864/76743 of epoch 2, 86.61 ms/it, loss 0.471542
Finished training it 37888/76743 of epoch 2, 87.73 ms/it, loss 0.475288
Finished training it 37888/76743 of epoch 2, 88.28 ms/it, loss 0.471423
Finished training it 37888/76743 of epoch 2, 87.22 ms/it, loss 0.473854
Finished training it 37888/76743 of epoch 2, 87.94 ms/it, loss 0.474120
Finished training it 38912/76743 of epoch 2, 87.02 ms/it, loss 0.475002
Finished training it 38912/76743 of epoch 2, 86.44 ms/it, loss 0.473913
Finished training it 38912/76743 of epoch 2, 86.85 ms/it, loss 0.475580
Finished training it 38912/76743 of epoch 2, 87.22 ms/it, loss 0.473437
Finished training it 39936/76743 of epoch 2, 86.58 ms/it, loss 0.471250
Finished training it 39936/76743 of epoch 2, 86.50 ms/it, loss 0.472889
Finished training it 39936/76743 of epoch 2, 86.08 ms/it, loss 0.476451
Finished training it 39936/76743 of epoch 2, 86.44 ms/it, loss 0.473294
Finished training it 40960/76743 of epoch 2, 87.35 ms/it, loss 0.475024
Finished training it 40960/76743 of epoch 2, 87.46 ms/it, loss 0.470610
Finished training it 40960/76743 of epoch 2, 87.41 ms/it, loss 0.471385
Finished training it 40960/76743 of epoch 2, 87.26 ms/it, loss 0.473768
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534915.0
get out
0 has test check 2534915.0 and sample count 3274240
 accuracy 77.420 %, best 77.420 %, roc auc score 0.7680, best 0.7680
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534915.0
get out
1 has test check 2534915.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 87.01 ms/it, loss 0.474435
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 86.94 ms/it, loss 0.474698
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534915.0
get out
3 has test check 2534915.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 86.30 ms/it, loss 0.472932
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534915.0
get out
2 has test check 2534915.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 86.91 ms/it, loss 0.472186
Finished training it 43008/76743 of epoch 2, 86.74 ms/it, loss 0.473137
Finished training it 43008/76743 of epoch 2, 86.45 ms/it, loss 0.475732
Finished training it 43008/76743 of epoch 2, 87.09 ms/it, loss 0.470645
Finished training it 43008/76743 of epoch 2, 86.29 ms/it, loss 0.477475
Finished training it 44032/76743 of epoch 2, 86.55 ms/it, loss 0.474729
Finished training it 44032/76743 of epoch 2, 86.34 ms/it, loss 0.472750
Finished training it 44032/76743 of epoch 2, 86.02 ms/it, loss 0.474181
Finished training it 44032/76743 of epoch 2, 86.04 ms/it, loss 0.474729
Finished training it 45056/76743 of epoch 2, 86.53 ms/it, loss 0.476322
Finished training it 45056/76743 of epoch 2, 86.50 ms/it, loss 0.472498
Finished training it 45056/76743 of epoch 2, 86.94 ms/it, loss 0.474267
Finished training it 45056/76743 of epoch 2, 86.56 ms/it, loss 0.475311
Finished training it 46080/76743 of epoch 2, 86.94 ms/it, loss 0.474896
Finished training it 46080/76743 of epoch 2, 86.48 ms/it, loss 0.472962
Finished training it 46080/76743 of epoch 2, 86.55 ms/it, loss 0.473534
Finished training it 46080/76743 of epoch 2, 86.78 ms/it, loss 0.474919
Finished training it 47104/76743 of epoch 2, 86.56 ms/it, loss 0.471789
Finished training it 47104/76743 of epoch 2, 86.81 ms/it, loss 0.471879
Finished training it 47104/76743 of epoch 2, 86.12 ms/it, loss 0.471305
Finished training it 47104/76743 of epoch 2, 86.32 ms/it, loss 0.470584
Finished training it 48128/76743 of epoch 2, 87.64 ms/it, loss 0.471733
Finished training it 48128/76743 of epoch 2, 87.45 ms/it, loss 0.475236
Finished training it 48128/76743 of epoch 2, 87.19 ms/it, loss 0.472218
Finished training it 48128/76743 of epoch 2, 87.78 ms/it, loss 0.472063
Finished training it 49152/76743 of epoch 2, 88.91 ms/it, loss 0.472329
Finished training it 49152/76743 of epoch 2, 89.39 ms/it, loss 0.472074
Finished training it 49152/76743 of epoch 2, 89.50 ms/it, loss 0.474083
Finished training it 49152/76743 of epoch 2, 89.27 ms/it, loss 0.472738
Finished training it 50176/76743 of epoch 2, 86.55 ms/it, loss 0.473107
Finished training it 50176/76743 of epoch 2, 86.47 ms/it, loss 0.472858
Finished training it 50176/76743 of epoch 2, 86.66 ms/it, loss 0.475134
Finished training it 50176/76743 of epoch 2, 86.91 ms/it, loss 0.474946
Finished training it 51200/76743 of epoch 2, 86.88 ms/it, loss 0.472055
Finished training it 51200/76743 of epoch 2, 86.59 ms/it, loss 0.473481
Finished training it 51200/76743 of epoch 2, 86.49 ms/it, loss 0.476607
Finished training it 51200/76743 of epoch 2, 86.84 ms/it, loss 0.474305
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535306.0
get out
0 has test check 2535306.0 and sample count 3274240
 accuracy 77.432 %, best 77.432 %, roc auc score 0.7678, best 0.7680
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535306.0
get out
1 has test check 2535306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 86.69 ms/it, loss 0.474753
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 86.66 ms/it, loss 0.471028
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535306.0
get out
3 has test check 2535306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 85.89 ms/it, loss 0.473133
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535306.0
get out
2 has test check 2535306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 86.39 ms/it, loss 0.472100
Finished training it 53248/76743 of epoch 2, 86.78 ms/it, loss 0.474048
Finished training it 53248/76743 of epoch 2, 87.08 ms/it, loss 0.472938
Finished training it 53248/76743 of epoch 2, 86.67 ms/it, loss 0.473542
Finished training it 53248/76743 of epoch 2, 86.40 ms/it, loss 0.473874
Finished training it 54272/76743 of epoch 2, 86.44 ms/it, loss 0.473035
Finished training it 54272/76743 of epoch 2, 86.86 ms/it, loss 0.475176
Finished training it 54272/76743 of epoch 2, 86.31 ms/it, loss 0.472131
Finished training it 54272/76743 of epoch 2, 86.70 ms/it, loss 0.473704
Finished training it 55296/76743 of epoch 2, 98.20 ms/it, loss 0.471012
Finished training it 55296/76743 of epoch 2, 97.89 ms/it, loss 0.475698
Finished training it 55296/76743 of epoch 2, 98.02 ms/it, loss 0.472294
Finished training it 55296/76743 of epoch 2, 98.05 ms/it, loss 0.472818
Finished training it 56320/76743 of epoch 2, 86.57 ms/it, loss 0.471308
Finished training it 56320/76743 of epoch 2, 87.13 ms/it, loss 0.474362
Finished training it 56320/76743 of epoch 2, 87.13 ms/it, loss 0.470037
Finished training it 56320/76743 of epoch 2, 86.49 ms/it, loss 0.473221
Finished training it 57344/76743 of epoch 2, 86.99 ms/it, loss 0.470572
Finished training it 57344/76743 of epoch 2, 87.13 ms/it, loss 0.476909
Finished training it 57344/76743 of epoch 2, 86.64 ms/it, loss 0.472525
Finished training it 57344/76743 of epoch 2, 86.69 ms/it, loss 0.474178
Finished training it 58368/76743 of epoch 2, 87.19 ms/it, loss 0.472278
Finished training it 58368/76743 of epoch 2, 86.34 ms/it, loss 0.473199
Finished training it 58368/76743 of epoch 2, 86.96 ms/it, loss 0.474858
Finished training it 58368/76743 of epoch 2, 86.52 ms/it, loss 0.475366
Finished training it 59392/76743 of epoch 2, 87.37 ms/it, loss 0.474793
Finished training it 59392/76743 of epoch 2, 87.13 ms/it, loss 0.472652
Finished training it 59392/76743 of epoch 2, 87.09 ms/it, loss 0.471147
Finished training it 59392/76743 of epoch 2, 87.12 ms/it, loss 0.472900
Finished training it 60416/76743 of epoch 2, 86.96 ms/it, loss 0.472390
Finished training it 60416/76743 of epoch 2, 87.21 ms/it, loss 0.470976
Finished training it 60416/76743 of epoch 2, 87.35 ms/it, loss 0.471097
Finished training it 60416/76743 of epoch 2, 87.14 ms/it, loss 0.471794
Finished training it 61440/76743 of epoch 2, 86.35 ms/it, loss 0.474559
Finished training it 61440/76743 of epoch 2, 86.16 ms/it, loss 0.471546
Finished training it 61440/76743 of epoch 2, 86.48 ms/it, loss 0.474026
Finished training it 61440/76743 of epoch 2, 86.50 ms/it, loss 0.472678
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534147.0
get out
0 has test check 2534147.0 and sample count 3274240
 accuracy 77.396 %, best 77.432 %, roc auc score 0.7684, best 0.7684
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534147.0
get out
1 has test check 2534147.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.95 ms/it, loss 0.473821
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534147.0
get out
3 has test check 2534147.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.39 ms/it, loss 0.472338
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534147.0
get out
2 has test check 2534147.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.67 ms/it, loss 0.471199
Finished training it 62464/76743 of epoch 2, 87.07 ms/it, loss 0.473427
Finished training it 63488/76743 of epoch 2, 86.84 ms/it, loss 0.471874
Finished training it 63488/76743 of epoch 2, 86.40 ms/it, loss 0.471176
Finished training it 63488/76743 of epoch 2, 86.20 ms/it, loss 0.472275
Finished training it 63488/76743 of epoch 2, 86.60 ms/it, loss 0.474405
Finished training it 64512/76743 of epoch 2, 86.53 ms/it, loss 0.470366
Finished training it 64512/76743 of epoch 2, 86.46 ms/it, loss 0.471284
Finished training it 64512/76743 of epoch 2, 86.86 ms/it, loss 0.471391
Finished training it 64512/76743 of epoch 2, 86.78 ms/it, loss 0.473911
Finished training it 65536/76743 of epoch 2, 86.41 ms/it, loss 0.475241
Finished training it 65536/76743 of epoch 2, 85.99 ms/it, loss 0.472857
Finished training it 65536/76743 of epoch 2, 86.59 ms/it, loss 0.472890
Finished training it 65536/76743 of epoch 2, 85.98 ms/it, loss 0.472505
Finished training it 66560/76743 of epoch 2, 86.73 ms/it, loss 0.473435
Finished training it 66560/76743 of epoch 2, 87.29 ms/it, loss 0.472917
Finished training it 66560/76743 of epoch 2, 86.79 ms/it, loss 0.473239
Finished training it 66560/76743 of epoch 2, 87.19 ms/it, loss 0.472003
Finished training it 67584/76743 of epoch 2, 86.96 ms/it, loss 0.472969
Finished training it 67584/76743 of epoch 2, 87.15 ms/it, loss 0.473664
Finished training it 67584/76743 of epoch 2, 86.57 ms/it, loss 0.471723
Finished training it 67584/76743 of epoch 2, 86.61 ms/it, loss 0.475735
Finished training it 68608/76743 of epoch 2, 86.99 ms/it, loss 0.471162
Finished training it 68608/76743 of epoch 2, 86.49 ms/it, loss 0.471813
Finished training it 68608/76743 of epoch 2, 86.46 ms/it, loss 0.473280
Finished training it 68608/76743 of epoch 2, 86.70 ms/it, loss 0.472740
Finished training it 69632/76743 of epoch 2, 87.41 ms/it, loss 0.471089
Finished training it 69632/76743 of epoch 2, 87.22 ms/it, loss 0.472704
Finished training it 69632/76743 of epoch 2, 87.01 ms/it, loss 0.474050
Finished training it 69632/76743 of epoch 2, 86.84 ms/it, loss 0.473992
Finished training it 70656/76743 of epoch 2, 89.32 ms/it, loss 0.472386
Finished training it 70656/76743 of epoch 2, 88.65 ms/it, loss 0.470780
Finished training it 70656/76743 of epoch 2, 89.26 ms/it, loss 0.473190
Finished training it 70656/76743 of epoch 2, 88.79 ms/it, loss 0.472498
Finished training it 71680/76743 of epoch 2, 96.68 ms/it, loss 0.474338
Finished training it 71680/76743 of epoch 2, 96.99 ms/it, loss 0.472437
Finished training it 71680/76743 of epoch 2, 97.48 ms/it, loss 0.469372
Finished training it 71680/76743 of epoch 2, 97.53 ms/it, loss 0.469432
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2538056.0
get out
0 has test check 2538056.0 and sample count 3274240
 accuracy 77.516 %, best 77.516 %, roc auc score 0.7699, best 0.7699
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 86.44 ms/it, loss 0.474367
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2538056.0
get out
3 has test check 2538056.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.20 ms/it, loss 0.470778
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2538056.0
get out
1 has test check 2538056.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.56 ms/it, loss 0.472898
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2538056.0
get out
2 has test check 2538056.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.12 ms/it, loss 0.472701
Finished training it 73728/76743 of epoch 2, 85.91 ms/it, loss 0.470980
Finished training it 73728/76743 of epoch 2, 86.26 ms/it, loss 0.475073
Finished training it 73728/76743 of epoch 2, 86.48 ms/it, loss 0.471128
Finished training it 73728/76743 of epoch 2, 85.78 ms/it, loss 0.473486
Finished training it 74752/76743 of epoch 2, 86.80 ms/it, loss 0.473158
Finished training it 74752/76743 of epoch 2, 86.62 ms/it, loss 0.472483
Finished training it 74752/76743 of epoch 2, 86.39 ms/it, loss 0.473333
Finished training it 74752/76743 of epoch 2, 86.32 ms/it, loss 0.473174
Finished training it 75776/76743 of epoch 2, 98.29 ms/it, loss 0.471963
Finished training it 75776/76743 of epoch 2, 98.62 ms/it, loss 0.473421
Finished training it 75776/76743 of epoch 2, 99.02 ms/it, loss 0.473590
Finished training it 75776/76743 of epoch 2, 98.54 ms/it, loss 0.473796
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.17 ms/it, loss 0.472325
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.32 ms/it, loss 0.469481
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.03 ms/it, loss 0.473859
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.54 ms/it, loss 0.473590
Finished training it 2048/76743 of epoch 3, 86.49 ms/it, loss 0.471837
Finished training it 2048/76743 of epoch 3, 86.73 ms/it, loss 0.469320
Finished training it 2048/76743 of epoch 3, 86.37 ms/it, loss 0.471813
Finished training it 2048/76743 of epoch 3, 86.22 ms/it, loss 0.473828
Finished training it 3072/76743 of epoch 3, 86.62 ms/it, loss 0.472175
Finished training it 3072/76743 of epoch 3, 86.98 ms/it, loss 0.469814
Finished training it 3072/76743 of epoch 3, 86.56 ms/it, loss 0.469968
Finished training it 3072/76743 of epoch 3, 86.79 ms/it, loss 0.469806
Finished training it 4096/76743 of epoch 3, 98.89 ms/it, loss 0.471848
Finished training it 4096/76743 of epoch 3, 99.43 ms/it, loss 0.472872
Finished training it 4096/76743 of epoch 3, 98.93 ms/it, loss 0.473965
Finished training it 4096/76743 of epoch 3, 99.49 ms/it, loss 0.471146
Finished training it 5120/76743 of epoch 3, 100.96 ms/it, loss 0.473566
Finished training it 5120/76743 of epoch 3, 101.82 ms/it, loss 0.471827
Finished training it 5120/76743 of epoch 3, 100.95 ms/it, loss 0.470835
Finished training it 5120/76743 of epoch 3, 101.70 ms/it, loss 0.469717
Finished training it 6144/76743 of epoch 3, 86.18 ms/it, loss 0.473726
Finished training it 6144/76743 of epoch 3, 86.00 ms/it, loss 0.470189
Finished training it 6144/76743 of epoch 3, 86.07 ms/it, loss 0.470661
Finished training it 6144/76743 of epoch 3, 86.30 ms/it, loss 0.470870
Finished training it 7168/76743 of epoch 3, 86.77 ms/it, loss 0.472461
Finished training it 7168/76743 of epoch 3, 86.96 ms/it, loss 0.474575
Finished training it 7168/76743 of epoch 3, 86.91 ms/it, loss 0.474042
Finished training it 7168/76743 of epoch 3, 86.76 ms/it, loss 0.472433
Finished training it 8192/76743 of epoch 3, 87.08 ms/it, loss 0.472452
Finished training it 8192/76743 of epoch 3, 87.09 ms/it, loss 0.471681
Finished training it 8192/76743 of epoch 3, 87.13 ms/it, loss 0.470956
Finished training it 8192/76743 of epoch 3, 87.20 ms/it, loss 0.473493
Finished training it 9216/76743 of epoch 3, 86.64 ms/it, loss 0.473849
Finished training it 9216/76743 of epoch 3, 86.89 ms/it, loss 0.471583
Finished training it 9216/76743 of epoch 3, 86.84 ms/it, loss 0.473860
Finished training it 9216/76743 of epoch 3, 87.04 ms/it, loss 0.472911
Finished training it 10240/76743 of epoch 3, 87.11 ms/it, loss 0.473249
Finished training it 10240/76743 of epoch 3, 87.12 ms/it, loss 0.472408
Finished training it 10240/76743 of epoch 3, 87.22 ms/it, loss 0.470057
Finished training it 10240/76743 of epoch 3, 87.42 ms/it, loss 0.471746
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2538570.0
get out
0 has test check 2538570.0 and sample count 3274240
 accuracy 77.532 %, best 77.532 %, roc auc score 0.7703, best 0.7703
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2538570.0
get out
3 has test check 2538570.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 86.39 ms/it, loss 0.469694
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2538570.0
get out
2 has test check 2538570.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 86.51 ms/it, loss 0.470519
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 86.90 ms/it, loss 0.472407
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2538570.0
get out
1 has test check 2538570.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 86.99 ms/it, loss 0.470424
Finished training it 12288/76743 of epoch 3, 86.77 ms/it, loss 0.472616
Finished training it 12288/76743 of epoch 3, 86.38 ms/it, loss 0.470262
Finished training it 12288/76743 of epoch 3, 86.90 ms/it, loss 0.472997
Finished training it 12288/76743 of epoch 3, 86.26 ms/it, loss 0.474010
Finished training it 13312/76743 of epoch 3, 86.54 ms/it, loss 0.473337
Finished training it 13312/76743 of epoch 3, 86.82 ms/it, loss 0.471761
Finished training it 13312/76743 of epoch 3, 86.15 ms/it, loss 0.471661
Finished training it 13312/76743 of epoch 3, 86.24 ms/it, loss 0.473340
Finished training it 14336/76743 of epoch 3, 87.17 ms/it, loss 0.473748
Finished training it 14336/76743 of epoch 3, 87.03 ms/it, loss 0.471293
Finished training it 14336/76743 of epoch 3, 86.64 ms/it, loss 0.472249
Finished training it 14336/76743 of epoch 3, 86.72 ms/it, loss 0.469688
Finished training it 15360/76743 of epoch 3, 86.99 ms/it, loss 0.473107
Finished training it 15360/76743 of epoch 3, 86.89 ms/it, loss 0.471401
Finished training it 15360/76743 of epoch 3, 86.58 ms/it, loss 0.469885
Finished training it 15360/76743 of epoch 3, 86.59 ms/it, loss 0.470103
Finished training it 16384/76743 of epoch 3, 88.27 ms/it, loss 0.473609
Finished training it 16384/76743 of epoch 3, 88.57 ms/it, loss 0.471305
Finished training it 16384/76743 of epoch 3, 88.32 ms/it, loss 0.470372
Finished training it 16384/76743 of epoch 3, 88.42 ms/it, loss 0.470084
Finished training it 17408/76743 of epoch 3, 87.10 ms/it, loss 0.469200
Finished training it 17408/76743 of epoch 3, 87.27 ms/it, loss 0.473524
Finished training it 17408/76743 of epoch 3, 86.71 ms/it, loss 0.470862
Finished training it 17408/76743 of epoch 3, 86.84 ms/it, loss 0.472864
Finished training it 18432/76743 of epoch 3, 86.81 ms/it, loss 0.472654
Finished training it 18432/76743 of epoch 3, 87.15 ms/it, loss 0.472985
Finished training it 18432/76743 of epoch 3, 86.90 ms/it, loss 0.472056
Finished training it 18432/76743 of epoch 3, 86.99 ms/it, loss 0.471884
Finished training it 19456/76743 of epoch 3, 86.36 ms/it, loss 0.474064
Finished training it 19456/76743 of epoch 3, 87.01 ms/it, loss 0.472652
Finished training it 19456/76743 of epoch 3, 86.52 ms/it, loss 0.472597
Finished training it 19456/76743 of epoch 3, 86.87 ms/it, loss 0.472391
Finished training it 20480/76743 of epoch 3, 86.95 ms/it, loss 0.472959
Finished training it 20480/76743 of epoch 3, 86.61 ms/it, loss 0.474234
Finished training it 20480/76743 of epoch 3, 87.18 ms/it, loss 0.473003
Finished training it 20480/76743 of epoch 3, 86.71 ms/it, loss 0.474097
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540293.0
get out
0 has test check 2540293.0 and sample count 3274240
 accuracy 77.584 %, best 77.584 %, roc auc score 0.7719, best 0.7719
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 86.68 ms/it, loss 0.470536
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540293.0
get out
3 has test check 2540293.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.50 ms/it, loss 0.470221
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540293.0
get out
1 has test check 2540293.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.82 ms/it, loss 0.472157
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540293.0
get out
2 has test check 2540293.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.47 ms/it, loss 0.469377
Finished training it 22528/76743 of epoch 3, 86.00 ms/it, loss 0.472184
Finished training it 22528/76743 of epoch 3, 86.44 ms/it, loss 0.471543
Finished training it 22528/76743 of epoch 3, 86.75 ms/it, loss 0.472644
Finished training it 22528/76743 of epoch 3, 86.18 ms/it, loss 0.468351
Finished training it 23552/76743 of epoch 3, 86.35 ms/it, loss 0.469722
Finished training it 23552/76743 of epoch 3, 86.33 ms/it, loss 0.471168
Finished training it 23552/76743 of epoch 3, 86.75 ms/it, loss 0.470594
Finished training it 23552/76743 of epoch 3, 86.90 ms/it, loss 0.468590
Finished training it 24576/76743 of epoch 3, 97.90 ms/it, loss 0.469713
Finished training it 24576/76743 of epoch 3, 97.83 ms/it, loss 0.471089
Finished training it 24576/76743 of epoch 3, 98.07 ms/it, loss 0.471094
Finished training it 24576/76743 of epoch 3, 97.86 ms/it, loss 0.475087
Finished training it 25600/76743 of epoch 3, 86.84 ms/it, loss 0.474106
Finished training it 25600/76743 of epoch 3, 86.84 ms/it, loss 0.472230
Finished training it 25600/76743 of epoch 3, 86.55 ms/it, loss 0.471109
Finished training it 25600/76743 of epoch 3, 86.48 ms/it, loss 0.471208
Finished training it 26624/76743 of epoch 3, 89.24 ms/it, loss 0.471632
Finished training it 26624/76743 of epoch 3, 89.32 ms/it, loss 0.470198
Finished training it 26624/76743 of epoch 3, 88.87 ms/it, loss 0.471981
Finished training it 26624/76743 of epoch 3, 88.87 ms/it, loss 0.470833
Finished training it 27648/76743 of epoch 3, 87.29 ms/it, loss 0.473422
Finished training it 27648/76743 of epoch 3, 87.66 ms/it, loss 0.471548
Finished training it 27648/76743 of epoch 3, 87.41 ms/it, loss 0.472225
Finished training it 27648/76743 of epoch 3, 87.46 ms/it, loss 0.473292
Finished training it 28672/76743 of epoch 3, 86.43 ms/it, loss 0.470926
Finished training it 28672/76743 of epoch 3, 86.60 ms/it, loss 0.474222
Finished training it 28672/76743 of epoch 3, 86.86 ms/it, loss 0.470024
Finished training it 28672/76743 of epoch 3, 86.52 ms/it, loss 0.473045
Finished training it 29696/76743 of epoch 3, 87.37 ms/it, loss 0.471201
Finished training it 29696/76743 of epoch 3, 87.15 ms/it, loss 0.471405
Finished training it 29696/76743 of epoch 3, 87.15 ms/it, loss 0.469559
Finished training it 29696/76743 of epoch 3, 87.23 ms/it, loss 0.470442
Finished training it 30720/76743 of epoch 3, 86.88 ms/it, loss 0.470919
Finished training it 30720/76743 of epoch 3, 86.96 ms/it, loss 0.466131
Finished training it 30720/76743 of epoch 3, 86.91 ms/it, loss 0.471214
Finished training it 30720/76743 of epoch 3, 87.15 ms/it, loss 0.469478
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540047.0
get out
0 has test check 2540047.0 and sample count 3274240
 accuracy 77.577 %, best 77.584 %, roc auc score 0.7718, best 0.7719
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540047.0
get out
1 has test check 2540047.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.51 ms/it, loss 0.470835
Finished training it 31744/76743 of epoch 3, 86.40 ms/it, loss 0.473364
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540047.0
get out
2 has test check 2540047.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.14 ms/it, loss 0.472804
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540047.0
get out
3 has test check 2540047.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.09 ms/it, loss 0.473418
Finished training it 32768/76743 of epoch 3, 86.72 ms/it, loss 0.471747
Finished training it 32768/76743 of epoch 3, 86.28 ms/it, loss 0.471548
Finished training it 32768/76743 of epoch 3, 86.43 ms/it, loss 0.470047
Finished training it 32768/76743 of epoch 3, 86.49 ms/it, loss 0.470559
Finished training it 33792/76743 of epoch 3, 86.40 ms/it, loss 0.470916
Finished training it 33792/76743 of epoch 3, 86.56 ms/it, loss 0.473528
Finished training it 33792/76743 of epoch 3, 86.87 ms/it, loss 0.472746
Finished training it 33792/76743 of epoch 3, 86.28 ms/it, loss 0.472323
Finished training it 34816/76743 of epoch 3, 86.87 ms/it, loss 0.472469
Finished training it 34816/76743 of epoch 3, 86.48 ms/it, loss 0.472095
Finished training it 34816/76743 of epoch 3, 86.51 ms/it, loss 0.469778
Finished training it 34816/76743 of epoch 3, 86.60 ms/it, loss 0.469033
Finished training it 35840/76743 of epoch 3, 86.75 ms/it, loss 0.470295
Finished training it 35840/76743 of epoch 3, 86.90 ms/it, loss 0.471332
Finished training it 35840/76743 of epoch 3, 86.67 ms/it, loss 0.470622
Finished training it 35840/76743 of epoch 3, 87.10 ms/it, loss 0.472791
Finished training it 36864/76743 of epoch 3, 86.59 ms/it, loss 0.472508
Finished training it 36864/76743 of epoch 3, 86.21 ms/it, loss 0.471729
Finished training it 36864/76743 of epoch 3, 86.65 ms/it, loss 0.471747
Finished training it 36864/76743 of epoch 3, 86.20 ms/it, loss 0.468440
Finished training it 37888/76743 of epoch 3, 86.49 ms/it, loss 0.467711
Finished training it 37888/76743 of epoch 3, 85.99 ms/it, loss 0.472151
Finished training it 37888/76743 of epoch 3, 86.23 ms/it, loss 0.470840
Finished training it 37888/76743 of epoch 3, 85.98 ms/it, loss 0.470373
Finished training it 38912/76743 of epoch 3, 86.66 ms/it, loss 0.471401
Finished training it 38912/76743 of epoch 3, 86.27 ms/it, loss 0.470825
Finished training it 38912/76743 of epoch 3, 86.37 ms/it, loss 0.472195
Finished training it 38912/76743 of epoch 3, 86.83 ms/it, loss 0.469552
Finished training it 39936/76743 of epoch 3, 85.92 ms/it, loss 0.473003
Finished training it 39936/76743 of epoch 3, 86.16 ms/it, loss 0.469695
Finished training it 39936/76743 of epoch 3, 85.88 ms/it, loss 0.470001
Finished training it 39936/76743 of epoch 3, 86.32 ms/it, loss 0.468328
Finished training it 40960/76743 of epoch 3, 87.17 ms/it, loss 0.471946
Finished training it 40960/76743 of epoch 3, 87.13 ms/it, loss 0.470662
Finished training it 40960/76743 of epoch 3, 87.32 ms/it, loss 0.468235
Finished training it 40960/76743 of epoch 3, 87.60 ms/it, loss 0.467453
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540529.0
get out
0 has test check 2540529.0 and sample count 3274240
 accuracy 77.591 %, best 77.591 %, roc auc score 0.7720, best 0.7720
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540529.0
get out
3 has test check 2540529.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 86.74 ms/it, loss 0.469829
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540529.0
get out
2 has test check 2540529.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 86.79 ms/it, loss 0.469295
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 86.98 ms/it, loss 0.471736
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540529.0
get out
1 has test check 2540529.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 87.14 ms/it, loss 0.470820
Finished training it 43008/76743 of epoch 3, 86.68 ms/it, loss 0.474389
Finished training it 43008/76743 of epoch 3, 86.73 ms/it, loss 0.469972
Finished training it 43008/76743 of epoch 3, 86.99 ms/it, loss 0.467715
Finished training it 43008/76743 of epoch 3, 86.65 ms/it, loss 0.472300
Finished training it 44032/76743 of epoch 3, 86.41 ms/it, loss 0.469662
Finished training it 44032/76743 of epoch 3, 86.18 ms/it, loss 0.471741
Finished training it 44032/76743 of epoch 3, 86.09 ms/it, loss 0.470969
Finished training it 44032/76743 of epoch 3, 86.52 ms/it, loss 0.471761
Finished training it 45056/76743 of epoch 3, 86.47 ms/it, loss 0.473105
Finished training it 45056/76743 of epoch 3, 86.99 ms/it, loss 0.470981
Finished training it 45056/76743 of epoch 3, 86.72 ms/it, loss 0.472001
Finished training it 45056/76743 of epoch 3, 86.43 ms/it, loss 0.469341
Finished training it 46080/76743 of epoch 3, 97.56 ms/it, loss 0.471475
Finished training it 46080/76743 of epoch 3, 97.44 ms/it, loss 0.471692
Finished training it 46080/76743 of epoch 3, 96.82 ms/it, loss 0.470235
Finished training it 46080/76743 of epoch 3, 97.24 ms/it, loss 0.469477
Finished training it 47104/76743 of epoch 3, 86.49 ms/it, loss 0.467630
Finished training it 47104/76743 of epoch 3, 86.55 ms/it, loss 0.468372
Finished training it 47104/76743 of epoch 3, 86.38 ms/it, loss 0.468027
Finished training it 47104/76743 of epoch 3, 86.70 ms/it, loss 0.468781
Finished training it 48128/76743 of epoch 3, 86.81 ms/it, loss 0.468479
Finished training it 48128/76743 of epoch 3, 86.51 ms/it, loss 0.469050
Finished training it 48128/76743 of epoch 3, 87.01 ms/it, loss 0.468473
Finished training it 48128/76743 of epoch 3, 86.61 ms/it, loss 0.472084
Finished training it 49152/76743 of epoch 3, 86.84 ms/it, loss 0.469294
Finished training it 49152/76743 of epoch 3, 86.41 ms/it, loss 0.469368
Finished training it 49152/76743 of epoch 3, 87.00 ms/it, loss 0.470728
Finished training it 49152/76743 of epoch 3, 86.45 ms/it, loss 0.469148
Finished training it 50176/76743 of epoch 3, 87.08 ms/it, loss 0.472267
Finished training it 50176/76743 of epoch 3, 86.62 ms/it, loss 0.469532
Finished training it 50176/76743 of epoch 3, 87.37 ms/it, loss 0.471919
Finished training it 50176/76743 of epoch 3, 86.77 ms/it, loss 0.470175
Finished training it 51200/76743 of epoch 3, 87.20 ms/it, loss 0.470976
Finished training it 51200/76743 of epoch 3, 87.42 ms/it, loss 0.468373
Finished training it 51200/76743 of epoch 3, 86.79 ms/it, loss 0.473420
Finished training it 51200/76743 of epoch 3, 86.90 ms/it, loss 0.470249
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540965.0
get out
0 has test check 2540965.0 and sample count 3274240
 accuracy 77.605 %, best 77.605 %, roc auc score 0.7723, best 0.7723
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 86.85 ms/it, loss 0.467606
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540965.0
get out
1 has test check 2540965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 86.98 ms/it, loss 0.471796
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540965.0
get out
3 has test check 2540965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 86.63 ms/it, loss 0.470343
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540965.0
get out
2 has test check 2540965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 86.63 ms/it, loss 0.469203
Finished training it 53248/76743 of epoch 3, 86.85 ms/it, loss 0.470730
Finished training it 53248/76743 of epoch 3, 86.96 ms/it, loss 0.469550
Finished training it 53248/76743 of epoch 3, 86.42 ms/it, loss 0.470438
Finished training it 53248/76743 of epoch 3, 86.50 ms/it, loss 0.470647
Finished training it 54272/76743 of epoch 3, 86.84 ms/it, loss 0.472025
Finished training it 54272/76743 of epoch 3, 86.30 ms/it, loss 0.468862
Finished training it 54272/76743 of epoch 3, 86.61 ms/it, loss 0.470909
Finished training it 54272/76743 of epoch 3, 86.49 ms/it, loss 0.469484
Finished training it 55296/76743 of epoch 3, 86.94 ms/it, loss 0.467806
Finished training it 55296/76743 of epoch 3, 86.55 ms/it, loss 0.472726
Finished training it 55296/76743 of epoch 3, 86.54 ms/it, loss 0.469686
Finished training it 55296/76743 of epoch 3, 86.62 ms/it, loss 0.469494
Finished training it 56320/76743 of epoch 3, 86.54 ms/it, loss 0.468163
Finished training it 56320/76743 of epoch 3, 86.89 ms/it, loss 0.471212
Finished training it 56320/76743 of epoch 3, 87.00 ms/it, loss 0.466952
Finished training it 56320/76743 of epoch 3, 86.58 ms/it, loss 0.469944
Finished training it 57344/76743 of epoch 3, 86.96 ms/it, loss 0.467859
Finished training it 57344/76743 of epoch 3, 87.22 ms/it, loss 0.473915
Finished training it 57344/76743 of epoch 3, 86.72 ms/it, loss 0.469204
Finished training it 57344/76743 of epoch 3, 86.91 ms/it, loss 0.471323
Finished training it 58368/76743 of epoch 3, 86.76 ms/it, loss 0.471755
Finished training it 58368/76743 of epoch 3, 86.62 ms/it, loss 0.471720
Finished training it 58368/76743 of epoch 3, 86.48 ms/it, loss 0.470467
Finished training it 58368/76743 of epoch 3, 87.01 ms/it, loss 0.469560
Finished training it 59392/76743 of epoch 3, 86.29 ms/it, loss 0.469785
Finished training it 59392/76743 of epoch 3, 86.41 ms/it, loss 0.471330
Finished training it 59392/76743 of epoch 3, 85.97 ms/it, loss 0.468466
Finished training it 59392/76743 of epoch 3, 86.16 ms/it, loss 0.469675
Finished training it 60416/76743 of epoch 3, 86.50 ms/it, loss 0.469521
Finished training it 60416/76743 of epoch 3, 86.55 ms/it, loss 0.468742
Finished training it 60416/76743 of epoch 3, 86.93 ms/it, loss 0.467881
Finished training it 60416/76743 of epoch 3, 86.45 ms/it, loss 0.467441
Finished training it 61440/76743 of epoch 3, 86.41 ms/it, loss 0.471473
Finished training it 61440/76743 of epoch 3, 86.77 ms/it, loss 0.469324
Finished training it 61440/76743 of epoch 3, 86.21 ms/it, loss 0.471200
Finished training it 61440/76743 of epoch 3, 86.14 ms/it, loss 0.468565
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541655.0
get out
0 has test check 2541655.0 and sample count 3274240
 accuracy 77.626 %, best 77.626 %, roc auc score 0.7727, best 0.7727
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541655.0
get out
1 has test check 2541655.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 86.98 ms/it, loss 0.470795
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 86.85 ms/it, loss 0.470382
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541655.0
get out
3 has test check 2541655.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 86.36 ms/it, loss 0.469333
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541655.0
get out
2 has test check 2541655.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 86.49 ms/it, loss 0.468240
Finished training it 63488/76743 of epoch 3, 87.10 ms/it, loss 0.468856
Finished training it 63488/76743 of epoch 3, 86.94 ms/it, loss 0.471142
Finished training it 63488/76743 of epoch 3, 86.62 ms/it, loss 0.468144
Finished training it 63488/76743 of epoch 3, 86.57 ms/it, loss 0.469830
Finished training it 64512/76743 of epoch 3, 86.53 ms/it, loss 0.468578
Finished training it 64512/76743 of epoch 3, 87.22 ms/it, loss 0.468283
Finished training it 64512/76743 of epoch 3, 86.95 ms/it, loss 0.470825
Finished training it 64512/76743 of epoch 3, 86.59 ms/it, loss 0.467778
Finished training it 65536/76743 of epoch 3, 86.48 ms/it, loss 0.469820
Finished training it 65536/76743 of epoch 3, 86.87 ms/it, loss 0.469938
Finished training it 65536/76743 of epoch 3, 86.52 ms/it, loss 0.470192
Finished training it 65536/76743 of epoch 3, 86.75 ms/it, loss 0.472080
Finished training it 66560/76743 of epoch 3, 96.80 ms/it, loss 0.469778
Finished training it 66560/76743 of epoch 3, 96.31 ms/it, loss 0.470303
Finished training it 66560/76743 of epoch 3, 96.60 ms/it, loss 0.469184
Finished training it 66560/76743 of epoch 3, 96.32 ms/it, loss 0.470209
Finished training it 67584/76743 of epoch 3, 86.07 ms/it, loss 0.472995
Finished training it 67584/76743 of epoch 3, 86.42 ms/it, loss 0.470530
Finished training it 67584/76743 of epoch 3, 86.15 ms/it, loss 0.468867
Finished training it 67584/76743 of epoch 3, 86.36 ms/it, loss 0.469972
Finished training it 68608/76743 of epoch 3, 86.86 ms/it, loss 0.468580
Finished training it 68608/76743 of epoch 3, 86.48 ms/it, loss 0.470028
Finished training it 68608/76743 of epoch 3, 86.56 ms/it, loss 0.469858
Finished training it 68608/76743 of epoch 3, 86.39 ms/it, loss 0.469397
Finished training it 69632/76743 of epoch 3, 86.40 ms/it, loss 0.471279
Finished training it 69632/76743 of epoch 3, 86.75 ms/it, loss 0.467783
Finished training it 69632/76743 of epoch 3, 86.46 ms/it, loss 0.470872
Finished training it 69632/76743 of epoch 3, 86.63 ms/it, loss 0.470273
Finished training it 70656/76743 of epoch 3, 86.27 ms/it, loss 0.467591
Finished training it 70656/76743 of epoch 3, 86.28 ms/it, loss 0.469640
Finished training it 70656/76743 of epoch 3, 86.68 ms/it, loss 0.470427
Finished training it 70656/76743 of epoch 3, 86.77 ms/it, loss 0.469380
Finished training it 71680/76743 of epoch 3, 86.85 ms/it, loss 0.466302
Finished training it 71680/76743 of epoch 3, 86.70 ms/it, loss 0.471598
Finished training it 71680/76743 of epoch 3, 87.07 ms/it, loss 0.466538
Finished training it 71680/76743 of epoch 3, 86.61 ms/it, loss 0.469730
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542640.0
get out
0 has test check 2542640.0 and sample count 3274240
 accuracy 77.656 %, best 77.656 %, roc auc score 0.7739, best 0.7739
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542640.0
get out
3 has test check 2542640.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 86.46 ms/it, loss 0.468409
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542640.0
get out
2 has test check 2542640.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 86.58 ms/it, loss 0.469976
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542640.0
get out
1 has test check 2542640.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 86.96 ms/it, loss 0.470071
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 86.75 ms/it, loss 0.471300
Finished training it 73728/76743 of epoch 3, 87.20 ms/it, loss 0.467989
Finished training it 73728/76743 of epoch 3, 87.09 ms/it, loss 0.472207
Finished training it 73728/76743 of epoch 3, 86.88 ms/it, loss 0.470609
Finished training it 73728/76743 of epoch 3, 86.88 ms/it, loss 0.468264
Finished training it 74752/76743 of epoch 3, 87.29 ms/it, loss 0.470090
Finished training it 74752/76743 of epoch 3, 87.01 ms/it, loss 0.470039
Finished training it 74752/76743 of epoch 3, 86.62 ms/it, loss 0.470609
Finished training it 74752/76743 of epoch 3, 86.59 ms/it, loss 0.470242
Finished training it 75776/76743 of epoch 3, 86.47 ms/it, loss 0.470942
Finished training it 75776/76743 of epoch 3, 86.13 ms/it, loss 0.469063
Finished training it 75776/76743 of epoch 3, 86.74 ms/it, loss 0.470716
Finished training it 75776/76743 of epoch 3, 86.12 ms/it, loss 0.470810
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 87.49 ms/it, loss 0.470570
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 87.32 ms/it, loss 0.466218
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 88.00 ms/it, loss 0.471452
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 87.34 ms/it, loss 0.469487
Finished training it 2048/76743 of epoch 4, 86.21 ms/it, loss 0.469378
Finished training it 2048/76743 of epoch 4, 86.47 ms/it, loss 0.468778
Finished training it 2048/76743 of epoch 4, 86.11 ms/it, loss 0.470988
Finished training it 2048/76743 of epoch 4, 86.68 ms/it, loss 0.466925
Finished training it 3072/76743 of epoch 4, 86.56 ms/it, loss 0.467023
Finished training it 3072/76743 of epoch 4, 86.41 ms/it, loss 0.466985
Finished training it 3072/76743 of epoch 4, 85.95 ms/it, loss 0.467055
Finished training it 3072/76743 of epoch 4, 86.13 ms/it, loss 0.469933
Finished training it 4096/76743 of epoch 4, 86.61 ms/it, loss 0.470341
Finished training it 4096/76743 of epoch 4, 86.92 ms/it, loss 0.468578
Finished training it 4096/76743 of epoch 4, 86.41 ms/it, loss 0.471163
Finished training it 4096/76743 of epoch 4, 86.38 ms/it, loss 0.469337
Finished training it 5120/76743 of epoch 4, 87.05 ms/it, loss 0.469093
Finished training it 5120/76743 of epoch 4, 86.44 ms/it, loss 0.471259
Finished training it 5120/76743 of epoch 4, 86.86 ms/it, loss 0.466993
Finished training it 5120/76743 of epoch 4, 86.43 ms/it, loss 0.468512
Finished training it 6144/76743 of epoch 4, 85.91 ms/it, loss 0.467288
Finished training it 6144/76743 of epoch 4, 86.54 ms/it, loss 0.467968
Finished training it 6144/76743 of epoch 4, 86.28 ms/it, loss 0.470836
Finished training it 6144/76743 of epoch 4, 86.01 ms/it, loss 0.468038
Finished training it 7168/76743 of epoch 4, 86.83 ms/it, loss 0.471119
Finished training it 7168/76743 of epoch 4, 87.01 ms/it, loss 0.471976
Finished training it 7168/76743 of epoch 4, 86.61 ms/it, loss 0.469908
Finished training it 7168/76743 of epoch 4, 86.59 ms/it, loss 0.469584
Finished training it 8192/76743 of epoch 4, 101.74 ms/it, loss 0.470706
Finished training it 8192/76743 of epoch 4, 101.77 ms/it, loss 0.468219
Finished training it 8192/76743 of epoch 4, 101.09 ms/it, loss 0.469893
Finished training it 8192/76743 of epoch 4, 101.07 ms/it, loss 0.469357
Finished training it 9216/76743 of epoch 4, 102.09 ms/it, loss 0.469858
Finished training it 9216/76743 of epoch 4, 101.12 ms/it, loss 0.471344
Finished training it 9216/76743 of epoch 4, 101.66 ms/it, loss 0.469046
Finished training it 9216/76743 of epoch 4, 101.40 ms/it, loss 0.471006
Finished training it 10240/76743 of epoch 4, 101.43 ms/it, loss 0.470016
Finished training it 10240/76743 of epoch 4, 101.96 ms/it, loss 0.468996
Finished training it 10240/76743 of epoch 4, 101.60 ms/it, loss 0.470531
Finished training it 10240/76743 of epoch 4, 101.56 ms/it, loss 0.467442
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542967.0
get out
0 has test check 2542967.0 and sample count 3274240
 accuracy 77.666 %, best 77.666 %, roc auc score 0.7740, best 0.7740
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542967.0
get out
3 has test check 2542967.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.33 ms/it, loss 0.466947
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 87.39 ms/it, loss 0.469537
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542967.0
get out
2 has test check 2542967.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.31 ms/it, loss 0.467449
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542967.0
get out
1 has test check 2542967.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.67 ms/it, loss 0.467757
Finished training it 12288/76743 of epoch 4, 87.14 ms/it, loss 0.467218
Finished training it 12288/76743 of epoch 4, 87.32 ms/it, loss 0.470371
Finished training it 12288/76743 of epoch 4, 87.30 ms/it, loss 0.469875
Finished training it 12288/76743 of epoch 4, 86.98 ms/it, loss 0.470799
Finished training it 13312/76743 of epoch 4, 97.19 ms/it, loss 0.470389
Finished training it 13312/76743 of epoch 4, 97.42 ms/it, loss 0.468598
Finished training it 13312/76743 of epoch 4, 97.38 ms/it, loss 0.470864
Finished training it 13312/76743 of epoch 4, 97.90 ms/it, loss 0.468765
Finished training it 14336/76743 of epoch 4, 87.78 ms/it, loss 0.471248
Finished training it 14336/76743 of epoch 4, 87.88 ms/it, loss 0.468214
Finished training it 14336/76743 of epoch 4, 87.52 ms/it, loss 0.467062
Finished training it 14336/76743 of epoch 4, 87.39 ms/it, loss 0.469370
Finished training it 15360/76743 of epoch 4, 87.30 ms/it, loss 0.468835
Finished training it 15360/76743 of epoch 4, 86.89 ms/it, loss 0.466980
Finished training it 15360/76743 of epoch 4, 87.25 ms/it, loss 0.470895
Finished training it 15360/76743 of epoch 4, 87.06 ms/it, loss 0.467801
Finished training it 16384/76743 of epoch 4, 87.03 ms/it, loss 0.467372
Finished training it 16384/76743 of epoch 4, 87.41 ms/it, loss 0.468498
Finished training it 16384/76743 of epoch 4, 86.93 ms/it, loss 0.471563
Finished training it 16384/76743 of epoch 4, 87.05 ms/it, loss 0.467917
Finished training it 17408/76743 of epoch 4, 87.08 ms/it, loss 0.471048
Finished training it 17408/76743 of epoch 4, 86.99 ms/it, loss 0.466525
Finished training it 17408/76743 of epoch 4, 86.87 ms/it, loss 0.469940
Finished training it 17408/76743 of epoch 4, 86.70 ms/it, loss 0.467973
Finished training it 18432/76743 of epoch 4, 88.23 ms/it, loss 0.470087
Finished training it 18432/76743 of epoch 4, 87.75 ms/it, loss 0.469477
Finished training it 18432/76743 of epoch 4, 88.09 ms/it, loss 0.469190
Finished training it 18432/76743 of epoch 4, 87.95 ms/it, loss 0.469810
Finished training it 19456/76743 of epoch 4, 90.40 ms/it, loss 0.469588
Finished training it 19456/76743 of epoch 4, 90.32 ms/it, loss 0.470380
Finished training it 19456/76743 of epoch 4, 90.23 ms/it, loss 0.471773
Finished training it 19456/76743 of epoch 4, 90.64 ms/it, loss 0.469835
Finished training it 20480/76743 of epoch 4, 90.93 ms/it, loss 0.471871
Finished training it 20480/76743 of epoch 4, 91.45 ms/it, loss 0.470845
Finished training it 20480/76743 of epoch 4, 90.96 ms/it, loss 0.470366
Finished training it 20480/76743 of epoch 4, 90.86 ms/it, loss 0.471800
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543504.0
get out
0 has test check 2543504.0 and sample count 3274240
 accuracy 77.682 %, best 77.682 %, roc auc score 0.7748, best 0.7748
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 87.04 ms/it, loss 0.467943
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543504.0
get out
1 has test check 2543504.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.38 ms/it, loss 0.470095
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543504.0
get out
2 has test check 2543504.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.12 ms/it, loss 0.467087
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543504.0
get out
3 has test check 2543504.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.04 ms/it, loss 0.467687
Finished training it 22528/76743 of epoch 4, 86.88 ms/it, loss 0.466250
Finished training it 22528/76743 of epoch 4, 87.23 ms/it, loss 0.468459
Finished training it 22528/76743 of epoch 4, 86.92 ms/it, loss 0.469229
Finished training it 22528/76743 of epoch 4, 87.30 ms/it, loss 0.470045
Finished training it 23552/76743 of epoch 4, 87.41 ms/it, loss 0.467547
Finished training it 23552/76743 of epoch 4, 87.53 ms/it, loss 0.465718
Finished training it 23552/76743 of epoch 4, 87.30 ms/it, loss 0.467569
Finished training it 23552/76743 of epoch 4, 87.15 ms/it, loss 0.468513
Finished training it 24576/76743 of epoch 4, 86.97 ms/it, loss 0.468501
Finished training it 24576/76743 of epoch 4, 86.86 ms/it, loss 0.472641
Finished training it 24576/76743 of epoch 4, 87.32 ms/it, loss 0.467159
Finished training it 24576/76743 of epoch 4, 87.07 ms/it, loss 0.468384
Finished training it 25600/76743 of epoch 4, 88.12 ms/it, loss 0.469643
Finished training it 25600/76743 of epoch 4, 87.66 ms/it, loss 0.468458
Finished training it 25600/76743 of epoch 4, 87.65 ms/it, loss 0.468931
Finished training it 25600/76743 of epoch 4, 87.84 ms/it, loss 0.471335
Finished training it 26624/76743 of epoch 4, 87.31 ms/it, loss 0.468220
Finished training it 26624/76743 of epoch 4, 87.70 ms/it, loss 0.468082
Finished training it 26624/76743 of epoch 4, 87.45 ms/it, loss 0.469497
Finished training it 26624/76743 of epoch 4, 87.93 ms/it, loss 0.468782
Finished training it 27648/76743 of epoch 4, 87.66 ms/it, loss 0.470791
Finished training it 27648/76743 of epoch 4, 87.85 ms/it, loss 0.468917
Finished training it 27648/76743 of epoch 4, 87.22 ms/it, loss 0.469764
Finished training it 27648/76743 of epoch 4, 87.24 ms/it, loss 0.470810
Finished training it 28672/76743 of epoch 4, 86.69 ms/it, loss 0.468414
Finished training it 28672/76743 of epoch 4, 86.78 ms/it, loss 0.470530
Finished training it 28672/76743 of epoch 4, 87.41 ms/it, loss 0.467611
Finished training it 28672/76743 of epoch 4, 87.24 ms/it, loss 0.471765
Finished training it 29696/76743 of epoch 4, 89.74 ms/it, loss 0.469404
Finished training it 29696/76743 of epoch 4, 89.72 ms/it, loss 0.467096
Finished training it 29696/76743 of epoch 4, 89.79 ms/it, loss 0.467984
Finished training it 29696/76743 of epoch 4, 90.18 ms/it, loss 0.468458
Finished training it 30720/76743 of epoch 4, 91.03 ms/it, loss 0.463784
Finished training it 30720/76743 of epoch 4, 90.74 ms/it, loss 0.468745
Finished training it 30720/76743 of epoch 4, 90.69 ms/it, loss 0.468850
Finished training it 30720/76743 of epoch 4, 91.20 ms/it, loss 0.467241
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542854.0
get out
0 has test check 2542854.0 and sample count 3274240
 accuracy 77.662 %, best 77.682 %, roc auc score 0.7743, best 0.7748
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542854.0
get out
1 has test check 2542854.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 90.86 ms/it, loss 0.468245
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542854.0
get out
2 has test check 2542854.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 90.51 ms/it, loss 0.470335
Finished training it 31744/76743 of epoch 4, 90.73 ms/it, loss 0.470862
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542854.0
get out
3 has test check 2542854.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 90.65 ms/it, loss 0.471150
Finished training it 32768/76743 of epoch 4, 91.02 ms/it, loss 0.467456
Finished training it 32768/76743 of epoch 4, 91.09 ms/it, loss 0.469108
Finished training it 32768/76743 of epoch 4, 91.66 ms/it, loss 0.469531
Finished training it 32768/76743 of epoch 4, 91.30 ms/it, loss 0.467873
Finished training it 33792/76743 of epoch 4, 89.03 ms/it, loss 0.469695
Finished training it 33792/76743 of epoch 4, 88.91 ms/it, loss 0.471153
Finished training it 33792/76743 of epoch 4, 89.24 ms/it, loss 0.470011
Finished training it 33792/76743 of epoch 4, 88.86 ms/it, loss 0.468315
Finished training it 34816/76743 of epoch 4, 97.37 ms/it, loss 0.466635
Finished training it 34816/76743 of epoch 4, 97.31 ms/it, loss 0.469580
Finished training it 34816/76743 of epoch 4, 97.36 ms/it, loss 0.467204
Finished training it 34816/76743 of epoch 4, 97.75 ms/it, loss 0.469788
Finished training it 35840/76743 of epoch 4, 87.66 ms/it, loss 0.468209
Finished training it 35840/76743 of epoch 4, 87.62 ms/it, loss 0.468261
Finished training it 35840/76743 of epoch 4, 87.86 ms/it, loss 0.468730
Finished training it 35840/76743 of epoch 4, 88.70 ms/it, loss 0.470534
Finished training it 36864/76743 of epoch 4, 88.07 ms/it, loss 0.469334
Finished training it 36864/76743 of epoch 4, 87.34 ms/it, loss 0.466041
Finished training it 36864/76743 of epoch 4, 87.70 ms/it, loss 0.469836
Finished training it 36864/76743 of epoch 4, 87.38 ms/it, loss 0.469169
Finished training it 37888/76743 of epoch 4, 87.95 ms/it, loss 0.464867
Finished training it 37888/76743 of epoch 4, 87.54 ms/it, loss 0.467958
Finished training it 37888/76743 of epoch 4, 87.67 ms/it, loss 0.468404
Finished training it 37888/76743 of epoch 4, 87.48 ms/it, loss 0.469505
Finished training it 38912/76743 of epoch 4, 87.38 ms/it, loss 0.469430
Finished training it 38912/76743 of epoch 4, 87.02 ms/it, loss 0.468274
Finished training it 38912/76743 of epoch 4, 87.33 ms/it, loss 0.470066
Finished training it 38912/76743 of epoch 4, 87.65 ms/it, loss 0.467649
Finished training it 39936/76743 of epoch 4, 87.41 ms/it, loss 0.470271
Finished training it 39936/76743 of epoch 4, 87.72 ms/it, loss 0.466812
Finished training it 39936/76743 of epoch 4, 87.86 ms/it, loss 0.465416
Finished training it 39936/76743 of epoch 4, 87.38 ms/it, loss 0.468161
Finished training it 40960/76743 of epoch 4, 87.85 ms/it, loss 0.468501
Finished training it 40960/76743 of epoch 4, 88.14 ms/it, loss 0.465317
Finished training it 40960/76743 of epoch 4, 87.77 ms/it, loss 0.469633
Finished training it 40960/76743 of epoch 4, 87.99 ms/it, loss 0.465909
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544853.0
get out
0 has test check 2544853.0 and sample count 3274240
 accuracy 77.723 %, best 77.723 %, roc auc score 0.7755, best 0.7755
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544853.0
get out
1 has test check 2544853.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 90.49 ms/it, loss 0.468575
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 90.51 ms/it, loss 0.469441
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544853.0
get out
3 has test check 2544853.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 90.51 ms/it, loss 0.467187
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544853.0
get out
2 has test check 2544853.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 90.59 ms/it, loss 0.466621
Finished training it 43008/76743 of epoch 4, 90.81 ms/it, loss 0.465452
Finished training it 43008/76743 of epoch 4, 90.41 ms/it, loss 0.471517
Finished training it 43008/76743 of epoch 4, 90.41 ms/it, loss 0.470044
Finished training it 43008/76743 of epoch 4, 90.60 ms/it, loss 0.467353
Finished training it 44032/76743 of epoch 4, 87.94 ms/it, loss 0.467225
Finished training it 44032/76743 of epoch 4, 88.07 ms/it, loss 0.469152
Finished training it 44032/76743 of epoch 4, 87.93 ms/it, loss 0.468910
Finished training it 44032/76743 of epoch 4, 88.22 ms/it, loss 0.469167
Finished training it 45056/76743 of epoch 4, 87.45 ms/it, loss 0.467250
Finished training it 45056/76743 of epoch 4, 87.80 ms/it, loss 0.468607
Finished training it 45056/76743 of epoch 4, 87.90 ms/it, loss 0.469675
Finished training it 45056/76743 of epoch 4, 87.47 ms/it, loss 0.470701
Finished training it 46080/76743 of epoch 4, 88.01 ms/it, loss 0.468904
Finished training it 46080/76743 of epoch 4, 87.62 ms/it, loss 0.467769
Finished training it 46080/76743 of epoch 4, 88.16 ms/it, loss 0.469248
Finished training it 46080/76743 of epoch 4, 87.67 ms/it, loss 0.467501
Finished training it 47104/76743 of epoch 4, 87.60 ms/it, loss 0.466158
Finished training it 47104/76743 of epoch 4, 87.28 ms/it, loss 0.466141
Finished training it 47104/76743 of epoch 4, 86.99 ms/it, loss 0.465538
Finished training it 47104/76743 of epoch 4, 87.26 ms/it, loss 0.465645
Finished training it 48128/76743 of epoch 4, 87.54 ms/it, loss 0.470008
Finished training it 48128/76743 of epoch 4, 87.72 ms/it, loss 0.466333
Finished training it 48128/76743 of epoch 4, 88.03 ms/it, loss 0.466544
Finished training it 48128/76743 of epoch 4, 87.65 ms/it, loss 0.466512
Finished training it 49152/76743 of epoch 4, 87.39 ms/it, loss 0.466760
Finished training it 49152/76743 of epoch 4, 87.30 ms/it, loss 0.466777
Finished training it 49152/76743 of epoch 4, 87.82 ms/it, loss 0.468759
Finished training it 49152/76743 of epoch 4, 87.67 ms/it, loss 0.466763
Finished training it 50176/76743 of epoch 4, 87.20 ms/it, loss 0.469926
Finished training it 50176/76743 of epoch 4, 87.01 ms/it, loss 0.467369
Finished training it 50176/76743 of epoch 4, 86.95 ms/it, loss 0.467393
Finished training it 50176/76743 of epoch 4, 87.31 ms/it, loss 0.469568
Finished training it 51200/76743 of epoch 4, 87.95 ms/it, loss 0.468075
Finished training it 51200/76743 of epoch 4, 88.41 ms/it, loss 0.468992
Finished training it 51200/76743 of epoch 4, 87.91 ms/it, loss 0.471562
Finished training it 51200/76743 of epoch 4, 88.53 ms/it, loss 0.466270
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545110.0
get out
0 has test check 2545110.0 and sample count 3274240
 accuracy 77.731 %, best 77.731 %, roc auc score 0.7756, best 0.7756
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545110.0
get out
1 has test check 2545110.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.54 ms/it, loss 0.469381
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545110.0
get out
2 has test check 2545110.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.46 ms/it, loss 0.466630
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545110.0
get out
3 has test check 2545110.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.37 ms/it, loss 0.467797
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 90.36 ms/it, loss 0.465386
Finished training it 53248/76743 of epoch 4, 90.66 ms/it, loss 0.468474
Finished training it 53248/76743 of epoch 4, 90.69 ms/it, loss 0.468161
Finished training it 53248/76743 of epoch 4, 90.78 ms/it, loss 0.467149
Finished training it 53248/76743 of epoch 4, 90.56 ms/it, loss 0.468158
Finished training it 54272/76743 of epoch 4, 89.12 ms/it, loss 0.466555
Finished training it 54272/76743 of epoch 4, 89.57 ms/it, loss 0.469972
Finished training it 54272/76743 of epoch 4, 89.35 ms/it, loss 0.468378
Finished training it 54272/76743 of epoch 4, 89.23 ms/it, loss 0.467588
Finished training it 55296/76743 of epoch 4, 98.26 ms/it, loss 0.465540
Finished training it 55296/76743 of epoch 4, 97.59 ms/it, loss 0.470781
Finished training it 55296/76743 of epoch 4, 98.30 ms/it, loss 0.467132
Finished training it 55296/76743 of epoch 4, 98.01 ms/it, loss 0.467413
Finished training it 56320/76743 of epoch 4, 88.47 ms/it, loss 0.464629
Finished training it 56320/76743 of epoch 4, 88.10 ms/it, loss 0.468854
Finished training it 56320/76743 of epoch 4, 87.86 ms/it, loss 0.467919
Finished training it 56320/76743 of epoch 4, 87.78 ms/it, loss 0.465217
Finished training it 57344/76743 of epoch 4, 86.62 ms/it, loss 0.467085
Finished training it 57344/76743 of epoch 4, 87.03 ms/it, loss 0.465580
Finished training it 57344/76743 of epoch 4, 87.18 ms/it, loss 0.472027
Finished training it 57344/76743 of epoch 4, 86.64 ms/it, loss 0.469091
Finished training it 58368/76743 of epoch 4, 87.97 ms/it, loss 0.468244
Finished training it 58368/76743 of epoch 4, 88.38 ms/it, loss 0.469688
Finished training it 58368/76743 of epoch 4, 88.56 ms/it, loss 0.466999
Finished training it 58368/76743 of epoch 4, 88.12 ms/it, loss 0.469639
Finished training it 59392/76743 of epoch 4, 87.92 ms/it, loss 0.469355
Finished training it 59392/76743 of epoch 4, 87.83 ms/it, loss 0.467584
Finished training it 59392/76743 of epoch 4, 87.51 ms/it, loss 0.466506
Finished training it 59392/76743 of epoch 4, 87.61 ms/it, loss 0.466882
Finished training it 60416/76743 of epoch 4, 87.56 ms/it, loss 0.466937
Finished training it 60416/76743 of epoch 4, 87.85 ms/it, loss 0.465936
Finished training it 60416/76743 of epoch 4, 87.86 ms/it, loss 0.466880
Finished training it 60416/76743 of epoch 4, 87.55 ms/it, loss 0.465500
Finished training it 61440/76743 of epoch 4, 88.47 ms/it, loss 0.467237
Finished training it 61440/76743 of epoch 4, 88.27 ms/it, loss 0.469225
Finished training it 61440/76743 of epoch 4, 87.80 ms/it, loss 0.466393
Finished training it 61440/76743 of epoch 4, 87.71 ms/it, loss 0.468962
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546109.0
get out
0 has test check 2546109.0 and sample count 3274240
 accuracy 77.762 %, best 77.762 %, roc auc score 0.7758, best 0.7758
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546109.0
get out
3 has test check 2546109.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.71 ms/it, loss 0.467124
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 90.88 ms/it, loss 0.468557
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546109.0
get out
2 has test check 2546109.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.72 ms/it, loss 0.466009
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546109.0
get out
1 has test check 2546109.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.83 ms/it, loss 0.468366
Finished training it 63488/76743 of epoch 4, 90.29 ms/it, loss 0.466107
Finished training it 63488/76743 of epoch 4, 90.26 ms/it, loss 0.467471
Finished training it 63488/76743 of epoch 4, 90.30 ms/it, loss 0.469034
Finished training it 63488/76743 of epoch 4, 90.44 ms/it, loss 0.466591
Finished training it 64512/76743 of epoch 4, 89.29 ms/it, loss 0.466241
Finished training it 64512/76743 of epoch 4, 89.02 ms/it, loss 0.465371
Finished training it 64512/76743 of epoch 4, 89.18 ms/it, loss 0.465786
Finished training it 64512/76743 of epoch 4, 89.20 ms/it, loss 0.468608
Finished training it 65536/76743 of epoch 4, 87.85 ms/it, loss 0.467698
Finished training it 65536/76743 of epoch 4, 88.08 ms/it, loss 0.470431
Finished training it 65536/76743 of epoch 4, 88.37 ms/it, loss 0.468016
Finished training it 65536/76743 of epoch 4, 87.92 ms/it, loss 0.468184
Finished training it 66560/76743 of epoch 4, 88.24 ms/it, loss 0.468206
Finished training it 66560/76743 of epoch 4, 88.15 ms/it, loss 0.468373
Finished training it 66560/76743 of epoch 4, 88.55 ms/it, loss 0.467023
Finished training it 66560/76743 of epoch 4, 88.88 ms/it, loss 0.467486
Finished training it 67584/76743 of epoch 4, 88.10 ms/it, loss 0.468231
Finished training it 67584/76743 of epoch 4, 87.77 ms/it, loss 0.468621
Finished training it 67584/76743 of epoch 4, 87.55 ms/it, loss 0.466550
Finished training it 67584/76743 of epoch 4, 87.58 ms/it, loss 0.471113
Finished training it 68608/76743 of epoch 4, 87.74 ms/it, loss 0.467121
Finished training it 68608/76743 of epoch 4, 88.04 ms/it, loss 0.466440
Finished training it 68608/76743 of epoch 4, 87.66 ms/it, loss 0.468204
Finished training it 68608/76743 of epoch 4, 87.73 ms/it, loss 0.467801
Finished training it 69632/76743 of epoch 4, 87.60 ms/it, loss 0.469121
Finished training it 69632/76743 of epoch 4, 87.67 ms/it, loss 0.468392
Finished training it 69632/76743 of epoch 4, 87.55 ms/it, loss 0.468739
Finished training it 69632/76743 of epoch 4, 87.94 ms/it, loss 0.465832
Finished training it 70656/76743 of epoch 4, 88.14 ms/it, loss 0.467543
Finished training it 70656/76743 of epoch 4, 87.63 ms/it, loss 0.468010
Finished training it 70656/76743 of epoch 4, 88.14 ms/it, loss 0.468771
Finished training it 70656/76743 of epoch 4, 87.76 ms/it, loss 0.466065
Finished training it 71680/76743 of epoch 4, 87.29 ms/it, loss 0.464586
Finished training it 71680/76743 of epoch 4, 87.03 ms/it, loss 0.469459
Finished training it 71680/76743 of epoch 4, 86.91 ms/it, loss 0.467946
Finished training it 71680/76743 of epoch 4, 87.65 ms/it, loss 0.464855
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545813.0
get out
0 has test check 2545813.0 and sample count 3274240
 accuracy 77.753 %, best 77.762 %, roc auc score 0.7764, best 0.7764
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545813.0
get out
3 has test check 2545813.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 89.82 ms/it, loss 0.466601
Finished training it 72704/76743 of epoch 4, 89.93 ms/it, loss 0.469713
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545813.0
get out
1 has test check 2545813.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 89.97 ms/it, loss 0.468253
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545813.0
get out
2 has test check 2545813.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 89.80 ms/it, loss 0.468391
Finished training it 73728/76743 of epoch 4, 90.73 ms/it, loss 0.466466
Finished training it 73728/76743 of epoch 4, 90.36 ms/it, loss 0.468705
Finished training it 73728/76743 of epoch 4, 90.47 ms/it, loss 0.466434
Finished training it 73728/76743 of epoch 4, 90.62 ms/it, loss 0.469978
Finished training it 74752/76743 of epoch 4, 89.47 ms/it, loss 0.467837
Finished training it 74752/76743 of epoch 4, 89.09 ms/it, loss 0.468306
Finished training it 74752/76743 of epoch 4, 89.60 ms/it, loss 0.468367
Finished training it 74752/76743 of epoch 4, 89.21 ms/it, loss 0.468333
Finished training it 75776/76743 of epoch 4, 87.44 ms/it, loss 0.468557
Finished training it 75776/76743 of epoch 4, 87.73 ms/it, loss 0.469041
Finished training it 75776/76743 of epoch 4, 87.30 ms/it, loss 0.467123
Finished training it 75776/76743 of epoch 4, 87.80 ms/it, loss 0.468467
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 93.66 ms/it, loss 0.464546
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.64 ms/it, loss 0.469456
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 93.91 ms/it, loss 0.467852
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.17 ms/it, loss 0.468622
Finished training it 2048/76743 of epoch 5, 87.48 ms/it, loss 0.467330
Finished training it 2048/76743 of epoch 5, 87.26 ms/it, loss 0.469288
Finished training it 2048/76743 of epoch 5, 87.78 ms/it, loss 0.465067
Finished training it 2048/76743 of epoch 5, 87.21 ms/it, loss 0.467366
Finished training it 3072/76743 of epoch 5, 88.03 ms/it, loss 0.465311
Finished training it 3072/76743 of epoch 5, 88.12 ms/it, loss 0.465615
Finished training it 3072/76743 of epoch 5, 87.88 ms/it, loss 0.465113
Finished training it 3072/76743 of epoch 5, 87.87 ms/it, loss 0.468232
Finished training it 4096/76743 of epoch 5, 87.45 ms/it, loss 0.467661
Finished training it 4096/76743 of epoch 5, 87.96 ms/it, loss 0.466940
Finished training it 4096/76743 of epoch 5, 87.52 ms/it, loss 0.469729
Finished training it 4096/76743 of epoch 5, 87.85 ms/it, loss 0.468302
Finished training it 5120/76743 of epoch 5, 88.15 ms/it, loss 0.467290
Finished training it 5120/76743 of epoch 5, 87.69 ms/it, loss 0.465404
Finished training it 5120/76743 of epoch 5, 87.58 ms/it, loss 0.467086
Finished training it 5120/76743 of epoch 5, 87.61 ms/it, loss 0.469764
Finished training it 6144/76743 of epoch 5, 87.42 ms/it, loss 0.469133
Finished training it 6144/76743 of epoch 5, 87.09 ms/it, loss 0.466473
Finished training it 6144/76743 of epoch 5, 87.56 ms/it, loss 0.466522
Finished training it 6144/76743 of epoch 5, 86.94 ms/it, loss 0.465797
Finished training it 7168/76743 of epoch 5, 87.70 ms/it, loss 0.468914
Finished training it 7168/76743 of epoch 5, 87.91 ms/it, loss 0.470309
Finished training it 7168/76743 of epoch 5, 87.44 ms/it, loss 0.467621
Finished training it 7168/76743 of epoch 5, 87.68 ms/it, loss 0.467749
Finished training it 8192/76743 of epoch 5, 87.68 ms/it, loss 0.466480
Finished training it 8192/76743 of epoch 5, 87.90 ms/it, loss 0.469025
Finished training it 8192/76743 of epoch 5, 87.40 ms/it, loss 0.468500
Finished training it 8192/76743 of epoch 5, 87.42 ms/it, loss 0.467564
Finished training it 9216/76743 of epoch 5, 88.92 ms/it, loss 0.469168
Finished training it 9216/76743 of epoch 5, 89.21 ms/it, loss 0.468346
Finished training it 9216/76743 of epoch 5, 88.98 ms/it, loss 0.467618
Finished training it 9216/76743 of epoch 5, 88.90 ms/it, loss 0.469582
Finished training it 10240/76743 of epoch 5, 90.31 ms/it, loss 0.466796
Finished training it 10240/76743 of epoch 5, 90.14 ms/it, loss 0.465530
Finished training it 10240/76743 of epoch 5, 90.18 ms/it, loss 0.468356
Finished training it 10240/76743 of epoch 5, 90.15 ms/it, loss 0.468090
Testing at - 10240/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544339.0
get out
0 has test check 2544339.0 and sample count 3274240
 accuracy 77.708 %, best 77.762 %, roc auc score 0.7756, best 0.7764
Finished training it 11264/76743 of epoch 5, 87.72 ms/it, loss 0.467923
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544339.0
get out
1 has test check 2544339.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.69 ms/it, loss 0.465856
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544339.0
get out
3 has test check 2544339.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.37 ms/it, loss 0.464728
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544339.0
get out
2 has test check 2544339.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.34 ms/it, loss 0.466022
Finished training it 12288/76743 of epoch 5, 87.93 ms/it, loss 0.465706
Finished training it 12288/76743 of epoch 5, 88.21 ms/it, loss 0.468104
Finished training it 12288/76743 of epoch 5, 88.05 ms/it, loss 0.468812
Finished training it 12288/76743 of epoch 5, 88.61 ms/it, loss 0.468336
Finished training it 13312/76743 of epoch 5, 88.24 ms/it, loss 0.467382
Finished training it 13312/76743 of epoch 5, 87.95 ms/it, loss 0.469192
Finished training it 13312/76743 of epoch 5, 87.39 ms/it, loss 0.466868
Finished training it 13312/76743 of epoch 5, 87.38 ms/it, loss 0.468380
Finished training it 14336/76743 of epoch 5, 87.77 ms/it, loss 0.469062
Finished training it 14336/76743 of epoch 5, 87.26 ms/it, loss 0.467402
Finished training it 14336/76743 of epoch 5, 87.67 ms/it, loss 0.466356
Finished training it 14336/76743 of epoch 5, 87.30 ms/it, loss 0.465095
Finished training it 15360/76743 of epoch 5, 87.04 ms/it, loss 0.465877
Finished training it 15360/76743 of epoch 5, 87.15 ms/it, loss 0.465285
Finished training it 15360/76743 of epoch 5, 87.42 ms/it, loss 0.467003
Finished training it 15360/76743 of epoch 5, 87.77 ms/it, loss 0.468684
Finished training it 16384/76743 of epoch 5, 87.04 ms/it, loss 0.465630
Finished training it 16384/76743 of epoch 5, 87.64 ms/it, loss 0.467166
Finished training it 16384/76743 of epoch 5, 87.22 ms/it, loss 0.469463
Finished training it 16384/76743 of epoch 5, 87.48 ms/it, loss 0.465739
Finished training it 17408/76743 of epoch 5, 87.64 ms/it, loss 0.464568
Finished training it 17408/76743 of epoch 5, 86.98 ms/it, loss 0.468430
Finished training it 17408/76743 of epoch 5, 86.98 ms/it, loss 0.466037
Finished training it 17408/76743 of epoch 5, 87.70 ms/it, loss 0.469506
Finished training it 18432/76743 of epoch 5, 88.61 ms/it, loss 0.468176
Finished training it 18432/76743 of epoch 5, 88.32 ms/it, loss 0.467698
Finished training it 18432/76743 of epoch 5, 88.15 ms/it, loss 0.468261
Finished training it 18432/76743 of epoch 5, 88.74 ms/it, loss 0.467405
Finished training it 19456/76743 of epoch 5, 88.16 ms/it, loss 0.469510
Finished training it 19456/76743 of epoch 5, 88.41 ms/it, loss 0.467741
Finished training it 19456/76743 of epoch 5, 88.68 ms/it, loss 0.467657
Finished training it 19456/76743 of epoch 5, 88.43 ms/it, loss 0.468322
Finished training it 20480/76743 of epoch 5, 90.16 ms/it, loss 0.468453
Finished training it 20480/76743 of epoch 5, 90.34 ms/it, loss 0.469466
Finished training it 20480/76743 of epoch 5, 90.26 ms/it, loss 0.469875
Finished training it 20480/76743 of epoch 5, 90.34 ms/it, loss 0.468888
Testing at - 20480/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546848.0
get out
0 has test check 2546848.0 and sample count 3274240
 accuracy 77.784 %, best 77.784 %, roc auc score 0.7773, best 0.7773
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 5, 87.41 ms/it, loss 0.465901
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546848.0
get out
3 has test check 2546848.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 86.92 ms/it, loss 0.466320
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546848.0
get out
2 has test check 2546848.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 87.06 ms/it, loss 0.464845
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546848.0
get out
1 has test check 2546848.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 87.23 ms/it, loss 0.468127
Finished training it 22528/76743 of epoch 5, 87.59 ms/it, loss 0.464496
Finished training it 22528/76743 of epoch 5, 87.60 ms/it, loss 0.467712
Finished training it 22528/76743 of epoch 5, 87.80 ms/it, loss 0.468202
Finished training it 22528/76743 of epoch 5, 88.07 ms/it, loss 0.466806
Finished training it 23552/76743 of epoch 5, 87.26 ms/it, loss 0.465485
Finished training it 23552/76743 of epoch 5, 87.52 ms/it, loss 0.463646
Finished training it 23552/76743 of epoch 5, 86.88 ms/it, loss 0.465860
Finished training it 23552/76743 of epoch 5, 86.77 ms/it, loss 0.466384
Finished training it 24576/76743 of epoch 5, 98.64 ms/it, loss 0.465766
Finished training it 24576/76743 of epoch 5, 98.56 ms/it, loss 0.466263
Finished training it 24576/76743 of epoch 5, 98.42 ms/it, loss 0.466589
Finished training it 24576/76743 of epoch 5, 98.37 ms/it, loss 0.470895
Finished training it 25600/76743 of epoch 5, 88.13 ms/it, loss 0.467898
Finished training it 25600/76743 of epoch 5, 88.02 ms/it, loss 0.469575
Finished training it 25600/76743 of epoch 5, 87.59 ms/it, loss 0.467354
Finished training it 25600/76743 of epoch 5, 87.72 ms/it, loss 0.466624
Finished training it 26624/76743 of epoch 5, 86.78 ms/it, loss 0.467523
Finished training it 26624/76743 of epoch 5, 87.33 ms/it, loss 0.465885
Finished training it 26624/76743 of epoch 5, 87.01 ms/it, loss 0.466301
Finished training it 26624/76743 of epoch 5, 87.00 ms/it, loss 0.466739
Finished training it 27648/76743 of epoch 5, 86.62 ms/it, loss 0.469176
Finished training it 27648/76743 of epoch 5, 86.85 ms/it, loss 0.468891
Finished training it 27648/76743 of epoch 5, 87.06 ms/it, loss 0.466912
Finished training it 27648/76743 of epoch 5, 86.62 ms/it, loss 0.467641
Finished training it 28672/76743 of epoch 5, 88.28 ms/it, loss 0.465422
Finished training it 28672/76743 of epoch 5, 87.69 ms/it, loss 0.468457
Finished training it 28672/76743 of epoch 5, 87.99 ms/it, loss 0.469683
Finished training it 28672/76743 of epoch 5, 87.80 ms/it, loss 0.466467
Finished training it 29696/76743 of epoch 5, 88.67 ms/it, loss 0.466782
Finished training it 29696/76743 of epoch 5, 88.66 ms/it, loss 0.467456
Finished training it 29696/76743 of epoch 5, 88.25 ms/it, loss 0.465562
Finished training it 29696/76743 of epoch 5, 88.32 ms/it, loss 0.466195
Finished training it 30720/76743 of epoch 5, 90.93 ms/it, loss 0.464905
Finished training it 30720/76743 of epoch 5, 90.68 ms/it, loss 0.466697
Finished training it 30720/76743 of epoch 5, 90.88 ms/it, loss 0.461646
Finished training it 30720/76743 of epoch 5, 90.60 ms/it, loss 0.467144
Testing at - 30720/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545891.0
get out
0 has test check 2545891.0 and sample count 3274240
 accuracy 77.755 %, best 77.784 %, roc auc score 0.7771, best 0.7773
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545891.0
get out
1 has test check 2545891.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 90.09 ms/it, loss 0.466097
Finished training it 31744/76743 of epoch 5, 89.99 ms/it, loss 0.468839
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545891.0
get out
3 has test check 2545891.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 89.98 ms/it, loss 0.468892
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545891.0
get out
2 has test check 2545891.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 89.88 ms/it, loss 0.468514
Finished training it 32768/76743 of epoch 5, 90.59 ms/it, loss 0.465935
Finished training it 32768/76743 of epoch 5, 90.48 ms/it, loss 0.467298
Finished training it 32768/76743 of epoch 5, 90.48 ms/it, loss 0.465563
Finished training it 32768/76743 of epoch 5, 90.99 ms/it, loss 0.467516
Finished training it 33792/76743 of epoch 5, 89.74 ms/it, loss 0.468186
Finished training it 33792/76743 of epoch 5, 90.06 ms/it, loss 0.468370
Finished training it 33792/76743 of epoch 5, 89.63 ms/it, loss 0.469345
Finished training it 33792/76743 of epoch 5, 89.78 ms/it, loss 0.466575
Finished training it 34816/76743 of epoch 5, 87.57 ms/it, loss 0.464929
Finished training it 34816/76743 of epoch 5, 87.67 ms/it, loss 0.468001
Finished training it 34816/76743 of epoch 5, 87.08 ms/it, loss 0.465028
Finished training it 34816/76743 of epoch 5, 87.22 ms/it, loss 0.467573
Finished training it 35840/76743 of epoch 5, 87.70 ms/it, loss 0.466872
Finished training it 35840/76743 of epoch 5, 87.34 ms/it, loss 0.466590
Finished training it 35840/76743 of epoch 5, 87.71 ms/it, loss 0.468808
Finished training it 35840/76743 of epoch 5, 87.34 ms/it, loss 0.466355
Finished training it 36864/76743 of epoch 5, 86.81 ms/it, loss 0.467471
Finished training it 36864/76743 of epoch 5, 86.97 ms/it, loss 0.464167
Finished training it 36864/76743 of epoch 5, 87.39 ms/it, loss 0.467442
Finished training it 36864/76743 of epoch 5, 87.32 ms/it, loss 0.468302
Finished training it 37888/76743 of epoch 5, 87.92 ms/it, loss 0.467730
Finished training it 37888/76743 of epoch 5, 88.41 ms/it, loss 0.463146
Finished training it 37888/76743 of epoch 5, 88.09 ms/it, loss 0.466740
Finished training it 37888/76743 of epoch 5, 87.94 ms/it, loss 0.465916
Finished training it 38912/76743 of epoch 5, 87.32 ms/it, loss 0.467821
Finished training it 38912/76743 of epoch 5, 86.98 ms/it, loss 0.468242
Finished training it 38912/76743 of epoch 5, 87.04 ms/it, loss 0.466676
Finished training it 38912/76743 of epoch 5, 87.61 ms/it, loss 0.465914
Finished training it 39936/76743 of epoch 5, 88.01 ms/it, loss 0.466239
Finished training it 39936/76743 of epoch 5, 88.33 ms/it, loss 0.463931
Finished training it 39936/76743 of epoch 5, 88.02 ms/it, loss 0.465040
Finished training it 39936/76743 of epoch 5, 87.75 ms/it, loss 0.468561
Finished training it 40960/76743 of epoch 5, 87.93 ms/it, loss 0.463797
Finished training it 40960/76743 of epoch 5, 87.82 ms/it, loss 0.467621
Finished training it 40960/76743 of epoch 5, 88.19 ms/it, loss 0.463269
Finished training it 40960/76743 of epoch 5, 87.74 ms/it, loss 0.466563
Testing at - 40960/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546951.0
get out
0 has test check 2546951.0 and sample count 3274240
 accuracy 77.788 %, best 77.788 %, roc auc score 0.7773, best 0.7773
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 5, 90.11 ms/it, loss 0.467497
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546951.0
get out
3 has test check 2546951.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 90.23 ms/it, loss 0.465312
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546951.0
get out
1 has test check 2546951.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 90.26 ms/it, loss 0.466473
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546951.0
get out
2 has test check 2546951.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 90.13 ms/it, loss 0.464973
Finished training it 43008/76743 of epoch 5, 90.58 ms/it, loss 0.468297
Finished training it 43008/76743 of epoch 5, 90.54 ms/it, loss 0.469883
Finished training it 43008/76743 of epoch 5, 90.71 ms/it, loss 0.463599
Finished training it 43008/76743 of epoch 5, 90.46 ms/it, loss 0.465469
Finished training it 44032/76743 of epoch 5, 90.20 ms/it, loss 0.465412
Finished training it 44032/76743 of epoch 5, 90.25 ms/it, loss 0.466973
Finished training it 44032/76743 of epoch 5, 90.34 ms/it, loss 0.467277
Finished training it 44032/76743 of epoch 5, 90.08 ms/it, loss 0.467201
Finished training it 45056/76743 of epoch 5, 93.09 ms/it, loss 0.464963
Finished training it 45056/76743 of epoch 5, 92.92 ms/it, loss 0.469480
Finished training it 45056/76743 of epoch 5, 93.49 ms/it, loss 0.467150
Finished training it 45056/76743 of epoch 5, 93.79 ms/it, loss 0.468198
Finished training it 46080/76743 of epoch 5, 93.61 ms/it, loss 0.467503
Finished training it 46080/76743 of epoch 5, 93.43 ms/it, loss 0.465350
Finished training it 46080/76743 of epoch 5, 93.56 ms/it, loss 0.465438
Finished training it 46080/76743 of epoch 5, 94.23 ms/it, loss 0.467555
Finished training it 47104/76743 of epoch 5, 88.11 ms/it, loss 0.464395
Finished training it 47104/76743 of epoch 5, 87.84 ms/it, loss 0.463884
Finished training it 47104/76743 of epoch 5, 87.80 ms/it, loss 0.463766
Finished training it 47104/76743 of epoch 5, 88.18 ms/it, loss 0.464288
Finished training it 48128/76743 of epoch 5, 87.16 ms/it, loss 0.464888
Finished training it 48128/76743 of epoch 5, 87.82 ms/it, loss 0.464819
Finished training it 48128/76743 of epoch 5, 87.63 ms/it, loss 0.464307
Finished training it 48128/76743 of epoch 5, 87.11 ms/it, loss 0.467890
Finished training it 49152/76743 of epoch 5, 87.22 ms/it, loss 0.465388
Finished training it 49152/76743 of epoch 5, 87.44 ms/it, loss 0.466493
Finished training it 49152/76743 of epoch 5, 86.84 ms/it, loss 0.465055
Finished training it 49152/76743 of epoch 5, 87.11 ms/it, loss 0.464912
Finished training it 50176/76743 of epoch 5, 87.62 ms/it, loss 0.468200
Finished training it 50176/76743 of epoch 5, 87.84 ms/it, loss 0.468042
Finished training it 50176/76743 of epoch 5, 87.49 ms/it, loss 0.465805
Finished training it 50176/76743 of epoch 5, 87.42 ms/it, loss 0.465411
Finished training it 51200/76743 of epoch 5, 88.87 ms/it, loss 0.464654
Finished training it 51200/76743 of epoch 5, 88.68 ms/it, loss 0.467085
Finished training it 51200/76743 of epoch 5, 88.23 ms/it, loss 0.466069
Finished training it 51200/76743 of epoch 5, 88.32 ms/it, loss 0.469311
Testing at - 51200/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547573.0
get out
0 has test check 2547573.0 and sample count 3274240
 accuracy 77.807 %, best 77.807 %, roc auc score 0.7775, best 0.7775
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547573.0
get out
1 has test check 2547573.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.26 ms/it, loss 0.467104
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547573.0
get out
2 has test check 2547573.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.21 ms/it, loss 0.464764
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 5, 90.17 ms/it, loss 0.463589
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547573.0
get out
3 has test check 2547573.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.17 ms/it, loss 0.465995
Finished training it 53248/76743 of epoch 5, 90.61 ms/it, loss 0.466330
Finished training it 53248/76743 of epoch 5, 90.71 ms/it, loss 0.466939
Finished training it 53248/76743 of epoch 5, 90.68 ms/it, loss 0.466252
Finished training it 53248/76743 of epoch 5, 90.87 ms/it, loss 0.465325
Finished training it 54272/76743 of epoch 5, 88.91 ms/it, loss 0.464285
Finished training it 54272/76743 of epoch 5, 89.02 ms/it, loss 0.466441
Finished training it 54272/76743 of epoch 5, 88.90 ms/it, loss 0.465368
Finished training it 54272/76743 of epoch 5, 89.15 ms/it, loss 0.468098
Finished training it 55296/76743 of epoch 5, 87.56 ms/it, loss 0.464825
Finished training it 55296/76743 of epoch 5, 87.39 ms/it, loss 0.468909
Finished training it 55296/76743 of epoch 5, 87.87 ms/it, loss 0.463761
Finished training it 55296/76743 of epoch 5, 87.18 ms/it, loss 0.465379
Finished training it 56320/76743 of epoch 5, 87.43 ms/it, loss 0.463498
Finished training it 56320/76743 of epoch 5, 88.07 ms/it, loss 0.462842
Finished training it 56320/76743 of epoch 5, 87.75 ms/it, loss 0.466999
Finished training it 56320/76743 of epoch 5, 87.40 ms/it, loss 0.466228
Finished training it 57344/76743 of epoch 5, 87.82 ms/it, loss 0.464822
Finished training it 57344/76743 of epoch 5, 87.81 ms/it, loss 0.467067
Finished training it 57344/76743 of epoch 5, 88.34 ms/it, loss 0.463944
Finished training it 57344/76743 of epoch 5, 88.60 ms/it, loss 0.470391
Finished training it 58368/76743 of epoch 5, 87.57 ms/it, loss 0.465382
Finished training it 58368/76743 of epoch 5, 87.06 ms/it, loss 0.467823
Finished training it 58368/76743 of epoch 5, 86.92 ms/it, loss 0.466120
Finished training it 58368/76743 of epoch 5, 87.43 ms/it, loss 0.468143
Finished training it 59392/76743 of epoch 5, 88.05 ms/it, loss 0.465869
Finished training it 59392/76743 of epoch 5, 88.24 ms/it, loss 0.467150
Finished training it 59392/76743 of epoch 5, 87.85 ms/it, loss 0.465180
Finished training it 59392/76743 of epoch 5, 87.82 ms/it, loss 0.464717
Finished training it 60416/76743 of epoch 5, 87.38 ms/it, loss 0.465155
Finished training it 60416/76743 of epoch 5, 87.02 ms/it, loss 0.463207
Finished training it 60416/76743 of epoch 5, 87.03 ms/it, loss 0.465059
Finished training it 60416/76743 of epoch 5, 87.63 ms/it, loss 0.464019
Finished training it 61440/76743 of epoch 5, 87.91 ms/it, loss 0.467537
Finished training it 61440/76743 of epoch 5, 87.50 ms/it, loss 0.464777
Finished training it 61440/76743 of epoch 5, 88.16 ms/it, loss 0.465712
Finished training it 61440/76743 of epoch 5, 87.53 ms/it, loss 0.467076
Testing at - 61440/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547907.0
get out
0 has test check 2547907.0 and sample count 3274240
 accuracy 77.817 %, best 77.817 %, roc auc score 0.7777, best 0.7777
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547907.0
get out
2 has test check 2547907.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.15 ms/it, loss 0.464179
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 5, 90.24 ms/it, loss 0.467106
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547907.0
get out
3 has test check 2547907.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.16 ms/it, loss 0.465591
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547907.0
get out
1 has test check 2547907.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.38 ms/it, loss 0.466737
Finished training it 63488/76743 of epoch 5, 90.19 ms/it, loss 0.465810
Finished training it 63488/76743 of epoch 5, 90.45 ms/it, loss 0.464735
Finished training it 63488/76743 of epoch 5, 90.21 ms/it, loss 0.464210
Finished training it 63488/76743 of epoch 5, 90.33 ms/it, loss 0.467544
Finished training it 64512/76743 of epoch 5, 89.88 ms/it, loss 0.466861
Finished training it 64512/76743 of epoch 5, 90.17 ms/it, loss 0.464236
Finished training it 64512/76743 of epoch 5, 89.93 ms/it, loss 0.464063
Finished training it 64512/76743 of epoch 5, 89.73 ms/it, loss 0.463426
Finished training it 65536/76743 of epoch 5, 98.32 ms/it, loss 0.466382
Finished training it 65536/76743 of epoch 5, 97.74 ms/it, loss 0.465678
Finished training it 65536/76743 of epoch 5, 99.15 ms/it, loss 0.466280
Finished training it 65536/76743 of epoch 5, 98.64 ms/it, loss 0.468405
Finished training it 66560/76743 of epoch 5, 88.68 ms/it, loss 0.465686
Finished training it 66560/76743 of epoch 5, 88.05 ms/it, loss 0.466537
Finished training it 66560/76743 of epoch 5, 88.46 ms/it, loss 0.465203
Finished training it 66560/76743 of epoch 5, 88.23 ms/it, loss 0.466404
Finished training it 67584/76743 of epoch 5, 87.91 ms/it, loss 0.466735
Finished training it 67584/76743 of epoch 5, 88.19 ms/it, loss 0.466237
Finished training it 67584/76743 of epoch 5, 87.79 ms/it, loss 0.465237
Finished training it 67584/76743 of epoch 5, 87.53 ms/it, loss 0.469281
Finished training it 68608/76743 of epoch 5, 87.83 ms/it, loss 0.464725
Finished training it 68608/76743 of epoch 5, 87.16 ms/it, loss 0.465486
Finished training it 68608/76743 of epoch 5, 87.50 ms/it, loss 0.466060
Finished training it 68608/76743 of epoch 5, 87.19 ms/it, loss 0.466581
Finished training it 69632/76743 of epoch 5, 87.53 ms/it, loss 0.467090
Finished training it 69632/76743 of epoch 5, 87.38 ms/it, loss 0.467342
Finished training it 69632/76743 of epoch 5, 87.88 ms/it, loss 0.464071
Finished training it 69632/76743 of epoch 5, 87.87 ms/it, loss 0.466317
Finished training it 70656/76743 of epoch 5, 87.55 ms/it, loss 0.466865
Finished training it 70656/76743 of epoch 5, 87.47 ms/it, loss 0.466240
Finished training it 70656/76743 of epoch 5, 87.51 ms/it, loss 0.464062
Finished training it 70656/76743 of epoch 5, 88.03 ms/it, loss 0.465780
Finished training it 71680/76743 of epoch 5, 87.92 ms/it, loss 0.462510
Finished training it 71680/76743 of epoch 5, 87.47 ms/it, loss 0.467358
Finished training it 71680/76743 of epoch 5, 87.77 ms/it, loss 0.463484
Finished training it 71680/76743 of epoch 5, 87.50 ms/it, loss 0.466176
Testing at - 71680/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548947.0
get out
0 has test check 2548947.0 and sample count 3274240
 accuracy 77.849 %, best 77.849 %, roc auc score 0.7786, best 0.7786
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548947.0
get out
3 has test check 2548947.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.41 ms/it, loss 0.464904
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 5, 90.47 ms/it, loss 0.467711
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548947.0
get out
2 has test check 2548947.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.32 ms/it, loss 0.466684
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548947.0
get out
1 has test check 2548947.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.71 ms/it, loss 0.466314
Finished training it 73728/76743 of epoch 5, 90.42 ms/it, loss 0.468488
Finished training it 73728/76743 of epoch 5, 90.79 ms/it, loss 0.465234
Finished training it 73728/76743 of epoch 5, 90.45 ms/it, loss 0.464861
Finished training it 73728/76743 of epoch 5, 90.42 ms/it, loss 0.466226
Finished training it 74752/76743 of epoch 5, 89.86 ms/it, loss 0.465845
Finished training it 74752/76743 of epoch 5, 90.39 ms/it, loss 0.466764
Finished training it 74752/76743 of epoch 5, 89.75 ms/it, loss 0.466316
Finished training it 74752/76743 of epoch 5, 89.78 ms/it, loss 0.466376
Finished training it 75776/76743 of epoch 5, 87.42 ms/it, loss 0.467660
Finished training it 75776/76743 of epoch 5, 87.77 ms/it, loss 0.466595
Finished training it 75776/76743 of epoch 5, 87.28 ms/it, loss 0.465138
Finished training it 75776/76743 of epoch 5, 87.35 ms/it, loss 0.467067
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 89.51 ms/it, loss 0.467538
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 88.64 ms/it, loss 0.462822
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 88.81 ms/it, loss 0.467107
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 88.68 ms/it, loss 0.466299
Finished training it 2048/76743 of epoch 6, 87.69 ms/it, loss 0.465627
Finished training it 2048/76743 of epoch 6, 88.07 ms/it, loss 0.465811
Finished training it 2048/76743 of epoch 6, 87.67 ms/it, loss 0.467348
Finished training it 2048/76743 of epoch 6, 88.30 ms/it, loss 0.463508
Finished training it 3072/76743 of epoch 6, 87.72 ms/it, loss 0.462874
Finished training it 3072/76743 of epoch 6, 87.60 ms/it, loss 0.466158
Finished training it 3072/76743 of epoch 6, 87.78 ms/it, loss 0.463517
Finished training it 3072/76743 of epoch 6, 88.14 ms/it, loss 0.463548
Finished training it 4096/76743 of epoch 6, 87.92 ms/it, loss 0.464842
Finished training it 4096/76743 of epoch 6, 87.62 ms/it, loss 0.466322
Finished training it 4096/76743 of epoch 6, 87.50 ms/it, loss 0.467658
Finished training it 4096/76743 of epoch 6, 87.52 ms/it, loss 0.465758
Finished training it 5120/76743 of epoch 6, 87.37 ms/it, loss 0.465422
Finished training it 5120/76743 of epoch 6, 87.51 ms/it, loss 0.467482
Finished training it 5120/76743 of epoch 6, 88.01 ms/it, loss 0.465440
Finished training it 5120/76743 of epoch 6, 87.71 ms/it, loss 0.463232
Finished training it 6144/76743 of epoch 6, 88.32 ms/it, loss 0.464819
Finished training it 6144/76743 of epoch 6, 88.23 ms/it, loss 0.467543
Finished training it 6144/76743 of epoch 6, 88.07 ms/it, loss 0.464181
Finished training it 6144/76743 of epoch 6, 88.13 ms/it, loss 0.464402
Finished training it 7168/76743 of epoch 6, 87.45 ms/it, loss 0.465970
Finished training it 7168/76743 of epoch 6, 87.65 ms/it, loss 0.466998
Finished training it 7168/76743 of epoch 6, 87.51 ms/it, loss 0.465922
Finished training it 7168/76743 of epoch 6, 87.99 ms/it, loss 0.468490
Finished training it 8192/76743 of epoch 6, 87.17 ms/it, loss 0.466707
Finished training it 8192/76743 of epoch 6, 87.55 ms/it, loss 0.464622
Finished training it 8192/76743 of epoch 6, 87.28 ms/it, loss 0.465507
Finished training it 8192/76743 of epoch 6, 87.68 ms/it, loss 0.467026
Finished training it 9216/76743 of epoch 6, 88.17 ms/it, loss 0.467188
Finished training it 9216/76743 of epoch 6, 88.36 ms/it, loss 0.465662
Finished training it 9216/76743 of epoch 6, 88.27 ms/it, loss 0.468184
Finished training it 9216/76743 of epoch 6, 88.57 ms/it, loss 0.466655
Finished training it 10240/76743 of epoch 6, 96.55 ms/it, loss 0.466193
Finished training it 10240/76743 of epoch 6, 97.03 ms/it, loss 0.466442
Finished training it 10240/76743 of epoch 6, 97.31 ms/it, loss 0.465576
Finished training it 10240/76743 of epoch 6, 96.33 ms/it, loss 0.463967
Testing at - 10240/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548523.0
get out
0 has test check 2548523.0 and sample count 3274240
 accuracy 77.836 %, best 77.849 %, roc auc score 0.7782, best 0.7786
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548523.0
get out
1 has test check 2548523.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 87.88 ms/it, loss 0.464716
Finished training it 11264/76743 of epoch 6, 87.81 ms/it, loss 0.466144
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548523.0
get out
3 has test check 2548523.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 87.27 ms/it, loss 0.463059
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548523.0
get out
2 has test check 2548523.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 87.31 ms/it, loss 0.464481
Finished training it 12288/76743 of epoch 6, 88.01 ms/it, loss 0.467347
Finished training it 12288/76743 of epoch 6, 88.28 ms/it, loss 0.466958
Finished training it 12288/76743 of epoch 6, 88.13 ms/it, loss 0.466735
Finished training it 12288/76743 of epoch 6, 87.86 ms/it, loss 0.463874
Finished training it 13312/76743 of epoch 6, 93.87 ms/it, loss 0.465622
Finished training it 13312/76743 of epoch 6, 93.73 ms/it, loss 0.467597
Finished training it 13312/76743 of epoch 6, 93.53 ms/it, loss 0.467122
Finished training it 13312/76743 of epoch 6, 93.80 ms/it, loss 0.465658
Finished training it 14336/76743 of epoch 6, 92.47 ms/it, loss 0.467044
Finished training it 14336/76743 of epoch 6, 92.17 ms/it, loss 0.463152
Finished training it 14336/76743 of epoch 6, 92.68 ms/it, loss 0.464449
Finished training it 14336/76743 of epoch 6, 92.16 ms/it, loss 0.465751
Finished training it 15360/76743 of epoch 6, 88.14 ms/it, loss 0.465529
Finished training it 15360/76743 of epoch 6, 88.36 ms/it, loss 0.466816
Finished training it 15360/76743 of epoch 6, 87.96 ms/it, loss 0.464751
Finished training it 15360/76743 of epoch 6, 87.89 ms/it, loss 0.463798
Finished training it 16384/76743 of epoch 6, 87.63 ms/it, loss 0.467856
Finished training it 16384/76743 of epoch 6, 87.85 ms/it, loss 0.463868
Finished training it 16384/76743 of epoch 6, 87.58 ms/it, loss 0.464198
Finished training it 16384/76743 of epoch 6, 88.07 ms/it, loss 0.465628
Finished training it 17408/76743 of epoch 6, 88.16 ms/it, loss 0.462602
Finished training it 17408/76743 of epoch 6, 87.67 ms/it, loss 0.466242
Finished training it 17408/76743 of epoch 6, 88.28 ms/it, loss 0.467579
Finished training it 17408/76743 of epoch 6, 87.65 ms/it, loss 0.464567
Finished training it 18432/76743 of epoch 6, 87.98 ms/it, loss 0.466750
Finished training it 18432/76743 of epoch 6, 87.42 ms/it, loss 0.466345
Finished training it 18432/76743 of epoch 6, 87.62 ms/it, loss 0.465801
Finished training it 18432/76743 of epoch 6, 87.43 ms/it, loss 0.466181
Finished training it 19456/76743 of epoch 6, 88.39 ms/it, loss 0.466187
Finished training it 19456/76743 of epoch 6, 88.14 ms/it, loss 0.466489
Finished training it 19456/76743 of epoch 6, 88.46 ms/it, loss 0.466107
Finished training it 19456/76743 of epoch 6, 88.06 ms/it, loss 0.467694
Finished training it 20480/76743 of epoch 6, 90.15 ms/it, loss 0.468047
Finished training it 20480/76743 of epoch 6, 90.49 ms/it, loss 0.467119
Finished training it 20480/76743 of epoch 6, 90.27 ms/it, loss 0.466848
Finished training it 20480/76743 of epoch 6, 90.18 ms/it, loss 0.468077
Testing at - 20480/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549311.0
get out
0 has test check 2549311.0 and sample count 3274240
 accuracy 77.860 %, best 77.860 %, roc auc score 0.7788, best 0.7788
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549311.0
get out
1 has test check 2549311.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.17 ms/it, loss 0.466640
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549311.0
get out
3 has test check 2549311.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.26 ms/it, loss 0.464302
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 6, 88.32 ms/it, loss 0.464405
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549311.0
get out
2 has test check 2549311.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.19 ms/it, loss 0.463087
Finished training it 22528/76743 of epoch 6, 87.70 ms/it, loss 0.462928
Finished training it 22528/76743 of epoch 6, 88.14 ms/it, loss 0.466388
Finished training it 22528/76743 of epoch 6, 87.45 ms/it, loss 0.466143
Finished training it 22528/76743 of epoch 6, 87.91 ms/it, loss 0.464393
Finished training it 23552/76743 of epoch 6, 87.55 ms/it, loss 0.465178
Finished training it 23552/76743 of epoch 6, 87.63 ms/it, loss 0.464584
Finished training it 23552/76743 of epoch 6, 88.15 ms/it, loss 0.461912
Finished training it 23552/76743 of epoch 6, 87.72 ms/it, loss 0.464082
Finished training it 24576/76743 of epoch 6, 88.01 ms/it, loss 0.469655
Finished training it 24576/76743 of epoch 6, 87.60 ms/it, loss 0.464841
Finished training it 24576/76743 of epoch 6, 87.68 ms/it, loss 0.465718
Finished training it 24576/76743 of epoch 6, 88.09 ms/it, loss 0.464151
Finished training it 25600/76743 of epoch 6, 87.53 ms/it, loss 0.468069
Finished training it 25600/76743 of epoch 6, 88.02 ms/it, loss 0.466127
Finished training it 25600/76743 of epoch 6, 87.30 ms/it, loss 0.465258
Finished training it 25600/76743 of epoch 6, 87.39 ms/it, loss 0.465768
Finished training it 26624/76743 of epoch 6, 88.06 ms/it, loss 0.464527
Finished training it 26624/76743 of epoch 6, 88.10 ms/it, loss 0.465393
Finished training it 26624/76743 of epoch 6, 88.06 ms/it, loss 0.465843
Finished training it 26624/76743 of epoch 6, 88.27 ms/it, loss 0.464126
Finished training it 27648/76743 of epoch 6, 87.52 ms/it, loss 0.467662
Finished training it 27648/76743 of epoch 6, 87.55 ms/it, loss 0.467094
Finished training it 27648/76743 of epoch 6, 87.45 ms/it, loss 0.466024
Finished training it 27648/76743 of epoch 6, 87.78 ms/it, loss 0.465420
Finished training it 28672/76743 of epoch 6, 87.63 ms/it, loss 0.466918
Finished training it 28672/76743 of epoch 6, 87.45 ms/it, loss 0.465080
Finished training it 28672/76743 of epoch 6, 87.63 ms/it, loss 0.468270
Finished training it 28672/76743 of epoch 6, 87.76 ms/it, loss 0.463817
Finished training it 29696/76743 of epoch 6, 88.64 ms/it, loss 0.465194
Finished training it 29696/76743 of epoch 6, 87.91 ms/it, loss 0.463989
Finished training it 29696/76743 of epoch 6, 88.22 ms/it, loss 0.465804
Finished training it 29696/76743 of epoch 6, 88.19 ms/it, loss 0.464103
Finished training it 30720/76743 of epoch 6, 90.44 ms/it, loss 0.465671
Finished training it 30720/76743 of epoch 6, 90.28 ms/it, loss 0.464913
Finished training it 30720/76743 of epoch 6, 90.33 ms/it, loss 0.460080
Finished training it 30720/76743 of epoch 6, 90.54 ms/it, loss 0.463350
Testing at - 30720/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547051.0
get out
0 has test check 2547051.0 and sample count 3274240
 accuracy 77.791 %, best 77.860 %, roc auc score 0.7788, best 0.7788
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547051.0
get out
3 has test check 2547051.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 90.12 ms/it, loss 0.467576
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547051.0
get out
1 has test check 2547051.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 90.16 ms/it, loss 0.464821
Finished training it 31744/76743 of epoch 6, 89.84 ms/it, loss 0.467264
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547051.0
get out
2 has test check 2547051.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 89.97 ms/it, loss 0.466916
Finished training it 32768/76743 of epoch 6, 90.69 ms/it, loss 0.466164
Finished training it 32768/76743 of epoch 6, 90.53 ms/it, loss 0.465843
Finished training it 32768/76743 of epoch 6, 90.53 ms/it, loss 0.464919
Finished training it 32768/76743 of epoch 6, 90.47 ms/it, loss 0.464071
Finished training it 33792/76743 of epoch 6, 90.12 ms/it, loss 0.465154
Finished training it 33792/76743 of epoch 6, 90.18 ms/it, loss 0.466495
Finished training it 33792/76743 of epoch 6, 90.35 ms/it, loss 0.466771
Finished training it 33792/76743 of epoch 6, 90.05 ms/it, loss 0.467848
Finished training it 34816/76743 of epoch 6, 94.17 ms/it, loss 0.463803
Finished training it 34816/76743 of epoch 6, 93.94 ms/it, loss 0.463398
Finished training it 34816/76743 of epoch 6, 94.89 ms/it, loss 0.466436
Finished training it 34816/76743 of epoch 6, 94.19 ms/it, loss 0.466298
Finished training it 35840/76743 of epoch 6, 92.16 ms/it, loss 0.465084
Finished training it 35840/76743 of epoch 6, 92.80 ms/it, loss 0.467436
Finished training it 35840/76743 of epoch 6, 92.36 ms/it, loss 0.464966
Finished training it 35840/76743 of epoch 6, 93.16 ms/it, loss 0.465113
Finished training it 36864/76743 of epoch 6, 87.28 ms/it, loss 0.462987
Finished training it 36864/76743 of epoch 6, 87.18 ms/it, loss 0.466042
Finished training it 36864/76743 of epoch 6, 87.77 ms/it, loss 0.466701
Finished training it 36864/76743 of epoch 6, 88.10 ms/it, loss 0.465927
Finished training it 37888/76743 of epoch 6, 87.12 ms/it, loss 0.464260
Finished training it 37888/76743 of epoch 6, 87.17 ms/it, loss 0.465800
Finished training it 37888/76743 of epoch 6, 87.78 ms/it, loss 0.461726
Finished training it 37888/76743 of epoch 6, 87.26 ms/it, loss 0.465046
Finished training it 38912/76743 of epoch 6, 87.10 ms/it, loss 0.466143
Finished training it 38912/76743 of epoch 6, 87.57 ms/it, loss 0.464454
Finished training it 38912/76743 of epoch 6, 87.08 ms/it, loss 0.466656
Finished training it 38912/76743 of epoch 6, 87.17 ms/it, loss 0.465687
Finished training it 39936/76743 of epoch 6, 88.10 ms/it, loss 0.463316
Finished training it 39936/76743 of epoch 6, 87.67 ms/it, loss 0.464814
Finished training it 39936/76743 of epoch 6, 87.92 ms/it, loss 0.463730
Finished training it 39936/76743 of epoch 6, 87.73 ms/it, loss 0.466998
Finished training it 40960/76743 of epoch 6, 87.99 ms/it, loss 0.462996
Finished training it 40960/76743 of epoch 6, 88.39 ms/it, loss 0.461709
Finished training it 40960/76743 of epoch 6, 87.79 ms/it, loss 0.466272
Finished training it 40960/76743 of epoch 6, 87.86 ms/it, loss 0.465092
Testing at - 40960/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549667.0
get out
0 has test check 2549667.0 and sample count 3274240
 accuracy 77.870 %, best 77.870 %, roc auc score 0.7787, best 0.7788
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 6, 90.51 ms/it, loss 0.465859
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549667.0
get out
1 has test check 2549667.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 90.49 ms/it, loss 0.464593
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549667.0
get out
3 has test check 2549667.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 90.51 ms/it, loss 0.463753
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549667.0
get out
2 has test check 2549667.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 90.54 ms/it, loss 0.464108
Finished training it 43008/76743 of epoch 6, 90.42 ms/it, loss 0.468629
Finished training it 43008/76743 of epoch 6, 90.63 ms/it, loss 0.462165
Finished training it 43008/76743 of epoch 6, 90.40 ms/it, loss 0.466619
Finished training it 43008/76743 of epoch 6, 90.37 ms/it, loss 0.464398
Finished training it 44032/76743 of epoch 6, 90.82 ms/it, loss 0.464492
Finished training it 44032/76743 of epoch 6, 90.70 ms/it, loss 0.465503
Finished training it 44032/76743 of epoch 6, 90.58 ms/it, loss 0.465834
Finished training it 44032/76743 of epoch 6, 90.87 ms/it, loss 0.466047
Finished training it 45056/76743 of epoch 6, 88.08 ms/it, loss 0.467637
Finished training it 45056/76743 of epoch 6, 88.08 ms/it, loss 0.466540
Finished training it 45056/76743 of epoch 6, 87.86 ms/it, loss 0.463898
Finished training it 45056/76743 of epoch 6, 88.18 ms/it, loss 0.465631
Finished training it 46080/76743 of epoch 6, 87.54 ms/it, loss 0.466116
Finished training it 46080/76743 of epoch 6, 87.39 ms/it, loss 0.464427
Finished training it 46080/76743 of epoch 6, 87.91 ms/it, loss 0.466176
Finished training it 46080/76743 of epoch 6, 87.43 ms/it, loss 0.464195
Finished training it 47104/76743 of epoch 6, 87.94 ms/it, loss 0.462938
Finished training it 47104/76743 of epoch 6, 87.63 ms/it, loss 0.463171
Finished training it 47104/76743 of epoch 6, 87.27 ms/it, loss 0.462506
Finished training it 47104/76743 of epoch 6, 87.45 ms/it, loss 0.462677
Finished training it 48128/76743 of epoch 6, 87.73 ms/it, loss 0.463000
Finished training it 48128/76743 of epoch 6, 87.48 ms/it, loss 0.463053
Finished training it 48128/76743 of epoch 6, 87.47 ms/it, loss 0.466806
Finished training it 48128/76743 of epoch 6, 87.80 ms/it, loss 0.463453
Finished training it 49152/76743 of epoch 6, 87.16 ms/it, loss 0.464043
Finished training it 49152/76743 of epoch 6, 87.58 ms/it, loss 0.465466
Finished training it 49152/76743 of epoch 6, 87.09 ms/it, loss 0.463501
Finished training it 49152/76743 of epoch 6, 87.10 ms/it, loss 0.463164
Finished training it 50176/76743 of epoch 6, 87.49 ms/it, loss 0.464160
Finished training it 50176/76743 of epoch 6, 87.99 ms/it, loss 0.466966
Finished training it 50176/76743 of epoch 6, 87.65 ms/it, loss 0.466794
Finished training it 50176/76743 of epoch 6, 87.27 ms/it, loss 0.464045
Finished training it 51200/76743 of epoch 6, 87.81 ms/it, loss 0.465739
Finished training it 51200/76743 of epoch 6, 88.08 ms/it, loss 0.462873
Finished training it 51200/76743 of epoch 6, 87.37 ms/it, loss 0.464672
Finished training it 51200/76743 of epoch 6, 87.39 ms/it, loss 0.467866
Testing at - 51200/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549148.0
get out
0 has test check 2549148.0 and sample count 3274240
 accuracy 77.855 %, best 77.870 %, roc auc score 0.7792, best 0.7792
Finished training it 52224/76743 of epoch 6, 90.61 ms/it, loss 0.462210
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549148.0
get out
2 has test check 2549148.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 90.53 ms/it, loss 0.463304
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549148.0
get out
3 has test check 2549148.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 90.72 ms/it, loss 0.464686
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549148.0
get out
1 has test check 2549148.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 90.81 ms/it, loss 0.465701
Finished training it 53248/76743 of epoch 6, 90.09 ms/it, loss 0.464255
Finished training it 53248/76743 of epoch 6, 89.91 ms/it, loss 0.465179
Finished training it 53248/76743 of epoch 6, 89.82 ms/it, loss 0.465093
Finished training it 53248/76743 of epoch 6, 90.07 ms/it, loss 0.464941
Finished training it 54272/76743 of epoch 6, 90.25 ms/it, loss 0.463347
Finished training it 54272/76743 of epoch 6, 90.58 ms/it, loss 0.467285
Finished training it 54272/76743 of epoch 6, 90.40 ms/it, loss 0.465508
Finished training it 54272/76743 of epoch 6, 90.29 ms/it, loss 0.464366
Finished training it 55296/76743 of epoch 6, 100.20 ms/it, loss 0.464102
Finished training it 55296/76743 of epoch 6, 100.52 ms/it, loss 0.463501
Finished training it 55296/76743 of epoch 6, 100.51 ms/it, loss 0.467360
Finished training it 55296/76743 of epoch 6, 100.47 ms/it, loss 0.462326
Finished training it 56320/76743 of epoch 6, 87.92 ms/it, loss 0.461575
Finished training it 56320/76743 of epoch 6, 87.85 ms/it, loss 0.465787
Finished training it 56320/76743 of epoch 6, 87.47 ms/it, loss 0.464806
Finished training it 56320/76743 of epoch 6, 87.80 ms/it, loss 0.462396
Finished training it 57344/76743 of epoch 6, 87.57 ms/it, loss 0.463567
Finished training it 57344/76743 of epoch 6, 87.80 ms/it, loss 0.462463
Finished training it 57344/76743 of epoch 6, 87.94 ms/it, loss 0.469062
Finished training it 57344/76743 of epoch 6, 87.82 ms/it, loss 0.466198
Finished training it 58368/76743 of epoch 6, 87.75 ms/it, loss 0.464169
Finished training it 58368/76743 of epoch 6, 87.49 ms/it, loss 0.466593
Finished training it 58368/76743 of epoch 6, 87.25 ms/it, loss 0.464561
Finished training it 58368/76743 of epoch 6, 87.31 ms/it, loss 0.466316
Finished training it 59392/76743 of epoch 6, 87.99 ms/it, loss 0.463630
Finished training it 59392/76743 of epoch 6, 88.47 ms/it, loss 0.466379
Finished training it 59392/76743 of epoch 6, 87.99 ms/it, loss 0.463384
Finished training it 59392/76743 of epoch 6, 88.10 ms/it, loss 0.464540
Finished training it 60416/76743 of epoch 6, 88.17 ms/it, loss 0.462952
Finished training it 60416/76743 of epoch 6, 87.66 ms/it, loss 0.461706
Finished training it 60416/76743 of epoch 6, 88.00 ms/it, loss 0.463591
Finished training it 60416/76743 of epoch 6, 87.75 ms/it, loss 0.463564
Finished training it 61440/76743 of epoch 6, 87.67 ms/it, loss 0.463124
Finished training it 61440/76743 of epoch 6, 88.21 ms/it, loss 0.464190
Finished training it 61440/76743 of epoch 6, 87.61 ms/it, loss 0.466122
Finished training it 61440/76743 of epoch 6, 87.92 ms/it, loss 0.466202
Testing at - 61440/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550128.0
get out
0 has test check 2550128.0 and sample count 3274240
 accuracy 77.885 %, best 77.885 %, roc auc score 0.7791, best 0.7792
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550128.0
get out
1 has test check 2550128.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 90.41 ms/it, loss 0.465378
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550128.0
get out
2 has test check 2550128.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 90.33 ms/it, loss 0.462843
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550128.0
get out
3 has test check 2550128.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 90.20 ms/it, loss 0.464432
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 6, 90.50 ms/it, loss 0.465612
Finished training it 63488/76743 of epoch 6, 90.33 ms/it, loss 0.464397
Finished training it 63488/76743 of epoch 6, 90.41 ms/it, loss 0.466406
Finished training it 63488/76743 of epoch 6, 90.57 ms/it, loss 0.463152
Finished training it 63488/76743 of epoch 6, 90.29 ms/it, loss 0.462913
Finished training it 64512/76743 of epoch 6, 90.57 ms/it, loss 0.462105
Finished training it 64512/76743 of epoch 6, 90.66 ms/it, loss 0.463441
Finished training it 64512/76743 of epoch 6, 90.66 ms/it, loss 0.466072
Finished training it 64512/76743 of epoch 6, 90.51 ms/it, loss 0.462791
Finished training it 65536/76743 of epoch 6, 89.68 ms/it, loss 0.464833
Finished training it 65536/76743 of epoch 6, 89.06 ms/it, loss 0.465120
Finished training it 65536/76743 of epoch 6, 89.33 ms/it, loss 0.467399
Finished training it 65536/76743 of epoch 6, 88.99 ms/it, loss 0.464786
Finished training it 66560/76743 of epoch 6, 87.73 ms/it, loss 0.465302
Finished training it 66560/76743 of epoch 6, 88.12 ms/it, loss 0.464713
Finished training it 66560/76743 of epoch 6, 87.86 ms/it, loss 0.463854
Finished training it 66560/76743 of epoch 6, 87.76 ms/it, loss 0.465215
Finished training it 67584/76743 of epoch 6, 88.55 ms/it, loss 0.465132
Finished training it 67584/76743 of epoch 6, 88.20 ms/it, loss 0.465220
Finished training it 67584/76743 of epoch 6, 88.16 ms/it, loss 0.463874
Finished training it 67584/76743 of epoch 6, 88.05 ms/it, loss 0.467866
Finished training it 68608/76743 of epoch 6, 87.15 ms/it, loss 0.465184
Finished training it 68608/76743 of epoch 6, 87.82 ms/it, loss 0.463485
Finished training it 68608/76743 of epoch 6, 87.60 ms/it, loss 0.464236
Finished training it 68608/76743 of epoch 6, 87.21 ms/it, loss 0.463968
Finished training it 69632/76743 of epoch 6, 87.26 ms/it, loss 0.466382
Finished training it 69632/76743 of epoch 6, 87.17 ms/it, loss 0.465885
Finished training it 69632/76743 of epoch 6, 87.72 ms/it, loss 0.464770
Finished training it 69632/76743 of epoch 6, 87.66 ms/it, loss 0.462822
Finished training it 70656/76743 of epoch 6, 87.77 ms/it, loss 0.464342
Finished training it 70656/76743 of epoch 6, 87.63 ms/it, loss 0.465560
Finished training it 70656/76743 of epoch 6, 87.37 ms/it, loss 0.464590
Finished training it 70656/76743 of epoch 6, 87.30 ms/it, loss 0.462819
Finished training it 71680/76743 of epoch 6, 87.38 ms/it, loss 0.461891
Finished training it 71680/76743 of epoch 6, 87.03 ms/it, loss 0.461374
Finished training it 71680/76743 of epoch 6, 86.97 ms/it, loss 0.466000
Finished training it 71680/76743 of epoch 6, 86.89 ms/it, loss 0.465004
Testing at - 71680/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551317.0
get out
0 has test check 2551317.0 and sample count 3274240
 accuracy 77.921 %, best 77.921 %, roc auc score 0.7798, best 0.7798
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551317.0
get out
3 has test check 2551317.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 90.73 ms/it, loss 0.463542
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 6, 90.92 ms/it, loss 0.466337
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551317.0
get out
1 has test check 2551317.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 90.89 ms/it, loss 0.464918
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551317.0
get out
2 has test check 2551317.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 90.67 ms/it, loss 0.464932
Finished training it 73728/76743 of epoch 6, 90.33 ms/it, loss 0.463567
Finished training it 73728/76743 of epoch 6, 90.41 ms/it, loss 0.466925
Finished training it 73728/76743 of epoch 6, 90.14 ms/it, loss 0.465397
Finished training it 73728/76743 of epoch 6, 90.44 ms/it, loss 0.463471
Finished training it 74752/76743 of epoch 6, 90.35 ms/it, loss 0.464389
Finished training it 74752/76743 of epoch 6, 90.54 ms/it, loss 0.465565
Finished training it 74752/76743 of epoch 6, 90.29 ms/it, loss 0.465109
Finished training it 74752/76743 of epoch 6, 90.26 ms/it, loss 0.464720
Finished training it 75776/76743 of epoch 6, 94.50 ms/it, loss 0.464997
Finished training it 75776/76743 of epoch 6, 94.24 ms/it, loss 0.463800
Finished training it 75776/76743 of epoch 6, 94.96 ms/it, loss 0.466218
Finished training it 75776/76743 of epoch 6, 94.30 ms/it, loss 0.465505
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 91.38 ms/it, loss 0.461473
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 91.43 ms/it, loss 0.465091
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 91.47 ms/it, loss 0.465449
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 92.35 ms/it, loss 0.466300
Finished training it 2048/76743 of epoch 7, 87.47 ms/it, loss 0.461926
Finished training it 2048/76743 of epoch 7, 87.10 ms/it, loss 0.465660
Finished training it 2048/76743 of epoch 7, 87.36 ms/it, loss 0.464158
Finished training it 2048/76743 of epoch 7, 87.19 ms/it, loss 0.464186
Finished training it 3072/76743 of epoch 7, 88.34 ms/it, loss 0.462006
Finished training it 3072/76743 of epoch 7, 88.55 ms/it, loss 0.462458
Finished training it 3072/76743 of epoch 7, 87.89 ms/it, loss 0.465108
Finished training it 3072/76743 of epoch 7, 87.73 ms/it, loss 0.461512
Finished training it 4096/76743 of epoch 7, 87.48 ms/it, loss 0.465009
Finished training it 4096/76743 of epoch 7, 87.28 ms/it, loss 0.464513
Finished training it 4096/76743 of epoch 7, 87.10 ms/it, loss 0.466500
Finished training it 4096/76743 of epoch 7, 87.83 ms/it, loss 0.463714
Finished training it 5120/76743 of epoch 7, 87.82 ms/it, loss 0.461723
Finished training it 5120/76743 of epoch 7, 87.24 ms/it, loss 0.466503
Finished training it 5120/76743 of epoch 7, 87.18 ms/it, loss 0.463740
Finished training it 5120/76743 of epoch 7, 87.75 ms/it, loss 0.463939
Finished training it 6144/76743 of epoch 7, 87.69 ms/it, loss 0.463136
Finished training it 6144/76743 of epoch 7, 88.24 ms/it, loss 0.463877
Finished training it 6144/76743 of epoch 7, 88.08 ms/it, loss 0.466432
Finished training it 6144/76743 of epoch 7, 87.65 ms/it, loss 0.462522
Finished training it 7168/76743 of epoch 7, 87.73 ms/it, loss 0.467390
Finished training it 7168/76743 of epoch 7, 87.32 ms/it, loss 0.464759
Finished training it 7168/76743 of epoch 7, 87.47 ms/it, loss 0.466012
Finished training it 7168/76743 of epoch 7, 87.13 ms/it, loss 0.464896
Finished training it 8192/76743 of epoch 7, 86.96 ms/it, loss 0.465323
Finished training it 8192/76743 of epoch 7, 87.11 ms/it, loss 0.464482
Finished training it 8192/76743 of epoch 7, 87.44 ms/it, loss 0.463486
Finished training it 8192/76743 of epoch 7, 87.62 ms/it, loss 0.465778
Finished training it 9216/76743 of epoch 7, 87.61 ms/it, loss 0.466717
Finished training it 9216/76743 of epoch 7, 87.61 ms/it, loss 0.465821
Finished training it 9216/76743 of epoch 7, 87.37 ms/it, loss 0.464516
Finished training it 9216/76743 of epoch 7, 87.79 ms/it, loss 0.465515
Finished training it 10240/76743 of epoch 7, 88.29 ms/it, loss 0.465415
Finished training it 10240/76743 of epoch 7, 88.63 ms/it, loss 0.462420
Finished training it 10240/76743 of epoch 7, 88.38 ms/it, loss 0.464563
Finished training it 10240/76743 of epoch 7, 88.75 ms/it, loss 0.464256
Testing at - 10240/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550198.0
get out
0 has test check 2550198.0 and sample count 3274240
 accuracy 77.887 %, best 77.921 %, roc auc score 0.7796, best 0.7798
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550198.0
get out
2 has test check 2550198.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 87.45 ms/it, loss 0.462798
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550198.0
get out
1 has test check 2550198.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 87.72 ms/it, loss 0.463238
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550198.0
get out
3 has test check 2550198.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 87.45 ms/it, loss 0.462143
Finished training it 11264/76743 of epoch 7, 87.74 ms/it, loss 0.464676
Finished training it 12288/76743 of epoch 7, 87.42 ms/it, loss 0.465799
Finished training it 12288/76743 of epoch 7, 87.59 ms/it, loss 0.465511
Finished training it 12288/76743 of epoch 7, 87.41 ms/it, loss 0.462266
Finished training it 12288/76743 of epoch 7, 88.10 ms/it, loss 0.465720
Finished training it 13312/76743 of epoch 7, 87.19 ms/it, loss 0.463692
Finished training it 13312/76743 of epoch 7, 87.87 ms/it, loss 0.463820
Finished training it 13312/76743 of epoch 7, 87.12 ms/it, loss 0.465949
Finished training it 13312/76743 of epoch 7, 87.52 ms/it, loss 0.465889
Finished training it 14336/76743 of epoch 7, 88.91 ms/it, loss 0.465727
Finished training it 14336/76743 of epoch 7, 88.44 ms/it, loss 0.463432
Finished training it 14336/76743 of epoch 7, 88.42 ms/it, loss 0.462001
Finished training it 14336/76743 of epoch 7, 88.30 ms/it, loss 0.464242
Finished training it 15360/76743 of epoch 7, 87.56 ms/it, loss 0.464266
Finished training it 15360/76743 of epoch 7, 87.34 ms/it, loss 0.463382
Finished training it 15360/76743 of epoch 7, 87.83 ms/it, loss 0.465407
Finished training it 15360/76743 of epoch 7, 87.43 ms/it, loss 0.462476
Finished training it 16384/76743 of epoch 7, 87.74 ms/it, loss 0.462346
Finished training it 16384/76743 of epoch 7, 87.80 ms/it, loss 0.464103
Finished training it 16384/76743 of epoch 7, 87.16 ms/it, loss 0.466498
Finished training it 16384/76743 of epoch 7, 87.08 ms/it, loss 0.462885
Finished training it 17408/76743 of epoch 7, 87.84 ms/it, loss 0.461554
Finished training it 17408/76743 of epoch 7, 88.06 ms/it, loss 0.465995
Finished training it 17408/76743 of epoch 7, 87.50 ms/it, loss 0.463349
Finished training it 17408/76743 of epoch 7, 87.46 ms/it, loss 0.465192
Finished training it 18432/76743 of epoch 7, 87.89 ms/it, loss 0.465265
Finished training it 18432/76743 of epoch 7, 87.66 ms/it, loss 0.465156
Finished training it 18432/76743 of epoch 7, 87.70 ms/it, loss 0.464649
Finished training it 18432/76743 of epoch 7, 87.55 ms/it, loss 0.464733
Finished training it 19456/76743 of epoch 7, 88.27 ms/it, loss 0.464413
Finished training it 19456/76743 of epoch 7, 87.73 ms/it, loss 0.465036
Finished training it 19456/76743 of epoch 7, 88.38 ms/it, loss 0.464780
Finished training it 19456/76743 of epoch 7, 87.83 ms/it, loss 0.466662
Finished training it 20480/76743 of epoch 7, 87.04 ms/it, loss 0.466574
Finished training it 20480/76743 of epoch 7, 87.05 ms/it, loss 0.466728
Finished training it 20480/76743 of epoch 7, 87.59 ms/it, loss 0.465561
Finished training it 20480/76743 of epoch 7, 87.48 ms/it, loss 0.465400
Testing at - 20480/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551338.0
get out
0 has test check 2551338.0 and sample count 3274240
 accuracy 77.922 %, best 77.922 %, roc auc score 0.7805, best 0.7805
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 7, 87.75 ms/it, loss 0.462740
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551338.0
get out
3 has test check 2551338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 87.48 ms/it, loss 0.462938
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551338.0
get out
2 has test check 2551338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 87.57 ms/it, loss 0.461451
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551338.0
get out
1 has test check 2551338.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 87.73 ms/it, loss 0.465044
Finished training it 22528/76743 of epoch 7, 87.59 ms/it, loss 0.461574
Finished training it 22528/76743 of epoch 7, 87.96 ms/it, loss 0.462863
Finished training it 22528/76743 of epoch 7, 87.41 ms/it, loss 0.464984
Finished training it 22528/76743 of epoch 7, 87.95 ms/it, loss 0.465034
Finished training it 23552/76743 of epoch 7, 87.14 ms/it, loss 0.463438
Finished training it 23552/76743 of epoch 7, 87.70 ms/it, loss 0.460694
Finished training it 23552/76743 of epoch 7, 87.02 ms/it, loss 0.463624
Finished training it 23552/76743 of epoch 7, 87.38 ms/it, loss 0.462946
Finished training it 24576/76743 of epoch 7, 98.45 ms/it, loss 0.468428
Finished training it 24576/76743 of epoch 7, 98.57 ms/it, loss 0.462746
Finished training it 24576/76743 of epoch 7, 98.06 ms/it, loss 0.463446
Finished training it 24576/76743 of epoch 7, 98.00 ms/it, loss 0.464283
Finished training it 25600/76743 of epoch 7, 87.57 ms/it, loss 0.464134
Finished training it 25600/76743 of epoch 7, 87.67 ms/it, loss 0.466450
Finished training it 25600/76743 of epoch 7, 87.45 ms/it, loss 0.464609
Finished training it 25600/76743 of epoch 7, 87.74 ms/it, loss 0.464671
Finished training it 26624/76743 of epoch 7, 88.38 ms/it, loss 0.464391
Finished training it 26624/76743 of epoch 7, 88.54 ms/it, loss 0.463050
Finished training it 26624/76743 of epoch 7, 88.13 ms/it, loss 0.463110
Finished training it 26624/76743 of epoch 7, 88.29 ms/it, loss 0.464828
Finished training it 27648/76743 of epoch 7, 87.83 ms/it, loss 0.467106
Finished training it 27648/76743 of epoch 7, 87.64 ms/it, loss 0.464829
Finished training it 27648/76743 of epoch 7, 88.29 ms/it, loss 0.464140
Finished training it 27648/76743 of epoch 7, 87.84 ms/it, loss 0.465688
Finished training it 28672/76743 of epoch 7, 87.41 ms/it, loss 0.463903
Finished training it 28672/76743 of epoch 7, 87.36 ms/it, loss 0.465593
Finished training it 28672/76743 of epoch 7, 87.97 ms/it, loss 0.466840
Finished training it 28672/76743 of epoch 7, 87.75 ms/it, loss 0.462381
Finished training it 29696/76743 of epoch 7, 88.51 ms/it, loss 0.464208
Finished training it 29696/76743 of epoch 7, 88.19 ms/it, loss 0.464682
Finished training it 29696/76743 of epoch 7, 87.98 ms/it, loss 0.462764
Finished training it 29696/76743 of epoch 7, 87.89 ms/it, loss 0.463493
Finished training it 30720/76743 of epoch 7, 87.33 ms/it, loss 0.463385
Finished training it 30720/76743 of epoch 7, 87.75 ms/it, loss 0.462365
Finished training it 30720/76743 of epoch 7, 87.70 ms/it, loss 0.458799
Finished training it 30720/76743 of epoch 7, 87.33 ms/it, loss 0.464393
Testing at - 30720/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549982.0
get out
0 has test check 2549982.0 and sample count 3274240
 accuracy 77.880 %, best 77.922 %, roc auc score 0.7801, best 0.7805
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549982.0
get out
3 has test check 2549982.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 90.42 ms/it, loss 0.466710
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549982.0
get out
2 has test check 2549982.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 90.32 ms/it, loss 0.465499
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549982.0
get out
1 has test check 2549982.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 90.52 ms/it, loss 0.463538
Finished training it 31744/76743 of epoch 7, 90.45 ms/it, loss 0.466158
Finished training it 32768/76743 of epoch 7, 90.40 ms/it, loss 0.463646
Finished training it 32768/76743 of epoch 7, 90.42 ms/it, loss 0.464754
Finished training it 32768/76743 of epoch 7, 90.57 ms/it, loss 0.462965
Finished training it 32768/76743 of epoch 7, 90.80 ms/it, loss 0.464827
Finished training it 33792/76743 of epoch 7, 90.15 ms/it, loss 0.464056
Finished training it 33792/76743 of epoch 7, 90.19 ms/it, loss 0.466149
Finished training it 33792/76743 of epoch 7, 90.46 ms/it, loss 0.465488
Finished training it 33792/76743 of epoch 7, 90.13 ms/it, loss 0.465694
Finished training it 34816/76743 of epoch 7, 90.65 ms/it, loss 0.464902
Finished training it 34816/76743 of epoch 7, 90.70 ms/it, loss 0.461980
Finished training it 34816/76743 of epoch 7, 90.89 ms/it, loss 0.465216
Finished training it 34816/76743 of epoch 7, 90.74 ms/it, loss 0.462115
Finished training it 35840/76743 of epoch 7, 89.11 ms/it, loss 0.463926
Finished training it 35840/76743 of epoch 7, 89.06 ms/it, loss 0.463953
Finished training it 35840/76743 of epoch 7, 89.15 ms/it, loss 0.466097
Finished training it 35840/76743 of epoch 7, 89.08 ms/it, loss 0.463899
Finished training it 36864/76743 of epoch 7, 87.42 ms/it, loss 0.465089
Finished training it 36864/76743 of epoch 7, 87.50 ms/it, loss 0.465475
Finished training it 36864/76743 of epoch 7, 87.55 ms/it, loss 0.461730
Finished training it 36864/76743 of epoch 7, 87.66 ms/it, loss 0.464787
Finished training it 37888/76743 of epoch 7, 87.43 ms/it, loss 0.463178
Finished training it 37888/76743 of epoch 7, 87.63 ms/it, loss 0.464427
Finished training it 37888/76743 of epoch 7, 88.02 ms/it, loss 0.460428
Finished training it 37888/76743 of epoch 7, 87.78 ms/it, loss 0.463678
Finished training it 38912/76743 of epoch 7, 87.38 ms/it, loss 0.464072
Finished training it 38912/76743 of epoch 7, 87.47 ms/it, loss 0.464810
Finished training it 38912/76743 of epoch 7, 87.81 ms/it, loss 0.463417
Finished training it 38912/76743 of epoch 7, 87.47 ms/it, loss 0.465521
Finished training it 39936/76743 of epoch 7, 87.27 ms/it, loss 0.462329
Finished training it 39936/76743 of epoch 7, 86.90 ms/it, loss 0.463434
Finished training it 39936/76743 of epoch 7, 87.04 ms/it, loss 0.465709
Finished training it 39936/76743 of epoch 7, 87.41 ms/it, loss 0.461413
Finished training it 40960/76743 of epoch 7, 87.34 ms/it, loss 0.463993
Finished training it 40960/76743 of epoch 7, 87.27 ms/it, loss 0.464965
Finished training it 40960/76743 of epoch 7, 87.68 ms/it, loss 0.460709
Finished training it 40960/76743 of epoch 7, 87.47 ms/it, loss 0.461538
Testing at - 40960/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551428.0
get out
0 has test check 2551428.0 and sample count 3274240
 accuracy 77.924 %, best 77.924 %, roc auc score 0.7803, best 0.7805
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 7, 89.97 ms/it, loss 0.464563
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551428.0
get out
1 has test check 2551428.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 90.29 ms/it, loss 0.463072
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551428.0
get out
3 has test check 2551428.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 90.05 ms/it, loss 0.462567
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551428.0
get out
2 has test check 2551428.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 90.09 ms/it, loss 0.462524
Finished training it 43008/76743 of epoch 7, 90.56 ms/it, loss 0.462887
Finished training it 43008/76743 of epoch 7, 90.56 ms/it, loss 0.467450
Finished training it 43008/76743 of epoch 7, 90.59 ms/it, loss 0.465042
Finished training it 43008/76743 of epoch 7, 90.70 ms/it, loss 0.461261
Finished training it 44032/76743 of epoch 7, 90.84 ms/it, loss 0.464546
Finished training it 44032/76743 of epoch 7, 90.52 ms/it, loss 0.464461
Finished training it 44032/76743 of epoch 7, 90.61 ms/it, loss 0.463145
Finished training it 44032/76743 of epoch 7, 90.58 ms/it, loss 0.464547
Finished training it 45056/76743 of epoch 7, 97.38 ms/it, loss 0.466811
Finished training it 45056/76743 of epoch 7, 97.42 ms/it, loss 0.462632
Finished training it 45056/76743 of epoch 7, 97.92 ms/it, loss 0.464438
Finished training it 45056/76743 of epoch 7, 97.07 ms/it, loss 0.465220
Finished training it 46080/76743 of epoch 7, 94.81 ms/it, loss 0.464788
Finished training it 46080/76743 of epoch 7, 95.24 ms/it, loss 0.464648
Finished training it 46080/76743 of epoch 7, 94.23 ms/it, loss 0.463044
Finished training it 46080/76743 of epoch 7, 94.27 ms/it, loss 0.462909
Finished training it 47104/76743 of epoch 7, 87.97 ms/it, loss 0.461230
Finished training it 47104/76743 of epoch 7, 88.01 ms/it, loss 0.461574
Finished training it 47104/76743 of epoch 7, 87.76 ms/it, loss 0.461135
Finished training it 47104/76743 of epoch 7, 88.20 ms/it, loss 0.462320
Finished training it 48128/76743 of epoch 7, 88.50 ms/it, loss 0.461771
Finished training it 48128/76743 of epoch 7, 88.32 ms/it, loss 0.462067
Finished training it 48128/76743 of epoch 7, 87.85 ms/it, loss 0.465793
Finished training it 48128/76743 of epoch 7, 87.81 ms/it, loss 0.462116
Finished training it 49152/76743 of epoch 7, 86.87 ms/it, loss 0.462265
Finished training it 49152/76743 of epoch 7, 87.02 ms/it, loss 0.462622
Finished training it 49152/76743 of epoch 7, 87.01 ms/it, loss 0.464566
Finished training it 49152/76743 of epoch 7, 86.71 ms/it, loss 0.462033
Finished training it 50176/76743 of epoch 7, 88.10 ms/it, loss 0.465681
Finished training it 50176/76743 of epoch 7, 88.25 ms/it, loss 0.466068
Finished training it 50176/76743 of epoch 7, 87.72 ms/it, loss 0.462868
Finished training it 50176/76743 of epoch 7, 87.58 ms/it, loss 0.462939
Finished training it 51200/76743 of epoch 7, 87.60 ms/it, loss 0.464680
Finished training it 51200/76743 of epoch 7, 87.38 ms/it, loss 0.463686
Finished training it 51200/76743 of epoch 7, 87.39 ms/it, loss 0.467247
Finished training it 51200/76743 of epoch 7, 87.74 ms/it, loss 0.462102
Testing at - 51200/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551420.0
get out
0 has test check 2551420.0 and sample count 3274240
 accuracy 77.924 %, best 77.924 %, roc auc score 0.7804, best 0.7805
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551420.0
get out
1 has test check 2551420.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 90.78 ms/it, loss 0.464470
Finished training it 52224/76743 of epoch 7, 90.74 ms/it, loss 0.460746
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551420.0
get out
3 has test check 2551420.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 90.65 ms/it, loss 0.463583
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551420.0
get out
2 has test check 2551420.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 90.52 ms/it, loss 0.462357
Finished training it 53248/76743 of epoch 7, 90.22 ms/it, loss 0.464290
Finished training it 53248/76743 of epoch 7, 90.50 ms/it, loss 0.462957
Finished training it 53248/76743 of epoch 7, 90.17 ms/it, loss 0.464180
Finished training it 53248/76743 of epoch 7, 90.29 ms/it, loss 0.463466
Finished training it 54272/76743 of epoch 7, 90.01 ms/it, loss 0.461992
Finished training it 54272/76743 of epoch 7, 90.18 ms/it, loss 0.464299
Finished training it 54272/76743 of epoch 7, 90.22 ms/it, loss 0.465535
Finished training it 54272/76743 of epoch 7, 89.97 ms/it, loss 0.462807
Finished training it 55296/76743 of epoch 7, 90.51 ms/it, loss 0.462299
Finished training it 55296/76743 of epoch 7, 90.45 ms/it, loss 0.462922
Finished training it 55296/76743 of epoch 7, 90.85 ms/it, loss 0.461387
Finished training it 55296/76743 of epoch 7, 90.49 ms/it, loss 0.466386
Finished training it 56320/76743 of epoch 7, 90.39 ms/it, loss 0.460369
Finished training it 56320/76743 of epoch 7, 90.09 ms/it, loss 0.463479
Finished training it 56320/76743 of epoch 7, 90.07 ms/it, loss 0.461455
Finished training it 56320/76743 of epoch 7, 90.15 ms/it, loss 0.464807
Finished training it 57344/76743 of epoch 7, 87.62 ms/it, loss 0.465039
Finished training it 57344/76743 of epoch 7, 87.57 ms/it, loss 0.462285
Finished training it 57344/76743 of epoch 7, 88.20 ms/it, loss 0.467694
Finished training it 57344/76743 of epoch 7, 87.76 ms/it, loss 0.461582
Finished training it 58368/76743 of epoch 7, 87.65 ms/it, loss 0.465420
Finished training it 58368/76743 of epoch 7, 87.72 ms/it, loss 0.463161
Finished training it 58368/76743 of epoch 7, 87.38 ms/it, loss 0.463849
Finished training it 58368/76743 of epoch 7, 87.45 ms/it, loss 0.465245
Finished training it 59392/76743 of epoch 7, 88.15 ms/it, loss 0.464991
Finished training it 59392/76743 of epoch 7, 87.62 ms/it, loss 0.462535
Finished training it 59392/76743 of epoch 7, 87.94 ms/it, loss 0.463440
Finished training it 59392/76743 of epoch 7, 87.56 ms/it, loss 0.461888
Finished training it 60416/76743 of epoch 7, 87.36 ms/it, loss 0.460892
Finished training it 60416/76743 of epoch 7, 87.99 ms/it, loss 0.461912
Finished training it 60416/76743 of epoch 7, 87.40 ms/it, loss 0.462384
Finished training it 60416/76743 of epoch 7, 87.82 ms/it, loss 0.462454
Finished training it 61440/76743 of epoch 7, 88.32 ms/it, loss 0.462794
Finished training it 61440/76743 of epoch 7, 87.39 ms/it, loss 0.462455
Finished training it 61440/76743 of epoch 7, 87.34 ms/it, loss 0.464801
Finished training it 61440/76743 of epoch 7, 88.09 ms/it, loss 0.464789
Testing at - 61440/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552899.0
get out
0 has test check 2552899.0 and sample count 3274240
 accuracy 77.969 %, best 77.969 %, roc auc score 0.7809, best 0.7809
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552899.0
get out
2 has test check 2552899.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 90.02 ms/it, loss 0.461635
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 7, 89.98 ms/it, loss 0.464279
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552899.0
get out
1 has test check 2552899.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 90.00 ms/it, loss 0.464266
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552899.0
get out
3 has test check 2552899.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 90.06 ms/it, loss 0.462940
Finished training it 63488/76743 of epoch 7, 90.48 ms/it, loss 0.464928
Finished training it 63488/76743 of epoch 7, 90.57 ms/it, loss 0.463082
Finished training it 63488/76743 of epoch 7, 90.82 ms/it, loss 0.462254
Finished training it 63488/76743 of epoch 7, 90.56 ms/it, loss 0.461994
Finished training it 64512/76743 of epoch 7, 90.20 ms/it, loss 0.460815
Finished training it 64512/76743 of epoch 7, 90.19 ms/it, loss 0.461733
Finished training it 64512/76743 of epoch 7, 90.13 ms/it, loss 0.464872
Finished training it 64512/76743 of epoch 7, 90.45 ms/it, loss 0.462064
Finished training it 65536/76743 of epoch 7, 96.60 ms/it, loss 0.463891
Finished training it 65536/76743 of epoch 7, 96.61 ms/it, loss 0.463351
Finished training it 65536/76743 of epoch 7, 96.02 ms/it, loss 0.465964
Finished training it 65536/76743 of epoch 7, 96.43 ms/it, loss 0.463300
Finished training it 66560/76743 of epoch 7, 95.88 ms/it, loss 0.464090
Finished training it 66560/76743 of epoch 7, 96.29 ms/it, loss 0.462660
Finished training it 66560/76743 of epoch 7, 96.59 ms/it, loss 0.463204
Finished training it 66560/76743 of epoch 7, 95.71 ms/it, loss 0.463916
Finished training it 67584/76743 of epoch 7, 89.50 ms/it, loss 0.463974
Finished training it 67584/76743 of epoch 7, 89.71 ms/it, loss 0.464147
Finished training it 67584/76743 of epoch 7, 89.51 ms/it, loss 0.462813
Finished training it 67584/76743 of epoch 7, 89.42 ms/it, loss 0.466531
Finished training it 68608/76743 of epoch 7, 86.73 ms/it, loss 0.463801
Finished training it 68608/76743 of epoch 7, 87.34 ms/it, loss 0.462534
Finished training it 68608/76743 of epoch 7, 86.69 ms/it, loss 0.462837
Finished training it 68608/76743 of epoch 7, 87.32 ms/it, loss 0.463482
Finished training it 69632/76743 of epoch 7, 88.22 ms/it, loss 0.464002
Finished training it 69632/76743 of epoch 7, 87.92 ms/it, loss 0.465248
Finished training it 69632/76743 of epoch 7, 88.49 ms/it, loss 0.461836
Finished training it 69632/76743 of epoch 7, 87.79 ms/it, loss 0.464508
Finished training it 70656/76743 of epoch 7, 88.18 ms/it, loss 0.463001
Finished training it 70656/76743 of epoch 7, 87.72 ms/it, loss 0.463732
Finished training it 70656/76743 of epoch 7, 87.87 ms/it, loss 0.464384
Finished training it 70656/76743 of epoch 7, 87.53 ms/it, loss 0.461600
Finished training it 71680/76743 of epoch 7, 87.66 ms/it, loss 0.460315
Finished training it 71680/76743 of epoch 7, 87.38 ms/it, loss 0.464029
Finished training it 71680/76743 of epoch 7, 87.95 ms/it, loss 0.460570
Finished training it 71680/76743 of epoch 7, 87.39 ms/it, loss 0.464785
Testing at - 71680/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552324.0
get out
0 has test check 2552324.0 and sample count 3274240
 accuracy 77.952 %, best 77.969 %, roc auc score 0.7806, best 0.7809
Finished training it 72704/76743 of epoch 7, 90.39 ms/it, loss 0.465285
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552324.0
get out
1 has test check 2552324.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 90.35 ms/it, loss 0.463500
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552324.0
get out
3 has test check 2552324.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 90.12 ms/it, loss 0.462624
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552324.0
get out
2 has test check 2552324.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 90.28 ms/it, loss 0.464154
Finished training it 73728/76743 of epoch 7, 90.10 ms/it, loss 0.465779
Finished training it 73728/76743 of epoch 7, 89.92 ms/it, loss 0.464094
Finished training it 73728/76743 of epoch 7, 90.07 ms/it, loss 0.462586
Finished training it 73728/76743 of epoch 7, 89.99 ms/it, loss 0.462733
Finished training it 74752/76743 of epoch 7, 90.51 ms/it, loss 0.463901
Finished training it 74752/76743 of epoch 7, 90.51 ms/it, loss 0.463448
Finished training it 74752/76743 of epoch 7, 90.71 ms/it, loss 0.464486
Finished training it 74752/76743 of epoch 7, 90.56 ms/it, loss 0.464366
Finished training it 75776/76743 of epoch 7, 90.59 ms/it, loss 0.462845
Finished training it 75776/76743 of epoch 7, 90.60 ms/it, loss 0.465033
Finished training it 75776/76743 of epoch 7, 90.46 ms/it, loss 0.464401
Finished training it 75776/76743 of epoch 7, 90.94 ms/it, loss 0.464457
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 91.61 ms/it, loss 0.463776
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 92.15 ms/it, loss 0.465127
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 91.43 ms/it, loss 0.464373
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 91.38 ms/it, loss 0.460131
Finished training it 2048/76743 of epoch 8, 87.69 ms/it, loss 0.463450
Finished training it 2048/76743 of epoch 8, 87.28 ms/it, loss 0.464888
Finished training it 2048/76743 of epoch 8, 87.81 ms/it, loss 0.460880
Finished training it 2048/76743 of epoch 8, 87.22 ms/it, loss 0.463209
Finished training it 3072/76743 of epoch 8, 87.42 ms/it, loss 0.463793
Finished training it 3072/76743 of epoch 8, 87.86 ms/it, loss 0.461489
Finished training it 3072/76743 of epoch 8, 87.66 ms/it, loss 0.461262
Finished training it 3072/76743 of epoch 8, 87.31 ms/it, loss 0.460179
Finished training it 4096/76743 of epoch 8, 88.08 ms/it, loss 0.462748
Finished training it 4096/76743 of epoch 8, 87.57 ms/it, loss 0.463338
Finished training it 4096/76743 of epoch 8, 87.71 ms/it, loss 0.464194
Finished training it 4096/76743 of epoch 8, 87.71 ms/it, loss 0.465314
Finished training it 5120/76743 of epoch 8, 87.03 ms/it, loss 0.462868
Finished training it 5120/76743 of epoch 8, 87.43 ms/it, loss 0.462860
Finished training it 5120/76743 of epoch 8, 87.49 ms/it, loss 0.460631
Finished training it 5120/76743 of epoch 8, 87.14 ms/it, loss 0.465035
Finished training it 6144/76743 of epoch 8, 87.31 ms/it, loss 0.462278
Finished training it 6144/76743 of epoch 8, 87.50 ms/it, loss 0.461738
Finished training it 6144/76743 of epoch 8, 87.89 ms/it, loss 0.465257
Finished training it 6144/76743 of epoch 8, 87.84 ms/it, loss 0.462448
Finished training it 7168/76743 of epoch 8, 87.88 ms/it, loss 0.464850
Finished training it 7168/76743 of epoch 8, 87.72 ms/it, loss 0.463560
Finished training it 7168/76743 of epoch 8, 87.66 ms/it, loss 0.463836
Finished training it 7168/76743 of epoch 8, 87.88 ms/it, loss 0.465941
Finished training it 8192/76743 of epoch 8, 87.66 ms/it, loss 0.463771
Finished training it 8192/76743 of epoch 8, 88.09 ms/it, loss 0.464281
Finished training it 8192/76743 of epoch 8, 87.91 ms/it, loss 0.462205
Finished training it 8192/76743 of epoch 8, 87.71 ms/it, loss 0.463035
Finished training it 9216/76743 of epoch 8, 92.85 ms/it, loss 0.464967
Finished training it 9216/76743 of epoch 8, 93.84 ms/it, loss 0.464155
Finished training it 9216/76743 of epoch 8, 93.23 ms/it, loss 0.463263
Finished training it 9216/76743 of epoch 8, 92.97 ms/it, loss 0.465538
Finished training it 10240/76743 of epoch 8, 92.25 ms/it, loss 0.464268
Finished training it 10240/76743 of epoch 8, 92.23 ms/it, loss 0.463593
Finished training it 10240/76743 of epoch 8, 92.67 ms/it, loss 0.461443
Finished training it 10240/76743 of epoch 8, 92.56 ms/it, loss 0.462986
Testing at - 10240/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552076.0
get out
0 has test check 2552076.0 and sample count 3274240
 accuracy 77.944 %, best 77.969 %, roc auc score 0.7808, best 0.7809
Finished training it 11264/76743 of epoch 8, 90.44 ms/it, loss 0.463749
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552076.0
get out
2 has test check 2552076.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 90.41 ms/it, loss 0.461506
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552076.0
get out
1 has test check 2552076.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 90.46 ms/it, loss 0.462066
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552076.0
get out
3 has test check 2552076.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 90.29 ms/it, loss 0.461064
Finished training it 12288/76743 of epoch 8, 95.14 ms/it, loss 0.464638
Finished training it 12288/76743 of epoch 8, 95.53 ms/it, loss 0.464532
Finished training it 12288/76743 of epoch 8, 95.23 ms/it, loss 0.464649
Finished training it 12288/76743 of epoch 8, 95.42 ms/it, loss 0.461443
Finished training it 13312/76743 of epoch 8, 91.82 ms/it, loss 0.462830
Finished training it 13312/76743 of epoch 8, 91.89 ms/it, loss 0.463211
Finished training it 13312/76743 of epoch 8, 92.30 ms/it, loss 0.465166
Finished training it 13312/76743 of epoch 8, 91.83 ms/it, loss 0.464868
Finished training it 14336/76743 of epoch 8, 87.57 ms/it, loss 0.464857
Finished training it 14336/76743 of epoch 8, 87.26 ms/it, loss 0.463500
Finished training it 14336/76743 of epoch 8, 87.33 ms/it, loss 0.460962
Finished training it 14336/76743 of epoch 8, 87.48 ms/it, loss 0.462549
Finished training it 15360/76743 of epoch 8, 87.99 ms/it, loss 0.461436
Finished training it 15360/76743 of epoch 8, 88.47 ms/it, loss 0.463229
Finished training it 15360/76743 of epoch 8, 88.01 ms/it, loss 0.462785
Finished training it 15360/76743 of epoch 8, 88.45 ms/it, loss 0.464360
Finished training it 16384/76743 of epoch 8, 87.82 ms/it, loss 0.463003
Finished training it 16384/76743 of epoch 8, 87.63 ms/it, loss 0.461493
Finished training it 16384/76743 of epoch 8, 87.58 ms/it, loss 0.465306
Finished training it 16384/76743 of epoch 8, 87.59 ms/it, loss 0.461730
Finished training it 17408/76743 of epoch 8, 87.50 ms/it, loss 0.462273
Finished training it 17408/76743 of epoch 8, 87.57 ms/it, loss 0.464354
Finished training it 17408/76743 of epoch 8, 88.02 ms/it, loss 0.464853
Finished training it 17408/76743 of epoch 8, 87.99 ms/it, loss 0.460531
Finished training it 18432/76743 of epoch 8, 87.54 ms/it, loss 0.463258
Finished training it 18432/76743 of epoch 8, 87.43 ms/it, loss 0.464090
Finished training it 18432/76743 of epoch 8, 87.89 ms/it, loss 0.464334
Finished training it 18432/76743 of epoch 8, 87.53 ms/it, loss 0.463508
Finished training it 19456/76743 of epoch 8, 87.39 ms/it, loss 0.464042
Finished training it 19456/76743 of epoch 8, 87.42 ms/it, loss 0.465850
Finished training it 19456/76743 of epoch 8, 87.64 ms/it, loss 0.463578
Finished training it 19456/76743 of epoch 8, 87.85 ms/it, loss 0.463582
Finished training it 20480/76743 of epoch 8, 88.06 ms/it, loss 0.464599
Finished training it 20480/76743 of epoch 8, 87.52 ms/it, loss 0.465582
Finished training it 20480/76743 of epoch 8, 87.61 ms/it, loss 0.465209
Finished training it 20480/76743 of epoch 8, 87.85 ms/it, loss 0.464137
Testing at - 20480/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552607.0
get out
0 has test check 2552607.0 and sample count 3274240
 accuracy 77.960 %, best 77.969 %, roc auc score 0.7813, best 0.7813
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552607.0
get out
1 has test check 2552607.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 90.34 ms/it, loss 0.464105
Finished training it 21504/76743 of epoch 8, 90.25 ms/it, loss 0.461550
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552607.0
get out
2 has test check 2552607.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 90.34 ms/it, loss 0.460693
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552607.0
get out
3 has test check 2552607.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 90.20 ms/it, loss 0.462082
Finished training it 22528/76743 of epoch 8, 89.79 ms/it, loss 0.463651
Finished training it 22528/76743 of epoch 8, 89.61 ms/it, loss 0.462171
Finished training it 22528/76743 of epoch 8, 89.61 ms/it, loss 0.460508
Finished training it 22528/76743 of epoch 8, 89.54 ms/it, loss 0.463708
Finished training it 23552/76743 of epoch 8, 88.16 ms/it, loss 0.461773
Finished training it 23552/76743 of epoch 8, 87.70 ms/it, loss 0.462106
Finished training it 23552/76743 of epoch 8, 87.57 ms/it, loss 0.462521
Finished training it 23552/76743 of epoch 8, 88.19 ms/it, loss 0.459734
Finished training it 24576/76743 of epoch 8, 87.71 ms/it, loss 0.461364
Finished training it 24576/76743 of epoch 8, 87.24 ms/it, loss 0.462090
Finished training it 24576/76743 of epoch 8, 87.48 ms/it, loss 0.467466
Finished training it 24576/76743 of epoch 8, 87.29 ms/it, loss 0.462913
Finished training it 25600/76743 of epoch 8, 88.20 ms/it, loss 0.463944
Finished training it 25600/76743 of epoch 8, 87.73 ms/it, loss 0.463172
Finished training it 25600/76743 of epoch 8, 88.39 ms/it, loss 0.465482
Finished training it 25600/76743 of epoch 8, 87.85 ms/it, loss 0.462627
Finished training it 26624/76743 of epoch 8, 87.84 ms/it, loss 0.463151
Finished training it 26624/76743 of epoch 8, 87.97 ms/it, loss 0.461836
Finished training it 26624/76743 of epoch 8, 87.70 ms/it, loss 0.462319
Finished training it 26624/76743 of epoch 8, 87.56 ms/it, loss 0.463586
Finished training it 27648/76743 of epoch 8, 87.38 ms/it, loss 0.463818
Finished training it 27648/76743 of epoch 8, 87.44 ms/it, loss 0.464547
Finished training it 27648/76743 of epoch 8, 87.26 ms/it, loss 0.465651
Finished training it 27648/76743 of epoch 8, 87.69 ms/it, loss 0.463157
Finished training it 28672/76743 of epoch 8, 87.85 ms/it, loss 0.462521
Finished training it 28672/76743 of epoch 8, 87.91 ms/it, loss 0.464685
Finished training it 28672/76743 of epoch 8, 88.15 ms/it, loss 0.465311
Finished training it 28672/76743 of epoch 8, 88.11 ms/it, loss 0.461499
Finished training it 29696/76743 of epoch 8, 88.00 ms/it, loss 0.463766
Finished training it 29696/76743 of epoch 8, 87.85 ms/it, loss 0.462757
Finished training it 29696/76743 of epoch 8, 87.68 ms/it, loss 0.461901
Finished training it 29696/76743 of epoch 8, 87.64 ms/it, loss 0.462521
Finished training it 30720/76743 of epoch 8, 87.80 ms/it, loss 0.462904
Finished training it 30720/76743 of epoch 8, 87.98 ms/it, loss 0.457272
Finished training it 30720/76743 of epoch 8, 88.15 ms/it, loss 0.461264
Finished training it 30720/76743 of epoch 8, 87.76 ms/it, loss 0.463320
Testing at - 30720/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552420.0
get out
0 has test check 2552420.0 and sample count 3274240
 accuracy 77.955 %, best 77.969 %, roc auc score 0.7813, best 0.7813
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552420.0
get out
2 has test check 2552420.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 90.46 ms/it, loss 0.464689
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552420.0
get out
3 has test check 2552420.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 90.22 ms/it, loss 0.465133
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552420.0
get out
1 has test check 2552420.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 90.51 ms/it, loss 0.462375
Finished training it 31744/76743 of epoch 8, 90.41 ms/it, loss 0.465139
Finished training it 32768/76743 of epoch 8, 90.38 ms/it, loss 0.461731
Finished training it 32768/76743 of epoch 8, 90.50 ms/it, loss 0.463668
Finished training it 32768/76743 of epoch 8, 90.42 ms/it, loss 0.462322
Finished training it 32768/76743 of epoch 8, 90.52 ms/it, loss 0.463224
Finished training it 33792/76743 of epoch 8, 90.91 ms/it, loss 0.464685
Finished training it 33792/76743 of epoch 8, 90.84 ms/it, loss 0.465278
Finished training it 33792/76743 of epoch 8, 90.81 ms/it, loss 0.464789
Finished training it 33792/76743 of epoch 8, 90.74 ms/it, loss 0.462849
Finished training it 34816/76743 of epoch 8, 101.41 ms/it, loss 0.463839
Finished training it 34816/76743 of epoch 8, 101.27 ms/it, loss 0.463811
Finished training it 34816/76743 of epoch 8, 101.46 ms/it, loss 0.461044
Finished training it 34816/76743 of epoch 8, 101.30 ms/it, loss 0.461381
Finished training it 35840/76743 of epoch 8, 90.39 ms/it, loss 0.462966
Finished training it 35840/76743 of epoch 8, 90.40 ms/it, loss 0.462472
Finished training it 35840/76743 of epoch 8, 90.35 ms/it, loss 0.462916
Finished training it 35840/76743 of epoch 8, 90.37 ms/it, loss 0.464925
Finished training it 36864/76743 of epoch 8, 90.38 ms/it, loss 0.463870
Finished training it 36864/76743 of epoch 8, 90.54 ms/it, loss 0.464096
Finished training it 36864/76743 of epoch 8, 90.38 ms/it, loss 0.460594
Finished training it 36864/76743 of epoch 8, 90.48 ms/it, loss 0.463465
Finished training it 37888/76743 of epoch 8, 90.53 ms/it, loss 0.462026
Finished training it 37888/76743 of epoch 8, 90.72 ms/it, loss 0.459333
Finished training it 37888/76743 of epoch 8, 90.71 ms/it, loss 0.463669
Finished training it 37888/76743 of epoch 8, 90.50 ms/it, loss 0.462564
Finished training it 38912/76743 of epoch 8, 89.27 ms/it, loss 0.462106
Finished training it 38912/76743 of epoch 8, 89.00 ms/it, loss 0.463808
Finished training it 38912/76743 of epoch 8, 88.89 ms/it, loss 0.464016
Finished training it 38912/76743 of epoch 8, 88.98 ms/it, loss 0.462857
Finished training it 39936/76743 of epoch 8, 87.86 ms/it, loss 0.461248
Finished training it 39936/76743 of epoch 8, 87.88 ms/it, loss 0.460612
Finished training it 39936/76743 of epoch 8, 87.59 ms/it, loss 0.462272
Finished training it 39936/76743 of epoch 8, 87.44 ms/it, loss 0.464454
Finished training it 40960/76743 of epoch 8, 88.34 ms/it, loss 0.459373
Finished training it 40960/76743 of epoch 8, 88.01 ms/it, loss 0.460646
Finished training it 40960/76743 of epoch 8, 87.82 ms/it, loss 0.462850
Finished training it 40960/76743 of epoch 8, 87.92 ms/it, loss 0.463800
Testing at - 40960/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552972.0
get out
0 has test check 2552972.0 and sample count 3274240
 accuracy 77.971 %, best 77.971 %, roc auc score 0.7813, best 0.7813
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552972.0
get out
1 has test check 2552972.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 90.66 ms/it, loss 0.462172
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552972.0
get out
3 has test check 2552972.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 90.54 ms/it, loss 0.461264
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 8, 90.80 ms/it, loss 0.463592
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552972.0
get out
2 has test check 2552972.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 90.50 ms/it, loss 0.461307
Finished training it 43008/76743 of epoch 8, 90.21 ms/it, loss 0.464272
Finished training it 43008/76743 of epoch 8, 90.14 ms/it, loss 0.465963
Finished training it 43008/76743 of epoch 8, 90.31 ms/it, loss 0.461821
Finished training it 43008/76743 of epoch 8, 90.44 ms/it, loss 0.459863
Finished training it 44032/76743 of epoch 8, 90.37 ms/it, loss 0.463420
Finished training it 44032/76743 of epoch 8, 90.32 ms/it, loss 0.461644
Finished training it 44032/76743 of epoch 8, 90.43 ms/it, loss 0.463472
Finished training it 44032/76743 of epoch 8, 90.48 ms/it, loss 0.463490
Finished training it 45056/76743 of epoch 8, 90.40 ms/it, loss 0.464211
Finished training it 45056/76743 of epoch 8, 90.43 ms/it, loss 0.463057
Finished training it 45056/76743 of epoch 8, 90.25 ms/it, loss 0.465485
Finished training it 45056/76743 of epoch 8, 90.34 ms/it, loss 0.461369
Finished training it 46080/76743 of epoch 8, 90.20 ms/it, loss 0.463949
Finished training it 46080/76743 of epoch 8, 90.14 ms/it, loss 0.461865
Finished training it 46080/76743 of epoch 8, 90.27 ms/it, loss 0.463289
Finished training it 46080/76743 of epoch 8, 90.09 ms/it, loss 0.461775
Finished training it 47104/76743 of epoch 8, 90.79 ms/it, loss 0.461069
Finished training it 47104/76743 of epoch 8, 90.83 ms/it, loss 0.460166
Finished training it 47104/76743 of epoch 8, 90.75 ms/it, loss 0.459554
Finished training it 47104/76743 of epoch 8, 90.89 ms/it, loss 0.460610
Finished training it 48128/76743 of epoch 8, 90.34 ms/it, loss 0.464935
Finished training it 48128/76743 of epoch 8, 90.58 ms/it, loss 0.460826
Finished training it 48128/76743 of epoch 8, 90.61 ms/it, loss 0.460772
Finished training it 48128/76743 of epoch 8, 90.50 ms/it, loss 0.460854
Finished training it 49152/76743 of epoch 8, 88.59 ms/it, loss 0.461115
Finished training it 49152/76743 of epoch 8, 88.75 ms/it, loss 0.463285
Finished training it 49152/76743 of epoch 8, 88.62 ms/it, loss 0.461771
Finished training it 49152/76743 of epoch 8, 88.53 ms/it, loss 0.460818
Finished training it 50176/76743 of epoch 8, 87.30 ms/it, loss 0.462067
Finished training it 50176/76743 of epoch 8, 87.75 ms/it, loss 0.464786
Finished training it 50176/76743 of epoch 8, 87.77 ms/it, loss 0.464627
Finished training it 50176/76743 of epoch 8, 87.50 ms/it, loss 0.461757
Finished training it 51200/76743 of epoch 8, 88.45 ms/it, loss 0.463681
Finished training it 51200/76743 of epoch 8, 88.53 ms/it, loss 0.461078
Finished training it 51200/76743 of epoch 8, 88.18 ms/it, loss 0.462483
Finished training it 51200/76743 of epoch 8, 88.08 ms/it, loss 0.465705
Testing at - 51200/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553692.0
get out
0 has test check 2553692.0 and sample count 3274240
 accuracy 77.993 %, best 77.993 %, roc auc score 0.7819, best 0.7819
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 8, 90.57 ms/it, loss 0.459527
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553692.0
get out
1 has test check 2553692.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 90.57 ms/it, loss 0.463412
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553692.0
get out
3 has test check 2553692.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 90.49 ms/it, loss 0.462093
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553692.0
get out
2 has test check 2553692.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 90.46 ms/it, loss 0.461499
Finished training it 53248/76743 of epoch 8, 90.53 ms/it, loss 0.463084
Finished training it 53248/76743 of epoch 8, 90.71 ms/it, loss 0.461924
Finished training it 53248/76743 of epoch 8, 90.49 ms/it, loss 0.462853
Finished training it 53248/76743 of epoch 8, 90.62 ms/it, loss 0.462694
Finished training it 54272/76743 of epoch 8, 90.73 ms/it, loss 0.461020
Finished training it 54272/76743 of epoch 8, 90.68 ms/it, loss 0.461770
Finished training it 54272/76743 of epoch 8, 90.67 ms/it, loss 0.462956
Finished training it 54272/76743 of epoch 8, 90.79 ms/it, loss 0.464536
Finished training it 55296/76743 of epoch 8, 95.18 ms/it, loss 0.465422
Finished training it 55296/76743 of epoch 8, 96.01 ms/it, loss 0.461609
Finished training it 55296/76743 of epoch 8, 95.31 ms/it, loss 0.460233
Finished training it 55296/76743 of epoch 8, 95.23 ms/it, loss 0.462100
Finished training it 56320/76743 of epoch 8, 96.33 ms/it, loss 0.463484
Finished training it 56320/76743 of epoch 8, 96.64 ms/it, loss 0.462791
Finished training it 56320/76743 of epoch 8, 96.62 ms/it, loss 0.458850
Finished training it 56320/76743 of epoch 8, 96.98 ms/it, loss 0.460484
Finished training it 57344/76743 of epoch 8, 89.97 ms/it, loss 0.463685
Finished training it 57344/76743 of epoch 8, 89.97 ms/it, loss 0.461471
Finished training it 57344/76743 of epoch 8, 90.12 ms/it, loss 0.466453
Finished training it 57344/76743 of epoch 8, 90.18 ms/it, loss 0.460529
Finished training it 58368/76743 of epoch 8, 90.17 ms/it, loss 0.464523
Finished training it 58368/76743 of epoch 8, 90.10 ms/it, loss 0.463118
Finished training it 58368/76743 of epoch 8, 90.12 ms/it, loss 0.464023
Finished training it 58368/76743 of epoch 8, 90.22 ms/it, loss 0.461835
Finished training it 59392/76743 of epoch 8, 89.93 ms/it, loss 0.464175
Finished training it 59392/76743 of epoch 8, 89.76 ms/it, loss 0.461230
Finished training it 59392/76743 of epoch 8, 89.70 ms/it, loss 0.462374
Finished training it 59392/76743 of epoch 8, 89.62 ms/it, loss 0.461328
Finished training it 60416/76743 of epoch 8, 87.72 ms/it, loss 0.461166
Finished training it 60416/76743 of epoch 8, 88.37 ms/it, loss 0.461272
Finished training it 60416/76743 of epoch 8, 87.84 ms/it, loss 0.461074
Finished training it 60416/76743 of epoch 8, 87.88 ms/it, loss 0.459720
Finished training it 61440/76743 of epoch 8, 87.91 ms/it, loss 0.463936
Finished training it 61440/76743 of epoch 8, 88.07 ms/it, loss 0.461641
Finished training it 61440/76743 of epoch 8, 87.77 ms/it, loss 0.463215
Finished training it 61440/76743 of epoch 8, 87.61 ms/it, loss 0.461048
Testing at - 61440/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554079.0
get out
0 has test check 2554079.0 and sample count 3274240
 accuracy 78.005 %, best 78.005 %, roc auc score 0.7821, best 0.7821
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554079.0
get out
3 has test check 2554079.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 90.27 ms/it, loss 0.462138
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554079.0
get out
2 has test check 2554079.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 90.18 ms/it, loss 0.460288
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 8, 90.41 ms/it, loss 0.463253
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554079.0
get out
1 has test check 2554079.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 90.32 ms/it, loss 0.463139
Finished training it 63488/76743 of epoch 8, 89.75 ms/it, loss 0.462130
Finished training it 63488/76743 of epoch 8, 90.07 ms/it, loss 0.464203
Finished training it 63488/76743 of epoch 8, 89.81 ms/it, loss 0.461116
Finished training it 63488/76743 of epoch 8, 89.98 ms/it, loss 0.461271
Finished training it 64512/76743 of epoch 8, 90.56 ms/it, loss 0.460020
Finished training it 64512/76743 of epoch 8, 90.61 ms/it, loss 0.463588
Finished training it 64512/76743 of epoch 8, 90.60 ms/it, loss 0.460846
Finished training it 64512/76743 of epoch 8, 90.74 ms/it, loss 0.461255
Finished training it 65536/76743 of epoch 8, 90.36 ms/it, loss 0.465100
Finished training it 65536/76743 of epoch 8, 90.34 ms/it, loss 0.462452
Finished training it 65536/76743 of epoch 8, 90.35 ms/it, loss 0.462375
Finished training it 65536/76743 of epoch 8, 90.32 ms/it, loss 0.463164
Finished training it 66560/76743 of epoch 8, 90.16 ms/it, loss 0.462556
Finished training it 66560/76743 of epoch 8, 90.34 ms/it, loss 0.461567
Finished training it 66560/76743 of epoch 8, 90.32 ms/it, loss 0.462042
Finished training it 66560/76743 of epoch 8, 90.24 ms/it, loss 0.462730
Finished training it 67584/76743 of epoch 8, 90.94 ms/it, loss 0.463114
Finished training it 67584/76743 of epoch 8, 90.87 ms/it, loss 0.465614
Finished training it 67584/76743 of epoch 8, 90.88 ms/it, loss 0.462952
Finished training it 67584/76743 of epoch 8, 90.85 ms/it, loss 0.461589
Finished training it 68608/76743 of epoch 8, 90.72 ms/it, loss 0.461756
Finished training it 68608/76743 of epoch 8, 90.71 ms/it, loss 0.462942
Finished training it 68608/76743 of epoch 8, 90.57 ms/it, loss 0.462346
Finished training it 68608/76743 of epoch 8, 90.50 ms/it, loss 0.461904
Finished training it 69632/76743 of epoch 8, 90.16 ms/it, loss 0.460530
Finished training it 69632/76743 of epoch 8, 89.94 ms/it, loss 0.463335
Finished training it 69632/76743 of epoch 8, 89.93 ms/it, loss 0.464039
Finished training it 69632/76743 of epoch 8, 90.17 ms/it, loss 0.462834
Finished training it 70656/76743 of epoch 8, 87.97 ms/it, loss 0.460970
Finished training it 70656/76743 of epoch 8, 88.15 ms/it, loss 0.462944
Finished training it 70656/76743 of epoch 8, 87.89 ms/it, loss 0.462664
Finished training it 70656/76743 of epoch 8, 88.04 ms/it, loss 0.461946
Finished training it 71680/76743 of epoch 8, 87.78 ms/it, loss 0.462877
Finished training it 71680/76743 of epoch 8, 87.80 ms/it, loss 0.463696
Finished training it 71680/76743 of epoch 8, 88.25 ms/it, loss 0.459393
Finished training it 71680/76743 of epoch 8, 88.45 ms/it, loss 0.459700
Testing at - 71680/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554163.0
get out
0 has test check 2554163.0 and sample count 3274240
 accuracy 78.008 %, best 78.008 %, roc auc score 0.7819, best 0.7821
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554163.0
get out
2 has test check 2554163.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 90.60 ms/it, loss 0.462464
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 8, 90.68 ms/it, loss 0.464099
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554163.0
get out
3 has test check 2554163.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 90.53 ms/it, loss 0.461282
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554163.0
get out
1 has test check 2554163.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 90.74 ms/it, loss 0.462582
Finished training it 73728/76743 of epoch 8, 90.55 ms/it, loss 0.462956
Finished training it 73728/76743 of epoch 8, 90.62 ms/it, loss 0.461250
Finished training it 73728/76743 of epoch 8, 90.66 ms/it, loss 0.461585
Finished training it 73728/76743 of epoch 8, 90.65 ms/it, loss 0.464819
Finished training it 74752/76743 of epoch 8, 90.22 ms/it, loss 0.462854
Finished training it 74752/76743 of epoch 8, 90.29 ms/it, loss 0.463360
Finished training it 74752/76743 of epoch 8, 90.20 ms/it, loss 0.462379
Finished training it 74752/76743 of epoch 8, 90.28 ms/it, loss 0.462795
Finished training it 75776/76743 of epoch 8, 97.86 ms/it, loss 0.462899
Finished training it 75776/76743 of epoch 8, 97.97 ms/it, loss 0.463356
Finished training it 75776/76743 of epoch 8, 97.97 ms/it, loss 0.461887
Finished training it 75776/76743 of epoch 8, 97.68 ms/it, loss 0.464024
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 94.09 ms/it, loss 0.459283
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 94.47 ms/it, loss 0.464177
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 94.09 ms/it, loss 0.462744
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 94.28 ms/it, loss 0.463010
Finished training it 2048/76743 of epoch 9, 90.70 ms/it, loss 0.462155
Finished training it 2048/76743 of epoch 9, 90.80 ms/it, loss 0.462147
Finished training it 2048/76743 of epoch 9, 90.70 ms/it, loss 0.459658
Finished training it 2048/76743 of epoch 9, 90.69 ms/it, loss 0.463846
Finished training it 3072/76743 of epoch 9, 90.89 ms/it, loss 0.460469
Finished training it 3072/76743 of epoch 9, 90.58 ms/it, loss 0.459109
Finished training it 3072/76743 of epoch 9, 90.64 ms/it, loss 0.462377
Finished training it 3072/76743 of epoch 9, 90.80 ms/it, loss 0.460045
Finished training it 4096/76743 of epoch 9, 87.91 ms/it, loss 0.464541
Finished training it 4096/76743 of epoch 9, 88.03 ms/it, loss 0.463130
Finished training it 4096/76743 of epoch 9, 87.80 ms/it, loss 0.462230
Finished training it 4096/76743 of epoch 9, 88.13 ms/it, loss 0.461953
Finished training it 5120/76743 of epoch 9, 87.92 ms/it, loss 0.463954
Finished training it 5120/76743 of epoch 9, 88.04 ms/it, loss 0.462250
Finished training it 5120/76743 of epoch 9, 88.26 ms/it, loss 0.459716
Finished training it 5120/76743 of epoch 9, 87.79 ms/it, loss 0.461866
Finished training it 6144/76743 of epoch 9, 88.03 ms/it, loss 0.464068
Finished training it 6144/76743 of epoch 9, 88.14 ms/it, loss 0.461295
Finished training it 6144/76743 of epoch 9, 87.62 ms/it, loss 0.461002
Finished training it 6144/76743 of epoch 9, 87.59 ms/it, loss 0.460334
Finished training it 7168/76743 of epoch 9, 88.27 ms/it, loss 0.463812
Finished training it 7168/76743 of epoch 9, 87.89 ms/it, loss 0.462338
Finished training it 7168/76743 of epoch 9, 87.91 ms/it, loss 0.462794
Finished training it 7168/76743 of epoch 9, 88.43 ms/it, loss 0.465367
Finished training it 8192/76743 of epoch 9, 88.01 ms/it, loss 0.461944
Finished training it 8192/76743 of epoch 9, 88.25 ms/it, loss 0.463437
Finished training it 8192/76743 of epoch 9, 88.04 ms/it, loss 0.463061
Finished training it 8192/76743 of epoch 9, 88.26 ms/it, loss 0.461550
Finished training it 9216/76743 of epoch 9, 88.13 ms/it, loss 0.462245
Finished training it 9216/76743 of epoch 9, 88.19 ms/it, loss 0.463269
Finished training it 9216/76743 of epoch 9, 87.87 ms/it, loss 0.463614
Finished training it 9216/76743 of epoch 9, 87.82 ms/it, loss 0.464105
Finished training it 10240/76743 of epoch 9, 87.73 ms/it, loss 0.460298
Finished training it 10240/76743 of epoch 9, 87.59 ms/it, loss 0.462474
Finished training it 10240/76743 of epoch 9, 87.67 ms/it, loss 0.463222
Finished training it 10240/76743 of epoch 9, 87.90 ms/it, loss 0.462028
Testing at - 10240/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554275.0
get out
0 has test check 2554275.0 and sample count 3274240
 accuracy 78.011 %, best 78.011 %, roc auc score 0.7819, best 0.7821
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554275.0
get out
3 has test check 2554275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 90.98 ms/it, loss 0.460012
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554275.0
get out
1 has test check 2554275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 90.96 ms/it, loss 0.461139
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 9, 90.84 ms/it, loss 0.462666
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554275.0
get out
2 has test check 2554275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 91.09 ms/it, loss 0.460670
Finished training it 12288/76743 of epoch 9, 90.40 ms/it, loss 0.463594
Finished training it 12288/76743 of epoch 9, 90.39 ms/it, loss 0.459997
Finished training it 12288/76743 of epoch 9, 90.37 ms/it, loss 0.463421
Finished training it 12288/76743 of epoch 9, 90.47 ms/it, loss 0.463240
Finished training it 13312/76743 of epoch 9, 90.64 ms/it, loss 0.463701
Finished training it 13312/76743 of epoch 9, 90.70 ms/it, loss 0.462093
Finished training it 13312/76743 of epoch 9, 90.62 ms/it, loss 0.462150
Finished training it 13312/76743 of epoch 9, 90.70 ms/it, loss 0.463939
Finished training it 14336/76743 of epoch 9, 88.97 ms/it, loss 0.462602
Finished training it 14336/76743 of epoch 9, 89.25 ms/it, loss 0.463604
Finished training it 14336/76743 of epoch 9, 89.00 ms/it, loss 0.459726
Finished training it 14336/76743 of epoch 9, 89.40 ms/it, loss 0.461261
Finished training it 15360/76743 of epoch 9, 87.95 ms/it, loss 0.462022
Finished training it 15360/76743 of epoch 9, 87.29 ms/it, loss 0.460698
Finished training it 15360/76743 of epoch 9, 87.87 ms/it, loss 0.463054
Finished training it 15360/76743 of epoch 9, 87.45 ms/it, loss 0.461608
Finished training it 16384/76743 of epoch 9, 88.10 ms/it, loss 0.462254
Finished training it 16384/76743 of epoch 9, 88.06 ms/it, loss 0.460551
Finished training it 16384/76743 of epoch 9, 87.72 ms/it, loss 0.460481
Finished training it 16384/76743 of epoch 9, 87.94 ms/it, loss 0.464314
Finished training it 17408/76743 of epoch 9, 86.85 ms/it, loss 0.461225
Finished training it 17408/76743 of epoch 9, 87.22 ms/it, loss 0.463864
Finished training it 17408/76743 of epoch 9, 86.78 ms/it, loss 0.463419
Finished training it 17408/76743 of epoch 9, 87.51 ms/it, loss 0.459671
Finished training it 18432/76743 of epoch 9, 87.67 ms/it, loss 0.463265
Finished training it 18432/76743 of epoch 9, 87.73 ms/it, loss 0.462435
Finished training it 18432/76743 of epoch 9, 87.96 ms/it, loss 0.463393
Finished training it 18432/76743 of epoch 9, 88.22 ms/it, loss 0.462775
Finished training it 19456/76743 of epoch 9, 87.90 ms/it, loss 0.462606
Finished training it 19456/76743 of epoch 9, 87.48 ms/it, loss 0.463102
Finished training it 19456/76743 of epoch 9, 87.38 ms/it, loss 0.464487
Finished training it 19456/76743 of epoch 9, 87.47 ms/it, loss 0.462397
Finished training it 20480/76743 of epoch 9, 88.49 ms/it, loss 0.463142
Finished training it 20480/76743 of epoch 9, 88.50 ms/it, loss 0.464086
Finished training it 20480/76743 of epoch 9, 88.22 ms/it, loss 0.464347
Finished training it 20480/76743 of epoch 9, 88.22 ms/it, loss 0.464365
Testing at - 20480/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553644.0
get out
0 has test check 2553644.0 and sample count 3274240
 accuracy 77.992 %, best 78.011 %, roc auc score 0.7820, best 0.7821
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553644.0
get out
2 has test check 2553644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 90.70 ms/it, loss 0.459415
Finished training it 21504/76743 of epoch 9, 90.85 ms/it, loss 0.460324
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553644.0
get out
1 has test check 2553644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 90.60 ms/it, loss 0.462972
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553644.0
get out
3 has test check 2553644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 90.83 ms/it, loss 0.461250
Finished training it 22528/76743 of epoch 9, 90.37 ms/it, loss 0.462703
Finished training it 22528/76743 of epoch 9, 90.31 ms/it, loss 0.463034
Finished training it 22528/76743 of epoch 9, 90.38 ms/it, loss 0.461100
Finished training it 22528/76743 of epoch 9, 90.43 ms/it, loss 0.459421
Finished training it 23552/76743 of epoch 9, 90.88 ms/it, loss 0.460848
Finished training it 23552/76743 of epoch 9, 90.84 ms/it, loss 0.461138
Finished training it 23552/76743 of epoch 9, 90.79 ms/it, loss 0.461566
Finished training it 23552/76743 of epoch 9, 90.98 ms/it, loss 0.458902
Finished training it 24576/76743 of epoch 9, 100.22 ms/it, loss 0.462531
Finished training it 24576/76743 of epoch 9, 100.32 ms/it, loss 0.461144
Finished training it 24576/76743 of epoch 9, 100.23 ms/it, loss 0.460234
Finished training it 24576/76743 of epoch 9, 100.74 ms/it, loss 0.466706
Finished training it 25600/76743 of epoch 9, 87.58 ms/it, loss 0.464519
Finished training it 25600/76743 of epoch 9, 87.36 ms/it, loss 0.462219
Finished training it 25600/76743 of epoch 9, 87.52 ms/it, loss 0.462849
Finished training it 25600/76743 of epoch 9, 87.29 ms/it, loss 0.461629
Finished training it 26624/76743 of epoch 9, 87.04 ms/it, loss 0.462708
Finished training it 26624/76743 of epoch 9, 87.05 ms/it, loss 0.461413
Finished training it 26624/76743 of epoch 9, 87.38 ms/it, loss 0.462064
Finished training it 26624/76743 of epoch 9, 87.36 ms/it, loss 0.460565
Finished training it 27648/76743 of epoch 9, 87.61 ms/it, loss 0.464422
Finished training it 27648/76743 of epoch 9, 87.79 ms/it, loss 0.463534
Finished training it 27648/76743 of epoch 9, 87.91 ms/it, loss 0.462251
Finished training it 27648/76743 of epoch 9, 87.45 ms/it, loss 0.462834
Finished training it 28672/76743 of epoch 9, 87.61 ms/it, loss 0.460574
Finished training it 28672/76743 of epoch 9, 87.10 ms/it, loss 0.463062
Finished training it 28672/76743 of epoch 9, 87.49 ms/it, loss 0.464308
Finished training it 28672/76743 of epoch 9, 87.12 ms/it, loss 0.461247
Finished training it 29696/76743 of epoch 9, 88.21 ms/it, loss 0.462686
Finished training it 29696/76743 of epoch 9, 87.93 ms/it, loss 0.460850
Finished training it 29696/76743 of epoch 9, 87.83 ms/it, loss 0.461537
Finished training it 29696/76743 of epoch 9, 88.35 ms/it, loss 0.461720
Finished training it 30720/76743 of epoch 9, 88.36 ms/it, loss 0.456593
Finished training it 30720/76743 of epoch 9, 87.96 ms/it, loss 0.461892
Finished training it 30720/76743 of epoch 9, 87.82 ms/it, loss 0.462155
Finished training it 30720/76743 of epoch 9, 88.11 ms/it, loss 0.460547
Testing at - 30720/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553581.0
get out
0 has test check 2553581.0 and sample count 3274240
 accuracy 77.990 %, best 78.011 %, roc auc score 0.7821, best 0.7821
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553581.0
get out
2 has test check 2553581.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 90.51 ms/it, loss 0.463976
Finished training it 31744/76743 of epoch 9, 90.70 ms/it, loss 0.463809
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553581.0
get out
1 has test check 2553581.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 90.68 ms/it, loss 0.461761
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553581.0
get out
3 has test check 2553581.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 90.47 ms/it, loss 0.464584
Finished training it 32768/76743 of epoch 9, 90.89 ms/it, loss 0.460938
Finished training it 32768/76743 of epoch 9, 90.85 ms/it, loss 0.463029
Finished training it 32768/76743 of epoch 9, 90.88 ms/it, loss 0.462496
Finished training it 32768/76743 of epoch 9, 90.97 ms/it, loss 0.461025
Finished training it 33792/76743 of epoch 9, 90.11 ms/it, loss 0.463438
Finished training it 33792/76743 of epoch 9, 90.14 ms/it, loss 0.464232
Finished training it 33792/76743 of epoch 9, 90.16 ms/it, loss 0.463308
Finished training it 33792/76743 of epoch 9, 90.27 ms/it, loss 0.462184
Finished training it 34816/76743 of epoch 9, 90.78 ms/it, loss 0.460321
Finished training it 34816/76743 of epoch 9, 90.68 ms/it, loss 0.460224
Finished training it 34816/76743 of epoch 9, 90.76 ms/it, loss 0.463243
Finished training it 34816/76743 of epoch 9, 90.59 ms/it, loss 0.462953
Finished training it 35840/76743 of epoch 9, 90.42 ms/it, loss 0.461880
Finished training it 35840/76743 of epoch 9, 90.24 ms/it, loss 0.461712
Finished training it 35840/76743 of epoch 9, 90.44 ms/it, loss 0.463760
Finished training it 35840/76743 of epoch 9, 90.26 ms/it, loss 0.461775
Finished training it 36864/76743 of epoch 9, 90.53 ms/it, loss 0.462381
Finished training it 36864/76743 of epoch 9, 90.37 ms/it, loss 0.462753
Finished training it 36864/76743 of epoch 9, 90.32 ms/it, loss 0.462758
Finished training it 36864/76743 of epoch 9, 90.37 ms/it, loss 0.459572
Finished training it 37888/76743 of epoch 9, 89.91 ms/it, loss 0.461297
Finished training it 37888/76743 of epoch 9, 89.97 ms/it, loss 0.462564
Finished training it 37888/76743 of epoch 9, 90.28 ms/it, loss 0.458400
Finished training it 37888/76743 of epoch 9, 90.15 ms/it, loss 0.461529
Finished training it 38912/76743 of epoch 9, 90.39 ms/it, loss 0.461152
Finished training it 38912/76743 of epoch 9, 90.09 ms/it, loss 0.462016
Finished training it 38912/76743 of epoch 9, 90.15 ms/it, loss 0.463122
Finished training it 38912/76743 of epoch 9, 90.27 ms/it, loss 0.462949
Finished training it 39936/76743 of epoch 9, 89.54 ms/it, loss 0.463609
Finished training it 39936/76743 of epoch 9, 89.75 ms/it, loss 0.460145
Finished training it 39936/76743 of epoch 9, 89.76 ms/it, loss 0.459326
Finished training it 39936/76743 of epoch 9, 89.58 ms/it, loss 0.461459
Finished training it 40960/76743 of epoch 9, 88.42 ms/it, loss 0.459789
Finished training it 40960/76743 of epoch 9, 87.76 ms/it, loss 0.462092
Finished training it 40960/76743 of epoch 9, 87.79 ms/it, loss 0.463085
Finished training it 40960/76743 of epoch 9, 88.38 ms/it, loss 0.458341
Testing at - 40960/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554158.0
get out
0 has test check 2554158.0 and sample count 3274240
 accuracy 78.008 %, best 78.011 %, roc auc score 0.7824, best 0.7824
Finished training it 41984/76743 of epoch 9, 90.59 ms/it, loss 0.462776
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554158.0
get out
3 has test check 2554158.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.49 ms/it, loss 0.460424
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554158.0
get out
1 has test check 2554158.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.49 ms/it, loss 0.461015
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554158.0
get out
2 has test check 2554158.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.52 ms/it, loss 0.460590
Finished training it 43008/76743 of epoch 9, 90.50 ms/it, loss 0.464443
Finished training it 43008/76743 of epoch 9, 90.57 ms/it, loss 0.460514
Finished training it 43008/76743 of epoch 9, 90.49 ms/it, loss 0.459032
Finished training it 43008/76743 of epoch 9, 90.54 ms/it, loss 0.463170
Finished training it 44032/76743 of epoch 9, 90.24 ms/it, loss 0.462408
Finished training it 44032/76743 of epoch 9, 90.31 ms/it, loss 0.462726
Finished training it 44032/76743 of epoch 9, 90.26 ms/it, loss 0.461131
Finished training it 44032/76743 of epoch 9, 90.37 ms/it, loss 0.462444
Finished training it 45056/76743 of epoch 9, 103.31 ms/it, loss 0.463143
Finished training it 45056/76743 of epoch 9, 103.25 ms/it, loss 0.460497
Finished training it 45056/76743 of epoch 9, 103.29 ms/it, loss 0.464454
Finished training it 45056/76743 of epoch 9, 103.45 ms/it, loss 0.462917
Finished training it 46080/76743 of epoch 9, 90.76 ms/it, loss 0.461120
Finished training it 46080/76743 of epoch 9, 90.93 ms/it, loss 0.462961
Finished training it 46080/76743 of epoch 9, 90.87 ms/it, loss 0.460988
Finished training it 46080/76743 of epoch 9, 90.92 ms/it, loss 0.462490
Finished training it 47104/76743 of epoch 9, 90.35 ms/it, loss 0.460143
Finished training it 47104/76743 of epoch 9, 90.27 ms/it, loss 0.459252
Finished training it 47104/76743 of epoch 9, 90.25 ms/it, loss 0.458963
Finished training it 47104/76743 of epoch 9, 90.26 ms/it, loss 0.459978
Finished training it 48128/76743 of epoch 9, 90.77 ms/it, loss 0.460125
Finished training it 48128/76743 of epoch 9, 90.82 ms/it, loss 0.459785
Finished training it 48128/76743 of epoch 9, 90.73 ms/it, loss 0.463892
Finished training it 48128/76743 of epoch 9, 90.80 ms/it, loss 0.459877
Finished training it 49152/76743 of epoch 9, 90.86 ms/it, loss 0.460617
Finished training it 49152/76743 of epoch 9, 90.86 ms/it, loss 0.459262
Finished training it 49152/76743 of epoch 9, 90.94 ms/it, loss 0.462215
Finished training it 49152/76743 of epoch 9, 90.91 ms/it, loss 0.459913
Finished training it 50176/76743 of epoch 9, 90.92 ms/it, loss 0.463881
Finished training it 50176/76743 of epoch 9, 90.58 ms/it, loss 0.460840
Finished training it 50176/76743 of epoch 9, 90.65 ms/it, loss 0.463442
Finished training it 50176/76743 of epoch 9, 90.69 ms/it, loss 0.461374
Finished training it 51200/76743 of epoch 9, 89.36 ms/it, loss 0.461845
Finished training it 51200/76743 of epoch 9, 89.31 ms/it, loss 0.462727
Finished training it 51200/76743 of epoch 9, 89.54 ms/it, loss 0.459904
Finished training it 51200/76743 of epoch 9, 89.15 ms/it, loss 0.464802
Testing at - 51200/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554368.0
get out
0 has test check 2554368.0 and sample count 3274240
 accuracy 78.014 %, best 78.014 %, roc auc score 0.7825, best 0.7825
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554368.0
get out
1 has test check 2554368.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 91.06 ms/it, loss 0.462289
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554368.0
get out
2 has test check 2554368.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 90.89 ms/it, loss 0.460632
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 9, 91.02 ms/it, loss 0.458843
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554368.0
get out
3 has test check 2554368.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 90.82 ms/it, loss 0.461155
Finished training it 53248/76743 of epoch 9, 90.23 ms/it, loss 0.461642
Finished training it 53248/76743 of epoch 9, 90.31 ms/it, loss 0.461208
Finished training it 53248/76743 of epoch 9, 90.26 ms/it, loss 0.462273
Finished training it 53248/76743 of epoch 9, 90.20 ms/it, loss 0.462201
Finished training it 54272/76743 of epoch 9, 89.84 ms/it, loss 0.463780
Finished training it 54272/76743 of epoch 9, 89.94 ms/it, loss 0.462170
Finished training it 54272/76743 of epoch 9, 89.84 ms/it, loss 0.460054
Finished training it 54272/76743 of epoch 9, 90.02 ms/it, loss 0.460984
Finished training it 55296/76743 of epoch 9, 90.39 ms/it, loss 0.461036
Finished training it 55296/76743 of epoch 9, 90.38 ms/it, loss 0.459324
Finished training it 55296/76743 of epoch 9, 90.23 ms/it, loss 0.464945
Finished training it 55296/76743 of epoch 9, 90.27 ms/it, loss 0.460989
Finished training it 56320/76743 of epoch 9, 90.55 ms/it, loss 0.459525
Finished training it 56320/76743 of epoch 9, 90.57 ms/it, loss 0.461911
Finished training it 56320/76743 of epoch 9, 90.62 ms/it, loss 0.462484
Finished training it 56320/76743 of epoch 9, 90.57 ms/it, loss 0.457741
Finished training it 57344/76743 of epoch 9, 90.15 ms/it, loss 0.465634
Finished training it 57344/76743 of epoch 9, 90.09 ms/it, loss 0.460324
Finished training it 57344/76743 of epoch 9, 90.06 ms/it, loss 0.462764
Finished training it 57344/76743 of epoch 9, 90.07 ms/it, loss 0.459580
Finished training it 58368/76743 of epoch 9, 90.72 ms/it, loss 0.463196
Finished training it 58368/76743 of epoch 9, 90.66 ms/it, loss 0.461042
Finished training it 58368/76743 of epoch 9, 90.68 ms/it, loss 0.461681
Finished training it 58368/76743 of epoch 9, 90.62 ms/it, loss 0.463762
Finished training it 59392/76743 of epoch 9, 90.62 ms/it, loss 0.460241
Finished training it 59392/76743 of epoch 9, 90.60 ms/it, loss 0.461590
Finished training it 59392/76743 of epoch 9, 90.73 ms/it, loss 0.463461
Finished training it 59392/76743 of epoch 9, 90.75 ms/it, loss 0.460214
Finished training it 60416/76743 of epoch 9, 90.52 ms/it, loss 0.459668
Finished training it 60416/76743 of epoch 9, 90.38 ms/it, loss 0.460220
Finished training it 60416/76743 of epoch 9, 90.32 ms/it, loss 0.458800
Finished training it 60416/76743 of epoch 9, 90.43 ms/it, loss 0.459897
Finished training it 61440/76743 of epoch 9, 89.66 ms/it, loss 0.460178
Finished training it 61440/76743 of epoch 9, 89.58 ms/it, loss 0.462586
Finished training it 61440/76743 of epoch 9, 89.77 ms/it, loss 0.460891
Finished training it 61440/76743 of epoch 9, 89.86 ms/it, loss 0.463073
Testing at - 61440/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555834.0
get out
0 has test check 2555834.0 and sample count 3274240
 accuracy 78.059 %, best 78.059 %, roc auc score 0.7829, best 0.7829
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555834.0
get out
2 has test check 2555834.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 90.33 ms/it, loss 0.459469
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555834.0
get out
1 has test check 2555834.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 90.40 ms/it, loss 0.462398
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 9, 90.44 ms/it, loss 0.461978
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555834.0
get out
3 has test check 2555834.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 90.26 ms/it, loss 0.461172
Finished training it 63488/76743 of epoch 9, 90.50 ms/it, loss 0.460067
Finished training it 63488/76743 of epoch 9, 90.50 ms/it, loss 0.461120
Finished training it 63488/76743 of epoch 9, 90.46 ms/it, loss 0.462990
Finished training it 63488/76743 of epoch 9, 90.57 ms/it, loss 0.460344
Finished training it 64512/76743 of epoch 9, 90.11 ms/it, loss 0.459292
Finished training it 64512/76743 of epoch 9, 90.25 ms/it, loss 0.460037
Finished training it 64512/76743 of epoch 9, 90.27 ms/it, loss 0.462685
Finished training it 64512/76743 of epoch 9, 89.97 ms/it, loss 0.459853
Finished training it 65536/76743 of epoch 9, 101.81 ms/it, loss 0.462118
Finished training it 65536/76743 of epoch 9, 101.60 ms/it, loss 0.461527
Finished training it 65536/76743 of epoch 9, 101.74 ms/it, loss 0.464167
Finished training it 65536/76743 of epoch 9, 101.74 ms/it, loss 0.462245
Finished training it 66560/76743 of epoch 9, 90.19 ms/it, loss 0.460992
Finished training it 66560/76743 of epoch 9, 90.24 ms/it, loss 0.461287
Finished training it 66560/76743 of epoch 9, 90.13 ms/it, loss 0.461975
Finished training it 66560/76743 of epoch 9, 90.11 ms/it, loss 0.461364
Finished training it 67584/76743 of epoch 9, 90.42 ms/it, loss 0.460822
Finished training it 67584/76743 of epoch 9, 90.36 ms/it, loss 0.462363
Finished training it 67584/76743 of epoch 9, 90.20 ms/it, loss 0.462151
Finished training it 67584/76743 of epoch 9, 90.24 ms/it, loss 0.464576
Finished training it 68608/76743 of epoch 9, 90.70 ms/it, loss 0.460854
Finished training it 68608/76743 of epoch 9, 90.46 ms/it, loss 0.461886
Finished training it 68608/76743 of epoch 9, 90.66 ms/it, loss 0.461589
Finished training it 68608/76743 of epoch 9, 90.43 ms/it, loss 0.460919
Finished training it 69632/76743 of epoch 9, 90.99 ms/it, loss 0.463204
Finished training it 69632/76743 of epoch 9, 90.87 ms/it, loss 0.462598
Finished training it 69632/76743 of epoch 9, 90.83 ms/it, loss 0.459700
Finished training it 69632/76743 of epoch 9, 90.94 ms/it, loss 0.461891
Finished training it 70656/76743 of epoch 9, 90.71 ms/it, loss 0.460987
Finished training it 70656/76743 of epoch 9, 90.55 ms/it, loss 0.462403
Finished training it 70656/76743 of epoch 9, 90.59 ms/it, loss 0.461624
Finished training it 70656/76743 of epoch 9, 90.68 ms/it, loss 0.459822
Finished training it 71680/76743 of epoch 9, 90.14 ms/it, loss 0.461830
Finished training it 71680/76743 of epoch 9, 90.20 ms/it, loss 0.458769
Finished training it 71680/76743 of epoch 9, 90.09 ms/it, loss 0.458276
Finished training it 71680/76743 of epoch 9, 90.04 ms/it, loss 0.463012
Testing at - 71680/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555217.0
get out
0 has test check 2555217.0 and sample count 3274240
 accuracy 78.040 %, best 78.059 %, roc auc score 0.7827, best 0.7829
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555217.0
get out
1 has test check 2555217.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 90.89 ms/it, loss 0.461736
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555217.0
get out
2 has test check 2555217.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 90.80 ms/it, loss 0.461778
Finished training it 72704/76743 of epoch 9, 90.84 ms/it, loss 0.463338
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555217.0
get out
3 has test check 2555217.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 90.81 ms/it, loss 0.460141
Finished training it 73728/76743 of epoch 9, 90.53 ms/it, loss 0.460859
Finished training it 73728/76743 of epoch 9, 90.38 ms/it, loss 0.462296
Finished training it 73728/76743 of epoch 9, 90.46 ms/it, loss 0.463894
Finished training it 73728/76743 of epoch 9, 90.48 ms/it, loss 0.460497
Finished training it 74752/76743 of epoch 9, 90.41 ms/it, loss 0.462435
Finished training it 74752/76743 of epoch 9, 90.43 ms/it, loss 0.461985
Finished training it 74752/76743 of epoch 9, 90.43 ms/it, loss 0.461582
Finished training it 74752/76743 of epoch 9, 90.28 ms/it, loss 0.461870
Finished training it 75776/76743 of epoch 9, 90.49 ms/it, loss 0.463117
Finished training it 75776/76743 of epoch 9, 90.35 ms/it, loss 0.462191
Finished training it 75776/76743 of epoch 9, 90.17 ms/it, loss 0.461117
Finished training it 75776/76743 of epoch 9, 90.36 ms/it, loss 0.462085
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552986.0
get out
0 has test check 2552986.0 and sample count 3274240
 accuracy 77.972 %, best 78.059 %, roc auc score 0.7827, best 0.7829
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552986.0
get out
1 has test check 2552986.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552986.0
get out
3 has test check 2552986.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552986.0
get out
2 has test check 2552986.0 and sample count 3274240
