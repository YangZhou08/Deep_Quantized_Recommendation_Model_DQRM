Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 52.54 ms/it, loss 0.512828
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 52.38 ms/it, loss 0.514101
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.58 ms/it, loss 0.516538
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.05 ms/it, loss 0.516740
Finished training it 2048/76743 of epoch 0, 50.08 ms/it, loss 0.501260
Finished training it 2048/76743 of epoch 0, 49.08 ms/it, loss 0.499341
Finished training it 2048/76743 of epoch 0, 50.12 ms/it, loss 0.499895
Finished training it 2048/76743 of epoch 0, 49.57 ms/it, loss 0.497958
Finished training it 3072/76743 of epoch 0, 49.79 ms/it, loss 0.492216
Finished training it 3072/76743 of epoch 0, 48.95 ms/it, loss 0.493224
Finished training it 3072/76743 of epoch 0, 49.39 ms/it, loss 0.491151
Finished training it 3072/76743 of epoch 0, 49.78 ms/it, loss 0.490443
Finished training it 4096/76743 of epoch 0, 49.44 ms/it, loss 0.482082
Finished training it 4096/76743 of epoch 0, 49.66 ms/it, loss 0.483231
Finished training it 4096/76743 of epoch 0, 48.90 ms/it, loss 0.485679
Finished training it 4096/76743 of epoch 0, 49.93 ms/it, loss 0.484506
Finished training it 5120/76743 of epoch 0, 49.04 ms/it, loss 0.477646
Finished training it 5120/76743 of epoch 0, 49.18 ms/it, loss 0.477047
Finished training it 5120/76743 of epoch 0, 48.69 ms/it, loss 0.476388
Finished training it 5120/76743 of epoch 0, 48.30 ms/it, loss 0.476806
Finished training it 6144/76743 of epoch 0, 48.98 ms/it, loss 0.473945
Finished training it 6144/76743 of epoch 0, 48.26 ms/it, loss 0.474208
Finished training it 6144/76743 of epoch 0, 48.74 ms/it, loss 0.474490
Finished training it 6144/76743 of epoch 0, 48.99 ms/it, loss 0.473461
Finished training it 7168/76743 of epoch 0, 49.18 ms/it, loss 0.471718
Finished training it 7168/76743 of epoch 0, 49.39 ms/it, loss 0.469376
Finished training it 7168/76743 of epoch 0, 49.40 ms/it, loss 0.471542
Finished training it 7168/76743 of epoch 0, 48.80 ms/it, loss 0.469231
Finished training it 8192/76743 of epoch 0, 48.26 ms/it, loss 0.472002
Finished training it 8192/76743 of epoch 0, 48.73 ms/it, loss 0.467361
Finished training it 8192/76743 of epoch 0, 49.03 ms/it, loss 0.470459
Finished training it 8192/76743 of epoch 0, 49.03 ms/it, loss 0.471688
Finished training it 9216/76743 of epoch 0, 49.32 ms/it, loss 0.467121
Finished training it 9216/76743 of epoch 0, 49.41 ms/it, loss 0.463555
Finished training it 9216/76743 of epoch 0, 49.06 ms/it, loss 0.465907
Finished training it 9216/76743 of epoch 0, 48.49 ms/it, loss 0.462419
Finished training it 10240/76743 of epoch 0, 48.83 ms/it, loss 0.464093
Finished training it 10240/76743 of epoch 0, 48.88 ms/it, loss 0.464607
Finished training it 10240/76743 of epoch 0, 48.83 ms/it, loss 0.464289
Finished training it 10240/76743 of epoch 0, 48.20 ms/it, loss 0.463259
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555312.0
get out
0 has test check 2555312.0 and sample count 3274240
 accuracy 78.043 %, best 78.043 %, roc auc score 0.7846, best 0.7846
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 49.31 ms/it, loss 0.463981
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555312.0
get out
3 has test check 2555312.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 50.38 ms/it, loss 0.463420
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555312.0
get out
1 has test check 2555312.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 50.21 ms/it, loss 0.462776
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555312.0
get out
2 has test check 2555312.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 49.78 ms/it, loss 0.462724
Finished training it 12288/76743 of epoch 0, 49.13 ms/it, loss 0.463936
Finished training it 12288/76743 of epoch 0, 48.85 ms/it, loss 0.459318
Finished training it 12288/76743 of epoch 0, 49.06 ms/it, loss 0.463458
Finished training it 12288/76743 of epoch 0, 48.69 ms/it, loss 0.462592
Finished training it 13312/76743 of epoch 0, 48.89 ms/it, loss 0.461101
Finished training it 13312/76743 of epoch 0, 49.51 ms/it, loss 0.461178
Finished training it 13312/76743 of epoch 0, 49.42 ms/it, loss 0.463315
Finished training it 13312/76743 of epoch 0, 49.72 ms/it, loss 0.460834
Finished training it 14336/76743 of epoch 0, 49.24 ms/it, loss 0.458945
Finished training it 14336/76743 of epoch 0, 49.10 ms/it, loss 0.460353
Finished training it 14336/76743 of epoch 0, 48.51 ms/it, loss 0.460059
Finished training it 14336/76743 of epoch 0, 48.74 ms/it, loss 0.460699
Finished training it 15360/76743 of epoch 0, 55.29 ms/it, loss 0.458968
Finished training it 15360/76743 of epoch 0, 54.66 ms/it, loss 0.462570
Finished training it 15360/76743 of epoch 0, 54.32 ms/it, loss 0.457732
Finished training it 15360/76743 of epoch 0, 54.65 ms/it, loss 0.459759
Finished training it 16384/76743 of epoch 0, 48.67 ms/it, loss 0.457680
Finished training it 16384/76743 of epoch 0, 49.25 ms/it, loss 0.460194
Finished training it 16384/76743 of epoch 0, 49.13 ms/it, loss 0.459900
Finished training it 16384/76743 of epoch 0, 49.07 ms/it, loss 0.459535
Finished training it 17408/76743 of epoch 0, 49.11 ms/it, loss 0.459557
Finished training it 17408/76743 of epoch 0, 49.17 ms/it, loss 0.458866
Finished training it 17408/76743 of epoch 0, 48.96 ms/it, loss 0.457410
Finished training it 17408/76743 of epoch 0, 48.94 ms/it, loss 0.459420
Finished training it 18432/76743 of epoch 0, 48.50 ms/it, loss 0.458532
Finished training it 18432/76743 of epoch 0, 48.83 ms/it, loss 0.462066
Finished training it 18432/76743 of epoch 0, 48.89 ms/it, loss 0.459532
Finished training it 18432/76743 of epoch 0, 48.65 ms/it, loss 0.457934
Finished training it 19456/76743 of epoch 0, 49.38 ms/it, loss 0.457739
Finished training it 19456/76743 of epoch 0, 49.29 ms/it, loss 0.456736
Finished training it 19456/76743 of epoch 0, 49.33 ms/it, loss 0.457087
Finished training it 19456/76743 of epoch 0, 49.19 ms/it, loss 0.455251
Finished training it 20480/76743 of epoch 0, 49.03 ms/it, loss 0.458968
Finished training it 20480/76743 of epoch 0, 49.27 ms/it, loss 0.458518
Finished training it 20480/76743 of epoch 0, 48.76 ms/it, loss 0.460756
Finished training it 20480/76743 of epoch 0, 49.04 ms/it, loss 0.460013
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2565615.0
get out
0 has test check 2565615.0 and sample count 3274240
 accuracy 78.358 %, best 78.358 %, roc auc score 0.7914, best 0.7914
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2565615.0
get out
2 has test check 2565615.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.62 ms/it, loss 0.455629
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 48.48 ms/it, loss 0.457166
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2565615.0
get out
3 has test check 2565615.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.84 ms/it, loss 0.458499
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2565615.0
get out
1 has test check 2565615.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.84 ms/it, loss 0.456519
Finished training it 22528/76743 of epoch 0, 48.98 ms/it, loss 0.457858
Finished training it 22528/76743 of epoch 0, 49.15 ms/it, loss 0.457408
Finished training it 22528/76743 of epoch 0, 49.31 ms/it, loss 0.458127
Finished training it 22528/76743 of epoch 0, 48.90 ms/it, loss 0.454670
Finished training it 23552/76743 of epoch 0, 48.68 ms/it, loss 0.456682
Finished training it 23552/76743 of epoch 0, 48.23 ms/it, loss 0.451914
Finished training it 23552/76743 of epoch 0, 48.48 ms/it, loss 0.453338
Finished training it 23552/76743 of epoch 0, 48.33 ms/it, loss 0.456996
Finished training it 24576/76743 of epoch 0, 48.95 ms/it, loss 0.453467
Finished training it 24576/76743 of epoch 0, 48.60 ms/it, loss 0.457213
Finished training it 24576/76743 of epoch 0, 48.71 ms/it, loss 0.455641
Finished training it 24576/76743 of epoch 0, 48.79 ms/it, loss 0.456907
Finished training it 25600/76743 of epoch 0, 48.71 ms/it, loss 0.454650
Finished training it 25600/76743 of epoch 0, 48.68 ms/it, loss 0.454833
Finished training it 25600/76743 of epoch 0, 48.65 ms/it, loss 0.456278
Finished training it 25600/76743 of epoch 0, 48.66 ms/it, loss 0.456357
Finished training it 26624/76743 of epoch 0, 48.79 ms/it, loss 0.455559
Finished training it 26624/76743 of epoch 0, 48.77 ms/it, loss 0.454976
Finished training it 26624/76743 of epoch 0, 48.62 ms/it, loss 0.456042
Finished training it 26624/76743 of epoch 0, 48.46 ms/it, loss 0.451887
Finished training it 27648/76743 of epoch 0, 49.26 ms/it, loss 0.455099
Finished training it 27648/76743 of epoch 0, 49.10 ms/it, loss 0.454196
Finished training it 27648/76743 of epoch 0, 49.07 ms/it, loss 0.456506
Finished training it 27648/76743 of epoch 0, 49.01 ms/it, loss 0.452860
Finished training it 28672/76743 of epoch 0, 48.71 ms/it, loss 0.455419
Finished training it 28672/76743 of epoch 0, 48.84 ms/it, loss 0.454387
Finished training it 28672/76743 of epoch 0, 48.77 ms/it, loss 0.455254
Finished training it 28672/76743 of epoch 0, 49.14 ms/it, loss 0.455945
Finished training it 29696/76743 of epoch 0, 48.75 ms/it, loss 0.452623
Finished training it 29696/76743 of epoch 0, 48.72 ms/it, loss 0.451363
Finished training it 29696/76743 of epoch 0, 48.58 ms/it, loss 0.451972
Finished training it 29696/76743 of epoch 0, 48.95 ms/it, loss 0.452534
Finished training it 30720/76743 of epoch 0, 48.81 ms/it, loss 0.453615
Finished training it 30720/76743 of epoch 0, 48.48 ms/it, loss 0.453596
Finished training it 30720/76743 of epoch 0, 48.55 ms/it, loss 0.451116
Finished training it 30720/76743 of epoch 0, 48.85 ms/it, loss 0.452785
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2569684.0
get out
0 has test check 2569684.0 and sample count 3274240
 accuracy 78.482 %, best 78.482 %, roc auc score 0.7943, best 0.7943
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 49.20 ms/it, loss 0.451936
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2569684.0
get out
2 has test check 2569684.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 49.21 ms/it, loss 0.455408
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2569684.0
get out
1 has test check 2569684.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 49.13 ms/it, loss 0.454382
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2569684.0
get out
3 has test check 2569684.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 49.36 ms/it, loss 0.452358
Finished training it 32768/76743 of epoch 0, 48.76 ms/it, loss 0.451361
Finished training it 32768/76743 of epoch 0, 48.55 ms/it, loss 0.451429
Finished training it 32768/76743 of epoch 0, 48.52 ms/it, loss 0.455073
Finished training it 32768/76743 of epoch 0, 48.65 ms/it, loss 0.453112
Finished training it 33792/76743 of epoch 0, 49.30 ms/it, loss 0.455580
Finished training it 33792/76743 of epoch 0, 49.25 ms/it, loss 0.452378
Finished training it 33792/76743 of epoch 0, 48.94 ms/it, loss 0.451870
Finished training it 33792/76743 of epoch 0, 49.16 ms/it, loss 0.453161
Finished training it 34816/76743 of epoch 0, 49.14 ms/it, loss 0.454411
Finished training it 34816/76743 of epoch 0, 49.31 ms/it, loss 0.448789
Finished training it 34816/76743 of epoch 0, 49.20 ms/it, loss 0.452826
Finished training it 34816/76743 of epoch 0, 49.30 ms/it, loss 0.453972
Finished training it 35840/76743 of epoch 0, 53.54 ms/it, loss 0.451820
Finished training it 35840/76743 of epoch 0, 53.43 ms/it, loss 0.454309
Finished training it 35840/76743 of epoch 0, 53.54 ms/it, loss 0.449765
Finished training it 35840/76743 of epoch 0, 48.94 ms/it, loss 0.450033
Finished training it 36864/76743 of epoch 0, 49.57 ms/it, loss 0.452150
Finished training it 36864/76743 of epoch 0, 49.60 ms/it, loss 0.455216
Finished training it 36864/76743 of epoch 0, 54.38 ms/it, loss 0.453086
Finished training it 36864/76743 of epoch 0, 50.00 ms/it, loss 0.451598
Finished training it 37888/76743 of epoch 0, 48.69 ms/it, loss 0.454330
Finished training it 37888/76743 of epoch 0, 48.76 ms/it, loss 0.452268
Finished training it 37888/76743 of epoch 0, 48.93 ms/it, loss 0.452448
Finished training it 37888/76743 of epoch 0, 48.67 ms/it, loss 0.453262
Finished training it 38912/76743 of epoch 0, 48.74 ms/it, loss 0.452503
Finished training it 38912/76743 of epoch 0, 48.85 ms/it, loss 0.449367
Finished training it 38912/76743 of epoch 0, 48.82 ms/it, loss 0.452283
Finished training it 38912/76743 of epoch 0, 49.07 ms/it, loss 0.453046
Finished training it 39936/76743 of epoch 0, 48.72 ms/it, loss 0.450996
Finished training it 39936/76743 of epoch 0, 48.77 ms/it, loss 0.451265
Finished training it 39936/76743 of epoch 0, 48.43 ms/it, loss 0.452441
Finished training it 39936/76743 of epoch 0, 48.59 ms/it, loss 0.450901
Finished training it 40960/76743 of epoch 0, 48.67 ms/it, loss 0.451844
Finished training it 40960/76743 of epoch 0, 48.68 ms/it, loss 0.450880
Finished training it 40960/76743 of epoch 0, 48.65 ms/it, loss 0.452582
Finished training it 40960/76743 of epoch 0, 48.74 ms/it, loss 0.452215
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2572747.0
get out
0 has test check 2572747.0 and sample count 3274240
 accuracy 78.575 %, best 78.575 %, roc auc score 0.7967, best 0.7967
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2572747.0
get out
2 has test check 2572747.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 49.16 ms/it, loss 0.451722
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2572747.0
get out
1 has test check 2572747.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 49.26 ms/it, loss 0.450255
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 49.07 ms/it, loss 0.448337
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2572747.0
get out
3 has test check 2572747.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 49.05 ms/it, loss 0.452687
Finished training it 43008/76743 of epoch 0, 48.50 ms/it, loss 0.448977
Finished training it 43008/76743 of epoch 0, 48.82 ms/it, loss 0.451678
Finished training it 43008/76743 of epoch 0, 48.71 ms/it, loss 0.452895
Finished training it 43008/76743 of epoch 0, 48.61 ms/it, loss 0.451531
Finished training it 44032/76743 of epoch 0, 48.75 ms/it, loss 0.450609
Finished training it 44032/76743 of epoch 0, 48.70 ms/it, loss 0.450584
Finished training it 44032/76743 of epoch 0, 48.79 ms/it, loss 0.452336
Finished training it 44032/76743 of epoch 0, 48.61 ms/it, loss 0.450408
Finished training it 45056/76743 of epoch 0, 48.52 ms/it, loss 0.451537
Finished training it 45056/76743 of epoch 0, 48.85 ms/it, loss 0.450362
Finished training it 45056/76743 of epoch 0, 48.72 ms/it, loss 0.449710
Finished training it 45056/76743 of epoch 0, 48.82 ms/it, loss 0.450651
Finished training it 46080/76743 of epoch 0, 48.98 ms/it, loss 0.448558
Finished training it 46080/76743 of epoch 0, 48.71 ms/it, loss 0.448841
Finished training it 46080/76743 of epoch 0, 48.92 ms/it, loss 0.449963
Finished training it 46080/76743 of epoch 0, 48.71 ms/it, loss 0.449073
Finished training it 47104/76743 of epoch 0, 48.79 ms/it, loss 0.450707
Finished training it 47104/76743 of epoch 0, 49.09 ms/it, loss 0.448177
Finished training it 47104/76743 of epoch 0, 49.15 ms/it, loss 0.450224
Finished training it 47104/76743 of epoch 0, 48.88 ms/it, loss 0.449320
Finished training it 48128/76743 of epoch 0, 48.90 ms/it, loss 0.448897
Finished training it 48128/76743 of epoch 0, 48.84 ms/it, loss 0.450143
Finished training it 48128/76743 of epoch 0, 48.89 ms/it, loss 0.448937
Finished training it 48128/76743 of epoch 0, 48.91 ms/it, loss 0.449903
Finished training it 49152/76743 of epoch 0, 48.57 ms/it, loss 0.450121
Finished training it 49152/76743 of epoch 0, 48.85 ms/it, loss 0.450451
Finished training it 49152/76743 of epoch 0, 48.74 ms/it, loss 0.449864
Finished training it 49152/76743 of epoch 0, 48.97 ms/it, loss 0.449193
Finished training it 50176/76743 of epoch 0, 49.33 ms/it, loss 0.448407
Finished training it 50176/76743 of epoch 0, 49.55 ms/it, loss 0.449455
Finished training it 50176/76743 of epoch 0, 49.20 ms/it, loss 0.451379
Finished training it 50176/76743 of epoch 0, 49.26 ms/it, loss 0.450296
Finished training it 51200/76743 of epoch 0, 48.53 ms/it, loss 0.450210
Finished training it 51200/76743 of epoch 0, 48.85 ms/it, loss 0.449425
Finished training it 51200/76743 of epoch 0, 48.86 ms/it, loss 0.449389
Finished training it 51200/76743 of epoch 0, 48.92 ms/it, loss 0.449302
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2566141.0
get out
0 has test check 2566141.0 and sample count 3274240
 accuracy 78.374 %, best 78.575 %, roc auc score 0.7979, best 0.7979
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2566141.0
get out
2 has test check 2566141.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 49.03 ms/it, loss 0.450286
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2566141.0
get out
1 has test check 2566141.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 49.09 ms/it, loss 0.448376
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2566141.0
get out
3 has test check 2566141.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 49.15 ms/it, loss 0.450654
Finished training it 52224/76743 of epoch 0, 49.15 ms/it, loss 0.449804
Finished training it 53248/76743 of epoch 0, 48.91 ms/it, loss 0.449241
Finished training it 53248/76743 of epoch 0, 49.26 ms/it, loss 0.450327
Finished training it 53248/76743 of epoch 0, 48.75 ms/it, loss 0.449838
Finished training it 53248/76743 of epoch 0, 49.03 ms/it, loss 0.447962
Finished training it 54272/76743 of epoch 0, 49.12 ms/it, loss 0.449857
Finished training it 54272/76743 of epoch 0, 49.39 ms/it, loss 0.447967
Finished training it 54272/76743 of epoch 0, 49.26 ms/it, loss 0.448312
Finished training it 54272/76743 of epoch 0, 49.30 ms/it, loss 0.449666
Finished training it 55296/76743 of epoch 0, 49.71 ms/it, loss 0.449209
Finished training it 55296/76743 of epoch 0, 49.61 ms/it, loss 0.450119
Finished training it 55296/76743 of epoch 0, 49.45 ms/it, loss 0.452204
Finished training it 55296/76743 of epoch 0, 49.28 ms/it, loss 0.450614
Finished training it 56320/76743 of epoch 0, 49.41 ms/it, loss 0.450028
Finished training it 56320/76743 of epoch 0, 48.84 ms/it, loss 0.451149
Finished training it 56320/76743 of epoch 0, 49.21 ms/it, loss 0.450054
Finished training it 56320/76743 of epoch 0, 49.31 ms/it, loss 0.448862
Finished training it 57344/76743 of epoch 0, 49.05 ms/it, loss 0.447255
Finished training it 57344/76743 of epoch 0, 48.90 ms/it, loss 0.449740
Finished training it 57344/76743 of epoch 0, 49.29 ms/it, loss 0.447005
Finished training it 57344/76743 of epoch 0, 49.37 ms/it, loss 0.448825
Finished training it 58368/76743 of epoch 0, 45.65 ms/it, loss 0.449139
Finished training it 58368/76743 of epoch 0, 45.77 ms/it, loss 0.449270
Finished training it 58368/76743 of epoch 0, 45.54 ms/it, loss 0.447613
Finished training it 58368/76743 of epoch 0, 45.35 ms/it, loss 0.447203
Finished training it 59392/76743 of epoch 0, 37.92 ms/it, loss 0.450683
Finished training it 59392/76743 of epoch 0, 37.96 ms/it, loss 0.446924
Finished training it 59392/76743 of epoch 0, 38.01 ms/it, loss 0.448256
Finished training it 59392/76743 of epoch 0, 37.80 ms/it, loss 0.449402
Finished training it 60416/76743 of epoch 0, 47.09 ms/it, loss 0.445578
Finished training it 60416/76743 of epoch 0, 46.65 ms/it, loss 0.450147
Finished training it 60416/76743 of epoch 0, 47.14 ms/it, loss 0.445555
Finished training it 60416/76743 of epoch 0, 46.84 ms/it, loss 0.448274
Finished training it 61440/76743 of epoch 0, 48.96 ms/it, loss 0.446176
Finished training it 61440/76743 of epoch 0, 48.92 ms/it, loss 0.451186
Finished training it 61440/76743 of epoch 0, 48.78 ms/it, loss 0.448502
Finished training it 61440/76743 of epoch 0, 48.85 ms/it, loss 0.447116
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576462.0
get out
0 has test check 2576462.0 and sample count 3274240
 accuracy 78.689 %, best 78.689 %, roc auc score 0.7991, best 0.7991
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576462.0
get out
2 has test check 2576462.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 48.93 ms/it, loss 0.449853
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576462.0
get out
1 has test check 2576462.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 48.98 ms/it, loss 0.450350
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 49.00 ms/it, loss 0.450611
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576462.0
get out
3 has test check 2576462.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 49.06 ms/it, loss 0.448743
Finished training it 63488/76743 of epoch 0, 49.00 ms/it, loss 0.450142
Finished training it 63488/76743 of epoch 0, 48.90 ms/it, loss 0.449339
Finished training it 63488/76743 of epoch 0, 48.69 ms/it, loss 0.446862
Finished training it 63488/76743 of epoch 0, 49.05 ms/it, loss 0.447443
Finished training it 64512/76743 of epoch 0, 49.43 ms/it, loss 0.446788
Finished training it 64512/76743 of epoch 0, 48.98 ms/it, loss 0.449280
Finished training it 64512/76743 of epoch 0, 49.10 ms/it, loss 0.447498
Finished training it 64512/76743 of epoch 0, 49.34 ms/it, loss 0.451013
Finished training it 65536/76743 of epoch 0, 55.68 ms/it, loss 0.448203
Finished training it 65536/76743 of epoch 0, 55.92 ms/it, loss 0.445411
Finished training it 65536/76743 of epoch 0, 49.35 ms/it, loss 0.447685
Finished training it 65536/76743 of epoch 0, 55.98 ms/it, loss 0.449956
Finished training it 66560/76743 of epoch 0, 49.13 ms/it, loss 0.444743
Finished training it 66560/76743 of epoch 0, 49.82 ms/it, loss 0.446222
Finished training it 66560/76743 of epoch 0, 49.58 ms/it, loss 0.448111
Finished training it 66560/76743 of epoch 0, 54.54 ms/it, loss 0.446415
Finished training it 67584/76743 of epoch 0, 49.41 ms/it, loss 0.448609
Finished training it 67584/76743 of epoch 0, 49.42 ms/it, loss 0.444514
Finished training it 67584/76743 of epoch 0, 49.43 ms/it, loss 0.450071
Finished training it 67584/76743 of epoch 0, 49.33 ms/it, loss 0.448943
Finished training it 68608/76743 of epoch 0, 49.52 ms/it, loss 0.447433
Finished training it 68608/76743 of epoch 0, 49.47 ms/it, loss 0.445997
Finished training it 68608/76743 of epoch 0, 49.52 ms/it, loss 0.448159
Finished training it 68608/76743 of epoch 0, 49.63 ms/it, loss 0.447650
Finished training it 69632/76743 of epoch 0, 49.21 ms/it, loss 0.448377
Finished training it 69632/76743 of epoch 0, 49.29 ms/it, loss 0.448378
Finished training it 69632/76743 of epoch 0, 49.29 ms/it, loss 0.447463
Finished training it 69632/76743 of epoch 0, 49.09 ms/it, loss 0.448535
Finished training it 70656/76743 of epoch 0, 49.47 ms/it, loss 0.447551
Finished training it 70656/76743 of epoch 0, 49.22 ms/it, loss 0.446652
Finished training it 70656/76743 of epoch 0, 48.93 ms/it, loss 0.446891
Finished training it 70656/76743 of epoch 0, 49.27 ms/it, loss 0.451823
Finished training it 71680/76743 of epoch 0, 48.93 ms/it, loss 0.447160
Finished training it 71680/76743 of epoch 0, 48.79 ms/it, loss 0.445701
Finished training it 71680/76743 of epoch 0, 48.95 ms/it, loss 0.445863
Finished training it 71680/76743 of epoch 0, 48.91 ms/it, loss 0.447553
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577801.0
get out
0 has test check 2577801.0 and sample count 3274240
 accuracy 78.730 %, best 78.730 %, roc auc score 0.8001, best 0.8001
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577801.0
get out
3 has test check 2577801.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 49.61 ms/it, loss 0.445384
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 49.78 ms/it, loss 0.449415
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577801.0
get out
2 has test check 2577801.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 49.58 ms/it, loss 0.448171
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577801.0
get out
1 has test check 2577801.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 49.65 ms/it, loss 0.447858
Finished training it 73728/76743 of epoch 0, 49.53 ms/it, loss 0.448089
Finished training it 73728/76743 of epoch 0, 49.50 ms/it, loss 0.445106
Finished training it 73728/76743 of epoch 0, 49.46 ms/it, loss 0.446857
Finished training it 73728/76743 of epoch 0, 49.46 ms/it, loss 0.447585
Finished training it 74752/76743 of epoch 0, 49.10 ms/it, loss 0.444618
Finished training it 74752/76743 of epoch 0, 49.26 ms/it, loss 0.446468
Finished training it 74752/76743 of epoch 0, 49.46 ms/it, loss 0.448316
Finished training it 74752/76743 of epoch 0, 49.32 ms/it, loss 0.446804
Finished training it 75776/76743 of epoch 0, 49.68 ms/it, loss 0.447670
Finished training it 75776/76743 of epoch 0, 49.39 ms/it, loss 0.448732
Finished training it 75776/76743 of epoch 0, 49.58 ms/it, loss 0.447480
Finished training it 75776/76743 of epoch 0, 49.37 ms/it, loss 0.446477
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.36 ms/it, loss 0.476244
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.23 ms/it, loss 0.478983
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.27 ms/it, loss 0.478158
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.46 ms/it, loss 0.476160
Finished training it 2048/76743 of epoch 1, 88.45 ms/it, loss 0.499719
Finished training it 2048/76743 of epoch 1, 88.59 ms/it, loss 0.500790
Finished training it 2048/76743 of epoch 1, 88.58 ms/it, loss 0.501836
Finished training it 2048/76743 of epoch 1, 88.39 ms/it, loss 0.498344
Finished training it 3072/76743 of epoch 1, 88.42 ms/it, loss 0.497875
Finished training it 3072/76743 of epoch 1, 88.57 ms/it, loss 0.499999
Finished training it 3072/76743 of epoch 1, 88.70 ms/it, loss 0.497454
Finished training it 3072/76743 of epoch 1, 88.65 ms/it, loss 0.499137
Finished training it 4096/76743 of epoch 1, 88.30 ms/it, loss 0.498444
Finished training it 4096/76743 of epoch 1, 88.34 ms/it, loss 0.495609
Finished training it 4096/76743 of epoch 1, 88.29 ms/it, loss 0.497302
Finished training it 4096/76743 of epoch 1, 88.59 ms/it, loss 0.496826
Finished training it 5120/76743 of epoch 1, 88.02 ms/it, loss 0.494993
Finished training it 5120/76743 of epoch 1, 88.02 ms/it, loss 0.495900
Finished training it 5120/76743 of epoch 1, 88.15 ms/it, loss 0.494758
Finished training it 5120/76743 of epoch 1, 88.10 ms/it, loss 0.494933
Finished training it 6144/76743 of epoch 1, 88.23 ms/it, loss 0.495459
Finished training it 6144/76743 of epoch 1, 88.18 ms/it, loss 0.494187
Finished training it 6144/76743 of epoch 1, 88.12 ms/it, loss 0.494274
Finished training it 6144/76743 of epoch 1, 88.16 ms/it, loss 0.495109
Finished training it 7168/76743 of epoch 1, 88.01 ms/it, loss 0.493863
Finished training it 7168/76743 of epoch 1, 88.10 ms/it, loss 0.491183
Finished training it 7168/76743 of epoch 1, 88.13 ms/it, loss 0.494343
Finished training it 7168/76743 of epoch 1, 88.07 ms/it, loss 0.491790
Finished training it 8192/76743 of epoch 1, 88.34 ms/it, loss 0.491102
Finished training it 8192/76743 of epoch 1, 88.36 ms/it, loss 0.496073
Finished training it 8192/76743 of epoch 1, 88.54 ms/it, loss 0.494927
Finished training it 8192/76743 of epoch 1, 88.43 ms/it, loss 0.494434
Finished training it 9216/76743 of epoch 1, 87.91 ms/it, loss 0.487679
Finished training it 9216/76743 of epoch 1, 87.93 ms/it, loss 0.491379
Finished training it 9216/76743 of epoch 1, 87.96 ms/it, loss 0.492862
Finished training it 9216/76743 of epoch 1, 88.14 ms/it, loss 0.487524
Finished training it 10240/76743 of epoch 1, 87.83 ms/it, loss 0.490759
Finished training it 10240/76743 of epoch 1, 87.86 ms/it, loss 0.490881
Finished training it 10240/76743 of epoch 1, 87.74 ms/it, loss 0.489843
Finished training it 10240/76743 of epoch 1, 87.91 ms/it, loss 0.491502
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2508528.0
get out
0 has test check 2508528.0 and sample count 3274240
 accuracy 76.614 %, best 78.730 %, roc auc score 0.7460, best 0.8001
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2508528.0
get out
3 has test check 2508528.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.78 ms/it, loss 0.491190
Finished training it 11264/76743 of epoch 1, 87.74 ms/it, loss 0.490213
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2508528.0
get out
1 has test check 2508528.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.61 ms/it, loss 0.489916
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2508528.0
get out
2 has test check 2508528.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.47 ms/it, loss 0.489809
Finished training it 12288/76743 of epoch 1, 87.97 ms/it, loss 0.488025
Finished training it 12288/76743 of epoch 1, 88.10 ms/it, loss 0.490996
Finished training it 12288/76743 of epoch 1, 87.91 ms/it, loss 0.489946
Finished training it 12288/76743 of epoch 1, 88.21 ms/it, loss 0.490659
Finished training it 13312/76743 of epoch 1, 88.25 ms/it, loss 0.489107
Finished training it 13312/76743 of epoch 1, 88.00 ms/it, loss 0.492466
Finished training it 13312/76743 of epoch 1, 88.29 ms/it, loss 0.490125
Finished training it 13312/76743 of epoch 1, 87.81 ms/it, loss 0.488047
Finished training it 14336/76743 of epoch 1, 95.11 ms/it, loss 0.490123
Finished training it 14336/76743 of epoch 1, 94.75 ms/it, loss 0.488855
Finished training it 14336/76743 of epoch 1, 95.02 ms/it, loss 0.488405
Finished training it 14336/76743 of epoch 1, 94.93 ms/it, loss 0.489877
Finished training it 15360/76743 of epoch 1, 95.46 ms/it, loss 0.489110
Finished training it 15360/76743 of epoch 1, 96.07 ms/it, loss 0.490803
Finished training it 15360/76743 of epoch 1, 95.36 ms/it, loss 0.486993
Finished training it 15360/76743 of epoch 1, 95.20 ms/it, loss 0.489847
Finished training it 16384/76743 of epoch 1, 88.31 ms/it, loss 0.488790
Finished training it 16384/76743 of epoch 1, 88.03 ms/it, loss 0.489936
Finished training it 16384/76743 of epoch 1, 88.08 ms/it, loss 0.487223
Finished training it 16384/76743 of epoch 1, 88.40 ms/it, loss 0.489852
Finished training it 17408/76743 of epoch 1, 88.10 ms/it, loss 0.489427
Finished training it 17408/76743 of epoch 1, 87.85 ms/it, loss 0.486297
Finished training it 17408/76743 of epoch 1, 88.23 ms/it, loss 0.489274
Finished training it 17408/76743 of epoch 1, 87.76 ms/it, loss 0.489399
Finished training it 18432/76743 of epoch 1, 88.43 ms/it, loss 0.489619
Finished training it 18432/76743 of epoch 1, 87.99 ms/it, loss 0.487314
Finished training it 18432/76743 of epoch 1, 87.95 ms/it, loss 0.489002
Finished training it 18432/76743 of epoch 1, 88.50 ms/it, loss 0.491215
Finished training it 19456/76743 of epoch 1, 87.80 ms/it, loss 0.487214
Finished training it 19456/76743 of epoch 1, 88.11 ms/it, loss 0.487733
Finished training it 19456/76743 of epoch 1, 88.05 ms/it, loss 0.488405
Finished training it 19456/76743 of epoch 1, 87.99 ms/it, loss 0.484790
Finished training it 20480/76743 of epoch 1, 87.90 ms/it, loss 0.490461
Finished training it 20480/76743 of epoch 1, 88.14 ms/it, loss 0.488312
Finished training it 20480/76743 of epoch 1, 88.06 ms/it, loss 0.490461
Finished training it 20480/76743 of epoch 1, 87.87 ms/it, loss 0.489357
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2513348.0
get out
0 has test check 2513348.0 and sample count 3274240
 accuracy 76.761 %, best 78.730 %, roc auc score 0.7502, best 0.8001
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2513348.0
get out
3 has test check 2513348.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 88.21 ms/it, loss 0.488274
Finished training it 21504/76743 of epoch 1, 87.98 ms/it, loss 0.487804
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2513348.0
get out
2 has test check 2513348.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 88.01 ms/it, loss 0.487870
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2513348.0
get out
1 has test check 2513348.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 88.25 ms/it, loss 0.486956
Finished training it 22528/76743 of epoch 1, 87.84 ms/it, loss 0.485620
Finished training it 22528/76743 of epoch 1, 88.19 ms/it, loss 0.488532
Finished training it 22528/76743 of epoch 1, 88.11 ms/it, loss 0.488943
Finished training it 22528/76743 of epoch 1, 88.07 ms/it, loss 0.488282
Finished training it 23552/76743 of epoch 1, 87.53 ms/it, loss 0.483716
Finished training it 23552/76743 of epoch 1, 87.48 ms/it, loss 0.483440
Finished training it 23552/76743 of epoch 1, 87.31 ms/it, loss 0.487513
Finished training it 23552/76743 of epoch 1, 87.75 ms/it, loss 0.487154
Finished training it 24576/76743 of epoch 1, 88.57 ms/it, loss 0.487193
Finished training it 24576/76743 of epoch 1, 88.76 ms/it, loss 0.486116
Finished training it 24576/76743 of epoch 1, 88.49 ms/it, loss 0.488175
Finished training it 24576/76743 of epoch 1, 88.87 ms/it, loss 0.484109
Finished training it 25600/76743 of epoch 1, 81.43 ms/it, loss 0.485292
Finished training it 25600/76743 of epoch 1, 81.18 ms/it, loss 0.485231
Finished training it 25600/76743 of epoch 1, 81.14 ms/it, loss 0.486995
Finished training it 25600/76743 of epoch 1, 81.41 ms/it, loss 0.487905
Finished training it 26624/76743 of epoch 1, 87.96 ms/it, loss 0.485304
Finished training it 26624/76743 of epoch 1, 88.18 ms/it, loss 0.486346
Finished training it 26624/76743 of epoch 1, 88.20 ms/it, loss 0.485762
Finished training it 26624/76743 of epoch 1, 88.31 ms/it, loss 0.482576
Finished training it 27648/76743 of epoch 1, 88.30 ms/it, loss 0.485728
Finished training it 27648/76743 of epoch 1, 88.27 ms/it, loss 0.486984
Finished training it 27648/76743 of epoch 1, 87.98 ms/it, loss 0.484507
Finished training it 27648/76743 of epoch 1, 88.05 ms/it, loss 0.483602
Finished training it 28672/76743 of epoch 1, 87.52 ms/it, loss 0.485750
Finished training it 28672/76743 of epoch 1, 87.95 ms/it, loss 0.486904
Finished training it 28672/76743 of epoch 1, 87.96 ms/it, loss 0.486399
Finished training it 28672/76743 of epoch 1, 87.73 ms/it, loss 0.486006
Finished training it 29696/76743 of epoch 1, 88.52 ms/it, loss 0.484443
Finished training it 29696/76743 of epoch 1, 88.24 ms/it, loss 0.483444
Finished training it 29696/76743 of epoch 1, 88.34 ms/it, loss 0.483476
Finished training it 29696/76743 of epoch 1, 88.45 ms/it, loss 0.482857
Finished training it 30720/76743 of epoch 1, 88.78 ms/it, loss 0.482968
Finished training it 30720/76743 of epoch 1, 88.42 ms/it, loss 0.484459
Finished training it 30720/76743 of epoch 1, 88.51 ms/it, loss 0.481228
Finished training it 30720/76743 of epoch 1, 88.55 ms/it, loss 0.483369
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2519467.0
get out
0 has test check 2519467.0 and sample count 3274240
 accuracy 76.948 %, best 78.730 %, roc auc score 0.7554, best 0.8001
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2519467.0
get out
2 has test check 2519467.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.91 ms/it, loss 0.485133
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2519467.0
get out
1 has test check 2519467.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.99 ms/it, loss 0.484153
Finished training it 31744/76743 of epoch 1, 87.97 ms/it, loss 0.483007
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2519467.0
get out
3 has test check 2519467.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 88.05 ms/it, loss 0.481268
Finished training it 32768/76743 of epoch 1, 87.75 ms/it, loss 0.482872
Finished training it 32768/76743 of epoch 1, 88.02 ms/it, loss 0.484311
Finished training it 32768/76743 of epoch 1, 87.79 ms/it, loss 0.484485
Finished training it 32768/76743 of epoch 1, 87.87 ms/it, loss 0.481803
Finished training it 33792/76743 of epoch 1, 88.18 ms/it, loss 0.483478
Finished training it 33792/76743 of epoch 1, 88.53 ms/it, loss 0.482372
Finished training it 33792/76743 of epoch 1, 88.45 ms/it, loss 0.483112
Finished training it 33792/76743 of epoch 1, 88.75 ms/it, loss 0.486582
Finished training it 34816/76743 of epoch 1, 99.52 ms/it, loss 0.478884
Finished training it 34816/76743 of epoch 1, 99.51 ms/it, loss 0.484924
Finished training it 34816/76743 of epoch 1, 100.19 ms/it, loss 0.482603
Finished training it 34816/76743 of epoch 1, 99.45 ms/it, loss 0.484367
Finished training it 35840/76743 of epoch 1, 87.59 ms/it, loss 0.483312
Finished training it 35840/76743 of epoch 1, 87.61 ms/it, loss 0.485013
Finished training it 35840/76743 of epoch 1, 87.73 ms/it, loss 0.479962
Finished training it 35840/76743 of epoch 1, 87.82 ms/it, loss 0.480170
Finished training it 36864/76743 of epoch 1, 87.89 ms/it, loss 0.483771
Finished training it 36864/76743 of epoch 1, 88.00 ms/it, loss 0.482719
Finished training it 36864/76743 of epoch 1, 87.85 ms/it, loss 0.484200
Finished training it 36864/76743 of epoch 1, 87.76 ms/it, loss 0.486579
Finished training it 37888/76743 of epoch 1, 88.12 ms/it, loss 0.484364
Finished training it 37888/76743 of epoch 1, 88.58 ms/it, loss 0.483076
Finished training it 37888/76743 of epoch 1, 88.25 ms/it, loss 0.483496
Finished training it 37888/76743 of epoch 1, 88.34 ms/it, loss 0.484200
Finished training it 38912/76743 of epoch 1, 88.46 ms/it, loss 0.483842
Finished training it 38912/76743 of epoch 1, 88.62 ms/it, loss 0.480832
Finished training it 38912/76743 of epoch 1, 88.23 ms/it, loss 0.482147
Finished training it 38912/76743 of epoch 1, 88.21 ms/it, loss 0.483487
Finished training it 39936/76743 of epoch 1, 88.28 ms/it, loss 0.482329
Finished training it 39936/76743 of epoch 1, 88.36 ms/it, loss 0.483092
Finished training it 39936/76743 of epoch 1, 87.96 ms/it, loss 0.480292
Finished training it 39936/76743 of epoch 1, 88.12 ms/it, loss 0.483307
Finished training it 40960/76743 of epoch 1, 88.38 ms/it, loss 0.482391
Finished training it 40960/76743 of epoch 1, 88.32 ms/it, loss 0.482150
Finished training it 40960/76743 of epoch 1, 88.49 ms/it, loss 0.481223
Finished training it 40960/76743 of epoch 1, 88.44 ms/it, loss 0.483965
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2520861.0
get out
0 has test check 2520861.0 and sample count 3274240
 accuracy 76.991 %, best 78.730 %, roc auc score 0.7570, best 0.8001
Finished training it 41984/76743 of epoch 1, 88.35 ms/it, loss 0.479154
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2520861.0
get out
2 has test check 2520861.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 88.15 ms/it, loss 0.482358
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2520861.0
get out
1 has test check 2520861.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 88.39 ms/it, loss 0.480699
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2520861.0
get out
3 has test check 2520861.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 88.47 ms/it, loss 0.482588
Finished training it 43008/76743 of epoch 1, 88.03 ms/it, loss 0.483317
Finished training it 43008/76743 of epoch 1, 88.34 ms/it, loss 0.481547
Finished training it 43008/76743 of epoch 1, 87.99 ms/it, loss 0.479081
Finished training it 43008/76743 of epoch 1, 88.05 ms/it, loss 0.483193
Finished training it 44032/76743 of epoch 1, 88.46 ms/it, loss 0.483482
Finished training it 44032/76743 of epoch 1, 88.19 ms/it, loss 0.480081
Finished training it 44032/76743 of epoch 1, 88.27 ms/it, loss 0.481583
Finished training it 44032/76743 of epoch 1, 88.32 ms/it, loss 0.481608
Finished training it 45056/76743 of epoch 1, 88.09 ms/it, loss 0.480279
Finished training it 45056/76743 of epoch 1, 87.93 ms/it, loss 0.481440
Finished training it 45056/76743 of epoch 1, 88.33 ms/it, loss 0.481482
Finished training it 45056/76743 of epoch 1, 88.02 ms/it, loss 0.481070
Finished training it 46080/76743 of epoch 1, 88.43 ms/it, loss 0.478879
Finished training it 46080/76743 of epoch 1, 88.09 ms/it, loss 0.480381
Finished training it 46080/76743 of epoch 1, 88.10 ms/it, loss 0.480845
Finished training it 46080/76743 of epoch 1, 88.10 ms/it, loss 0.479950
Finished training it 47104/76743 of epoch 1, 88.58 ms/it, loss 0.480513
Finished training it 47104/76743 of epoch 1, 88.53 ms/it, loss 0.480640
Finished training it 47104/76743 of epoch 1, 88.55 ms/it, loss 0.481459
Finished training it 47104/76743 of epoch 1, 88.78 ms/it, loss 0.478116
Finished training it 48128/76743 of epoch 1, 87.82 ms/it, loss 0.480840
Finished training it 48128/76743 of epoch 1, 87.88 ms/it, loss 0.480479
Finished training it 48128/76743 of epoch 1, 88.02 ms/it, loss 0.480104
Finished training it 48128/76743 of epoch 1, 87.84 ms/it, loss 0.481666
Finished training it 49152/76743 of epoch 1, 87.67 ms/it, loss 0.479673
Finished training it 49152/76743 of epoch 1, 88.15 ms/it, loss 0.480026
Finished training it 49152/76743 of epoch 1, 87.87 ms/it, loss 0.480349
Finished training it 49152/76743 of epoch 1, 87.85 ms/it, loss 0.479585
Finished training it 50176/76743 of epoch 1, 87.89 ms/it, loss 0.480229
Finished training it 50176/76743 of epoch 1, 87.80 ms/it, loss 0.479300
Finished training it 50176/76743 of epoch 1, 88.21 ms/it, loss 0.479218
Finished training it 50176/76743 of epoch 1, 87.98 ms/it, loss 0.481680
Finished training it 51200/76743 of epoch 1, 88.29 ms/it, loss 0.479975
Finished training it 51200/76743 of epoch 1, 88.23 ms/it, loss 0.480883
Finished training it 51200/76743 of epoch 1, 88.18 ms/it, loss 0.480036
Finished training it 51200/76743 of epoch 1, 88.71 ms/it, loss 0.479200
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2521816.0
get out
0 has test check 2521816.0 and sample count 3274240
 accuracy 77.020 %, best 78.730 %, roc auc score 0.7600, best 0.8001
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2521816.0
get out
2 has test check 2521816.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.31 ms/it, loss 0.481196
Finished training it 52224/76743 of epoch 1, 88.24 ms/it, loss 0.479510
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2521816.0
get out
1 has test check 2521816.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.50 ms/it, loss 0.479492
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2521816.0
get out
3 has test check 2521816.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.60 ms/it, loss 0.482192
Finished training it 53248/76743 of epoch 1, 88.80 ms/it, loss 0.480721
Finished training it 53248/76743 of epoch 1, 88.54 ms/it, loss 0.479289
Finished training it 53248/76743 of epoch 1, 88.76 ms/it, loss 0.480278
Finished training it 53248/76743 of epoch 1, 88.95 ms/it, loss 0.478600
Finished training it 54272/76743 of epoch 1, 82.98 ms/it, loss 0.479156
Finished training it 54272/76743 of epoch 1, 82.90 ms/it, loss 0.478666
Finished training it 54272/76743 of epoch 1, 82.63 ms/it, loss 0.479733
Finished training it 54272/76743 of epoch 1, 82.75 ms/it, loss 0.479402
Finished training it 55296/76743 of epoch 1, 101.58 ms/it, loss 0.480479
Finished training it 55296/76743 of epoch 1, 101.31 ms/it, loss 0.480822
Finished training it 55296/76743 of epoch 1, 101.77 ms/it, loss 0.479720
Finished training it 55296/76743 of epoch 1, 101.81 ms/it, loss 0.480510
Finished training it 56320/76743 of epoch 1, 88.50 ms/it, loss 0.480473
Finished training it 56320/76743 of epoch 1, 89.01 ms/it, loss 0.478739
Finished training it 56320/76743 of epoch 1, 88.60 ms/it, loss 0.481220
Finished training it 56320/76743 of epoch 1, 88.80 ms/it, loss 0.478604
Finished training it 57344/76743 of epoch 1, 88.86 ms/it, loss 0.477823
Finished training it 57344/76743 of epoch 1, 88.68 ms/it, loss 0.478761
Finished training it 57344/76743 of epoch 1, 88.63 ms/it, loss 0.476396
Finished training it 57344/76743 of epoch 1, 89.06 ms/it, loss 0.478588
Finished training it 58368/76743 of epoch 1, 88.02 ms/it, loss 0.477226
Finished training it 58368/76743 of epoch 1, 88.03 ms/it, loss 0.476949
Finished training it 58368/76743 of epoch 1, 88.25 ms/it, loss 0.480911
Finished training it 58368/76743 of epoch 1, 88.47 ms/it, loss 0.479411
Finished training it 59392/76743 of epoch 1, 88.61 ms/it, loss 0.480709
Finished training it 59392/76743 of epoch 1, 88.56 ms/it, loss 0.479670
Finished training it 59392/76743 of epoch 1, 88.81 ms/it, loss 0.479353
Finished training it 59392/76743 of epoch 1, 88.90 ms/it, loss 0.476746
Finished training it 60416/76743 of epoch 1, 88.64 ms/it, loss 0.475973
Finished training it 60416/76743 of epoch 1, 88.40 ms/it, loss 0.476352
Finished training it 60416/76743 of epoch 1, 88.32 ms/it, loss 0.480320
Finished training it 60416/76743 of epoch 1, 88.28 ms/it, loss 0.478438
Finished training it 61440/76743 of epoch 1, 88.34 ms/it, loss 0.477642
Finished training it 61440/76743 of epoch 1, 88.70 ms/it, loss 0.477346
Finished training it 61440/76743 of epoch 1, 88.55 ms/it, loss 0.480663
Finished training it 61440/76743 of epoch 1, 88.43 ms/it, loss 0.476505
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2528214.0
get out
0 has test check 2528214.0 and sample count 3274240
 accuracy 77.215 %, best 78.730 %, roc auc score 0.7623, best 0.8001
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2528214.0
get out
2 has test check 2528214.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 88.36 ms/it, loss 0.480321
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2528214.0
get out
3 has test check 2528214.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 88.50 ms/it, loss 0.479016
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2528214.0
get out
1 has test check 2528214.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 88.47 ms/it, loss 0.480540
Finished training it 62464/76743 of epoch 1, 88.35 ms/it, loss 0.481498
Finished training it 63488/76743 of epoch 1, 88.28 ms/it, loss 0.476713
Finished training it 63488/76743 of epoch 1, 88.50 ms/it, loss 0.477303
Finished training it 63488/76743 of epoch 1, 88.07 ms/it, loss 0.480138
Finished training it 63488/76743 of epoch 1, 88.40 ms/it, loss 0.480996
Finished training it 64512/76743 of epoch 1, 88.12 ms/it, loss 0.481137
Finished training it 64512/76743 of epoch 1, 87.90 ms/it, loss 0.477519
Finished training it 64512/76743 of epoch 1, 88.01 ms/it, loss 0.479648
Finished training it 64512/76743 of epoch 1, 88.27 ms/it, loss 0.476413
Finished training it 65536/76743 of epoch 1, 87.49 ms/it, loss 0.478352
Finished training it 65536/76743 of epoch 1, 87.53 ms/it, loss 0.478947
Finished training it 65536/76743 of epoch 1, 87.84 ms/it, loss 0.480377
Finished training it 65536/76743 of epoch 1, 87.71 ms/it, loss 0.474789
Finished training it 66560/76743 of epoch 1, 88.10 ms/it, loss 0.476517
Finished training it 66560/76743 of epoch 1, 87.86 ms/it, loss 0.475778
Finished training it 66560/76743 of epoch 1, 87.72 ms/it, loss 0.476339
Finished training it 66560/76743 of epoch 1, 87.93 ms/it, loss 0.479379
Finished training it 67584/76743 of epoch 1, 88.76 ms/it, loss 0.475365
Finished training it 67584/76743 of epoch 1, 88.29 ms/it, loss 0.479404
Finished training it 67584/76743 of epoch 1, 88.35 ms/it, loss 0.479551
Finished training it 67584/76743 of epoch 1, 88.53 ms/it, loss 0.479615
Finished training it 68608/76743 of epoch 1, 87.95 ms/it, loss 0.476930
Finished training it 68608/76743 of epoch 1, 88.23 ms/it, loss 0.477235
Finished training it 68608/76743 of epoch 1, 87.89 ms/it, loss 0.478255
Finished training it 68608/76743 of epoch 1, 88.15 ms/it, loss 0.479620
Finished training it 69632/76743 of epoch 1, 87.90 ms/it, loss 0.480087
Finished training it 69632/76743 of epoch 1, 88.37 ms/it, loss 0.477653
Finished training it 69632/76743 of epoch 1, 87.92 ms/it, loss 0.478803
Finished training it 69632/76743 of epoch 1, 88.30 ms/it, loss 0.477771
Finished training it 70656/76743 of epoch 1, 88.38 ms/it, loss 0.481922
Finished training it 70656/76743 of epoch 1, 88.80 ms/it, loss 0.476281
Finished training it 70656/76743 of epoch 1, 88.52 ms/it, loss 0.477491
Finished training it 70656/76743 of epoch 1, 88.39 ms/it, loss 0.478282
Finished training it 71680/76743 of epoch 1, 88.45 ms/it, loss 0.476624
Finished training it 71680/76743 of epoch 1, 88.69 ms/it, loss 0.476526
Finished training it 71680/76743 of epoch 1, 88.53 ms/it, loss 0.474507
Finished training it 71680/76743 of epoch 1, 88.82 ms/it, loss 0.478072
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2529598.0
get out
0 has test check 2529598.0 and sample count 3274240
 accuracy 77.258 %, best 78.730 %, roc auc score 0.7634, best 0.8001
Finished training it 72704/76743 of epoch 1, 87.98 ms/it, loss 0.479776
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2529598.0
get out
2 has test check 2529598.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 87.80 ms/it, loss 0.478381
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2529598.0
get out
1 has test check 2529598.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.07 ms/it, loss 0.478908
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2529598.0
get out
3 has test check 2529598.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.08 ms/it, loss 0.475271
Finished training it 73728/76743 of epoch 1, 88.19 ms/it, loss 0.478146
Finished training it 73728/76743 of epoch 1, 87.97 ms/it, loss 0.477053
Finished training it 73728/76743 of epoch 1, 87.72 ms/it, loss 0.476901
Finished training it 73728/76743 of epoch 1, 87.62 ms/it, loss 0.476202
Finished training it 74752/76743 of epoch 1, 87.82 ms/it, loss 0.475381
Finished training it 74752/76743 of epoch 1, 87.81 ms/it, loss 0.477369
Finished training it 74752/76743 of epoch 1, 88.14 ms/it, loss 0.477406
Finished training it 74752/76743 of epoch 1, 88.13 ms/it, loss 0.477811
Finished training it 75776/76743 of epoch 1, 93.68 ms/it, loss 0.478459
Finished training it 75776/76743 of epoch 1, 94.53 ms/it, loss 0.478146
Finished training it 75776/76743 of epoch 1, 93.69 ms/it, loss 0.479427
Finished training it 75776/76743 of epoch 1, 94.04 ms/it, loss 0.474890
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.04 ms/it, loss 0.475475
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.07 ms/it, loss 0.476260
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.79 ms/it, loss 0.477445
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.99 ms/it, loss 0.478774
Finished training it 2048/76743 of epoch 2, 88.01 ms/it, loss 0.474139
Finished training it 2048/76743 of epoch 2, 88.28 ms/it, loss 0.477826
Finished training it 2048/76743 of epoch 2, 87.94 ms/it, loss 0.475739
Finished training it 2048/76743 of epoch 2, 88.08 ms/it, loss 0.476665
Finished training it 3072/76743 of epoch 2, 88.28 ms/it, loss 0.475252
Finished training it 3072/76743 of epoch 2, 87.97 ms/it, loss 0.476659
Finished training it 3072/76743 of epoch 2, 88.29 ms/it, loss 0.477991
Finished training it 3072/76743 of epoch 2, 88.11 ms/it, loss 0.478709
Finished training it 4096/76743 of epoch 2, 88.66 ms/it, loss 0.476909
Finished training it 4096/76743 of epoch 2, 88.72 ms/it, loss 0.475561
Finished training it 4096/76743 of epoch 2, 88.42 ms/it, loss 0.478604
Finished training it 4096/76743 of epoch 2, 88.43 ms/it, loss 0.474402
Finished training it 5120/76743 of epoch 2, 88.39 ms/it, loss 0.476040
Finished training it 5120/76743 of epoch 2, 88.22 ms/it, loss 0.474879
Finished training it 5120/76743 of epoch 2, 88.42 ms/it, loss 0.475108
Finished training it 5120/76743 of epoch 2, 88.15 ms/it, loss 0.475034
Finished training it 6144/76743 of epoch 2, 88.67 ms/it, loss 0.476424
Finished training it 6144/76743 of epoch 2, 88.94 ms/it, loss 0.476433
Finished training it 6144/76743 of epoch 2, 88.88 ms/it, loss 0.475275
Finished training it 6144/76743 of epoch 2, 88.59 ms/it, loss 0.477275
Finished training it 7168/76743 of epoch 2, 88.59 ms/it, loss 0.474739
Finished training it 7168/76743 of epoch 2, 88.69 ms/it, loss 0.477413
Finished training it 7168/76743 of epoch 2, 88.34 ms/it, loss 0.477038
Finished training it 7168/76743 of epoch 2, 88.24 ms/it, loss 0.475068
Finished training it 8192/76743 of epoch 2, 88.98 ms/it, loss 0.478919
Finished training it 8192/76743 of epoch 2, 89.02 ms/it, loss 0.479622
Finished training it 8192/76743 of epoch 2, 88.48 ms/it, loss 0.479857
Finished training it 8192/76743 of epoch 2, 88.60 ms/it, loss 0.474783
Finished training it 9216/76743 of epoch 2, 88.17 ms/it, loss 0.476418
Finished training it 9216/76743 of epoch 2, 88.32 ms/it, loss 0.472798
Finished training it 9216/76743 of epoch 2, 88.41 ms/it, loss 0.477487
Finished training it 9216/76743 of epoch 2, 88.44 ms/it, loss 0.472781
Finished training it 10240/76743 of epoch 2, 88.34 ms/it, loss 0.476252
Finished training it 10240/76743 of epoch 2, 88.38 ms/it, loss 0.475197
Finished training it 10240/76743 of epoch 2, 88.26 ms/it, loss 0.475113
Finished training it 10240/76743 of epoch 2, 88.66 ms/it, loss 0.475881
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2531210.0
get out
0 has test check 2531210.0 and sample count 3274240
 accuracy 77.307 %, best 78.730 %, roc auc score 0.7648, best 0.8001
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2531210.0
get out
3 has test check 2531210.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 88.03 ms/it, loss 0.476451
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2531210.0
get out
2 has test check 2531210.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.77 ms/it, loss 0.475910
Finished training it 11264/76743 of epoch 2, 87.87 ms/it, loss 0.476249
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2531210.0
get out
1 has test check 2531210.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 88.08 ms/it, loss 0.475813
Finished training it 12288/76743 of epoch 2, 88.46 ms/it, loss 0.476369
Finished training it 12288/76743 of epoch 2, 88.76 ms/it, loss 0.477188
Finished training it 12288/76743 of epoch 2, 88.47 ms/it, loss 0.473923
Finished training it 12288/76743 of epoch 2, 88.66 ms/it, loss 0.477305
Finished training it 13312/76743 of epoch 2, 88.81 ms/it, loss 0.474595
Finished training it 13312/76743 of epoch 2, 88.87 ms/it, loss 0.478056
Finished training it 13312/76743 of epoch 2, 89.08 ms/it, loss 0.476161
Finished training it 13312/76743 of epoch 2, 89.01 ms/it, loss 0.475709
Finished training it 14336/76743 of epoch 2, 88.65 ms/it, loss 0.475420
Finished training it 14336/76743 of epoch 2, 88.44 ms/it, loss 0.476468
Finished training it 14336/76743 of epoch 2, 88.99 ms/it, loss 0.476446
Finished training it 14336/76743 of epoch 2, 88.80 ms/it, loss 0.474470
Finished training it 15360/76743 of epoch 2, 88.32 ms/it, loss 0.474644
Finished training it 15360/76743 of epoch 2, 88.39 ms/it, loss 0.477031
Finished training it 15360/76743 of epoch 2, 88.03 ms/it, loss 0.474126
Finished training it 15360/76743 of epoch 2, 87.99 ms/it, loss 0.475461
Finished training it 16384/76743 of epoch 2, 88.00 ms/it, loss 0.476803
Finished training it 16384/76743 of epoch 2, 88.47 ms/it, loss 0.476056
Finished training it 16384/76743 of epoch 2, 88.30 ms/it, loss 0.476641
Finished training it 16384/76743 of epoch 2, 88.11 ms/it, loss 0.474138
Finished training it 17408/76743 of epoch 2, 88.29 ms/it, loss 0.476461
Finished training it 17408/76743 of epoch 2, 88.18 ms/it, loss 0.475850
Finished training it 17408/76743 of epoch 2, 88.35 ms/it, loss 0.475563
Finished training it 17408/76743 of epoch 2, 88.23 ms/it, loss 0.473668
Finished training it 18432/76743 of epoch 2, 88.29 ms/it, loss 0.478625
Finished training it 18432/76743 of epoch 2, 88.20 ms/it, loss 0.476622
Finished training it 18432/76743 of epoch 2, 87.97 ms/it, loss 0.475917
Finished training it 18432/76743 of epoch 2, 87.91 ms/it, loss 0.474207
Finished training it 19456/76743 of epoch 2, 88.78 ms/it, loss 0.474213
Finished training it 19456/76743 of epoch 2, 88.47 ms/it, loss 0.474917
Finished training it 19456/76743 of epoch 2, 88.68 ms/it, loss 0.475895
Finished training it 19456/76743 of epoch 2, 88.55 ms/it, loss 0.471946
Finished training it 20480/76743 of epoch 2, 87.84 ms/it, loss 0.477816
Finished training it 20480/76743 of epoch 2, 87.92 ms/it, loss 0.476766
Finished training it 20480/76743 of epoch 2, 88.24 ms/it, loss 0.475650
Finished training it 20480/76743 of epoch 2, 88.15 ms/it, loss 0.478370
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533505.0
get out
0 has test check 2533505.0 and sample count 3274240
 accuracy 77.377 %, best 78.730 %, roc auc score 0.7666, best 0.8001
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533505.0
get out
3 has test check 2533505.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.14 ms/it, loss 0.476146
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533505.0
get out
1 has test check 2533505.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.15 ms/it, loss 0.474996
Finished training it 21504/76743 of epoch 2, 87.85 ms/it, loss 0.475397
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533505.0
get out
2 has test check 2533505.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.03 ms/it, loss 0.475331
Finished training it 22528/76743 of epoch 2, 88.41 ms/it, loss 0.476898
Finished training it 22528/76743 of epoch 2, 88.09 ms/it, loss 0.473584
Finished training it 22528/76743 of epoch 2, 88.13 ms/it, loss 0.476427
Finished training it 22528/76743 of epoch 2, 88.04 ms/it, loss 0.476229
Finished training it 23552/76743 of epoch 2, 88.46 ms/it, loss 0.475479
Finished training it 23552/76743 of epoch 2, 88.18 ms/it, loss 0.472269
Finished training it 23552/76743 of epoch 2, 87.96 ms/it, loss 0.476209
Finished training it 23552/76743 of epoch 2, 88.04 ms/it, loss 0.471082
Finished training it 24576/76743 of epoch 2, 100.48 ms/it, loss 0.472728
Finished training it 24576/76743 of epoch 2, 99.98 ms/it, loss 0.476560
Finished training it 24576/76743 of epoch 2, 100.33 ms/it, loss 0.474909
Finished training it 24576/76743 of epoch 2, 100.01 ms/it, loss 0.475511
Finished training it 25600/76743 of epoch 2, 88.58 ms/it, loss 0.476392
Finished training it 25600/76743 of epoch 2, 88.27 ms/it, loss 0.475640
Finished training it 25600/76743 of epoch 2, 88.42 ms/it, loss 0.473829
Finished training it 25600/76743 of epoch 2, 88.31 ms/it, loss 0.473781
Finished training it 26624/76743 of epoch 2, 88.31 ms/it, loss 0.471716
Finished training it 26624/76743 of epoch 2, 88.83 ms/it, loss 0.474144
Finished training it 26624/76743 of epoch 2, 88.43 ms/it, loss 0.474134
Finished training it 26624/76743 of epoch 2, 88.58 ms/it, loss 0.475151
Finished training it 27648/76743 of epoch 2, 89.22 ms/it, loss 0.474391
Finished training it 27648/76743 of epoch 2, 88.93 ms/it, loss 0.475712
Finished training it 27648/76743 of epoch 2, 88.71 ms/it, loss 0.473529
Finished training it 27648/76743 of epoch 2, 88.55 ms/it, loss 0.472940
Finished training it 28672/76743 of epoch 2, 88.55 ms/it, loss 0.475867
Finished training it 28672/76743 of epoch 2, 88.19 ms/it, loss 0.474995
Finished training it 28672/76743 of epoch 2, 88.17 ms/it, loss 0.475026
Finished training it 28672/76743 of epoch 2, 88.75 ms/it, loss 0.476410
Finished training it 29696/76743 of epoch 2, 88.30 ms/it, loss 0.472937
Finished training it 29696/76743 of epoch 2, 88.39 ms/it, loss 0.473195
Finished training it 29696/76743 of epoch 2, 88.71 ms/it, loss 0.473477
Finished training it 29696/76743 of epoch 2, 88.45 ms/it, loss 0.472285
Finished training it 30720/76743 of epoch 2, 88.33 ms/it, loss 0.473015
Finished training it 30720/76743 of epoch 2, 88.59 ms/it, loss 0.473426
Finished training it 30720/76743 of epoch 2, 88.37 ms/it, loss 0.470929
Finished training it 30720/76743 of epoch 2, 88.30 ms/it, loss 0.474172
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535196.0
get out
0 has test check 2535196.0 and sample count 3274240
 accuracy 77.429 %, best 78.730 %, roc auc score 0.7674, best 0.8001
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535196.0
get out
3 has test check 2535196.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 88.61 ms/it, loss 0.472204
Finished training it 31744/76743 of epoch 2, 88.33 ms/it, loss 0.473345
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535196.0
get out
1 has test check 2535196.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 88.56 ms/it, loss 0.474525
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535196.0
get out
2 has test check 2535196.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 88.51 ms/it, loss 0.475468
Finished training it 32768/76743 of epoch 2, 88.20 ms/it, loss 0.473332
Finished training it 32768/76743 of epoch 2, 88.49 ms/it, loss 0.474537
Finished training it 32768/76743 of epoch 2, 87.99 ms/it, loss 0.472359
Finished training it 32768/76743 of epoch 2, 88.27 ms/it, loss 0.475502
Finished training it 33792/76743 of epoch 2, 88.31 ms/it, loss 0.472823
Finished training it 33792/76743 of epoch 2, 88.07 ms/it, loss 0.473860
Finished training it 33792/76743 of epoch 2, 88.32 ms/it, loss 0.477008
Finished training it 33792/76743 of epoch 2, 88.10 ms/it, loss 0.473607
Finished training it 34816/76743 of epoch 2, 88.41 ms/it, loss 0.475529
Finished training it 34816/76743 of epoch 2, 88.57 ms/it, loss 0.474049
Finished training it 34816/76743 of epoch 2, 88.12 ms/it, loss 0.470012
Finished training it 34816/76743 of epoch 2, 88.13 ms/it, loss 0.476045
Finished training it 35840/76743 of epoch 2, 88.24 ms/it, loss 0.474112
Finished training it 35840/76743 of epoch 2, 88.33 ms/it, loss 0.475379
Finished training it 35840/76743 of epoch 2, 88.16 ms/it, loss 0.470722
Finished training it 35840/76743 of epoch 2, 88.48 ms/it, loss 0.470467
Finished training it 36864/76743 of epoch 2, 88.10 ms/it, loss 0.474811
Finished training it 36864/76743 of epoch 2, 88.40 ms/it, loss 0.474500
Finished training it 36864/76743 of epoch 2, 88.56 ms/it, loss 0.473597
Finished training it 36864/76743 of epoch 2, 88.14 ms/it, loss 0.477178
Finished training it 37888/76743 of epoch 2, 88.39 ms/it, loss 0.475214
Finished training it 37888/76743 of epoch 2, 88.75 ms/it, loss 0.474069
Finished training it 37888/76743 of epoch 2, 88.25 ms/it, loss 0.474306
Finished training it 37888/76743 of epoch 2, 88.47 ms/it, loss 0.475256
Finished training it 38912/76743 of epoch 2, 88.13 ms/it, loss 0.473086
Finished training it 38912/76743 of epoch 2, 88.48 ms/it, loss 0.475228
Finished training it 38912/76743 of epoch 2, 88.50 ms/it, loss 0.472063
Finished training it 38912/76743 of epoch 2, 88.16 ms/it, loss 0.473884
Finished training it 39936/76743 of epoch 2, 88.71 ms/it, loss 0.473380
Finished training it 39936/76743 of epoch 2, 88.42 ms/it, loss 0.473203
Finished training it 39936/76743 of epoch 2, 88.30 ms/it, loss 0.471670
Finished training it 39936/76743 of epoch 2, 88.66 ms/it, loss 0.474674
Finished training it 40960/76743 of epoch 2, 88.44 ms/it, loss 0.474365
Finished training it 40960/76743 of epoch 2, 88.41 ms/it, loss 0.474323
Finished training it 40960/76743 of epoch 2, 88.78 ms/it, loss 0.475598
Finished training it 40960/76743 of epoch 2, 88.90 ms/it, loss 0.473158
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535582.0
get out
0 has test check 2535582.0 and sample count 3274240
 accuracy 77.440 %, best 78.730 %, roc auc score 0.7681, best 0.8001
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535582.0
get out
2 has test check 2535582.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 87.53 ms/it, loss 0.473333
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535582.0
get out
3 has test check 2535582.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 87.82 ms/it, loss 0.474122
Finished training it 41984/76743 of epoch 2, 87.47 ms/it, loss 0.470715
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535582.0
get out
1 has test check 2535582.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 87.78 ms/it, loss 0.472417
Finished training it 43008/76743 of epoch 2, 88.62 ms/it, loss 0.474740
Finished training it 43008/76743 of epoch 2, 88.71 ms/it, loss 0.473325
Finished training it 43008/76743 of epoch 2, 88.33 ms/it, loss 0.474046
Finished training it 43008/76743 of epoch 2, 88.34 ms/it, loss 0.470308
Finished training it 44032/76743 of epoch 2, 88.23 ms/it, loss 0.475865
Finished training it 44032/76743 of epoch 2, 88.11 ms/it, loss 0.472583
Finished training it 44032/76743 of epoch 2, 87.82 ms/it, loss 0.472112
Finished training it 44032/76743 of epoch 2, 87.82 ms/it, loss 0.473384
Finished training it 45056/76743 of epoch 2, 93.25 ms/it, loss 0.473612
Finished training it 45056/76743 of epoch 2, 92.98 ms/it, loss 0.472290
Finished training it 45056/76743 of epoch 2, 93.06 ms/it, loss 0.472778
Finished training it 45056/76743 of epoch 2, 93.26 ms/it, loss 0.473271
Finished training it 46080/76743 of epoch 2, 94.83 ms/it, loss 0.471323
Finished training it 46080/76743 of epoch 2, 94.20 ms/it, loss 0.472076
Finished training it 46080/76743 of epoch 2, 93.79 ms/it, loss 0.472703
Finished training it 46080/76743 of epoch 2, 94.26 ms/it, loss 0.472491
Finished training it 47104/76743 of epoch 2, 87.83 ms/it, loss 0.472419
Finished training it 47104/76743 of epoch 2, 87.94 ms/it, loss 0.473083
Finished training it 47104/76743 of epoch 2, 88.15 ms/it, loss 0.470746
Finished training it 47104/76743 of epoch 2, 87.79 ms/it, loss 0.473800
Finished training it 48128/76743 of epoch 2, 88.39 ms/it, loss 0.473303
Finished training it 48128/76743 of epoch 2, 88.57 ms/it, loss 0.472163
Finished training it 48128/76743 of epoch 2, 88.27 ms/it, loss 0.472624
Finished training it 48128/76743 of epoch 2, 88.45 ms/it, loss 0.472422
Finished training it 49152/76743 of epoch 2, 88.42 ms/it, loss 0.472752
Finished training it 49152/76743 of epoch 2, 88.11 ms/it, loss 0.472320
Finished training it 49152/76743 of epoch 2, 88.48 ms/it, loss 0.472854
Finished training it 49152/76743 of epoch 2, 88.15 ms/it, loss 0.472002
Finished training it 50176/76743 of epoch 2, 88.16 ms/it, loss 0.471516
Finished training it 50176/76743 of epoch 2, 87.99 ms/it, loss 0.474301
Finished training it 50176/76743 of epoch 2, 87.89 ms/it, loss 0.473265
Finished training it 50176/76743 of epoch 2, 87.89 ms/it, loss 0.472101
Finished training it 51200/76743 of epoch 2, 88.15 ms/it, loss 0.473459
Finished training it 51200/76743 of epoch 2, 88.37 ms/it, loss 0.472934
Finished training it 51200/76743 of epoch 2, 88.51 ms/it, loss 0.471983
Finished training it 51200/76743 of epoch 2, 88.14 ms/it, loss 0.472929
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534972.0
get out
0 has test check 2534972.0 and sample count 3274240
 accuracy 77.422 %, best 78.730 %, roc auc score 0.7693, best 0.8001
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534972.0
get out
1 has test check 2534972.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 88.07 ms/it, loss 0.472035
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534972.0
get out
3 has test check 2534972.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 87.98 ms/it, loss 0.474903
Finished training it 52224/76743 of epoch 2, 87.75 ms/it, loss 0.473112
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534972.0
get out
2 has test check 2534972.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 87.75 ms/it, loss 0.474291
Finished training it 53248/76743 of epoch 2, 88.76 ms/it, loss 0.471359
Finished training it 53248/76743 of epoch 2, 88.75 ms/it, loss 0.472643
Finished training it 53248/76743 of epoch 2, 88.48 ms/it, loss 0.472867
Finished training it 53248/76743 of epoch 2, 88.48 ms/it, loss 0.471910
Finished training it 54272/76743 of epoch 2, 87.91 ms/it, loss 0.472310
Finished training it 54272/76743 of epoch 2, 87.97 ms/it, loss 0.472342
Finished training it 54272/76743 of epoch 2, 88.09 ms/it, loss 0.472489
Finished training it 54272/76743 of epoch 2, 88.00 ms/it, loss 0.471592
Finished training it 55296/76743 of epoch 2, 88.13 ms/it, loss 0.474601
Finished training it 55296/76743 of epoch 2, 88.33 ms/it, loss 0.473003
Finished training it 55296/76743 of epoch 2, 88.34 ms/it, loss 0.474063
Finished training it 55296/76743 of epoch 2, 88.14 ms/it, loss 0.474439
Finished training it 56320/76743 of epoch 2, 88.45 ms/it, loss 0.472647
Finished training it 56320/76743 of epoch 2, 88.47 ms/it, loss 0.471977
Finished training it 56320/76743 of epoch 2, 88.20 ms/it, loss 0.474392
Finished training it 56320/76743 of epoch 2, 88.24 ms/it, loss 0.474072
Finished training it 57344/76743 of epoch 2, 88.59 ms/it, loss 0.472139
Finished training it 57344/76743 of epoch 2, 88.27 ms/it, loss 0.469988
Finished training it 57344/76743 of epoch 2, 88.66 ms/it, loss 0.471173
Finished training it 57344/76743 of epoch 2, 88.36 ms/it, loss 0.472344
Finished training it 58368/76743 of epoch 2, 88.40 ms/it, loss 0.474092
Finished training it 58368/76743 of epoch 2, 88.04 ms/it, loss 0.470247
Finished training it 58368/76743 of epoch 2, 88.12 ms/it, loss 0.471338
Finished training it 58368/76743 of epoch 2, 88.32 ms/it, loss 0.473354
Finished training it 59392/76743 of epoch 2, 88.25 ms/it, loss 0.472665
Finished training it 59392/76743 of epoch 2, 88.09 ms/it, loss 0.472998
Finished training it 59392/76743 of epoch 2, 88.22 ms/it, loss 0.470606
Finished training it 59392/76743 of epoch 2, 87.99 ms/it, loss 0.474395
Finished training it 60416/76743 of epoch 2, 88.76 ms/it, loss 0.470479
Finished training it 60416/76743 of epoch 2, 88.49 ms/it, loss 0.474072
Finished training it 60416/76743 of epoch 2, 88.66 ms/it, loss 0.469203
Finished training it 60416/76743 of epoch 2, 88.49 ms/it, loss 0.472022
Finished training it 61440/76743 of epoch 2, 88.25 ms/it, loss 0.471066
Finished training it 61440/76743 of epoch 2, 88.60 ms/it, loss 0.474446
Finished training it 61440/76743 of epoch 2, 88.32 ms/it, loss 0.470439
Finished training it 61440/76743 of epoch 2, 88.50 ms/it, loss 0.471369
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537557.0
get out
0 has test check 2537557.0 and sample count 3274240
 accuracy 77.501 %, best 78.730 %, roc auc score 0.7697, best 0.8001
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537557.0
get out
3 has test check 2537557.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 88.54 ms/it, loss 0.472610
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537557.0
get out
1 has test check 2537557.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 88.57 ms/it, loss 0.474678
Finished training it 62464/76743 of epoch 2, 88.27 ms/it, loss 0.475575
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537557.0
get out
2 has test check 2537557.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 88.42 ms/it, loss 0.473731
Finished training it 63488/76743 of epoch 2, 88.56 ms/it, loss 0.471015
Finished training it 63488/76743 of epoch 2, 88.53 ms/it, loss 0.475164
Finished training it 63488/76743 of epoch 2, 88.42 ms/it, loss 0.470614
Finished training it 63488/76743 of epoch 2, 88.40 ms/it, loss 0.474741
Finished training it 64512/76743 of epoch 2, 88.50 ms/it, loss 0.475052
Finished training it 64512/76743 of epoch 2, 88.23 ms/it, loss 0.471399
Finished training it 64512/76743 of epoch 2, 88.33 ms/it, loss 0.473596
Finished training it 64512/76743 of epoch 2, 88.51 ms/it, loss 0.470644
Finished training it 65536/76743 of epoch 2, 99.02 ms/it, loss 0.472355
Finished training it 65536/76743 of epoch 2, 98.95 ms/it, loss 0.473014
Finished training it 65536/76743 of epoch 2, 99.03 ms/it, loss 0.474554
Finished training it 65536/76743 of epoch 2, 98.95 ms/it, loss 0.468780
Finished training it 66560/76743 of epoch 2, 88.51 ms/it, loss 0.470889
Finished training it 66560/76743 of epoch 2, 88.55 ms/it, loss 0.472994
Finished training it 66560/76743 of epoch 2, 88.32 ms/it, loss 0.469336
Finished training it 66560/76743 of epoch 2, 88.32 ms/it, loss 0.471095
Finished training it 67584/76743 of epoch 2, 87.74 ms/it, loss 0.473885
Finished training it 67584/76743 of epoch 2, 87.78 ms/it, loss 0.473429
Finished training it 67584/76743 of epoch 2, 87.86 ms/it, loss 0.473467
Finished training it 67584/76743 of epoch 2, 87.95 ms/it, loss 0.469325
Finished training it 68608/76743 of epoch 2, 88.16 ms/it, loss 0.470949
Finished training it 68608/76743 of epoch 2, 88.20 ms/it, loss 0.472159
Finished training it 68608/76743 of epoch 2, 88.39 ms/it, loss 0.473946
Finished training it 68608/76743 of epoch 2, 88.42 ms/it, loss 0.472050
Finished training it 69632/76743 of epoch 2, 87.97 ms/it, loss 0.472728
Finished training it 69632/76743 of epoch 2, 88.32 ms/it, loss 0.472304
Finished training it 69632/76743 of epoch 2, 87.81 ms/it, loss 0.473638
Finished training it 69632/76743 of epoch 2, 88.26 ms/it, loss 0.471877
Finished training it 70656/76743 of epoch 2, 88.53 ms/it, loss 0.475792
Finished training it 70656/76743 of epoch 2, 88.69 ms/it, loss 0.470856
Finished training it 70656/76743 of epoch 2, 88.66 ms/it, loss 0.471480
Finished training it 70656/76743 of epoch 2, 88.50 ms/it, loss 0.472163
Finished training it 71680/76743 of epoch 2, 87.97 ms/it, loss 0.470970
Finished training it 71680/76743 of epoch 2, 88.02 ms/it, loss 0.469278
Finished training it 71680/76743 of epoch 2, 88.30 ms/it, loss 0.472294
Finished training it 71680/76743 of epoch 2, 88.13 ms/it, loss 0.470310
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540070.0
get out
0 has test check 2540070.0 and sample count 3274240
 accuracy 77.577 %, best 78.730 %, roc auc score 0.7714, best 0.8001
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540070.0
get out
2 has test check 2540070.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 88.21 ms/it, loss 0.472681
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540070.0
get out
3 has test check 2540070.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 88.39 ms/it, loss 0.469935
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540070.0
get out
1 has test check 2540070.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 88.42 ms/it, loss 0.472766
Finished training it 72704/76743 of epoch 2, 88.13 ms/it, loss 0.473987
Finished training it 73728/76743 of epoch 2, 88.49 ms/it, loss 0.472470
Finished training it 73728/76743 of epoch 2, 88.45 ms/it, loss 0.471344
Finished training it 73728/76743 of epoch 2, 88.30 ms/it, loss 0.470378
Finished training it 73728/76743 of epoch 2, 88.30 ms/it, loss 0.472064
Finished training it 74752/76743 of epoch 2, 88.44 ms/it, loss 0.471646
Finished training it 74752/76743 of epoch 2, 88.11 ms/it, loss 0.471472
Finished training it 74752/76743 of epoch 2, 88.37 ms/it, loss 0.471999
Finished training it 74752/76743 of epoch 2, 88.15 ms/it, loss 0.469537
Finished training it 75776/76743 of epoch 2, 88.81 ms/it, loss 0.469372
Finished training it 75776/76743 of epoch 2, 88.47 ms/it, loss 0.472711
Finished training it 75776/76743 of epoch 2, 88.66 ms/it, loss 0.472360
Finished training it 75776/76743 of epoch 2, 88.44 ms/it, loss 0.473316
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.98 ms/it, loss 0.472800
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 89.09 ms/it, loss 0.469509
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.99 ms/it, loss 0.470269
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.99 ms/it, loss 0.471940
Finished training it 2048/76743 of epoch 3, 88.42 ms/it, loss 0.469888
Finished training it 2048/76743 of epoch 3, 88.61 ms/it, loss 0.470861
Finished training it 2048/76743 of epoch 3, 88.69 ms/it, loss 0.471970
Finished training it 2048/76743 of epoch 3, 88.41 ms/it, loss 0.468767
Finished training it 3072/76743 of epoch 3, 88.81 ms/it, loss 0.472123
Finished training it 3072/76743 of epoch 3, 88.75 ms/it, loss 0.469781
Finished training it 3072/76743 of epoch 3, 88.48 ms/it, loss 0.470946
Finished training it 3072/76743 of epoch 3, 88.47 ms/it, loss 0.473199
Finished training it 4096/76743 of epoch 3, 88.16 ms/it, loss 0.468981
Finished training it 4096/76743 of epoch 3, 88.47 ms/it, loss 0.469896
Finished training it 4096/76743 of epoch 3, 88.16 ms/it, loss 0.472443
Finished training it 4096/76743 of epoch 3, 88.43 ms/it, loss 0.471138
Finished training it 5120/76743 of epoch 3, 88.68 ms/it, loss 0.470205
Finished training it 5120/76743 of epoch 3, 88.50 ms/it, loss 0.470746
Finished training it 5120/76743 of epoch 3, 88.24 ms/it, loss 0.469584
Finished training it 5120/76743 of epoch 3, 88.39 ms/it, loss 0.469056
Finished training it 6144/76743 of epoch 3, 89.45 ms/it, loss 0.470424
Finished training it 6144/76743 of epoch 3, 89.31 ms/it, loss 0.471913
Finished training it 6144/76743 of epoch 3, 89.49 ms/it, loss 0.470134
Finished training it 6144/76743 of epoch 3, 89.21 ms/it, loss 0.471283
Finished training it 7168/76743 of epoch 3, 88.72 ms/it, loss 0.469033
Finished training it 7168/76743 of epoch 3, 88.56 ms/it, loss 0.471968
Finished training it 7168/76743 of epoch 3, 88.44 ms/it, loss 0.471565
Finished training it 7168/76743 of epoch 3, 88.39 ms/it, loss 0.469186
Finished training it 8192/76743 of epoch 3, 88.87 ms/it, loss 0.474028
Finished training it 8192/76743 of epoch 3, 88.58 ms/it, loss 0.469284
Finished training it 8192/76743 of epoch 3, 88.53 ms/it, loss 0.474299
Finished training it 8192/76743 of epoch 3, 88.74 ms/it, loss 0.473049
Finished training it 9216/76743 of epoch 3, 93.55 ms/it, loss 0.471285
Finished training it 9216/76743 of epoch 3, 93.82 ms/it, loss 0.472175
Finished training it 9216/76743 of epoch 3, 93.70 ms/it, loss 0.467170
Finished training it 9216/76743 of epoch 3, 93.70 ms/it, loss 0.467138
Finished training it 10240/76743 of epoch 3, 94.40 ms/it, loss 0.470120
Finished training it 10240/76743 of epoch 3, 94.31 ms/it, loss 0.470347
Finished training it 10240/76743 of epoch 3, 94.58 ms/it, loss 0.471418
Finished training it 10240/76743 of epoch 3, 93.99 ms/it, loss 0.469256
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2539190.0
get out
0 has test check 2539190.0 and sample count 3274240
 accuracy 77.551 %, best 78.730 %, roc auc score 0.7719, best 0.8001
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2539190.0
get out
1 has test check 2539190.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.09 ms/it, loss 0.469961
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2539190.0
get out
2 has test check 2539190.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.02 ms/it, loss 0.470551
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2539190.0
get out
3 has test check 2539190.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.26 ms/it, loss 0.470876
Finished training it 11264/76743 of epoch 3, 87.90 ms/it, loss 0.470898
Finished training it 12288/76743 of epoch 3, 93.29 ms/it, loss 0.467814
Finished training it 12288/76743 of epoch 3, 93.92 ms/it, loss 0.470364
Finished training it 12288/76743 of epoch 3, 93.58 ms/it, loss 0.471683
Finished training it 12288/76743 of epoch 3, 93.56 ms/it, loss 0.471952
Finished training it 13312/76743 of epoch 3, 94.86 ms/it, loss 0.469609
Finished training it 13312/76743 of epoch 3, 95.02 ms/it, loss 0.470359
Finished training it 13312/76743 of epoch 3, 94.25 ms/it, loss 0.469927
Finished training it 13312/76743 of epoch 3, 94.67 ms/it, loss 0.472567
Finished training it 14336/76743 of epoch 3, 88.52 ms/it, loss 0.470229
Finished training it 14336/76743 of epoch 3, 88.45 ms/it, loss 0.471290
Finished training it 14336/76743 of epoch 3, 88.79 ms/it, loss 0.471062
Finished training it 14336/76743 of epoch 3, 88.76 ms/it, loss 0.469741
Finished training it 15360/76743 of epoch 3, 88.69 ms/it, loss 0.472322
Finished training it 15360/76743 of epoch 3, 88.56 ms/it, loss 0.468901
Finished training it 15360/76743 of epoch 3, 88.91 ms/it, loss 0.469583
Finished training it 15360/76743 of epoch 3, 88.62 ms/it, loss 0.470233
Finished training it 16384/76743 of epoch 3, 88.41 ms/it, loss 0.472112
Finished training it 16384/76743 of epoch 3, 88.61 ms/it, loss 0.471399
Finished training it 16384/76743 of epoch 3, 88.37 ms/it, loss 0.469209
Finished training it 16384/76743 of epoch 3, 88.64 ms/it, loss 0.470921
Finished training it 17408/76743 of epoch 3, 89.06 ms/it, loss 0.470976
Finished training it 17408/76743 of epoch 3, 88.90 ms/it, loss 0.471196
Finished training it 17408/76743 of epoch 3, 89.05 ms/it, loss 0.470675
Finished training it 17408/76743 of epoch 3, 88.90 ms/it, loss 0.469290
Finished training it 18432/76743 of epoch 3, 88.32 ms/it, loss 0.471875
Finished training it 18432/76743 of epoch 3, 88.43 ms/it, loss 0.469158
Finished training it 18432/76743 of epoch 3, 88.59 ms/it, loss 0.474331
Finished training it 18432/76743 of epoch 3, 88.51 ms/it, loss 0.471653
Finished training it 19456/76743 of epoch 3, 88.77 ms/it, loss 0.469626
Finished training it 19456/76743 of epoch 3, 88.56 ms/it, loss 0.470260
Finished training it 19456/76743 of epoch 3, 88.84 ms/it, loss 0.471055
Finished training it 19456/76743 of epoch 3, 88.35 ms/it, loss 0.466837
Finished training it 20480/76743 of epoch 3, 88.31 ms/it, loss 0.472947
Finished training it 20480/76743 of epoch 3, 88.50 ms/it, loss 0.471588
Finished training it 20480/76743 of epoch 3, 88.48 ms/it, loss 0.472738
Finished training it 20480/76743 of epoch 3, 88.32 ms/it, loss 0.472084
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540987.0
get out
0 has test check 2540987.0 and sample count 3274240
 accuracy 77.605 %, best 78.730 %, roc auc score 0.7726, best 0.8001
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540987.0
get out
3 has test check 2540987.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 88.59 ms/it, loss 0.472057
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540987.0
get out
2 has test check 2540987.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 88.42 ms/it, loss 0.470410
Finished training it 21504/76743 of epoch 3, 88.41 ms/it, loss 0.470717
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540987.0
get out
1 has test check 2540987.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 88.63 ms/it, loss 0.470150
Finished training it 22528/76743 of epoch 3, 88.47 ms/it, loss 0.468739
Finished training it 22528/76743 of epoch 3, 88.45 ms/it, loss 0.471554
Finished training it 22528/76743 of epoch 3, 88.66 ms/it, loss 0.472632
Finished training it 22528/76743 of epoch 3, 88.69 ms/it, loss 0.471603
Finished training it 23552/76743 of epoch 3, 88.42 ms/it, loss 0.467463
Finished training it 23552/76743 of epoch 3, 88.10 ms/it, loss 0.466420
Finished training it 23552/76743 of epoch 3, 88.25 ms/it, loss 0.471054
Finished training it 23552/76743 of epoch 3, 88.44 ms/it, loss 0.471116
Finished training it 24576/76743 of epoch 3, 87.23 ms/it, loss 0.471519
Finished training it 24576/76743 of epoch 3, 87.48 ms/it, loss 0.467528
Finished training it 24576/76743 of epoch 3, 87.14 ms/it, loss 0.471908
Finished training it 24576/76743 of epoch 3, 87.38 ms/it, loss 0.470170
Finished training it 25600/76743 of epoch 3, 80.33 ms/it, loss 0.469151
Finished training it 25600/76743 of epoch 3, 80.21 ms/it, loss 0.471312
Finished training it 25600/76743 of epoch 3, 80.44 ms/it, loss 0.471361
Finished training it 25600/76743 of epoch 3, 80.50 ms/it, loss 0.468957
Finished training it 26624/76743 of epoch 3, 88.72 ms/it, loss 0.469936
Finished training it 26624/76743 of epoch 3, 88.67 ms/it, loss 0.469619
Finished training it 26624/76743 of epoch 3, 88.79 ms/it, loss 0.470455
Finished training it 26624/76743 of epoch 3, 88.54 ms/it, loss 0.466927
Finished training it 27648/76743 of epoch 3, 88.17 ms/it, loss 0.468922
Finished training it 27648/76743 of epoch 3, 88.26 ms/it, loss 0.467868
Finished training it 27648/76743 of epoch 3, 88.45 ms/it, loss 0.471401
Finished training it 27648/76743 of epoch 3, 88.47 ms/it, loss 0.469847
Finished training it 28672/76743 of epoch 3, 88.28 ms/it, loss 0.470674
Finished training it 28672/76743 of epoch 3, 88.45 ms/it, loss 0.472034
Finished training it 28672/76743 of epoch 3, 88.31 ms/it, loss 0.470190
Finished training it 28672/76743 of epoch 3, 88.47 ms/it, loss 0.471125
Finished training it 29696/76743 of epoch 3, 88.37 ms/it, loss 0.468804
Finished training it 29696/76743 of epoch 3, 88.56 ms/it, loss 0.468781
Finished training it 29696/76743 of epoch 3, 88.63 ms/it, loss 0.467541
Finished training it 29696/76743 of epoch 3, 88.31 ms/it, loss 0.468175
Finished training it 30720/76743 of epoch 3, 88.79 ms/it, loss 0.469145
Finished training it 30720/76743 of epoch 3, 88.58 ms/it, loss 0.468701
Finished training it 30720/76743 of epoch 3, 88.62 ms/it, loss 0.469152
Finished training it 30720/76743 of epoch 3, 88.70 ms/it, loss 0.466423
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541798.0
get out
0 has test check 2541798.0 and sample count 3274240
 accuracy 77.630 %, best 78.730 %, roc auc score 0.7727, best 0.8001
Finished training it 31744/76743 of epoch 3, 88.02 ms/it, loss 0.468638
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541798.0
get out
3 has test check 2541798.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 88.28 ms/it, loss 0.467685
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541798.0
get out
2 has test check 2541798.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 87.98 ms/it, loss 0.471196
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541798.0
get out
1 has test check 2541798.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 88.24 ms/it, loss 0.470158
Finished training it 32768/76743 of epoch 3, 88.47 ms/it, loss 0.471277
Finished training it 32768/76743 of epoch 3, 88.53 ms/it, loss 0.470408
Finished training it 32768/76743 of epoch 3, 88.33 ms/it, loss 0.468713
Finished training it 32768/76743 of epoch 3, 88.30 ms/it, loss 0.468195
Finished training it 33792/76743 of epoch 3, 88.45 ms/it, loss 0.469824
Finished training it 33792/76743 of epoch 3, 88.73 ms/it, loss 0.468852
Finished training it 33792/76743 of epoch 3, 88.62 ms/it, loss 0.473135
Finished training it 33792/76743 of epoch 3, 88.38 ms/it, loss 0.469661
Finished training it 34816/76743 of epoch 3, 99.09 ms/it, loss 0.471128
Finished training it 34816/76743 of epoch 3, 98.90 ms/it, loss 0.465667
Finished training it 34816/76743 of epoch 3, 99.13 ms/it, loss 0.469727
Finished training it 34816/76743 of epoch 3, 98.87 ms/it, loss 0.472235
Finished training it 35840/76743 of epoch 3, 88.65 ms/it, loss 0.467024
Finished training it 35840/76743 of epoch 3, 88.56 ms/it, loss 0.469993
Finished training it 35840/76743 of epoch 3, 88.72 ms/it, loss 0.471194
Finished training it 35840/76743 of epoch 3, 88.31 ms/it, loss 0.466766
Finished training it 36864/76743 of epoch 3, 88.60 ms/it, loss 0.472811
Finished training it 36864/76743 of epoch 3, 88.63 ms/it, loss 0.470974
Finished training it 36864/76743 of epoch 3, 88.99 ms/it, loss 0.470122
Finished training it 36864/76743 of epoch 3, 88.91 ms/it, loss 0.468984
Finished training it 37888/76743 of epoch 3, 88.89 ms/it, loss 0.469878
Finished training it 37888/76743 of epoch 3, 89.12 ms/it, loss 0.471343
Finished training it 37888/76743 of epoch 3, 89.13 ms/it, loss 0.470084
Finished training it 37888/76743 of epoch 3, 88.93 ms/it, loss 0.470839
Finished training it 38912/76743 of epoch 3, 88.48 ms/it, loss 0.467523
Finished training it 38912/76743 of epoch 3, 88.54 ms/it, loss 0.470773
Finished training it 38912/76743 of epoch 3, 88.29 ms/it, loss 0.469385
Finished training it 38912/76743 of epoch 3, 88.37 ms/it, loss 0.469224
Finished training it 39936/76743 of epoch 3, 88.16 ms/it, loss 0.468004
Finished training it 39936/76743 of epoch 3, 88.36 ms/it, loss 0.469017
Finished training it 39936/76743 of epoch 3, 88.29 ms/it, loss 0.470501
Finished training it 39936/76743 of epoch 3, 88.19 ms/it, loss 0.469305
Finished training it 40960/76743 of epoch 3, 88.70 ms/it, loss 0.471555
Finished training it 40960/76743 of epoch 3, 88.42 ms/it, loss 0.470617
Finished training it 40960/76743 of epoch 3, 88.67 ms/it, loss 0.469085
Finished training it 40960/76743 of epoch 3, 88.48 ms/it, loss 0.470157
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541800.0
get out
0 has test check 2541800.0 and sample count 3274240
 accuracy 77.630 %, best 78.730 %, roc auc score 0.7734, best 0.8001
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541800.0
get out
2 has test check 2541800.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.02 ms/it, loss 0.469784
Finished training it 41984/76743 of epoch 3, 88.01 ms/it, loss 0.466883
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541800.0
get out
3 has test check 2541800.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.23 ms/it, loss 0.470721
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541800.0
get out
1 has test check 2541800.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.25 ms/it, loss 0.468637
Finished training it 43008/76743 of epoch 3, 88.46 ms/it, loss 0.466155
Finished training it 43008/76743 of epoch 3, 88.64 ms/it, loss 0.469381
Finished training it 43008/76743 of epoch 3, 88.46 ms/it, loss 0.470322
Finished training it 43008/76743 of epoch 3, 88.72 ms/it, loss 0.471175
Finished training it 44032/76743 of epoch 3, 88.56 ms/it, loss 0.469159
Finished training it 44032/76743 of epoch 3, 88.76 ms/it, loss 0.468906
Finished training it 44032/76743 of epoch 3, 88.52 ms/it, loss 0.468094
Finished training it 44032/76743 of epoch 3, 88.73 ms/it, loss 0.471642
Finished training it 45056/76743 of epoch 3, 88.45 ms/it, loss 0.469761
Finished training it 45056/76743 of epoch 3, 88.36 ms/it, loss 0.468651
Finished training it 45056/76743 of epoch 3, 88.19 ms/it, loss 0.469401
Finished training it 45056/76743 of epoch 3, 88.10 ms/it, loss 0.468046
Finished training it 46080/76743 of epoch 3, 88.42 ms/it, loss 0.468299
Finished training it 46080/76743 of epoch 3, 88.66 ms/it, loss 0.467461
Finished training it 46080/76743 of epoch 3, 88.30 ms/it, loss 0.468567
Finished training it 46080/76743 of epoch 3, 88.72 ms/it, loss 0.467982
Finished training it 47104/76743 of epoch 3, 88.54 ms/it, loss 0.466296
Finished training it 47104/76743 of epoch 3, 88.31 ms/it, loss 0.468414
Finished training it 47104/76743 of epoch 3, 88.27 ms/it, loss 0.469317
Finished training it 47104/76743 of epoch 3, 88.57 ms/it, loss 0.468696
Finished training it 48128/76743 of epoch 3, 88.61 ms/it, loss 0.468579
Finished training it 48128/76743 of epoch 3, 88.95 ms/it, loss 0.468100
Finished training it 48128/76743 of epoch 3, 88.86 ms/it, loss 0.468673
Finished training it 48128/76743 of epoch 3, 88.63 ms/it, loss 0.469180
Finished training it 49152/76743 of epoch 3, 88.66 ms/it, loss 0.468864
Finished training it 49152/76743 of epoch 3, 88.68 ms/it, loss 0.468887
Finished training it 49152/76743 of epoch 3, 88.45 ms/it, loss 0.468882
Finished training it 49152/76743 of epoch 3, 88.47 ms/it, loss 0.468074
Finished training it 50176/76743 of epoch 3, 88.48 ms/it, loss 0.470674
Finished training it 50176/76743 of epoch 3, 88.35 ms/it, loss 0.468151
Finished training it 50176/76743 of epoch 3, 88.27 ms/it, loss 0.468566
Finished training it 50176/76743 of epoch 3, 88.18 ms/it, loss 0.469534
Finished training it 51200/76743 of epoch 3, 88.27 ms/it, loss 0.469082
Finished training it 51200/76743 of epoch 3, 88.20 ms/it, loss 0.468246
Finished training it 51200/76743 of epoch 3, 88.04 ms/it, loss 0.469044
Finished training it 51200/76743 of epoch 3, 87.99 ms/it, loss 0.468709
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541357.0
get out
0 has test check 2541357.0 and sample count 3274240
 accuracy 77.617 %, best 78.730 %, roc auc score 0.7741, best 0.8001
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541357.0
get out
1 has test check 2541357.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 88.46 ms/it, loss 0.468150
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541357.0
get out
3 has test check 2541357.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 88.45 ms/it, loss 0.470638
Finished training it 52224/76743 of epoch 3, 88.31 ms/it, loss 0.469080
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541357.0
get out
2 has test check 2541357.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 88.37 ms/it, loss 0.470046
Finished training it 53248/76743 of epoch 3, 82.95 ms/it, loss 0.469052
Finished training it 53248/76743 of epoch 3, 83.11 ms/it, loss 0.466967
Finished training it 53248/76743 of epoch 3, 83.14 ms/it, loss 0.468558
Finished training it 53248/76743 of epoch 3, 82.87 ms/it, loss 0.468344
Finished training it 54272/76743 of epoch 3, 86.03 ms/it, loss 0.468703
Finished training it 54272/76743 of epoch 3, 85.89 ms/it, loss 0.468285
Finished training it 54272/76743 of epoch 3, 86.03 ms/it, loss 0.467730
Finished training it 54272/76743 of epoch 3, 85.96 ms/it, loss 0.469084
Finished training it 55296/76743 of epoch 3, 94.18 ms/it, loss 0.470339
Finished training it 55296/76743 of epoch 3, 93.85 ms/it, loss 0.468993
Finished training it 55296/76743 of epoch 3, 93.85 ms/it, loss 0.470535
Finished training it 55296/76743 of epoch 3, 93.43 ms/it, loss 0.471222
Finished training it 56320/76743 of epoch 3, 93.65 ms/it, loss 0.467932
Finished training it 56320/76743 of epoch 3, 93.68 ms/it, loss 0.469976
Finished training it 56320/76743 of epoch 3, 93.66 ms/it, loss 0.468345
Finished training it 56320/76743 of epoch 3, 93.47 ms/it, loss 0.470238
Finished training it 57344/76743 of epoch 3, 88.46 ms/it, loss 0.466553
Finished training it 57344/76743 of epoch 3, 88.69 ms/it, loss 0.467289
Finished training it 57344/76743 of epoch 3, 88.75 ms/it, loss 0.467770
Finished training it 57344/76743 of epoch 3, 88.51 ms/it, loss 0.468495
Finished training it 58368/76743 of epoch 3, 88.89 ms/it, loss 0.469653
Finished training it 58368/76743 of epoch 3, 88.65 ms/it, loss 0.467173
Finished training it 58368/76743 of epoch 3, 88.64 ms/it, loss 0.469982
Finished training it 58368/76743 of epoch 3, 88.57 ms/it, loss 0.466988
Finished training it 59392/76743 of epoch 3, 88.46 ms/it, loss 0.468775
Finished training it 59392/76743 of epoch 3, 88.58 ms/it, loss 0.467257
Finished training it 59392/76743 of epoch 3, 88.29 ms/it, loss 0.469290
Finished training it 59392/76743 of epoch 3, 88.30 ms/it, loss 0.470305
Finished training it 60416/76743 of epoch 3, 88.61 ms/it, loss 0.466118
Finished training it 60416/76743 of epoch 3, 88.45 ms/it, loss 0.470130
Finished training it 60416/76743 of epoch 3, 88.56 ms/it, loss 0.467970
Finished training it 60416/76743 of epoch 3, 88.62 ms/it, loss 0.465688
Finished training it 61440/76743 of epoch 3, 88.45 ms/it, loss 0.467259
Finished training it 61440/76743 of epoch 3, 88.43 ms/it, loss 0.466691
Finished training it 61440/76743 of epoch 3, 88.75 ms/it, loss 0.467472
Finished training it 61440/76743 of epoch 3, 88.70 ms/it, loss 0.470926
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543506.0
get out
0 has test check 2543506.0 and sample count 3274240
 accuracy 77.682 %, best 78.730 %, roc auc score 0.7742, best 0.8001
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543506.0
get out
1 has test check 2543506.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 87.93 ms/it, loss 0.470491
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543506.0
get out
3 has test check 2543506.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 88.01 ms/it, loss 0.468772
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543506.0
get out
2 has test check 2543506.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 87.73 ms/it, loss 0.469538
Finished training it 62464/76743 of epoch 3, 87.82 ms/it, loss 0.471375
Finished training it 63488/76743 of epoch 3, 88.62 ms/it, loss 0.466836
Finished training it 63488/76743 of epoch 3, 88.47 ms/it, loss 0.470663
Finished training it 63488/76743 of epoch 3, 88.77 ms/it, loss 0.466750
Finished training it 63488/76743 of epoch 3, 88.80 ms/it, loss 0.471314
Finished training it 64512/76743 of epoch 3, 88.56 ms/it, loss 0.469620
Finished training it 64512/76743 of epoch 3, 88.73 ms/it, loss 0.471393
Finished training it 64512/76743 of epoch 3, 88.41 ms/it, loss 0.468108
Finished training it 64512/76743 of epoch 3, 88.66 ms/it, loss 0.467055
Finished training it 65536/76743 of epoch 3, 88.82 ms/it, loss 0.469014
Finished training it 65536/76743 of epoch 3, 89.01 ms/it, loss 0.465157
Finished training it 65536/76743 of epoch 3, 88.79 ms/it, loss 0.468369
Finished training it 65536/76743 of epoch 3, 88.90 ms/it, loss 0.470912
Finished training it 66560/76743 of epoch 3, 88.24 ms/it, loss 0.467764
Finished training it 66560/76743 of epoch 3, 88.30 ms/it, loss 0.465750
Finished training it 66560/76743 of epoch 3, 88.40 ms/it, loss 0.467627
Finished training it 66560/76743 of epoch 3, 88.50 ms/it, loss 0.469164
Finished training it 67584/76743 of epoch 3, 88.62 ms/it, loss 0.465772
Finished training it 67584/76743 of epoch 3, 88.73 ms/it, loss 0.469892
Finished training it 67584/76743 of epoch 3, 88.63 ms/it, loss 0.469943
Finished training it 67584/76743 of epoch 3, 88.49 ms/it, loss 0.470211
Finished training it 68608/76743 of epoch 3, 88.20 ms/it, loss 0.467374
Finished training it 68608/76743 of epoch 3, 88.27 ms/it, loss 0.468652
Finished training it 68608/76743 of epoch 3, 88.46 ms/it, loss 0.469980
Finished training it 68608/76743 of epoch 3, 88.48 ms/it, loss 0.468401
Finished training it 69632/76743 of epoch 3, 88.60 ms/it, loss 0.470176
Finished training it 69632/76743 of epoch 3, 88.78 ms/it, loss 0.467953
Finished training it 69632/76743 of epoch 3, 88.81 ms/it, loss 0.468990
Finished training it 69632/76743 of epoch 3, 88.64 ms/it, loss 0.468821
Finished training it 70656/76743 of epoch 3, 88.74 ms/it, loss 0.467777
Finished training it 70656/76743 of epoch 3, 88.66 ms/it, loss 0.472199
Finished training it 70656/76743 of epoch 3, 88.70 ms/it, loss 0.468708
Finished training it 70656/76743 of epoch 3, 88.92 ms/it, loss 0.467611
Finished training it 71680/76743 of epoch 3, 88.72 ms/it, loss 0.468746
Finished training it 71680/76743 of epoch 3, 88.41 ms/it, loss 0.465534
Finished training it 71680/76743 of epoch 3, 88.29 ms/it, loss 0.467355
Finished training it 71680/76743 of epoch 3, 88.74 ms/it, loss 0.466778
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544319.0
get out
0 has test check 2544319.0 and sample count 3274240
 accuracy 77.707 %, best 78.730 %, roc auc score 0.7750, best 0.8001
Finished training it 72704/76743 of epoch 3, 88.16 ms/it, loss 0.470599
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544319.0
get out
3 has test check 2544319.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 88.23 ms/it, loss 0.466384
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544319.0
get out
2 has test check 2544319.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 88.23 ms/it, loss 0.469712
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544319.0
get out
1 has test check 2544319.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 88.30 ms/it, loss 0.469320
Finished training it 73728/76743 of epoch 3, 88.67 ms/it, loss 0.466724
Finished training it 73728/76743 of epoch 3, 89.04 ms/it, loss 0.468304
Finished training it 73728/76743 of epoch 3, 88.81 ms/it, loss 0.468362
Finished training it 73728/76743 of epoch 3, 88.91 ms/it, loss 0.469247
Finished training it 74752/76743 of epoch 3, 88.39 ms/it, loss 0.465991
Finished training it 74752/76743 of epoch 3, 88.33 ms/it, loss 0.467703
Finished training it 74752/76743 of epoch 3, 88.53 ms/it, loss 0.468419
Finished training it 74752/76743 of epoch 3, 88.68 ms/it, loss 0.468347
Finished training it 75776/76743 of epoch 3, 93.59 ms/it, loss 0.469262
Finished training it 75776/76743 of epoch 3, 93.59 ms/it, loss 0.468858
Finished training it 75776/76743 of epoch 3, 93.29 ms/it, loss 0.470049
Finished training it 75776/76743 of epoch 3, 93.60 ms/it, loss 0.466679
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.56 ms/it, loss 0.466906
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.49 ms/it, loss 0.468808
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.53 ms/it, loss 0.466111
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.36 ms/it, loss 0.469660
Finished training it 2048/76743 of epoch 4, 89.01 ms/it, loss 0.468011
Finished training it 2048/76743 of epoch 4, 88.94 ms/it, loss 0.469295
Finished training it 2048/76743 of epoch 4, 88.71 ms/it, loss 0.466681
Finished training it 2048/76743 of epoch 4, 88.69 ms/it, loss 0.465922
Finished training it 3072/76743 of epoch 4, 89.04 ms/it, loss 0.467618
Finished training it 3072/76743 of epoch 4, 89.30 ms/it, loss 0.469174
Finished training it 3072/76743 of epoch 4, 89.07 ms/it, loss 0.470097
Finished training it 3072/76743 of epoch 4, 89.19 ms/it, loss 0.466093
Finished training it 4096/76743 of epoch 4, 88.66 ms/it, loss 0.464912
Finished training it 4096/76743 of epoch 4, 88.81 ms/it, loss 0.467649
Finished training it 4096/76743 of epoch 4, 88.84 ms/it, loss 0.466820
Finished training it 4096/76743 of epoch 4, 88.57 ms/it, loss 0.469364
Finished training it 5120/76743 of epoch 4, 88.32 ms/it, loss 0.467712
Finished training it 5120/76743 of epoch 4, 88.13 ms/it, loss 0.465481
Finished training it 5120/76743 of epoch 4, 88.02 ms/it, loss 0.466302
Finished training it 5120/76743 of epoch 4, 88.25 ms/it, loss 0.466736
Finished training it 6144/76743 of epoch 4, 88.51 ms/it, loss 0.468636
Finished training it 6144/76743 of epoch 4, 88.56 ms/it, loss 0.467904
Finished training it 6144/76743 of epoch 4, 88.82 ms/it, loss 0.467333
Finished training it 6144/76743 of epoch 4, 88.78 ms/it, loss 0.467336
Finished training it 7168/76743 of epoch 4, 89.03 ms/it, loss 0.468865
Finished training it 7168/76743 of epoch 4, 89.03 ms/it, loss 0.465816
Finished training it 7168/76743 of epoch 4, 88.65 ms/it, loss 0.466313
Finished training it 7168/76743 of epoch 4, 88.75 ms/it, loss 0.468379
Finished training it 8192/76743 of epoch 4, 88.65 ms/it, loss 0.469607
Finished training it 8192/76743 of epoch 4, 88.39 ms/it, loss 0.471373
Finished training it 8192/76743 of epoch 4, 88.33 ms/it, loss 0.465713
Finished training it 8192/76743 of epoch 4, 88.75 ms/it, loss 0.470972
Finished training it 9216/76743 of epoch 4, 88.70 ms/it, loss 0.464307
Finished training it 9216/76743 of epoch 4, 88.62 ms/it, loss 0.467913
Finished training it 9216/76743 of epoch 4, 88.83 ms/it, loss 0.464560
Finished training it 9216/76743 of epoch 4, 88.92 ms/it, loss 0.468850
Finished training it 10240/76743 of epoch 4, 88.57 ms/it, loss 0.467123
Finished training it 10240/76743 of epoch 4, 88.35 ms/it, loss 0.466666
Finished training it 10240/76743 of epoch 4, 88.66 ms/it, loss 0.467967
Finished training it 10240/76743 of epoch 4, 88.40 ms/it, loss 0.465978
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543373.0
get out
0 has test check 2543373.0 and sample count 3274240
 accuracy 77.678 %, best 78.730 %, roc auc score 0.7756, best 0.8001
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543373.0
get out
3 has test check 2543373.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 88.06 ms/it, loss 0.467978
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543373.0
get out
2 has test check 2543373.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.91 ms/it, loss 0.467121
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543373.0
get out
1 has test check 2543373.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 88.19 ms/it, loss 0.466875
Finished training it 11264/76743 of epoch 4, 87.88 ms/it, loss 0.467751
Finished training it 12288/76743 of epoch 4, 88.67 ms/it, loss 0.468365
Finished training it 12288/76743 of epoch 4, 88.81 ms/it, loss 0.469126
Finished training it 12288/76743 of epoch 4, 88.44 ms/it, loss 0.467486
Finished training it 12288/76743 of epoch 4, 88.42 ms/it, loss 0.464270
Finished training it 13312/76743 of epoch 4, 88.36 ms/it, loss 0.467086
Finished training it 13312/76743 of epoch 4, 88.30 ms/it, loss 0.469177
Finished training it 13312/76743 of epoch 4, 88.42 ms/it, loss 0.467117
Finished training it 13312/76743 of epoch 4, 88.41 ms/it, loss 0.466631
Finished training it 14336/76743 of epoch 4, 88.55 ms/it, loss 0.466270
Finished training it 14336/76743 of epoch 4, 88.26 ms/it, loss 0.467961
Finished training it 14336/76743 of epoch 4, 88.25 ms/it, loss 0.467206
Finished training it 14336/76743 of epoch 4, 88.56 ms/it, loss 0.467511
Finished training it 15360/76743 of epoch 4, 88.88 ms/it, loss 0.466118
Finished training it 15360/76743 of epoch 4, 88.80 ms/it, loss 0.469602
Finished training it 15360/76743 of epoch 4, 88.53 ms/it, loss 0.467184
Finished training it 15360/76743 of epoch 4, 88.55 ms/it, loss 0.465633
Finished training it 16384/76743 of epoch 4, 88.54 ms/it, loss 0.465975
Finished training it 16384/76743 of epoch 4, 88.62 ms/it, loss 0.468670
Finished training it 16384/76743 of epoch 4, 88.79 ms/it, loss 0.467557
Finished training it 16384/76743 of epoch 4, 88.81 ms/it, loss 0.468024
Finished training it 17408/76743 of epoch 4, 88.65 ms/it, loss 0.467898
Finished training it 17408/76743 of epoch 4, 88.72 ms/it, loss 0.467926
Finished training it 17408/76743 of epoch 4, 88.58 ms/it, loss 0.467912
Finished training it 17408/76743 of epoch 4, 88.49 ms/it, loss 0.466006
Finished training it 18432/76743 of epoch 4, 88.44 ms/it, loss 0.471123
Finished training it 18432/76743 of epoch 4, 88.36 ms/it, loss 0.466352
Finished training it 18432/76743 of epoch 4, 88.62 ms/it, loss 0.468216
Finished training it 18432/76743 of epoch 4, 88.31 ms/it, loss 0.468216
Finished training it 19456/76743 of epoch 4, 88.92 ms/it, loss 0.467359
Finished training it 19456/76743 of epoch 4, 89.03 ms/it, loss 0.466026
Finished training it 19456/76743 of epoch 4, 88.82 ms/it, loss 0.466662
Finished training it 19456/76743 of epoch 4, 88.80 ms/it, loss 0.463671
Finished training it 20480/76743 of epoch 4, 88.36 ms/it, loss 0.469763
Finished training it 20480/76743 of epoch 4, 88.43 ms/it, loss 0.468673
Finished training it 20480/76743 of epoch 4, 88.75 ms/it, loss 0.469810
Finished training it 20480/76743 of epoch 4, 88.63 ms/it, loss 0.468188
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545825.0
get out
0 has test check 2545825.0 and sample count 3274240
 accuracy 77.753 %, best 78.730 %, roc auc score 0.7764, best 0.8001
Finished training it 21504/76743 of epoch 4, 87.53 ms/it, loss 0.467643
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545825.0
get out
1 has test check 2545825.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.79 ms/it, loss 0.466795
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545825.0
get out
3 has test check 2545825.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.78 ms/it, loss 0.468738
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545825.0
get out
2 has test check 2545825.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 87.67 ms/it, loss 0.467506
Finished training it 22528/76743 of epoch 4, 88.13 ms/it, loss 0.468338
Finished training it 22528/76743 of epoch 4, 88.09 ms/it, loss 0.465305
Finished training it 22528/76743 of epoch 4, 88.19 ms/it, loss 0.468124
Finished training it 22528/76743 of epoch 4, 88.28 ms/it, loss 0.469335
Finished training it 23552/76743 of epoch 4, 88.15 ms/it, loss 0.463141
Finished training it 23552/76743 of epoch 4, 88.30 ms/it, loss 0.468118
Finished training it 23552/76743 of epoch 4, 88.30 ms/it, loss 0.464004
Finished training it 23552/76743 of epoch 4, 88.28 ms/it, loss 0.468052
Finished training it 24576/76743 of epoch 4, 99.36 ms/it, loss 0.468632
Finished training it 24576/76743 of epoch 4, 99.44 ms/it, loss 0.468859
Finished training it 24576/76743 of epoch 4, 99.56 ms/it, loss 0.464296
Finished training it 24576/76743 of epoch 4, 99.82 ms/it, loss 0.466895
Finished training it 25600/76743 of epoch 4, 88.48 ms/it, loss 0.465572
Finished training it 25600/76743 of epoch 4, 88.48 ms/it, loss 0.468007
Finished training it 25600/76743 of epoch 4, 88.21 ms/it, loss 0.468053
Finished training it 25600/76743 of epoch 4, 88.30 ms/it, loss 0.466253
Finished training it 26624/76743 of epoch 4, 88.18 ms/it, loss 0.463937
Finished training it 26624/76743 of epoch 4, 88.31 ms/it, loss 0.467132
Finished training it 26624/76743 of epoch 4, 88.51 ms/it, loss 0.467326
Finished training it 26624/76743 of epoch 4, 88.42 ms/it, loss 0.467064
Finished training it 27648/76743 of epoch 4, 88.25 ms/it, loss 0.466829
Finished training it 27648/76743 of epoch 4, 88.11 ms/it, loss 0.464649
Finished training it 27648/76743 of epoch 4, 88.13 ms/it, loss 0.465634
Finished training it 27648/76743 of epoch 4, 88.37 ms/it, loss 0.468914
Finished training it 28672/76743 of epoch 4, 88.38 ms/it, loss 0.467503
Finished training it 28672/76743 of epoch 4, 88.37 ms/it, loss 0.467050
Finished training it 28672/76743 of epoch 4, 88.53 ms/it, loss 0.468947
Finished training it 28672/76743 of epoch 4, 88.67 ms/it, loss 0.467600
Finished training it 29696/76743 of epoch 4, 88.43 ms/it, loss 0.464580
Finished training it 29696/76743 of epoch 4, 88.40 ms/it, loss 0.465258
Finished training it 29696/76743 of epoch 4, 88.26 ms/it, loss 0.465094
Finished training it 29696/76743 of epoch 4, 88.50 ms/it, loss 0.465746
Finished training it 30720/76743 of epoch 4, 88.62 ms/it, loss 0.463370
Finished training it 30720/76743 of epoch 4, 88.30 ms/it, loss 0.465259
Finished training it 30720/76743 of epoch 4, 88.44 ms/it, loss 0.466052
Finished training it 30720/76743 of epoch 4, 88.51 ms/it, loss 0.466570
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547137.0
get out
0 has test check 2547137.0 and sample count 3274240
 accuracy 77.793 %, best 78.730 %, roc auc score 0.7769, best 0.8001
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547137.0
get out
1 has test check 2547137.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 88.47 ms/it, loss 0.466578
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547137.0
get out
2 has test check 2547137.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 88.33 ms/it, loss 0.467793
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547137.0
get out
3 has test check 2547137.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 88.51 ms/it, loss 0.464497
Finished training it 31744/76743 of epoch 4, 88.12 ms/it, loss 0.465470
Finished training it 32768/76743 of epoch 4, 88.67 ms/it, loss 0.466788
Finished training it 32768/76743 of epoch 4, 88.65 ms/it, loss 0.468491
Finished training it 32768/76743 of epoch 4, 88.43 ms/it, loss 0.465746
Finished training it 32768/76743 of epoch 4, 88.29 ms/it, loss 0.464914
Finished training it 33792/76743 of epoch 4, 88.43 ms/it, loss 0.465552
Finished training it 33792/76743 of epoch 4, 88.19 ms/it, loss 0.466506
Finished training it 33792/76743 of epoch 4, 88.20 ms/it, loss 0.466758
Finished training it 33792/76743 of epoch 4, 88.47 ms/it, loss 0.469633
Finished training it 34816/76743 of epoch 4, 88.81 ms/it, loss 0.467814
Finished training it 34816/76743 of epoch 4, 88.62 ms/it, loss 0.469007
Finished training it 34816/76743 of epoch 4, 88.56 ms/it, loss 0.462415
Finished training it 34816/76743 of epoch 4, 88.77 ms/it, loss 0.466128
Finished training it 35840/76743 of epoch 4, 88.57 ms/it, loss 0.463272
Finished training it 35840/76743 of epoch 4, 88.70 ms/it, loss 0.463258
Finished training it 35840/76743 of epoch 4, 88.63 ms/it, loss 0.467894
Finished training it 35840/76743 of epoch 4, 88.61 ms/it, loss 0.466694
Finished training it 36864/76743 of epoch 4, 88.78 ms/it, loss 0.469354
Finished training it 36864/76743 of epoch 4, 88.76 ms/it, loss 0.467558
Finished training it 36864/76743 of epoch 4, 88.93 ms/it, loss 0.466747
Finished training it 36864/76743 of epoch 4, 88.96 ms/it, loss 0.465988
Finished training it 37888/76743 of epoch 4, 88.13 ms/it, loss 0.467693
Finished training it 37888/76743 of epoch 4, 88.50 ms/it, loss 0.466888
Finished training it 37888/76743 of epoch 4, 88.40 ms/it, loss 0.466347
Finished training it 37888/76743 of epoch 4, 88.38 ms/it, loss 0.468168
Finished training it 38912/76743 of epoch 4, 88.18 ms/it, loss 0.466080
Finished training it 38912/76743 of epoch 4, 88.27 ms/it, loss 0.467926
Finished training it 38912/76743 of epoch 4, 88.02 ms/it, loss 0.466336
Finished training it 38912/76743 of epoch 4, 88.28 ms/it, loss 0.464205
Finished training it 39936/76743 of epoch 4, 88.39 ms/it, loss 0.464537
Finished training it 39936/76743 of epoch 4, 88.31 ms/it, loss 0.466612
Finished training it 39936/76743 of epoch 4, 88.49 ms/it, loss 0.467819
Finished training it 39936/76743 of epoch 4, 88.48 ms/it, loss 0.465878
Finished training it 40960/76743 of epoch 4, 88.62 ms/it, loss 0.466034
Finished training it 40960/76743 of epoch 4, 88.30 ms/it, loss 0.467227
Finished training it 40960/76743 of epoch 4, 88.42 ms/it, loss 0.468341
Finished training it 40960/76743 of epoch 4, 88.21 ms/it, loss 0.467287
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547458.0
get out
0 has test check 2547458.0 and sample count 3274240
 accuracy 77.803 %, best 78.730 %, roc auc score 0.7776, best 0.8001
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547458.0
get out
2 has test check 2547458.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.45 ms/it, loss 0.466182
Finished training it 41984/76743 of epoch 4, 88.43 ms/it, loss 0.463652
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547458.0
get out
3 has test check 2547458.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.67 ms/it, loss 0.467596
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547458.0
get out
1 has test check 2547458.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.62 ms/it, loss 0.465675
Finished training it 43008/76743 of epoch 4, 88.12 ms/it, loss 0.467181
Finished training it 43008/76743 of epoch 4, 88.34 ms/it, loss 0.467967
Finished training it 43008/76743 of epoch 4, 88.14 ms/it, loss 0.463204
Finished training it 43008/76743 of epoch 4, 88.34 ms/it, loss 0.466405
Finished training it 44032/76743 of epoch 4, 88.63 ms/it, loss 0.466150
Finished training it 44032/76743 of epoch 4, 88.54 ms/it, loss 0.464790
Finished training it 44032/76743 of epoch 4, 88.64 ms/it, loss 0.468869
Finished training it 44032/76743 of epoch 4, 88.70 ms/it, loss 0.466084
Finished training it 45056/76743 of epoch 4, 93.50 ms/it, loss 0.466633
Finished training it 45056/76743 of epoch 4, 93.19 ms/it, loss 0.465731
Finished training it 45056/76743 of epoch 4, 92.94 ms/it, loss 0.464971
Finished training it 45056/76743 of epoch 4, 93.30 ms/it, loss 0.466829
Finished training it 46080/76743 of epoch 4, 95.02 ms/it, loss 0.464815
Finished training it 46080/76743 of epoch 4, 94.46 ms/it, loss 0.465435
Finished training it 46080/76743 of epoch 4, 95.07 ms/it, loss 0.464540
Finished training it 46080/76743 of epoch 4, 94.46 ms/it, loss 0.465685
Finished training it 47104/76743 of epoch 4, 88.22 ms/it, loss 0.463585
Finished training it 47104/76743 of epoch 4, 88.32 ms/it, loss 0.465903
Finished training it 47104/76743 of epoch 4, 88.07 ms/it, loss 0.466270
Finished training it 47104/76743 of epoch 4, 88.10 ms/it, loss 0.465414
Finished training it 48128/76743 of epoch 4, 88.77 ms/it, loss 0.465772
Finished training it 48128/76743 of epoch 4, 88.62 ms/it, loss 0.465730
Finished training it 48128/76743 of epoch 4, 88.89 ms/it, loss 0.464724
Finished training it 48128/76743 of epoch 4, 88.71 ms/it, loss 0.466396
Finished training it 49152/76743 of epoch 4, 88.48 ms/it, loss 0.465737
Finished training it 49152/76743 of epoch 4, 88.38 ms/it, loss 0.465025
Finished training it 49152/76743 of epoch 4, 88.24 ms/it, loss 0.465704
Finished training it 49152/76743 of epoch 4, 88.55 ms/it, loss 0.465829
Finished training it 50176/76743 of epoch 4, 88.59 ms/it, loss 0.464778
Finished training it 50176/76743 of epoch 4, 88.45 ms/it, loss 0.465881
Finished training it 50176/76743 of epoch 4, 88.55 ms/it, loss 0.467468
Finished training it 50176/76743 of epoch 4, 88.39 ms/it, loss 0.466059
Finished training it 51200/76743 of epoch 4, 88.36 ms/it, loss 0.465409
Finished training it 51200/76743 of epoch 4, 88.32 ms/it, loss 0.465599
Finished training it 51200/76743 of epoch 4, 88.31 ms/it, loss 0.465873
Finished training it 51200/76743 of epoch 4, 88.46 ms/it, loss 0.465060
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544650.0
get out
0 has test check 2544650.0 and sample count 3274240
 accuracy 77.717 %, best 78.730 %, roc auc score 0.7778, best 0.8001
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544650.0
get out
3 has test check 2544650.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 88.07 ms/it, loss 0.467943
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544650.0
get out
1 has test check 2544650.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 88.12 ms/it, loss 0.465188
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544650.0
get out
2 has test check 2544650.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 87.98 ms/it, loss 0.466931
Finished training it 52224/76743 of epoch 4, 87.94 ms/it, loss 0.466501
Finished training it 53248/76743 of epoch 4, 88.51 ms/it, loss 0.465438
Finished training it 53248/76743 of epoch 4, 88.65 ms/it, loss 0.464247
Finished training it 53248/76743 of epoch 4, 88.73 ms/it, loss 0.465706
Finished training it 53248/76743 of epoch 4, 88.57 ms/it, loss 0.466008
Finished training it 54272/76743 of epoch 4, 89.15 ms/it, loss 0.465555
Finished training it 54272/76743 of epoch 4, 88.89 ms/it, loss 0.465526
Finished training it 54272/76743 of epoch 4, 89.13 ms/it, loss 0.464833
Finished training it 54272/76743 of epoch 4, 88.99 ms/it, loss 0.465661
Finished training it 55296/76743 of epoch 4, 88.88 ms/it, loss 0.467333
Finished training it 55296/76743 of epoch 4, 88.83 ms/it, loss 0.466204
Finished training it 55296/76743 of epoch 4, 88.58 ms/it, loss 0.468545
Finished training it 55296/76743 of epoch 4, 88.68 ms/it, loss 0.467469
Finished training it 56320/76743 of epoch 4, 88.58 ms/it, loss 0.465181
Finished training it 56320/76743 of epoch 4, 88.53 ms/it, loss 0.465653
Finished training it 56320/76743 of epoch 4, 88.39 ms/it, loss 0.467941
Finished training it 56320/76743 of epoch 4, 88.34 ms/it, loss 0.467377
Finished training it 57344/76743 of epoch 4, 88.39 ms/it, loss 0.463816
Finished training it 57344/76743 of epoch 4, 88.62 ms/it, loss 0.464378
Finished training it 57344/76743 of epoch 4, 88.65 ms/it, loss 0.464984
Finished training it 57344/76743 of epoch 4, 88.55 ms/it, loss 0.465887
Finished training it 58368/76743 of epoch 4, 88.35 ms/it, loss 0.463495
Finished training it 58368/76743 of epoch 4, 88.52 ms/it, loss 0.466554
Finished training it 58368/76743 of epoch 4, 88.40 ms/it, loss 0.464311
Finished training it 58368/76743 of epoch 4, 88.53 ms/it, loss 0.467034
Finished training it 59392/76743 of epoch 4, 88.60 ms/it, loss 0.463869
Finished training it 59392/76743 of epoch 4, 88.47 ms/it, loss 0.465988
Finished training it 59392/76743 of epoch 4, 88.52 ms/it, loss 0.466343
Finished training it 59392/76743 of epoch 4, 88.33 ms/it, loss 0.467486
Finished training it 60416/76743 of epoch 4, 88.15 ms/it, loss 0.464796
Finished training it 60416/76743 of epoch 4, 88.25 ms/it, loss 0.467212
Finished training it 60416/76743 of epoch 4, 88.38 ms/it, loss 0.463515
Finished training it 60416/76743 of epoch 4, 88.35 ms/it, loss 0.462808
Finished training it 61440/76743 of epoch 4, 88.86 ms/it, loss 0.468497
Finished training it 61440/76743 of epoch 4, 88.59 ms/it, loss 0.464585
Finished training it 61440/76743 of epoch 4, 88.79 ms/it, loss 0.465019
Finished training it 61440/76743 of epoch 4, 88.58 ms/it, loss 0.463674
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547896.0
get out
0 has test check 2547896.0 and sample count 3274240
 accuracy 77.816 %, best 78.730 %, roc auc score 0.7780, best 0.8001
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547896.0
get out
3 has test check 2547896.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 88.16 ms/it, loss 0.465796
Finished training it 62464/76743 of epoch 4, 87.77 ms/it, loss 0.468589
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547896.0
get out
1 has test check 2547896.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 88.06 ms/it, loss 0.467938
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547896.0
get out
2 has test check 2547896.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 87.96 ms/it, loss 0.467054
Finished training it 63488/76743 of epoch 4, 88.36 ms/it, loss 0.467954
Finished training it 63488/76743 of epoch 4, 88.59 ms/it, loss 0.464175
Finished training it 63488/76743 of epoch 4, 88.51 ms/it, loss 0.464232
Finished training it 63488/76743 of epoch 4, 88.60 ms/it, loss 0.468582
Finished training it 64512/76743 of epoch 4, 89.02 ms/it, loss 0.468512
Finished training it 64512/76743 of epoch 4, 88.74 ms/it, loss 0.465386
Finished training it 64512/76743 of epoch 4, 88.94 ms/it, loss 0.464533
Finished training it 64512/76743 of epoch 4, 88.82 ms/it, loss 0.467141
Finished training it 65536/76743 of epoch 4, 98.91 ms/it, loss 0.462430
Finished training it 65536/76743 of epoch 4, 98.60 ms/it, loss 0.465523
Finished training it 65536/76743 of epoch 4, 98.80 ms/it, loss 0.466240
Finished training it 65536/76743 of epoch 4, 99.09 ms/it, loss 0.468177
Finished training it 66560/76743 of epoch 4, 88.01 ms/it, loss 0.464920
Finished training it 66560/76743 of epoch 4, 88.38 ms/it, loss 0.465100
Finished training it 66560/76743 of epoch 4, 88.18 ms/it, loss 0.466415
Finished training it 66560/76743 of epoch 4, 88.33 ms/it, loss 0.462815
Finished training it 67584/76743 of epoch 4, 88.79 ms/it, loss 0.462909
Finished training it 67584/76743 of epoch 4, 88.60 ms/it, loss 0.467432
Finished training it 67584/76743 of epoch 4, 88.74 ms/it, loss 0.467495
Finished training it 67584/76743 of epoch 4, 88.76 ms/it, loss 0.467023
Finished training it 68608/76743 of epoch 4, 88.98 ms/it, loss 0.466916
Finished training it 68608/76743 of epoch 4, 88.71 ms/it, loss 0.465713
Finished training it 68608/76743 of epoch 4, 88.99 ms/it, loss 0.465749
Finished training it 68608/76743 of epoch 4, 88.68 ms/it, loss 0.464582
Finished training it 69632/76743 of epoch 4, 88.73 ms/it, loss 0.466343
Finished training it 69632/76743 of epoch 4, 88.75 ms/it, loss 0.465428
Finished training it 69632/76743 of epoch 4, 88.53 ms/it, loss 0.466162
Finished training it 69632/76743 of epoch 4, 88.41 ms/it, loss 0.466982
Finished training it 70656/76743 of epoch 4, 87.34 ms/it, loss 0.465173
Finished training it 70656/76743 of epoch 4, 87.04 ms/it, loss 0.469563
Finished training it 70656/76743 of epoch 4, 87.09 ms/it, loss 0.466050
Finished training it 70656/76743 of epoch 4, 87.25 ms/it, loss 0.465339
Finished training it 71680/76743 of epoch 4, 81.79 ms/it, loss 0.465839
Finished training it 71680/76743 of epoch 4, 81.77 ms/it, loss 0.463137
Finished training it 71680/76743 of epoch 4, 81.87 ms/it, loss 0.463744
Finished training it 71680/76743 of epoch 4, 81.81 ms/it, loss 0.464819
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549274.0
get out
0 has test check 2549274.0 and sample count 3274240
 accuracy 77.858 %, best 78.730 %, roc auc score 0.7789, best 0.8001
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549274.0
get out
1 has test check 2549274.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 88.22 ms/it, loss 0.466018
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549274.0
get out
2 has test check 2549274.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 88.11 ms/it, loss 0.466854
Finished training it 72704/76743 of epoch 4, 87.98 ms/it, loss 0.467445
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549274.0
get out
3 has test check 2549274.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 88.27 ms/it, loss 0.463266
Finished training it 73728/76743 of epoch 4, 88.82 ms/it, loss 0.466230
Finished training it 73728/76743 of epoch 4, 88.81 ms/it, loss 0.465180
Finished training it 73728/76743 of epoch 4, 88.46 ms/it, loss 0.464084
Finished training it 73728/76743 of epoch 4, 88.56 ms/it, loss 0.465419
Finished training it 74752/76743 of epoch 4, 88.64 ms/it, loss 0.465471
Finished training it 74752/76743 of epoch 4, 88.45 ms/it, loss 0.465868
Finished training it 74752/76743 of epoch 4, 88.39 ms/it, loss 0.464983
Finished training it 74752/76743 of epoch 4, 88.47 ms/it, loss 0.463191
Finished training it 75776/76743 of epoch 4, 88.58 ms/it, loss 0.463758
Finished training it 75776/76743 of epoch 4, 88.40 ms/it, loss 0.467182
Finished training it 75776/76743 of epoch 4, 88.65 ms/it, loss 0.466579
Finished training it 75776/76743 of epoch 4, 88.31 ms/it, loss 0.466211
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549168.0
get out
0 has test check 2549168.0 and sample count 3274240
 accuracy 77.855 %, best 78.730 %, roc auc score 0.7788, best 0.8001
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549168.0
get out
2 has test check 2549168.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549168.0
get out
3 has test check 2549168.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549168.0
get out
1 has test check 2549168.0 and sample count 3274240
