Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.00 ms/it, loss 0.516702
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.37 ms/it, loss 0.513825
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 50.20 ms/it, loss 0.514677
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 50.94 ms/it, loss 0.514511
Finished training it 2048/76743 of epoch 0, 48.09 ms/it, loss 0.501566
Finished training it 2048/76743 of epoch 0, 48.33 ms/it, loss 0.499272
Finished training it 2048/76743 of epoch 0, 48.06 ms/it, loss 0.501884
Finished training it 2048/76743 of epoch 0, 48.18 ms/it, loss 0.500736
Finished training it 3072/76743 of epoch 0, 48.29 ms/it, loss 0.490610
Finished training it 3072/76743 of epoch 0, 48.56 ms/it, loss 0.488415
Finished training it 3072/76743 of epoch 0, 48.24 ms/it, loss 0.492719
Finished training it 3072/76743 of epoch 0, 48.42 ms/it, loss 0.487366
Finished training it 4096/76743 of epoch 0, 48.05 ms/it, loss 0.482558
Finished training it 4096/76743 of epoch 0, 48.30 ms/it, loss 0.484233
Finished training it 4096/76743 of epoch 0, 47.96 ms/it, loss 0.481851
Finished training it 4096/76743 of epoch 0, 47.93 ms/it, loss 0.484565
Finished training it 5120/76743 of epoch 0, 48.41 ms/it, loss 0.476415
Finished training it 5120/76743 of epoch 0, 48.07 ms/it, loss 0.474909
Finished training it 5120/76743 of epoch 0, 48.70 ms/it, loss 0.478779
Finished training it 5120/76743 of epoch 0, 48.42 ms/it, loss 0.480180
Finished training it 6144/76743 of epoch 0, 47.02 ms/it, loss 0.475484
Finished training it 6144/76743 of epoch 0, 47.45 ms/it, loss 0.474379
Finished training it 6144/76743 of epoch 0, 47.13 ms/it, loss 0.473378
Finished training it 6144/76743 of epoch 0, 47.98 ms/it, loss 0.473141
Finished training it 7168/76743 of epoch 0, 47.92 ms/it, loss 0.471760
Finished training it 7168/76743 of epoch 0, 48.36 ms/it, loss 0.472133
Finished training it 7168/76743 of epoch 0, 48.36 ms/it, loss 0.471414
Finished training it 7168/76743 of epoch 0, 47.88 ms/it, loss 0.468577
Finished training it 8192/76743 of epoch 0, 47.60 ms/it, loss 0.467696
Finished training it 8192/76743 of epoch 0, 47.29 ms/it, loss 0.469971
Finished training it 8192/76743 of epoch 0, 47.26 ms/it, loss 0.468344
Finished training it 8192/76743 of epoch 0, 47.49 ms/it, loss 0.466164
Finished training it 9216/76743 of epoch 0, 47.98 ms/it, loss 0.466513
Finished training it 9216/76743 of epoch 0, 47.49 ms/it, loss 0.466562
Finished training it 9216/76743 of epoch 0, 47.52 ms/it, loss 0.466731
Finished training it 9216/76743 of epoch 0, 48.31 ms/it, loss 0.467677
Finished training it 10240/76743 of epoch 0, 47.72 ms/it, loss 0.465191
Finished training it 10240/76743 of epoch 0, 48.06 ms/it, loss 0.465384
Finished training it 10240/76743 of epoch 0, 47.95 ms/it, loss 0.466347
Finished training it 10240/76743 of epoch 0, 47.68 ms/it, loss 0.469050
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549684.0
get out
0 has test check 2549684.0 and sample count 3274240
 accuracy 77.871 %, best 77.871 %, roc auc score 0.7831, best 0.7831
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549684.0
get out
3 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.36 ms/it, loss 0.461716
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549684.0
get out
2 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.38 ms/it, loss 0.463816
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549684.0
get out
1 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.62 ms/it, loss 0.464142
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 48.25 ms/it, loss 0.466849
Finished training it 12288/76743 of epoch 0, 48.62 ms/it, loss 0.466195
Finished training it 12288/76743 of epoch 0, 48.55 ms/it, loss 0.465625
Finished training it 12288/76743 of epoch 0, 48.24 ms/it, loss 0.459643
Finished training it 12288/76743 of epoch 0, 48.27 ms/it, loss 0.463956
Finished training it 13312/76743 of epoch 0, 48.14 ms/it, loss 0.462094
Finished training it 13312/76743 of epoch 0, 48.03 ms/it, loss 0.463700
Finished training it 13312/76743 of epoch 0, 48.53 ms/it, loss 0.462694
Finished training it 13312/76743 of epoch 0, 48.27 ms/it, loss 0.464322
Finished training it 14336/76743 of epoch 0, 48.15 ms/it, loss 0.461709
Finished training it 14336/76743 of epoch 0, 48.16 ms/it, loss 0.464586
Finished training it 14336/76743 of epoch 0, 47.71 ms/it, loss 0.461618
Finished training it 14336/76743 of epoch 0, 47.91 ms/it, loss 0.461938
Finished training it 15360/76743 of epoch 0, 54.04 ms/it, loss 0.462586
Finished training it 15360/76743 of epoch 0, 54.42 ms/it, loss 0.459805
Finished training it 15360/76743 of epoch 0, 53.72 ms/it, loss 0.462678
Finished training it 15360/76743 of epoch 0, 54.42 ms/it, loss 0.460717
Finished training it 16384/76743 of epoch 0, 48.79 ms/it, loss 0.462612
Finished training it 16384/76743 of epoch 0, 49.13 ms/it, loss 0.462164
Finished training it 16384/76743 of epoch 0, 48.71 ms/it, loss 0.461386
Finished training it 16384/76743 of epoch 0, 48.56 ms/it, loss 0.458125
Finished training it 17408/76743 of epoch 0, 47.93 ms/it, loss 0.458936
Finished training it 17408/76743 of epoch 0, 47.83 ms/it, loss 0.459083
Finished training it 17408/76743 of epoch 0, 48.12 ms/it, loss 0.461089
Finished training it 17408/76743 of epoch 0, 47.79 ms/it, loss 0.459490
Finished training it 18432/76743 of epoch 0, 49.08 ms/it, loss 0.458103
Finished training it 18432/76743 of epoch 0, 49.21 ms/it, loss 0.461185
Finished training it 18432/76743 of epoch 0, 48.78 ms/it, loss 0.456773
Finished training it 18432/76743 of epoch 0, 48.94 ms/it, loss 0.460314
Finished training it 19456/76743 of epoch 0, 48.74 ms/it, loss 0.459921
Finished training it 19456/76743 of epoch 0, 48.38 ms/it, loss 0.456601
Finished training it 19456/76743 of epoch 0, 48.68 ms/it, loss 0.459978
Finished training it 19456/76743 of epoch 0, 48.72 ms/it, loss 0.457812
Finished training it 20480/76743 of epoch 0, 49.14 ms/it, loss 0.459376
Finished training it 20480/76743 of epoch 0, 48.91 ms/it, loss 0.458472
Finished training it 20480/76743 of epoch 0, 49.10 ms/it, loss 0.459975
Finished training it 20480/76743 of epoch 0, 48.66 ms/it, loss 0.460096
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2563544.0
get out
0 has test check 2563544.0 and sample count 3274240
 accuracy 78.294 %, best 78.294 %, roc auc score 0.7900, best 0.7900
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2563544.0
get out
2 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.02 ms/it, loss 0.459840
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2563544.0
get out
1 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 47.97 ms/it, loss 0.458065
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 47.96 ms/it, loss 0.458370
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2563544.0
get out
3 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.20 ms/it, loss 0.456938
Finished training it 22528/76743 of epoch 0, 47.61 ms/it, loss 0.458241
Finished training it 22528/76743 of epoch 0, 48.02 ms/it, loss 0.459465
Finished training it 22528/76743 of epoch 0, 47.80 ms/it, loss 0.457982
Finished training it 22528/76743 of epoch 0, 48.14 ms/it, loss 0.455198
Finished training it 23552/76743 of epoch 0, 48.07 ms/it, loss 0.456892
Finished training it 23552/76743 of epoch 0, 48.08 ms/it, loss 0.457524
Finished training it 23552/76743 of epoch 0, 47.95 ms/it, loss 0.457775
Finished training it 23552/76743 of epoch 0, 47.73 ms/it, loss 0.454250
Finished training it 24576/76743 of epoch 0, 47.08 ms/it, loss 0.456124
Finished training it 24576/76743 of epoch 0, 47.14 ms/it, loss 0.454025
Finished training it 24576/76743 of epoch 0, 46.88 ms/it, loss 0.457948
Finished training it 24576/76743 of epoch 0, 47.54 ms/it, loss 0.457084
Finished training it 25600/76743 of epoch 0, 47.64 ms/it, loss 0.456683
Finished training it 25600/76743 of epoch 0, 48.42 ms/it, loss 0.455833
Finished training it 25600/76743 of epoch 0, 47.36 ms/it, loss 0.454512
Finished training it 25600/76743 of epoch 0, 47.85 ms/it, loss 0.454230
Finished training it 26624/76743 of epoch 0, 48.84 ms/it, loss 0.456683
Finished training it 26624/76743 of epoch 0, 48.36 ms/it, loss 0.451934
Finished training it 26624/76743 of epoch 0, 48.83 ms/it, loss 0.454859
Finished training it 26624/76743 of epoch 0, 48.49 ms/it, loss 0.455893
Finished training it 27648/76743 of epoch 0, 48.00 ms/it, loss 0.456670
Finished training it 27648/76743 of epoch 0, 48.09 ms/it, loss 0.457671
Finished training it 27648/76743 of epoch 0, 47.78 ms/it, loss 0.457426
Finished training it 27648/76743 of epoch 0, 47.41 ms/it, loss 0.454501
Finished training it 28672/76743 of epoch 0, 48.04 ms/it, loss 0.455609
Finished training it 28672/76743 of epoch 0, 48.73 ms/it, loss 0.453449
Finished training it 28672/76743 of epoch 0, 48.75 ms/it, loss 0.456664
Finished training it 28672/76743 of epoch 0, 48.09 ms/it, loss 0.457040
Finished training it 29696/76743 of epoch 0, 48.09 ms/it, loss 0.452302
Finished training it 29696/76743 of epoch 0, 48.14 ms/it, loss 0.456050
Finished training it 29696/76743 of epoch 0, 48.10 ms/it, loss 0.455268
Finished training it 29696/76743 of epoch 0, 48.29 ms/it, loss 0.456925
Finished training it 30720/76743 of epoch 0, 47.65 ms/it, loss 0.457169
Finished training it 30720/76743 of epoch 0, 47.55 ms/it, loss 0.453619
Finished training it 30720/76743 of epoch 0, 47.23 ms/it, loss 0.458074
Finished training it 30720/76743 of epoch 0, 47.87 ms/it, loss 0.451933
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2567182.0
get out
0 has test check 2567182.0 and sample count 3274240
 accuracy 78.405 %, best 78.405 %, roc auc score 0.7931, best 0.7931
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 49.02 ms/it, loss 0.454278
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2567182.0
get out
2 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 48.77 ms/it, loss 0.456337
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2567182.0
get out
3 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 48.90 ms/it, loss 0.453122
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2567182.0
get out
1 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 49.07 ms/it, loss 0.452170
Finished training it 32768/76743 of epoch 0, 48.34 ms/it, loss 0.456932
Finished training it 32768/76743 of epoch 0, 48.32 ms/it, loss 0.455261
Finished training it 32768/76743 of epoch 0, 48.52 ms/it, loss 0.457966
Finished training it 32768/76743 of epoch 0, 48.47 ms/it, loss 0.454398
Finished training it 33792/76743 of epoch 0, 48.31 ms/it, loss 0.454994
Finished training it 33792/76743 of epoch 0, 47.91 ms/it, loss 0.456479
Finished training it 33792/76743 of epoch 0, 47.88 ms/it, loss 0.452749
Finished training it 33792/76743 of epoch 0, 48.08 ms/it, loss 0.454992
Finished training it 34816/76743 of epoch 0, 47.88 ms/it, loss 0.454144
Finished training it 34816/76743 of epoch 0, 47.97 ms/it, loss 0.455914
Finished training it 34816/76743 of epoch 0, 47.83 ms/it, loss 0.456577
Finished training it 34816/76743 of epoch 0, 47.96 ms/it, loss 0.453789
Finished training it 35840/76743 of epoch 0, 47.96 ms/it, loss 0.452597
Finished training it 35840/76743 of epoch 0, 52.71 ms/it, loss 0.453834
Finished training it 35840/76743 of epoch 0, 53.16 ms/it, loss 0.452831
Finished training it 35840/76743 of epoch 0, 52.67 ms/it, loss 0.453189
Finished training it 36864/76743 of epoch 0, 49.05 ms/it, loss 0.454233
Finished training it 36864/76743 of epoch 0, 54.10 ms/it, loss 0.452190
Finished training it 36864/76743 of epoch 0, 51.20 ms/it, loss 0.458764
Finished training it 36864/76743 of epoch 0, 49.18 ms/it, loss 0.451942
Finished training it 37888/76743 of epoch 0, 48.58 ms/it, loss 0.455170
Finished training it 37888/76743 of epoch 0, 48.72 ms/it, loss 0.452137
Finished training it 37888/76743 of epoch 0, 49.02 ms/it, loss 0.452008
Finished training it 37888/76743 of epoch 0, 48.42 ms/it, loss 0.450566
Finished training it 38912/76743 of epoch 0, 47.87 ms/it, loss 0.450676
Finished training it 38912/76743 of epoch 0, 47.80 ms/it, loss 0.451677
Finished training it 38912/76743 of epoch 0, 47.58 ms/it, loss 0.453735
Finished training it 38912/76743 of epoch 0, 47.94 ms/it, loss 0.451404
Finished training it 39936/76743 of epoch 0, 48.12 ms/it, loss 0.453750
Finished training it 39936/76743 of epoch 0, 48.46 ms/it, loss 0.451172
Finished training it 39936/76743 of epoch 0, 48.04 ms/it, loss 0.450423
Finished training it 39936/76743 of epoch 0, 48.03 ms/it, loss 0.452518
Finished training it 40960/76743 of epoch 0, 48.68 ms/it, loss 0.453365
Finished training it 40960/76743 of epoch 0, 48.41 ms/it, loss 0.452239
Finished training it 40960/76743 of epoch 0, 48.72 ms/it, loss 0.450162
Finished training it 40960/76743 of epoch 0, 48.57 ms/it, loss 0.453485
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2570506.0
get out
0 has test check 2570506.0 and sample count 3274240
 accuracy 78.507 %, best 78.507 %, roc auc score 0.7957, best 0.7957
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2570506.0
get out
2 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.41 ms/it, loss 0.455943
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2570506.0
get out
3 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.23 ms/it, loss 0.451408
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2570506.0
get out
1 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.22 ms/it, loss 0.450535
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 48.03 ms/it, loss 0.452744
Finished training it 43008/76743 of epoch 0, 48.98 ms/it, loss 0.453662
Finished training it 43008/76743 of epoch 0, 48.71 ms/it, loss 0.452099
Finished training it 43008/76743 of epoch 0, 49.04 ms/it, loss 0.453108
Finished training it 43008/76743 of epoch 0, 49.16 ms/it, loss 0.456309
Finished training it 44032/76743 of epoch 0, 47.68 ms/it, loss 0.451356
Finished training it 44032/76743 of epoch 0, 47.60 ms/it, loss 0.452624
Finished training it 44032/76743 of epoch 0, 47.99 ms/it, loss 0.450840
Finished training it 44032/76743 of epoch 0, 47.90 ms/it, loss 0.450190
Finished training it 45056/76743 of epoch 0, 48.53 ms/it, loss 0.451714
Finished training it 45056/76743 of epoch 0, 48.79 ms/it, loss 0.452669
Finished training it 45056/76743 of epoch 0, 49.20 ms/it, loss 0.453146
Finished training it 45056/76743 of epoch 0, 48.92 ms/it, loss 0.449284
Finished training it 46080/76743 of epoch 0, 48.31 ms/it, loss 0.453401
Finished training it 46080/76743 of epoch 0, 48.12 ms/it, loss 0.452237
Finished training it 46080/76743 of epoch 0, 48.04 ms/it, loss 0.452386
Finished training it 46080/76743 of epoch 0, 47.85 ms/it, loss 0.450302
Finished training it 47104/76743 of epoch 0, 48.06 ms/it, loss 0.451179
Finished training it 47104/76743 of epoch 0, 47.54 ms/it, loss 0.451909
Finished training it 47104/76743 of epoch 0, 47.38 ms/it, loss 0.450972
Finished training it 47104/76743 of epoch 0, 47.98 ms/it, loss 0.451733
Finished training it 48128/76743 of epoch 0, 48.58 ms/it, loss 0.449727
Finished training it 48128/76743 of epoch 0, 47.71 ms/it, loss 0.450537
Finished training it 48128/76743 of epoch 0, 48.36 ms/it, loss 0.452259
Finished training it 48128/76743 of epoch 0, 48.03 ms/it, loss 0.451165
Finished training it 49152/76743 of epoch 0, 48.12 ms/it, loss 0.451423
Finished training it 49152/76743 of epoch 0, 47.47 ms/it, loss 0.454000
Finished training it 49152/76743 of epoch 0, 47.69 ms/it, loss 0.447497
Finished training it 49152/76743 of epoch 0, 47.55 ms/it, loss 0.453314
Finished training it 50176/76743 of epoch 0, 47.44 ms/it, loss 0.452101
Finished training it 50176/76743 of epoch 0, 47.74 ms/it, loss 0.452839
Finished training it 50176/76743 of epoch 0, 47.42 ms/it, loss 0.450412
Finished training it 50176/76743 of epoch 0, 47.63 ms/it, loss 0.450544
Finished training it 51200/76743 of epoch 0, 48.00 ms/it, loss 0.450810
Finished training it 51200/76743 of epoch 0, 47.91 ms/it, loss 0.450537
Finished training it 51200/76743 of epoch 0, 47.75 ms/it, loss 0.448926
Finished training it 51200/76743 of epoch 0, 47.66 ms/it, loss 0.451426
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2572931.0
get out
0 has test check 2572931.0 and sample count 3274240
 accuracy 78.581 %, best 78.581 %, roc auc score 0.7972, best 0.7972
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 47.57 ms/it, loss 0.449577
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2572931.0
get out
1 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.73 ms/it, loss 0.449948
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2572931.0
get out
3 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.78 ms/it, loss 0.451299
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2572931.0
get out
2 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.75 ms/it, loss 0.449063
Finished training it 53248/76743 of epoch 0, 48.76 ms/it, loss 0.448759
Finished training it 53248/76743 of epoch 0, 48.48 ms/it, loss 0.449518
Finished training it 53248/76743 of epoch 0, 48.01 ms/it, loss 0.452576
Finished training it 53248/76743 of epoch 0, 48.15 ms/it, loss 0.449495
Finished training it 54272/76743 of epoch 0, 48.08 ms/it, loss 0.447998
Finished training it 54272/76743 of epoch 0, 47.82 ms/it, loss 0.448124
Finished training it 54272/76743 of epoch 0, 47.66 ms/it, loss 0.451287
Finished training it 54272/76743 of epoch 0, 47.84 ms/it, loss 0.450736
Finished training it 55296/76743 of epoch 0, 47.80 ms/it, loss 0.447134
Finished training it 55296/76743 of epoch 0, 47.42 ms/it, loss 0.449762
Finished training it 55296/76743 of epoch 0, 47.46 ms/it, loss 0.449230
Finished training it 55296/76743 of epoch 0, 47.82 ms/it, loss 0.450467
Finished training it 56320/76743 of epoch 0, 47.52 ms/it, loss 0.451954
Finished training it 56320/76743 of epoch 0, 47.58 ms/it, loss 0.450524
Finished training it 56320/76743 of epoch 0, 47.60 ms/it, loss 0.452406
Finished training it 56320/76743 of epoch 0, 47.78 ms/it, loss 0.448759
Finished training it 57344/76743 of epoch 0, 47.83 ms/it, loss 0.449886
Finished training it 57344/76743 of epoch 0, 47.66 ms/it, loss 0.447740
Finished training it 57344/76743 of epoch 0, 47.49 ms/it, loss 0.451849
Finished training it 57344/76743 of epoch 0, 47.76 ms/it, loss 0.449816
Finished training it 58368/76743 of epoch 0, 48.39 ms/it, loss 0.450219
Finished training it 58368/76743 of epoch 0, 48.53 ms/it, loss 0.450902
Finished training it 58368/76743 of epoch 0, 48.17 ms/it, loss 0.450212
Finished training it 58368/76743 of epoch 0, 48.30 ms/it, loss 0.453564
Finished training it 59392/76743 of epoch 0, 48.29 ms/it, loss 0.448366
Finished training it 59392/76743 of epoch 0, 48.52 ms/it, loss 0.449740
Finished training it 59392/76743 of epoch 0, 48.04 ms/it, loss 0.448196
Finished training it 59392/76743 of epoch 0, 48.60 ms/it, loss 0.448999
Finished training it 60416/76743 of epoch 0, 47.90 ms/it, loss 0.448910
Finished training it 60416/76743 of epoch 0, 47.72 ms/it, loss 0.448607
Finished training it 60416/76743 of epoch 0, 48.16 ms/it, loss 0.448228
Finished training it 60416/76743 of epoch 0, 47.54 ms/it, loss 0.450813
Finished training it 61440/76743 of epoch 0, 47.69 ms/it, loss 0.449368
Finished training it 61440/76743 of epoch 0, 47.76 ms/it, loss 0.450739
Finished training it 61440/76743 of epoch 0, 48.35 ms/it, loss 0.448064
Finished training it 61440/76743 of epoch 0, 47.39 ms/it, loss 0.446945
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571885.0
get out
0 has test check 2571885.0 and sample count 3274240
 accuracy 78.549 %, best 78.581 %, roc auc score 0.7979, best 0.7979
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571885.0
get out
3 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 48.16 ms/it, loss 0.448282
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571885.0
get out
2 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 47.81 ms/it, loss 0.448746
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571885.0
get out
1 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 47.94 ms/it, loss 0.450529
Finished training it 62464/76743 of epoch 0, 47.59 ms/it, loss 0.449810
Finished training it 63488/76743 of epoch 0, 48.19 ms/it, loss 0.451776
Finished training it 63488/76743 of epoch 0, 47.96 ms/it, loss 0.451880
Finished training it 63488/76743 of epoch 0, 48.02 ms/it, loss 0.451507
Finished training it 63488/76743 of epoch 0, 47.70 ms/it, loss 0.446173
Finished training it 64512/76743 of epoch 0, 47.90 ms/it, loss 0.449187
Finished training it 64512/76743 of epoch 0, 48.46 ms/it, loss 0.451185
Finished training it 64512/76743 of epoch 0, 47.99 ms/it, loss 0.449693
Finished training it 64512/76743 of epoch 0, 47.66 ms/it, loss 0.448755
Finished training it 65536/76743 of epoch 0, 47.97 ms/it, loss 0.448244
Finished training it 65536/76743 of epoch 0, 52.63 ms/it, loss 0.449794
Finished training it 65536/76743 of epoch 0, 52.98 ms/it, loss 0.446315
Finished training it 65536/76743 of epoch 0, 52.19 ms/it, loss 0.449632
Finished training it 66560/76743 of epoch 0, 47.78 ms/it, loss 0.448398
Finished training it 66560/76743 of epoch 0, 48.33 ms/it, loss 0.448673
Finished training it 66560/76743 of epoch 0, 53.03 ms/it, loss 0.447972
Finished training it 66560/76743 of epoch 0, 49.28 ms/it, loss 0.450361
Finished training it 67584/76743 of epoch 0, 47.48 ms/it, loss 0.448834
Finished training it 67584/76743 of epoch 0, 48.02 ms/it, loss 0.449102
Finished training it 67584/76743 of epoch 0, 47.65 ms/it, loss 0.448287
Finished training it 67584/76743 of epoch 0, 47.40 ms/it, loss 0.445058
Finished training it 68608/76743 of epoch 0, 47.47 ms/it, loss 0.445898
Finished training it 68608/76743 of epoch 0, 47.98 ms/it, loss 0.448245
Finished training it 68608/76743 of epoch 0, 47.83 ms/it, loss 0.447740
Finished training it 68608/76743 of epoch 0, 47.93 ms/it, loss 0.450626
Finished training it 69632/76743 of epoch 0, 48.32 ms/it, loss 0.447932
Finished training it 69632/76743 of epoch 0, 48.27 ms/it, loss 0.448163
Finished training it 69632/76743 of epoch 0, 48.16 ms/it, loss 0.444949
Finished training it 69632/76743 of epoch 0, 48.31 ms/it, loss 0.448331
Finished training it 70656/76743 of epoch 0, 46.99 ms/it, loss 0.447669
Finished training it 70656/76743 of epoch 0, 47.28 ms/it, loss 0.447329
Finished training it 70656/76743 of epoch 0, 47.23 ms/it, loss 0.449160
Finished training it 70656/76743 of epoch 0, 47.16 ms/it, loss 0.449886
Finished training it 71680/76743 of epoch 0, 48.01 ms/it, loss 0.446542
Finished training it 71680/76743 of epoch 0, 47.82 ms/it, loss 0.448325
Finished training it 71680/76743 of epoch 0, 48.14 ms/it, loss 0.449354
Finished training it 71680/76743 of epoch 0, 48.08 ms/it, loss 0.448979
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576942.0
get out
0 has test check 2576942.0 and sample count 3274240
 accuracy 78.704 %, best 78.704 %, roc auc score 0.7993, best 0.7993
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576942.0
get out
2 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 48.42 ms/it, loss 0.446340
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576942.0
get out
1 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 47.98 ms/it, loss 0.448149
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576942.0
get out
3 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 47.70 ms/it, loss 0.446646
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 47.92 ms/it, loss 0.447217
Finished training it 73728/76743 of epoch 0, 47.69 ms/it, loss 0.447677
Finished training it 73728/76743 of epoch 0, 47.76 ms/it, loss 0.447925
Finished training it 73728/76743 of epoch 0, 47.41 ms/it, loss 0.447366
Finished training it 73728/76743 of epoch 0, 47.88 ms/it, loss 0.447332
Finished training it 74752/76743 of epoch 0, 47.48 ms/it, loss 0.445855
Finished training it 74752/76743 of epoch 0, 47.45 ms/it, loss 0.448510
Finished training it 74752/76743 of epoch 0, 47.74 ms/it, loss 0.448739
Finished training it 74752/76743 of epoch 0, 48.07 ms/it, loss 0.447953
Finished training it 75776/76743 of epoch 0, 48.25 ms/it, loss 0.444716
Finished training it 75776/76743 of epoch 0, 48.32 ms/it, loss 0.448732
Finished training it 75776/76743 of epoch 0, 48.25 ms/it, loss 0.445917
Finished training it 75776/76743 of epoch 0, 47.76 ms/it, loss 0.448973
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.01 ms/it, loss 0.476744
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 69.74 ms/it, loss 0.476866
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.45 ms/it, loss 0.476371
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 69.53 ms/it, loss 0.477531
Finished training it 2048/76743 of epoch 1, 87.26 ms/it, loss 0.500668
Finished training it 2048/76743 of epoch 1, 87.41 ms/it, loss 0.502980
Finished training it 2048/76743 of epoch 1, 87.38 ms/it, loss 0.502547
Finished training it 2048/76743 of epoch 1, 87.05 ms/it, loss 0.503358
Finished training it 3072/76743 of epoch 1, 88.82 ms/it, loss 0.496104
Finished training it 3072/76743 of epoch 1, 88.86 ms/it, loss 0.496704
Finished training it 3072/76743 of epoch 1, 88.30 ms/it, loss 0.501644
Finished training it 3072/76743 of epoch 1, 88.07 ms/it, loss 0.498453
Finished training it 4096/76743 of epoch 1, 88.16 ms/it, loss 0.496113
Finished training it 4096/76743 of epoch 1, 87.57 ms/it, loss 0.498255
Finished training it 4096/76743 of epoch 1, 88.16 ms/it, loss 0.498128
Finished training it 4096/76743 of epoch 1, 87.40 ms/it, loss 0.495785
Finished training it 5120/76743 of epoch 1, 87.18 ms/it, loss 0.491684
Finished training it 5120/76743 of epoch 1, 86.88 ms/it, loss 0.496175
Finished training it 5120/76743 of epoch 1, 86.86 ms/it, loss 0.495462
Finished training it 5120/76743 of epoch 1, 86.77 ms/it, loss 0.493357
Finished training it 6144/76743 of epoch 1, 88.02 ms/it, loss 0.493378
Finished training it 6144/76743 of epoch 1, 87.90 ms/it, loss 0.492526
Finished training it 6144/76743 of epoch 1, 87.51 ms/it, loss 0.494200
Finished training it 6144/76743 of epoch 1, 88.40 ms/it, loss 0.492107
Finished training it 7168/76743 of epoch 1, 88.78 ms/it, loss 0.492626
Finished training it 7168/76743 of epoch 1, 88.22 ms/it, loss 0.492245
Finished training it 7168/76743 of epoch 1, 88.76 ms/it, loss 0.489791
Finished training it 7168/76743 of epoch 1, 88.45 ms/it, loss 0.492852
Finished training it 8192/76743 of epoch 1, 88.22 ms/it, loss 0.489808
Finished training it 8192/76743 of epoch 1, 88.61 ms/it, loss 0.491103
Finished training it 8192/76743 of epoch 1, 88.05 ms/it, loss 0.492997
Finished training it 8192/76743 of epoch 1, 88.12 ms/it, loss 0.490739
Finished training it 9216/76743 of epoch 1, 88.10 ms/it, loss 0.491960
Finished training it 9216/76743 of epoch 1, 88.07 ms/it, loss 0.489144
Finished training it 9216/76743 of epoch 1, 88.16 ms/it, loss 0.490514
Finished training it 9216/76743 of epoch 1, 87.78 ms/it, loss 0.491486
Finished training it 10240/76743 of epoch 1, 88.02 ms/it, loss 0.489313
Finished training it 10240/76743 of epoch 1, 87.54 ms/it, loss 0.490693
Finished training it 10240/76743 of epoch 1, 88.04 ms/it, loss 0.492856
Finished training it 10240/76743 of epoch 1, 87.97 ms/it, loss 0.489619
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2510407.0
get out
0 has test check 2510407.0 and sample count 3274240
 accuracy 76.671 %, best 78.704 %, roc auc score 0.7481, best 0.7993
Finished training it 11264/76743 of epoch 1, 86.79 ms/it, loss 0.492037
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2510407.0
get out
2 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.88 ms/it, loss 0.489123
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2510407.0
get out
3 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.48 ms/it, loss 0.486768
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2510407.0
get out
1 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.60 ms/it, loss 0.490108
Finished training it 12288/76743 of epoch 1, 88.26 ms/it, loss 0.490738
Finished training it 12288/76743 of epoch 1, 88.15 ms/it, loss 0.485474
Finished training it 12288/76743 of epoch 1, 88.60 ms/it, loss 0.491502
Finished training it 12288/76743 of epoch 1, 88.13 ms/it, loss 0.489378
Finished training it 13312/76743 of epoch 1, 88.34 ms/it, loss 0.487112
Finished training it 13312/76743 of epoch 1, 88.20 ms/it, loss 0.488987
Finished training it 13312/76743 of epoch 1, 87.52 ms/it, loss 0.487694
Finished training it 13312/76743 of epoch 1, 87.73 ms/it, loss 0.489990
Finished training it 14336/76743 of epoch 1, 93.98 ms/it, loss 0.487668
Finished training it 14336/76743 of epoch 1, 93.66 ms/it, loss 0.489624
Finished training it 14336/76743 of epoch 1, 93.93 ms/it, loss 0.487749
Finished training it 14336/76743 of epoch 1, 93.66 ms/it, loss 0.488375
Finished training it 15360/76743 of epoch 1, 94.22 ms/it, loss 0.486481
Finished training it 15360/76743 of epoch 1, 94.32 ms/it, loss 0.487590
Finished training it 15360/76743 of epoch 1, 94.13 ms/it, loss 0.485702
Finished training it 15360/76743 of epoch 1, 94.53 ms/it, loss 0.488900
Finished training it 16384/76743 of epoch 1, 87.91 ms/it, loss 0.485999
Finished training it 16384/76743 of epoch 1, 87.69 ms/it, loss 0.486791
Finished training it 16384/76743 of epoch 1, 87.69 ms/it, loss 0.488814
Finished training it 16384/76743 of epoch 1, 88.00 ms/it, loss 0.488679
Finished training it 17408/76743 of epoch 1, 88.22 ms/it, loss 0.486185
Finished training it 17408/76743 of epoch 1, 88.18 ms/it, loss 0.486286
Finished training it 17408/76743 of epoch 1, 88.03 ms/it, loss 0.487614
Finished training it 17408/76743 of epoch 1, 88.01 ms/it, loss 0.484907
Finished training it 18432/76743 of epoch 1, 87.63 ms/it, loss 0.483557
Finished training it 18432/76743 of epoch 1, 87.83 ms/it, loss 0.486053
Finished training it 18432/76743 of epoch 1, 88.20 ms/it, loss 0.488406
Finished training it 18432/76743 of epoch 1, 87.20 ms/it, loss 0.487713
Finished training it 19456/76743 of epoch 1, 87.63 ms/it, loss 0.487415
Finished training it 19456/76743 of epoch 1, 88.09 ms/it, loss 0.486516
Finished training it 19456/76743 of epoch 1, 87.78 ms/it, loss 0.484773
Finished training it 19456/76743 of epoch 1, 87.88 ms/it, loss 0.484874
Finished training it 20480/76743 of epoch 1, 88.52 ms/it, loss 0.487061
Finished training it 20480/76743 of epoch 1, 88.90 ms/it, loss 0.486463
Finished training it 20480/76743 of epoch 1, 88.68 ms/it, loss 0.486882
Finished training it 20480/76743 of epoch 1, 88.16 ms/it, loss 0.484607
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2516253.0
get out
0 has test check 2516253.0 and sample count 3274240
 accuracy 76.850 %, best 78.704 %, roc auc score 0.7534, best 0.7993
Finished training it 21504/76743 of epoch 1, 87.93 ms/it, loss 0.485554
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2516253.0
get out
1 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 88.07 ms/it, loss 0.484749
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2516253.0
get out
3 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.69 ms/it, loss 0.485001
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2516253.0
get out
2 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.77 ms/it, loss 0.488156
Finished training it 22528/76743 of epoch 1, 88.36 ms/it, loss 0.486558
Finished training it 22528/76743 of epoch 1, 88.23 ms/it, loss 0.486885
Finished training it 22528/76743 of epoch 1, 87.91 ms/it, loss 0.483254
Finished training it 22528/76743 of epoch 1, 87.81 ms/it, loss 0.486356
Finished training it 23552/76743 of epoch 1, 88.08 ms/it, loss 0.484726
Finished training it 23552/76743 of epoch 1, 87.76 ms/it, loss 0.482556
Finished training it 23552/76743 of epoch 1, 88.00 ms/it, loss 0.485984
Finished training it 23552/76743 of epoch 1, 87.89 ms/it, loss 0.485435
Finished training it 24576/76743 of epoch 1, 88.22 ms/it, loss 0.482051
Finished training it 24576/76743 of epoch 1, 87.58 ms/it, loss 0.484726
Finished training it 24576/76743 of epoch 1, 87.71 ms/it, loss 0.486725
Finished training it 24576/76743 of epoch 1, 88.44 ms/it, loss 0.484045
Finished training it 25600/76743 of epoch 1, 87.65 ms/it, loss 0.482757
Finished training it 25600/76743 of epoch 1, 87.88 ms/it, loss 0.483939
Finished training it 25600/76743 of epoch 1, 87.74 ms/it, loss 0.483172
Finished training it 25600/76743 of epoch 1, 88.31 ms/it, loss 0.482741
Finished training it 26624/76743 of epoch 1, 87.49 ms/it, loss 0.479914
Finished training it 26624/76743 of epoch 1, 87.87 ms/it, loss 0.485007
Finished training it 26624/76743 of epoch 1, 88.19 ms/it, loss 0.485866
Finished training it 26624/76743 of epoch 1, 87.74 ms/it, loss 0.481898
Finished training it 27648/76743 of epoch 1, 87.98 ms/it, loss 0.484078
Finished training it 27648/76743 of epoch 1, 87.88 ms/it, loss 0.485588
Finished training it 27648/76743 of epoch 1, 87.93 ms/it, loss 0.486189
Finished training it 27648/76743 of epoch 1, 87.56 ms/it, loss 0.482653
Finished training it 28672/76743 of epoch 1, 88.20 ms/it, loss 0.481463
Finished training it 28672/76743 of epoch 1, 87.45 ms/it, loss 0.484874
Finished training it 28672/76743 of epoch 1, 87.85 ms/it, loss 0.483315
Finished training it 28672/76743 of epoch 1, 87.48 ms/it, loss 0.484869
Finished training it 29696/76743 of epoch 1, 87.90 ms/it, loss 0.485562
Finished training it 29696/76743 of epoch 1, 88.05 ms/it, loss 0.484192
Finished training it 29696/76743 of epoch 1, 88.10 ms/it, loss 0.479023
Finished training it 29696/76743 of epoch 1, 88.09 ms/it, loss 0.482860
Finished training it 30720/76743 of epoch 1, 87.22 ms/it, loss 0.485767
Finished training it 30720/76743 of epoch 1, 87.95 ms/it, loss 0.481471
Finished training it 30720/76743 of epoch 1, 87.39 ms/it, loss 0.485413
Finished training it 30720/76743 of epoch 1, 87.62 ms/it, loss 0.481972
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2519434.0
get out
0 has test check 2519434.0 and sample count 3274240
 accuracy 76.947 %, best 78.704 %, roc auc score 0.7554, best 0.7993
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2519434.0
get out
1 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.11 ms/it, loss 0.480955
Finished training it 31744/76743 of epoch 1, 86.95 ms/it, loss 0.481703
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2519434.0
get out
3 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.23 ms/it, loss 0.481879
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2519434.0
get out
2 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.96 ms/it, loss 0.484642
Finished training it 32768/76743 of epoch 1, 89.13 ms/it, loss 0.487365
Finished training it 32768/76743 of epoch 1, 88.21 ms/it, loss 0.484045
Finished training it 32768/76743 of epoch 1, 88.66 ms/it, loss 0.484762
Finished training it 32768/76743 of epoch 1, 88.93 ms/it, loss 0.482243
Finished training it 33792/76743 of epoch 1, 87.80 ms/it, loss 0.482858
Finished training it 33792/76743 of epoch 1, 87.68 ms/it, loss 0.482825
Finished training it 33792/76743 of epoch 1, 87.96 ms/it, loss 0.484377
Finished training it 33792/76743 of epoch 1, 87.83 ms/it, loss 0.480118
Finished training it 34816/76743 of epoch 1, 98.60 ms/it, loss 0.482637
Finished training it 34816/76743 of epoch 1, 98.47 ms/it, loss 0.483563
Finished training it 34816/76743 of epoch 1, 98.13 ms/it, loss 0.481547
Finished training it 34816/76743 of epoch 1, 98.01 ms/it, loss 0.484524
Finished training it 35840/76743 of epoch 1, 87.64 ms/it, loss 0.481940
Finished training it 35840/76743 of epoch 1, 88.03 ms/it, loss 0.481962
Finished training it 35840/76743 of epoch 1, 88.07 ms/it, loss 0.481203
Finished training it 35840/76743 of epoch 1, 88.21 ms/it, loss 0.481007
Finished training it 36864/76743 of epoch 1, 87.45 ms/it, loss 0.480275
Finished training it 36864/76743 of epoch 1, 87.87 ms/it, loss 0.482424
Finished training it 36864/76743 of epoch 1, 88.38 ms/it, loss 0.486623
Finished training it 36864/76743 of epoch 1, 88.23 ms/it, loss 0.480883
Finished training it 37888/76743 of epoch 1, 88.09 ms/it, loss 0.483051
Finished training it 37888/76743 of epoch 1, 88.64 ms/it, loss 0.479779
Finished training it 37888/76743 of epoch 1, 88.20 ms/it, loss 0.480577
Finished training it 37888/76743 of epoch 1, 87.96 ms/it, loss 0.480079
Finished training it 38912/76743 of epoch 1, 87.69 ms/it, loss 0.479566
Finished training it 38912/76743 of epoch 1, 88.04 ms/it, loss 0.479919
Finished training it 38912/76743 of epoch 1, 87.65 ms/it, loss 0.481910
Finished training it 38912/76743 of epoch 1, 87.72 ms/it, loss 0.478146
Finished training it 39936/76743 of epoch 1, 87.93 ms/it, loss 0.478717
Finished training it 39936/76743 of epoch 1, 87.93 ms/it, loss 0.481487
Finished training it 39936/76743 of epoch 1, 87.53 ms/it, loss 0.479583
Finished training it 39936/76743 of epoch 1, 87.90 ms/it, loss 0.481891
Finished training it 40960/76743 of epoch 1, 88.06 ms/it, loss 0.479310
Finished training it 40960/76743 of epoch 1, 87.51 ms/it, loss 0.482568
Finished training it 40960/76743 of epoch 1, 87.92 ms/it, loss 0.480382
Finished training it 40960/76743 of epoch 1, 87.65 ms/it, loss 0.481380
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2522346.0
get out
0 has test check 2522346.0 and sample count 3274240
 accuracy 77.036 %, best 78.704 %, roc auc score 0.7585, best 0.7993
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2522346.0
get out
1 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.51 ms/it, loss 0.478080
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2522346.0
get out
3 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.25 ms/it, loss 0.480475
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2522346.0
get out
2 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.11 ms/it, loss 0.485047
Finished training it 41984/76743 of epoch 1, 89.07 ms/it, loss 0.482181
Finished training it 43008/76743 of epoch 1, 88.30 ms/it, loss 0.481530
Finished training it 43008/76743 of epoch 1, 88.66 ms/it, loss 0.482755
Finished training it 43008/76743 of epoch 1, 88.36 ms/it, loss 0.481712
Finished training it 43008/76743 of epoch 1, 88.64 ms/it, loss 0.484103
Finished training it 44032/76743 of epoch 1, 87.35 ms/it, loss 0.480952
Finished training it 44032/76743 of epoch 1, 87.70 ms/it, loss 0.479428
Finished training it 44032/76743 of epoch 1, 87.80 ms/it, loss 0.480779
Finished training it 44032/76743 of epoch 1, 87.48 ms/it, loss 0.479874
Finished training it 45056/76743 of epoch 1, 88.24 ms/it, loss 0.479948
Finished training it 45056/76743 of epoch 1, 87.94 ms/it, loss 0.480746
Finished training it 45056/76743 of epoch 1, 88.02 ms/it, loss 0.481502
Finished training it 45056/76743 of epoch 1, 88.08 ms/it, loss 0.479408
Finished training it 46080/76743 of epoch 1, 87.68 ms/it, loss 0.480805
Finished training it 46080/76743 of epoch 1, 87.79 ms/it, loss 0.481201
Finished training it 46080/76743 of epoch 1, 87.55 ms/it, loss 0.479085
Finished training it 46080/76743 of epoch 1, 87.31 ms/it, loss 0.480154
Finished training it 47104/76743 of epoch 1, 88.39 ms/it, loss 0.480461
Finished training it 47104/76743 of epoch 1, 87.76 ms/it, loss 0.479963
Finished training it 47104/76743 of epoch 1, 87.75 ms/it, loss 0.479599
Finished training it 47104/76743 of epoch 1, 88.10 ms/it, loss 0.479998
Finished training it 48128/76743 of epoch 1, 89.06 ms/it, loss 0.479839
Finished training it 48128/76743 of epoch 1, 89.03 ms/it, loss 0.478693
Finished training it 48128/76743 of epoch 1, 89.11 ms/it, loss 0.478783
Finished training it 48128/76743 of epoch 1, 88.99 ms/it, loss 0.482049
Finished training it 49152/76743 of epoch 1, 88.05 ms/it, loss 0.482810
Finished training it 49152/76743 of epoch 1, 87.85 ms/it, loss 0.482564
Finished training it 49152/76743 of epoch 1, 88.38 ms/it, loss 0.479114
Finished training it 49152/76743 of epoch 1, 87.60 ms/it, loss 0.476939
Finished training it 50176/76743 of epoch 1, 88.38 ms/it, loss 0.479073
Finished training it 50176/76743 of epoch 1, 89.20 ms/it, loss 0.480833
Finished training it 50176/76743 of epoch 1, 88.78 ms/it, loss 0.478890
Finished training it 50176/76743 of epoch 1, 88.75 ms/it, loss 0.480178
Finished training it 51200/76743 of epoch 1, 87.69 ms/it, loss 0.479020
Finished training it 51200/76743 of epoch 1, 87.52 ms/it, loss 0.479799
Finished training it 51200/76743 of epoch 1, 87.93 ms/it, loss 0.479597
Finished training it 51200/76743 of epoch 1, 87.60 ms/it, loss 0.479644
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2525202.0
get out
0 has test check 2525202.0 and sample count 3274240
 accuracy 77.123 %, best 78.704 %, roc auc score 0.7605, best 0.7993
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2525202.0
get out
1 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.36 ms/it, loss 0.478441
Finished training it 52224/76743 of epoch 1, 88.12 ms/it, loss 0.479141
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2525202.0
get out
3 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.43 ms/it, loss 0.479095
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2525202.0
get out
2 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.35 ms/it, loss 0.477308
Finished training it 53248/76743 of epoch 1, 88.00 ms/it, loss 0.477567
Finished training it 53248/76743 of epoch 1, 87.73 ms/it, loss 0.479475
Finished training it 53248/76743 of epoch 1, 87.94 ms/it, loss 0.477555
Finished training it 53248/76743 of epoch 1, 87.94 ms/it, loss 0.481361
Finished training it 54272/76743 of epoch 1, 88.00 ms/it, loss 0.476612
Finished training it 54272/76743 of epoch 1, 87.87 ms/it, loss 0.480235
Finished training it 54272/76743 of epoch 1, 87.69 ms/it, loss 0.476583
Finished training it 54272/76743 of epoch 1, 87.30 ms/it, loss 0.478982
Finished training it 55296/76743 of epoch 1, 93.45 ms/it, loss 0.478187
Finished training it 55296/76743 of epoch 1, 93.81 ms/it, loss 0.477381
Finished training it 55296/76743 of epoch 1, 92.90 ms/it, loss 0.476472
Finished training it 55296/76743 of epoch 1, 93.05 ms/it, loss 0.480263
Finished training it 56320/76743 of epoch 1, 93.17 ms/it, loss 0.478249
Finished training it 56320/76743 of epoch 1, 92.77 ms/it, loss 0.480550
Finished training it 56320/76743 of epoch 1, 93.01 ms/it, loss 0.479755
Finished training it 56320/76743 of epoch 1, 92.62 ms/it, loss 0.480162
Finished training it 57344/76743 of epoch 1, 87.45 ms/it, loss 0.477751
Finished training it 57344/76743 of epoch 1, 87.50 ms/it, loss 0.480612
Finished training it 57344/76743 of epoch 1, 88.18 ms/it, loss 0.478054
Finished training it 57344/76743 of epoch 1, 87.75 ms/it, loss 0.479600
Finished training it 58368/76743 of epoch 1, 88.78 ms/it, loss 0.479331
Finished training it 58368/76743 of epoch 1, 88.81 ms/it, loss 0.478659
Finished training it 58368/76743 of epoch 1, 88.26 ms/it, loss 0.481855
Finished training it 58368/76743 of epoch 1, 88.30 ms/it, loss 0.478205
Finished training it 59392/76743 of epoch 1, 88.44 ms/it, loss 0.478459
Finished training it 59392/76743 of epoch 1, 88.45 ms/it, loss 0.477618
Finished training it 59392/76743 of epoch 1, 87.97 ms/it, loss 0.478250
Finished training it 59392/76743 of epoch 1, 88.09 ms/it, loss 0.477477
Finished training it 60416/76743 of epoch 1, 88.49 ms/it, loss 0.476815
Finished training it 60416/76743 of epoch 1, 88.06 ms/it, loss 0.476961
Finished training it 60416/76743 of epoch 1, 88.25 ms/it, loss 0.478518
Finished training it 60416/76743 of epoch 1, 87.70 ms/it, loss 0.479036
Finished training it 61440/76743 of epoch 1, 88.26 ms/it, loss 0.476642
Finished training it 61440/76743 of epoch 1, 88.24 ms/it, loss 0.477849
Finished training it 61440/76743 of epoch 1, 88.43 ms/it, loss 0.479464
Finished training it 61440/76743 of epoch 1, 87.86 ms/it, loss 0.480364
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2526115.0
get out
0 has test check 2526115.0 and sample count 3274240
 accuracy 77.151 %, best 78.704 %, roc auc score 0.7617, best 0.7993
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2526115.0
get out
3 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 87.95 ms/it, loss 0.476667
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2526115.0
get out
1 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 88.36 ms/it, loss 0.479368
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2526115.0
get out
2 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 87.76 ms/it, loss 0.477677
Finished training it 62464/76743 of epoch 1, 87.95 ms/it, loss 0.479592
Finished training it 63488/76743 of epoch 1, 88.57 ms/it, loss 0.479896
Finished training it 63488/76743 of epoch 1, 88.28 ms/it, loss 0.474226
Finished training it 63488/76743 of epoch 1, 88.70 ms/it, loss 0.481390
Finished training it 63488/76743 of epoch 1, 88.04 ms/it, loss 0.480865
Finished training it 64512/76743 of epoch 1, 87.75 ms/it, loss 0.480227
Finished training it 64512/76743 of epoch 1, 87.45 ms/it, loss 0.477628
Finished training it 64512/76743 of epoch 1, 87.56 ms/it, loss 0.477494
Finished training it 64512/76743 of epoch 1, 87.42 ms/it, loss 0.478573
Finished training it 65536/76743 of epoch 1, 87.70 ms/it, loss 0.478486
Finished training it 65536/76743 of epoch 1, 87.98 ms/it, loss 0.474954
Finished training it 65536/76743 of epoch 1, 87.39 ms/it, loss 0.477289
Finished training it 65536/76743 of epoch 1, 87.47 ms/it, loss 0.476502
Finished training it 66560/76743 of epoch 1, 87.74 ms/it, loss 0.477557
Finished training it 66560/76743 of epoch 1, 88.12 ms/it, loss 0.477075
Finished training it 66560/76743 of epoch 1, 88.16 ms/it, loss 0.477397
Finished training it 66560/76743 of epoch 1, 88.22 ms/it, loss 0.479923
Finished training it 67584/76743 of epoch 1, 88.34 ms/it, loss 0.477107
Finished training it 67584/76743 of epoch 1, 88.18 ms/it, loss 0.474307
Finished training it 67584/76743 of epoch 1, 88.19 ms/it, loss 0.476936
Finished training it 67584/76743 of epoch 1, 88.42 ms/it, loss 0.476977
Finished training it 68608/76743 of epoch 1, 88.49 ms/it, loss 0.476342
Finished training it 68608/76743 of epoch 1, 88.62 ms/it, loss 0.478706
Finished training it 68608/76743 of epoch 1, 88.48 ms/it, loss 0.476777
Finished training it 68608/76743 of epoch 1, 88.39 ms/it, loss 0.474963
Finished training it 69632/76743 of epoch 1, 88.94 ms/it, loss 0.476321
Finished training it 69632/76743 of epoch 1, 88.32 ms/it, loss 0.474262
Finished training it 69632/76743 of epoch 1, 88.85 ms/it, loss 0.475717
Finished training it 69632/76743 of epoch 1, 88.33 ms/it, loss 0.477470
Finished training it 70656/76743 of epoch 1, 89.73 ms/it, loss 0.476265
Finished training it 70656/76743 of epoch 1, 89.76 ms/it, loss 0.476834
Finished training it 70656/76743 of epoch 1, 89.51 ms/it, loss 0.478328
Finished training it 70656/76743 of epoch 1, 89.41 ms/it, loss 0.478120
Finished training it 71680/76743 of epoch 1, 89.24 ms/it, loss 0.476943
Finished training it 71680/76743 of epoch 1, 89.36 ms/it, loss 0.475283
Finished training it 71680/76743 of epoch 1, 89.08 ms/it, loss 0.477260
Finished training it 71680/76743 of epoch 1, 88.97 ms/it, loss 0.478507
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2529471.0
get out
0 has test check 2529471.0 and sample count 3274240
 accuracy 77.254 %, best 78.704 %, roc auc score 0.7638, best 0.7993
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2529471.0
get out
1 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.09 ms/it, loss 0.477651
Finished training it 72704/76743 of epoch 1, 87.76 ms/it, loss 0.476826
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2529471.0
get out
2 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 87.77 ms/it, loss 0.475263
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2529471.0
get out
3 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.07 ms/it, loss 0.475771
Finished training it 73728/76743 of epoch 1, 87.96 ms/it, loss 0.476415
Finished training it 73728/76743 of epoch 1, 87.79 ms/it, loss 0.476026
Finished training it 73728/76743 of epoch 1, 87.84 ms/it, loss 0.475823
Finished training it 73728/76743 of epoch 1, 87.55 ms/it, loss 0.476500
Finished training it 74752/76743 of epoch 1, 88.35 ms/it, loss 0.477357
Finished training it 74752/76743 of epoch 1, 88.36 ms/it, loss 0.475124
Finished training it 74752/76743 of epoch 1, 88.33 ms/it, loss 0.478557
Finished training it 74752/76743 of epoch 1, 88.01 ms/it, loss 0.476648
Finished training it 75776/76743 of epoch 1, 93.54 ms/it, loss 0.477980
Finished training it 75776/76743 of epoch 1, 93.41 ms/it, loss 0.476613
Finished training it 75776/76743 of epoch 1, 94.08 ms/it, loss 0.474237
Finished training it 75776/76743 of epoch 1, 94.17 ms/it, loss 0.474562
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.11 ms/it, loss 0.475236
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.72 ms/it, loss 0.476394
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.35 ms/it, loss 0.477190
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.09 ms/it, loss 0.475193
Finished training it 2048/76743 of epoch 2, 88.09 ms/it, loss 0.477360
Finished training it 2048/76743 of epoch 2, 88.06 ms/it, loss 0.475392
Finished training it 2048/76743 of epoch 2, 87.99 ms/it, loss 0.477521
Finished training it 2048/76743 of epoch 2, 87.91 ms/it, loss 0.477324
Finished training it 3072/76743 of epoch 2, 87.78 ms/it, loss 0.475776
Finished training it 3072/76743 of epoch 2, 87.72 ms/it, loss 0.478640
Finished training it 3072/76743 of epoch 2, 87.93 ms/it, loss 0.472468
Finished training it 3072/76743 of epoch 2, 87.98 ms/it, loss 0.474872
Finished training it 4096/76743 of epoch 2, 87.76 ms/it, loss 0.475188
Finished training it 4096/76743 of epoch 2, 87.98 ms/it, loss 0.474764
Finished training it 4096/76743 of epoch 2, 87.62 ms/it, loss 0.477921
Finished training it 4096/76743 of epoch 2, 87.95 ms/it, loss 0.477637
Finished training it 5120/76743 of epoch 2, 87.30 ms/it, loss 0.474859
Finished training it 5120/76743 of epoch 2, 87.58 ms/it, loss 0.477217
Finished training it 5120/76743 of epoch 2, 87.56 ms/it, loss 0.478462
Finished training it 5120/76743 of epoch 2, 87.45 ms/it, loss 0.472761
Finished training it 6144/76743 of epoch 2, 88.02 ms/it, loss 0.476638
Finished training it 6144/76743 of epoch 2, 88.27 ms/it, loss 0.475160
Finished training it 6144/76743 of epoch 2, 88.31 ms/it, loss 0.475240
Finished training it 6144/76743 of epoch 2, 88.23 ms/it, loss 0.476473
Finished training it 7168/76743 of epoch 2, 87.72 ms/it, loss 0.476499
Finished training it 7168/76743 of epoch 2, 87.64 ms/it, loss 0.473584
Finished training it 7168/76743 of epoch 2, 87.39 ms/it, loss 0.475574
Finished training it 7168/76743 of epoch 2, 87.67 ms/it, loss 0.476791
Finished training it 8192/76743 of epoch 2, 88.53 ms/it, loss 0.472882
Finished training it 8192/76743 of epoch 2, 88.28 ms/it, loss 0.474742
Finished training it 8192/76743 of epoch 2, 88.13 ms/it, loss 0.477009
Finished training it 8192/76743 of epoch 2, 88.44 ms/it, loss 0.474598
Finished training it 9216/76743 of epoch 2, 88.02 ms/it, loss 0.474618
Finished training it 9216/76743 of epoch 2, 87.96 ms/it, loss 0.476648
Finished training it 9216/76743 of epoch 2, 87.61 ms/it, loss 0.475849
Finished training it 9216/76743 of epoch 2, 87.72 ms/it, loss 0.474602
Finished training it 10240/76743 of epoch 2, 88.26 ms/it, loss 0.478308
Finished training it 10240/76743 of epoch 2, 88.27 ms/it, loss 0.475900
Finished training it 10240/76743 of epoch 2, 88.60 ms/it, loss 0.475591
Finished training it 10240/76743 of epoch 2, 88.45 ms/it, loss 0.474836
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2530982.0
get out
0 has test check 2530982.0 and sample count 3274240
 accuracy 77.300 %, best 78.704 %, roc auc score 0.7655, best 0.7993
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2530982.0
get out
1 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 88.06 ms/it, loss 0.475026
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2530982.0
get out
2 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.81 ms/it, loss 0.474754
Finished training it 11264/76743 of epoch 2, 87.90 ms/it, loss 0.477843
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2530982.0
get out
3 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.89 ms/it, loss 0.472637
Finished training it 12288/76743 of epoch 2, 87.85 ms/it, loss 0.477253
Finished training it 12288/76743 of epoch 2, 87.59 ms/it, loss 0.475798
Finished training it 12288/76743 of epoch 2, 87.58 ms/it, loss 0.471276
Finished training it 12288/76743 of epoch 2, 87.37 ms/it, loss 0.477149
Finished training it 13312/76743 of epoch 2, 88.10 ms/it, loss 0.474495
Finished training it 13312/76743 of epoch 2, 87.96 ms/it, loss 0.476647
Finished training it 13312/76743 of epoch 2, 88.13 ms/it, loss 0.474769
Finished training it 13312/76743 of epoch 2, 88.12 ms/it, loss 0.476262
Finished training it 14336/76743 of epoch 2, 87.89 ms/it, loss 0.474144
Finished training it 14336/76743 of epoch 2, 87.96 ms/it, loss 0.473611
Finished training it 14336/76743 of epoch 2, 88.10 ms/it, loss 0.474695
Finished training it 14336/76743 of epoch 2, 87.88 ms/it, loss 0.476864
Finished training it 15360/76743 of epoch 2, 88.49 ms/it, loss 0.473047
Finished training it 15360/76743 of epoch 2, 88.25 ms/it, loss 0.473926
Finished training it 15360/76743 of epoch 2, 88.57 ms/it, loss 0.475973
Finished training it 15360/76743 of epoch 2, 88.53 ms/it, loss 0.475011
Finished training it 16384/76743 of epoch 2, 88.61 ms/it, loss 0.475323
Finished training it 16384/76743 of epoch 2, 88.27 ms/it, loss 0.472684
Finished training it 16384/76743 of epoch 2, 88.14 ms/it, loss 0.476597
Finished training it 16384/76743 of epoch 2, 88.34 ms/it, loss 0.476801
Finished training it 17408/76743 of epoch 2, 88.08 ms/it, loss 0.472875
Finished training it 17408/76743 of epoch 2, 87.82 ms/it, loss 0.472435
Finished training it 17408/76743 of epoch 2, 87.96 ms/it, loss 0.474078
Finished training it 17408/76743 of epoch 2, 88.00 ms/it, loss 0.475640
Finished training it 18432/76743 of epoch 2, 88.37 ms/it, loss 0.475658
Finished training it 18432/76743 of epoch 2, 88.21 ms/it, loss 0.476605
Finished training it 18432/76743 of epoch 2, 87.99 ms/it, loss 0.474099
Finished training it 18432/76743 of epoch 2, 88.16 ms/it, loss 0.471871
Finished training it 19456/76743 of epoch 2, 87.92 ms/it, loss 0.472421
Finished training it 19456/76743 of epoch 2, 88.13 ms/it, loss 0.473286
Finished training it 19456/76743 of epoch 2, 88.12 ms/it, loss 0.475046
Finished training it 19456/76743 of epoch 2, 87.93 ms/it, loss 0.475285
Finished training it 20480/76743 of epoch 2, 88.00 ms/it, loss 0.475274
Finished training it 20480/76743 of epoch 2, 87.71 ms/it, loss 0.474770
Finished training it 20480/76743 of epoch 2, 88.03 ms/it, loss 0.474858
Finished training it 20480/76743 of epoch 2, 87.78 ms/it, loss 0.473372
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533419.0
get out
0 has test check 2533419.0 and sample count 3274240
 accuracy 77.374 %, best 78.704 %, roc auc score 0.7673, best 0.7993
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533419.0
get out
1 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.15 ms/it, loss 0.473876
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533419.0
get out
3 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.11 ms/it, loss 0.473643
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533419.0
get out
2 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 87.75 ms/it, loss 0.476905
Finished training it 21504/76743 of epoch 2, 87.81 ms/it, loss 0.474365
Finished training it 22528/76743 of epoch 2, 88.13 ms/it, loss 0.474763
Finished training it 22528/76743 of epoch 2, 88.21 ms/it, loss 0.475747
Finished training it 22528/76743 of epoch 2, 88.12 ms/it, loss 0.474561
Finished training it 22528/76743 of epoch 2, 88.44 ms/it, loss 0.471929
Finished training it 23552/76743 of epoch 2, 88.16 ms/it, loss 0.475516
Finished training it 23552/76743 of epoch 2, 88.24 ms/it, loss 0.474717
Finished training it 23552/76743 of epoch 2, 88.00 ms/it, loss 0.473951
Finished training it 23552/76743 of epoch 2, 88.08 ms/it, loss 0.471438
Finished training it 24576/76743 of epoch 2, 100.19 ms/it, loss 0.473420
Finished training it 24576/76743 of epoch 2, 99.62 ms/it, loss 0.471352
Finished training it 24576/76743 of epoch 2, 99.75 ms/it, loss 0.475626
Finished training it 24576/76743 of epoch 2, 100.21 ms/it, loss 0.473787
Finished training it 25600/76743 of epoch 2, 88.78 ms/it, loss 0.472035
Finished training it 25600/76743 of epoch 2, 88.79 ms/it, loss 0.473008
Finished training it 25600/76743 of epoch 2, 88.60 ms/it, loss 0.473720
Finished training it 25600/76743 of epoch 2, 88.47 ms/it, loss 0.472826
Finished training it 26624/76743 of epoch 2, 88.25 ms/it, loss 0.475623
Finished training it 26624/76743 of epoch 2, 88.20 ms/it, loss 0.471713
Finished training it 26624/76743 of epoch 2, 88.25 ms/it, loss 0.474653
Finished training it 26624/76743 of epoch 2, 88.17 ms/it, loss 0.469980
Finished training it 27648/76743 of epoch 2, 88.03 ms/it, loss 0.476055
Finished training it 27648/76743 of epoch 2, 87.76 ms/it, loss 0.472410
Finished training it 27648/76743 of epoch 2, 87.93 ms/it, loss 0.474534
Finished training it 27648/76743 of epoch 2, 87.96 ms/it, loss 0.474927
Finished training it 28672/76743 of epoch 2, 88.19 ms/it, loss 0.475670
Finished training it 28672/76743 of epoch 2, 88.33 ms/it, loss 0.471784
Finished training it 28672/76743 of epoch 2, 88.31 ms/it, loss 0.474014
Finished training it 28672/76743 of epoch 2, 88.31 ms/it, loss 0.474250
Finished training it 29696/76743 of epoch 2, 88.14 ms/it, loss 0.473946
Finished training it 29696/76743 of epoch 2, 88.20 ms/it, loss 0.469729
Finished training it 29696/76743 of epoch 2, 87.98 ms/it, loss 0.476240
Finished training it 29696/76743 of epoch 2, 87.93 ms/it, loss 0.474946
Finished training it 30720/76743 of epoch 2, 87.45 ms/it, loss 0.476247
Finished training it 30720/76743 of epoch 2, 87.69 ms/it, loss 0.472640
Finished training it 30720/76743 of epoch 2, 87.70 ms/it, loss 0.471338
Finished training it 30720/76743 of epoch 2, 87.59 ms/it, loss 0.476317
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534629.0
get out
0 has test check 2534629.0 and sample count 3274240
 accuracy 77.411 %, best 78.704 %, roc auc score 0.7680, best 0.7993
Finished training it 31744/76743 of epoch 2, 85.98 ms/it, loss 0.472831
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534629.0
get out
1 has test check 2534629.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.20 ms/it, loss 0.471856
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534629.0
get out
3 has test check 2534629.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.13 ms/it, loss 0.472409
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534629.0
get out
2 has test check 2534629.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 85.96 ms/it, loss 0.475128
Finished training it 32768/76743 of epoch 2, 86.04 ms/it, loss 0.477377
Finished training it 32768/76743 of epoch 2, 85.92 ms/it, loss 0.475604
Finished training it 32768/76743 of epoch 2, 86.02 ms/it, loss 0.473557
Finished training it 32768/76743 of epoch 2, 85.80 ms/it, loss 0.476069
Finished training it 33792/76743 of epoch 2, 86.65 ms/it, loss 0.475582
Finished training it 33792/76743 of epoch 2, 86.64 ms/it, loss 0.474006
Finished training it 33792/76743 of epoch 2, 86.77 ms/it, loss 0.473986
Finished training it 33792/76743 of epoch 2, 86.61 ms/it, loss 0.471466
Finished training it 34816/76743 of epoch 2, 85.81 ms/it, loss 0.473295
Finished training it 34816/76743 of epoch 2, 85.77 ms/it, loss 0.475864
Finished training it 34816/76743 of epoch 2, 85.79 ms/it, loss 0.474610
Finished training it 34816/76743 of epoch 2, 85.87 ms/it, loss 0.474170
Finished training it 35840/76743 of epoch 2, 85.95 ms/it, loss 0.473413
Finished training it 35840/76743 of epoch 2, 86.17 ms/it, loss 0.472395
Finished training it 35840/76743 of epoch 2, 86.06 ms/it, loss 0.472425
Finished training it 35840/76743 of epoch 2, 86.08 ms/it, loss 0.472920
Finished training it 36864/76743 of epoch 2, 85.92 ms/it, loss 0.477510
Finished training it 36864/76743 of epoch 2, 85.71 ms/it, loss 0.473752
Finished training it 36864/76743 of epoch 2, 85.82 ms/it, loss 0.472076
Finished training it 36864/76743 of epoch 2, 85.96 ms/it, loss 0.472242
Finished training it 37888/76743 of epoch 2, 86.01 ms/it, loss 0.474926
Finished training it 37888/76743 of epoch 2, 86.13 ms/it, loss 0.471247
Finished training it 37888/76743 of epoch 2, 86.16 ms/it, loss 0.471523
Finished training it 37888/76743 of epoch 2, 86.18 ms/it, loss 0.472065
Finished training it 38912/76743 of epoch 2, 86.10 ms/it, loss 0.471838
Finished training it 38912/76743 of epoch 2, 86.04 ms/it, loss 0.471171
Finished training it 38912/76743 of epoch 2, 86.07 ms/it, loss 0.470149
Finished training it 38912/76743 of epoch 2, 85.92 ms/it, loss 0.473128
Finished training it 39936/76743 of epoch 2, 86.68 ms/it, loss 0.473928
Finished training it 39936/76743 of epoch 2, 86.64 ms/it, loss 0.471493
Finished training it 39936/76743 of epoch 2, 86.60 ms/it, loss 0.470826
Finished training it 39936/76743 of epoch 2, 86.43 ms/it, loss 0.473525
Finished training it 40960/76743 of epoch 2, 86.29 ms/it, loss 0.473236
Finished training it 40960/76743 of epoch 2, 86.28 ms/it, loss 0.471402
Finished training it 40960/76743 of epoch 2, 86.01 ms/it, loss 0.474296
Finished training it 40960/76743 of epoch 2, 86.11 ms/it, loss 0.472714
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535626.0
get out
0 has test check 2535626.0 and sample count 3274240
 accuracy 77.442 %, best 78.704 %, roc auc score 0.7689, best 0.7993
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535626.0
get out
3 has test check 2535626.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 88.08 ms/it, loss 0.472462
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535626.0
get out
1 has test check 2535626.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 88.01 ms/it, loss 0.470475
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535626.0
get out
2 has test check 2535626.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 87.85 ms/it, loss 0.476614
Finished training it 41984/76743 of epoch 2, 88.05 ms/it, loss 0.474304
Finished training it 43008/76743 of epoch 2, 87.78 ms/it, loss 0.474071
Finished training it 43008/76743 of epoch 2, 87.92 ms/it, loss 0.474304
Finished training it 43008/76743 of epoch 2, 87.77 ms/it, loss 0.473428
Finished training it 43008/76743 of epoch 2, 88.14 ms/it, loss 0.475962
Finished training it 44032/76743 of epoch 2, 88.24 ms/it, loss 0.471915
Finished training it 44032/76743 of epoch 2, 88.38 ms/it, loss 0.471794
Finished training it 44032/76743 of epoch 2, 88.11 ms/it, loss 0.472787
Finished training it 44032/76743 of epoch 2, 87.87 ms/it, loss 0.473313
Finished training it 45056/76743 of epoch 2, 93.60 ms/it, loss 0.473263
Finished training it 45056/76743 of epoch 2, 93.29 ms/it, loss 0.471369
Finished training it 45056/76743 of epoch 2, 93.74 ms/it, loss 0.471189
Finished training it 45056/76743 of epoch 2, 93.54 ms/it, loss 0.473658
Finished training it 46080/76743 of epoch 2, 94.38 ms/it, loss 0.471061
Finished training it 46080/76743 of epoch 2, 94.00 ms/it, loss 0.472541
Finished training it 46080/76743 of epoch 2, 94.72 ms/it, loss 0.473625
Finished training it 46080/76743 of epoch 2, 94.44 ms/it, loss 0.472487
Finished training it 47104/76743 of epoch 2, 87.64 ms/it, loss 0.472947
Finished training it 47104/76743 of epoch 2, 87.61 ms/it, loss 0.472513
Finished training it 47104/76743 of epoch 2, 87.57 ms/it, loss 0.471684
Finished training it 47104/76743 of epoch 2, 87.84 ms/it, loss 0.472050
Finished training it 48128/76743 of epoch 2, 87.83 ms/it, loss 0.472780
Finished training it 48128/76743 of epoch 2, 87.97 ms/it, loss 0.471493
Finished training it 48128/76743 of epoch 2, 87.97 ms/it, loss 0.474444
Finished training it 48128/76743 of epoch 2, 87.72 ms/it, loss 0.471362
Finished training it 49152/76743 of epoch 2, 87.99 ms/it, loss 0.474970
Finished training it 49152/76743 of epoch 2, 87.76 ms/it, loss 0.469620
Finished training it 49152/76743 of epoch 2, 87.79 ms/it, loss 0.475309
Finished training it 49152/76743 of epoch 2, 87.91 ms/it, loss 0.472482
Finished training it 50176/76743 of epoch 2, 87.17 ms/it, loss 0.471241
Finished training it 50176/76743 of epoch 2, 87.21 ms/it, loss 0.473910
Finished training it 50176/76743 of epoch 2, 87.40 ms/it, loss 0.473605
Finished training it 50176/76743 of epoch 2, 87.11 ms/it, loss 0.471359
Finished training it 51200/76743 of epoch 2, 88.69 ms/it, loss 0.472744
Finished training it 51200/76743 of epoch 2, 88.73 ms/it, loss 0.472163
Finished training it 51200/76743 of epoch 2, 88.56 ms/it, loss 0.472834
Finished training it 51200/76743 of epoch 2, 88.64 ms/it, loss 0.472156
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2536065.0
get out
0 has test check 2536065.0 and sample count 3274240
 accuracy 77.455 %, best 78.704 %, roc auc score 0.7690, best 0.7993
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2536065.0
get out
1 has test check 2536065.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 87.91 ms/it, loss 0.470724
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2536065.0
get out
3 has test check 2536065.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 87.93 ms/it, loss 0.472041
Finished training it 52224/76743 of epoch 2, 87.87 ms/it, loss 0.472318
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2536065.0
get out
2 has test check 2536065.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 87.79 ms/it, loss 0.470155
Finished training it 53248/76743 of epoch 2, 87.92 ms/it, loss 0.470682
Finished training it 53248/76743 of epoch 2, 88.22 ms/it, loss 0.474097
Finished training it 53248/76743 of epoch 2, 88.10 ms/it, loss 0.470694
Finished training it 53248/76743 of epoch 2, 87.72 ms/it, loss 0.472243
Finished training it 54272/76743 of epoch 2, 88.69 ms/it, loss 0.473037
Finished training it 54272/76743 of epoch 2, 88.75 ms/it, loss 0.469696
Finished training it 54272/76743 of epoch 2, 88.63 ms/it, loss 0.469369
Finished training it 54272/76743 of epoch 2, 88.85 ms/it, loss 0.471698
Finished training it 55296/76743 of epoch 2, 87.89 ms/it, loss 0.470036
Finished training it 55296/76743 of epoch 2, 87.62 ms/it, loss 0.471562
Finished training it 55296/76743 of epoch 2, 87.88 ms/it, loss 0.473422
Finished training it 55296/76743 of epoch 2, 87.87 ms/it, loss 0.470385
Finished training it 56320/76743 of epoch 2, 87.31 ms/it, loss 0.474054
Finished training it 56320/76743 of epoch 2, 87.13 ms/it, loss 0.471509
Finished training it 56320/76743 of epoch 2, 87.08 ms/it, loss 0.473436
Finished training it 56320/76743 of epoch 2, 87.27 ms/it, loss 0.473402
Finished training it 57344/76743 of epoch 2, 88.72 ms/it, loss 0.473807
Finished training it 57344/76743 of epoch 2, 88.80 ms/it, loss 0.471949
Finished training it 57344/76743 of epoch 2, 88.84 ms/it, loss 0.471048
Finished training it 57344/76743 of epoch 2, 88.70 ms/it, loss 0.470506
Finished training it 58368/76743 of epoch 2, 87.75 ms/it, loss 0.472668
Finished training it 58368/76743 of epoch 2, 87.80 ms/it, loss 0.472074
Finished training it 58368/76743 of epoch 2, 87.87 ms/it, loss 0.473005
Finished training it 58368/76743 of epoch 2, 88.00 ms/it, loss 0.475541
Finished training it 59392/76743 of epoch 2, 87.11 ms/it, loss 0.470710
Finished training it 59392/76743 of epoch 2, 87.23 ms/it, loss 0.471689
Finished training it 59392/76743 of epoch 2, 87.19 ms/it, loss 0.471314
Finished training it 59392/76743 of epoch 2, 87.39 ms/it, loss 0.471582
Finished training it 60416/76743 of epoch 2, 87.37 ms/it, loss 0.470313
Finished training it 60416/76743 of epoch 2, 87.51 ms/it, loss 0.469668
Finished training it 60416/76743 of epoch 2, 87.57 ms/it, loss 0.471347
Finished training it 60416/76743 of epoch 2, 87.35 ms/it, loss 0.472544
Finished training it 61440/76743 of epoch 2, 87.30 ms/it, loss 0.473557
Finished training it 61440/76743 of epoch 2, 87.42 ms/it, loss 0.472812
Finished training it 61440/76743 of epoch 2, 87.27 ms/it, loss 0.469505
Finished training it 61440/76743 of epoch 2, 87.37 ms/it, loss 0.471409
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537017.0
get out
0 has test check 2537017.0 and sample count 3274240
 accuracy 77.484 %, best 78.704 %, roc auc score 0.7704, best 0.7993
Finished training it 62464/76743 of epoch 2, 87.60 ms/it, loss 0.472771
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537017.0
get out
3 has test check 2537017.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 87.43 ms/it, loss 0.470053
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537017.0
get out
2 has test check 2537017.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 87.55 ms/it, loss 0.471118
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537017.0
get out
1 has test check 2537017.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 87.48 ms/it, loss 0.472565
Finished training it 63488/76743 of epoch 2, 87.32 ms/it, loss 0.473442
Finished training it 63488/76743 of epoch 2, 87.43 ms/it, loss 0.474445
Finished training it 63488/76743 of epoch 2, 87.38 ms/it, loss 0.475584
Finished training it 63488/76743 of epoch 2, 87.37 ms/it, loss 0.468468
Finished training it 64512/76743 of epoch 2, 87.82 ms/it, loss 0.471353
Finished training it 64512/76743 of epoch 2, 87.84 ms/it, loss 0.472297
Finished training it 64512/76743 of epoch 2, 87.84 ms/it, loss 0.474342
Finished training it 64512/76743 of epoch 2, 87.79 ms/it, loss 0.471202
Finished training it 65536/76743 of epoch 2, 99.00 ms/it, loss 0.471363
Finished training it 65536/76743 of epoch 2, 99.27 ms/it, loss 0.471873
Finished training it 65536/76743 of epoch 2, 99.16 ms/it, loss 0.469009
Finished training it 65536/76743 of epoch 2, 98.85 ms/it, loss 0.470588
Finished training it 66560/76743 of epoch 2, 88.89 ms/it, loss 0.473722
Finished training it 66560/76743 of epoch 2, 88.80 ms/it, loss 0.470963
Finished training it 66560/76743 of epoch 2, 88.82 ms/it, loss 0.471446
Finished training it 66560/76743 of epoch 2, 88.70 ms/it, loss 0.470776
Finished training it 67584/76743 of epoch 2, 87.99 ms/it, loss 0.470947
Finished training it 67584/76743 of epoch 2, 87.99 ms/it, loss 0.470682
Finished training it 67584/76743 of epoch 2, 87.85 ms/it, loss 0.471092
Finished training it 67584/76743 of epoch 2, 87.82 ms/it, loss 0.467902
Finished training it 68608/76743 of epoch 2, 87.64 ms/it, loss 0.468638
Finished training it 68608/76743 of epoch 2, 88.02 ms/it, loss 0.470745
Finished training it 68608/76743 of epoch 2, 87.97 ms/it, loss 0.473054
Finished training it 68608/76743 of epoch 2, 87.96 ms/it, loss 0.470443
Finished training it 69632/76743 of epoch 2, 88.20 ms/it, loss 0.469948
Finished training it 69632/76743 of epoch 2, 88.28 ms/it, loss 0.470368
Finished training it 69632/76743 of epoch 2, 88.00 ms/it, loss 0.468470
Finished training it 69632/76743 of epoch 2, 88.07 ms/it, loss 0.471340
Finished training it 70656/76743 of epoch 2, 88.24 ms/it, loss 0.471850
Finished training it 70656/76743 of epoch 2, 88.16 ms/it, loss 0.470572
Finished training it 70656/76743 of epoch 2, 88.31 ms/it, loss 0.472382
Finished training it 70656/76743 of epoch 2, 88.36 ms/it, loss 0.469898
Finished training it 71680/76743 of epoch 2, 88.00 ms/it, loss 0.472128
Finished training it 71680/76743 of epoch 2, 87.94 ms/it, loss 0.471525
Finished training it 71680/76743 of epoch 2, 87.86 ms/it, loss 0.469659
Finished training it 71680/76743 of epoch 2, 87.80 ms/it, loss 0.470863
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2539793.0
get out
0 has test check 2539793.0 and sample count 3274240
 accuracy 77.569 %, best 78.704 %, roc auc score 0.7717, best 0.7993
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2539793.0
get out
2 has test check 2539793.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 87.58 ms/it, loss 0.469112
Finished training it 72704/76743 of epoch 2, 87.68 ms/it, loss 0.470545
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2539793.0
get out
1 has test check 2539793.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 87.91 ms/it, loss 0.471175
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2539793.0
get out
3 has test check 2539793.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 87.79 ms/it, loss 0.469676
Finished training it 73728/76743 of epoch 2, 88.57 ms/it, loss 0.470576
Finished training it 73728/76743 of epoch 2, 88.40 ms/it, loss 0.470010
Finished training it 73728/76743 of epoch 2, 88.31 ms/it, loss 0.470542
Finished training it 73728/76743 of epoch 2, 88.72 ms/it, loss 0.470253
Finished training it 74752/76743 of epoch 2, 88.35 ms/it, loss 0.469341
Finished training it 74752/76743 of epoch 2, 88.46 ms/it, loss 0.472640
Finished training it 74752/76743 of epoch 2, 88.34 ms/it, loss 0.471023
Finished training it 74752/76743 of epoch 2, 88.20 ms/it, loss 0.470715
Finished training it 75776/76743 of epoch 2, 87.21 ms/it, loss 0.471763
Finished training it 75776/76743 of epoch 2, 87.45 ms/it, loss 0.468133
Finished training it 75776/76743 of epoch 2, 87.53 ms/it, loss 0.468512
Finished training it 75776/76743 of epoch 2, 87.32 ms/it, loss 0.470703
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.78 ms/it, loss 0.471248
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 89.23 ms/it, loss 0.469334
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 89.08 ms/it, loss 0.470580
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 89.58 ms/it, loss 0.469219
Finished training it 2048/76743 of epoch 3, 88.06 ms/it, loss 0.469583
Finished training it 2048/76743 of epoch 3, 87.80 ms/it, loss 0.471336
Finished training it 2048/76743 of epoch 3, 88.00 ms/it, loss 0.471430
Finished training it 2048/76743 of epoch 3, 87.72 ms/it, loss 0.471120
Finished training it 3072/76743 of epoch 3, 88.57 ms/it, loss 0.466849
Finished training it 3072/76743 of epoch 3, 88.24 ms/it, loss 0.470255
Finished training it 3072/76743 of epoch 3, 88.41 ms/it, loss 0.469469
Finished training it 3072/76743 of epoch 3, 88.11 ms/it, loss 0.472960
Finished training it 4096/76743 of epoch 3, 88.27 ms/it, loss 0.469203
Finished training it 4096/76743 of epoch 3, 88.10 ms/it, loss 0.469790
Finished training it 4096/76743 of epoch 3, 88.21 ms/it, loss 0.472040
Finished training it 4096/76743 of epoch 3, 87.91 ms/it, loss 0.471904
Finished training it 5120/76743 of epoch 3, 88.11 ms/it, loss 0.467569
Finished training it 5120/76743 of epoch 3, 88.00 ms/it, loss 0.469586
Finished training it 5120/76743 of epoch 3, 88.35 ms/it, loss 0.471314
Finished training it 5120/76743 of epoch 3, 88.37 ms/it, loss 0.473246
Finished training it 6144/76743 of epoch 3, 87.79 ms/it, loss 0.471779
Finished training it 6144/76743 of epoch 3, 87.80 ms/it, loss 0.470126
Finished training it 6144/76743 of epoch 3, 88.06 ms/it, loss 0.469350
Finished training it 6144/76743 of epoch 3, 87.94 ms/it, loss 0.470916
Finished training it 7168/76743 of epoch 3, 88.49 ms/it, loss 0.471199
Finished training it 7168/76743 of epoch 3, 88.25 ms/it, loss 0.470602
Finished training it 7168/76743 of epoch 3, 88.26 ms/it, loss 0.468507
Finished training it 7168/76743 of epoch 3, 88.49 ms/it, loss 0.470893
Finished training it 8192/76743 of epoch 3, 87.87 ms/it, loss 0.469636
Finished training it 8192/76743 of epoch 3, 88.09 ms/it, loss 0.467402
Finished training it 8192/76743 of epoch 3, 88.22 ms/it, loss 0.469692
Finished training it 8192/76743 of epoch 3, 87.79 ms/it, loss 0.471459
Finished training it 9216/76743 of epoch 3, 93.25 ms/it, loss 0.470439
Finished training it 9216/76743 of epoch 3, 93.63 ms/it, loss 0.469739
Finished training it 9216/76743 of epoch 3, 93.46 ms/it, loss 0.470748
Finished training it 9216/76743 of epoch 3, 93.65 ms/it, loss 0.469086
Finished training it 10240/76743 of epoch 3, 94.94 ms/it, loss 0.473503
Finished training it 10240/76743 of epoch 3, 95.01 ms/it, loss 0.470336
Finished training it 10240/76743 of epoch 3, 95.26 ms/it, loss 0.469892
Finished training it 10240/76743 of epoch 3, 95.48 ms/it, loss 0.469431
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2539150.0
get out
0 has test check 2539150.0 and sample count 3274240
 accuracy 77.549 %, best 78.704 %, roc auc score 0.7720, best 0.7993
Finished training it 11264/76743 of epoch 3, 88.05 ms/it, loss 0.472548
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2539150.0
get out
1 has test check 2539150.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.21 ms/it, loss 0.469772
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2539150.0
get out
3 has test check 2539150.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.14 ms/it, loss 0.467332
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2539150.0
get out
2 has test check 2539150.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 88.05 ms/it, loss 0.469488
Finished training it 12288/76743 of epoch 3, 93.56 ms/it, loss 0.472049
Finished training it 12288/76743 of epoch 3, 93.71 ms/it, loss 0.470782
Finished training it 12288/76743 of epoch 3, 93.66 ms/it, loss 0.466069
Finished training it 12288/76743 of epoch 3, 93.67 ms/it, loss 0.472260
Finished training it 13312/76743 of epoch 3, 93.98 ms/it, loss 0.471723
Finished training it 13312/76743 of epoch 3, 94.29 ms/it, loss 0.469227
Finished training it 13312/76743 of epoch 3, 93.49 ms/it, loss 0.469068
Finished training it 13312/76743 of epoch 3, 94.26 ms/it, loss 0.470814
Finished training it 14336/76743 of epoch 3, 87.87 ms/it, loss 0.470077
Finished training it 14336/76743 of epoch 3, 88.18 ms/it, loss 0.468797
Finished training it 14336/76743 of epoch 3, 88.16 ms/it, loss 0.469723
Finished training it 14336/76743 of epoch 3, 88.12 ms/it, loss 0.472271
Finished training it 15360/76743 of epoch 3, 87.84 ms/it, loss 0.469941
Finished training it 15360/76743 of epoch 3, 87.69 ms/it, loss 0.469232
Finished training it 15360/76743 of epoch 3, 88.03 ms/it, loss 0.467844
Finished training it 15360/76743 of epoch 3, 87.97 ms/it, loss 0.471449
Finished training it 16384/76743 of epoch 3, 88.39 ms/it, loss 0.471507
Finished training it 16384/76743 of epoch 3, 88.37 ms/it, loss 0.469936
Finished training it 16384/76743 of epoch 3, 88.16 ms/it, loss 0.467503
Finished training it 16384/76743 of epoch 3, 88.16 ms/it, loss 0.471810
Finished training it 17408/76743 of epoch 3, 88.02 ms/it, loss 0.468692
Finished training it 17408/76743 of epoch 3, 88.31 ms/it, loss 0.468057
Finished training it 17408/76743 of epoch 3, 87.94 ms/it, loss 0.467958
Finished training it 17408/76743 of epoch 3, 88.39 ms/it, loss 0.470752
Finished training it 18432/76743 of epoch 3, 88.43 ms/it, loss 0.471122
Finished training it 18432/76743 of epoch 3, 88.54 ms/it, loss 0.471773
Finished training it 18432/76743 of epoch 3, 88.36 ms/it, loss 0.467144
Finished training it 18432/76743 of epoch 3, 88.14 ms/it, loss 0.469544
Finished training it 19456/76743 of epoch 3, 88.06 ms/it, loss 0.471139
Finished training it 19456/76743 of epoch 3, 88.21 ms/it, loss 0.468349
Finished training it 19456/76743 of epoch 3, 88.49 ms/it, loss 0.470221
Finished training it 19456/76743 of epoch 3, 88.15 ms/it, loss 0.467159
Finished training it 20480/76743 of epoch 3, 87.68 ms/it, loss 0.469929
Finished training it 20480/76743 of epoch 3, 87.82 ms/it, loss 0.470115
Finished training it 20480/76743 of epoch 3, 87.66 ms/it, loss 0.468822
Finished training it 20480/76743 of epoch 3, 87.92 ms/it, loss 0.470156
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541794.0
get out
0 has test check 2541794.0 and sample count 3274240
 accuracy 77.630 %, best 78.704 %, roc auc score 0.7738, best 0.7993
Finished training it 21504/76743 of epoch 3, 87.20 ms/it, loss 0.469876
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541794.0
get out
2 has test check 2541794.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 87.27 ms/it, loss 0.472128
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541794.0
get out
1 has test check 2541794.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 87.53 ms/it, loss 0.468979
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541794.0
get out
3 has test check 2541794.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 87.48 ms/it, loss 0.469338
Finished training it 22528/76743 of epoch 3, 87.98 ms/it, loss 0.469956
Finished training it 22528/76743 of epoch 3, 88.07 ms/it, loss 0.470854
Finished training it 22528/76743 of epoch 3, 87.98 ms/it, loss 0.466576
Finished training it 22528/76743 of epoch 3, 87.91 ms/it, loss 0.469501
Finished training it 23552/76743 of epoch 3, 88.34 ms/it, loss 0.470034
Finished training it 23552/76743 of epoch 3, 88.10 ms/it, loss 0.469150
Finished training it 23552/76743 of epoch 3, 88.41 ms/it, loss 0.470987
Finished training it 23552/76743 of epoch 3, 88.06 ms/it, loss 0.466171
Finished training it 24576/76743 of epoch 3, 87.97 ms/it, loss 0.467133
Finished training it 24576/76743 of epoch 3, 88.30 ms/it, loss 0.469350
Finished training it 24576/76743 of epoch 3, 87.80 ms/it, loss 0.471029
Finished training it 24576/76743 of epoch 3, 87.90 ms/it, loss 0.469177
Finished training it 25600/76743 of epoch 3, 88.44 ms/it, loss 0.467181
Finished training it 25600/76743 of epoch 3, 87.85 ms/it, loss 0.467832
Finished training it 25600/76743 of epoch 3, 88.16 ms/it, loss 0.469196
Finished training it 25600/76743 of epoch 3, 88.21 ms/it, loss 0.468890
Finished training it 26624/76743 of epoch 3, 88.10 ms/it, loss 0.465311
Finished training it 26624/76743 of epoch 3, 88.33 ms/it, loss 0.469482
Finished training it 26624/76743 of epoch 3, 88.33 ms/it, loss 0.471213
Finished training it 26624/76743 of epoch 3, 88.08 ms/it, loss 0.467361
Finished training it 27648/76743 of epoch 3, 87.94 ms/it, loss 0.470202
Finished training it 27648/76743 of epoch 3, 88.21 ms/it, loss 0.469951
Finished training it 27648/76743 of epoch 3, 87.83 ms/it, loss 0.468179
Finished training it 27648/76743 of epoch 3, 88.24 ms/it, loss 0.471032
Finished training it 28672/76743 of epoch 3, 88.33 ms/it, loss 0.469421
Finished training it 28672/76743 of epoch 3, 88.27 ms/it, loss 0.469445
Finished training it 28672/76743 of epoch 3, 88.06 ms/it, loss 0.470679
Finished training it 28672/76743 of epoch 3, 88.30 ms/it, loss 0.467198
Finished training it 29696/76743 of epoch 3, 88.22 ms/it, loss 0.465112
Finished training it 29696/76743 of epoch 3, 87.94 ms/it, loss 0.471168
Finished training it 29696/76743 of epoch 3, 88.17 ms/it, loss 0.469204
Finished training it 29696/76743 of epoch 3, 87.88 ms/it, loss 0.469980
Finished training it 30720/76743 of epoch 3, 87.64 ms/it, loss 0.466946
Finished training it 30720/76743 of epoch 3, 87.50 ms/it, loss 0.471700
Finished training it 30720/76743 of epoch 3, 87.60 ms/it, loss 0.468074
Finished training it 30720/76743 of epoch 3, 87.37 ms/it, loss 0.471506
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542325.0
get out
0 has test check 2542325.0 and sample count 3274240
 accuracy 77.646 %, best 78.704 %, roc auc score 0.7738, best 0.7993
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542325.0
get out
3 has test check 2542325.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 88.27 ms/it, loss 0.467419
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542325.0
get out
1 has test check 2542325.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 88.16 ms/it, loss 0.467131
Finished training it 31744/76743 of epoch 3, 88.09 ms/it, loss 0.468646
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542325.0
get out
2 has test check 2542325.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 87.99 ms/it, loss 0.470117
Finished training it 32768/76743 of epoch 3, 88.11 ms/it, loss 0.472567
Finished training it 32768/76743 of epoch 3, 87.79 ms/it, loss 0.471275
Finished training it 32768/76743 of epoch 3, 88.03 ms/it, loss 0.471081
Finished training it 32768/76743 of epoch 3, 87.89 ms/it, loss 0.468792
Finished training it 33792/76743 of epoch 3, 88.59 ms/it, loss 0.471023
Finished training it 33792/76743 of epoch 3, 88.65 ms/it, loss 0.469966
Finished training it 33792/76743 of epoch 3, 88.28 ms/it, loss 0.469464
Finished training it 33792/76743 of epoch 3, 88.40 ms/it, loss 0.466826
Finished training it 34816/76743 of epoch 3, 100.55 ms/it, loss 0.468694
Finished training it 34816/76743 of epoch 3, 100.31 ms/it, loss 0.469807
Finished training it 34816/76743 of epoch 3, 100.52 ms/it, loss 0.471238
Finished training it 34816/76743 of epoch 3, 100.81 ms/it, loss 0.470488
Finished training it 35840/76743 of epoch 3, 88.34 ms/it, loss 0.468082
Finished training it 35840/76743 of epoch 3, 88.21 ms/it, loss 0.467833
Finished training it 35840/76743 of epoch 3, 88.10 ms/it, loss 0.468835
Finished training it 35840/76743 of epoch 3, 88.33 ms/it, loss 0.468798
Finished training it 36864/76743 of epoch 3, 87.64 ms/it, loss 0.467428
Finished training it 36864/76743 of epoch 3, 87.59 ms/it, loss 0.469190
Finished training it 36864/76743 of epoch 3, 87.92 ms/it, loss 0.467848
Finished training it 36864/76743 of epoch 3, 87.87 ms/it, loss 0.473018
Finished training it 37888/76743 of epoch 3, 88.33 ms/it, loss 0.467195
Finished training it 37888/76743 of epoch 3, 88.30 ms/it, loss 0.467780
Finished training it 37888/76743 of epoch 3, 87.93 ms/it, loss 0.470392
Finished training it 37888/76743 of epoch 3, 88.13 ms/it, loss 0.466976
Finished training it 38912/76743 of epoch 3, 88.54 ms/it, loss 0.467449
Finished training it 38912/76743 of epoch 3, 88.32 ms/it, loss 0.468871
Finished training it 38912/76743 of epoch 3, 88.35 ms/it, loss 0.466092
Finished training it 38912/76743 of epoch 3, 88.60 ms/it, loss 0.467848
Finished training it 39936/76743 of epoch 3, 87.88 ms/it, loss 0.467075
Finished training it 39936/76743 of epoch 3, 87.89 ms/it, loss 0.466555
Finished training it 39936/76743 of epoch 3, 87.88 ms/it, loss 0.469188
Finished training it 39936/76743 of epoch 3, 87.56 ms/it, loss 0.468433
Finished training it 40960/76743 of epoch 3, 88.23 ms/it, loss 0.470254
Finished training it 40960/76743 of epoch 3, 88.54 ms/it, loss 0.466122
Finished training it 40960/76743 of epoch 3, 88.46 ms/it, loss 0.469127
Finished training it 40960/76743 of epoch 3, 88.35 ms/it, loss 0.468499
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542954.0
get out
0 has test check 2542954.0 and sample count 3274240
 accuracy 77.665 %, best 78.704 %, roc auc score 0.7745, best 0.7993
Finished training it 41984/76743 of epoch 3, 88.20 ms/it, loss 0.469976
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542954.0
get out
2 has test check 2542954.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.23 ms/it, loss 0.472371
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542954.0
get out
3 has test check 2542954.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.43 ms/it, loss 0.468193
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542954.0
get out
1 has test check 2542954.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 88.27 ms/it, loss 0.466444
Finished training it 43008/76743 of epoch 3, 88.11 ms/it, loss 0.470042
Finished training it 43008/76743 of epoch 3, 88.22 ms/it, loss 0.469875
Finished training it 43008/76743 of epoch 3, 88.27 ms/it, loss 0.471696
Finished training it 43008/76743 of epoch 3, 88.07 ms/it, loss 0.469115
Finished training it 44032/76743 of epoch 3, 88.01 ms/it, loss 0.466982
Finished training it 44032/76743 of epoch 3, 88.04 ms/it, loss 0.467354
Finished training it 44032/76743 of epoch 3, 87.81 ms/it, loss 0.469047
Finished training it 44032/76743 of epoch 3, 87.94 ms/it, loss 0.468067
Finished training it 45056/76743 of epoch 3, 88.06 ms/it, loss 0.469379
Finished training it 45056/76743 of epoch 3, 87.97 ms/it, loss 0.468576
Finished training it 45056/76743 of epoch 3, 87.98 ms/it, loss 0.466808
Finished training it 45056/76743 of epoch 3, 87.76 ms/it, loss 0.466756
Finished training it 46080/76743 of epoch 3, 88.56 ms/it, loss 0.468880
Finished training it 46080/76743 of epoch 3, 88.46 ms/it, loss 0.467123
Finished training it 46080/76743 of epoch 3, 88.79 ms/it, loss 0.468129
Finished training it 46080/76743 of epoch 3, 88.73 ms/it, loss 0.469460
Finished training it 47104/76743 of epoch 3, 87.97 ms/it, loss 0.468552
Finished training it 47104/76743 of epoch 3, 88.01 ms/it, loss 0.468038
Finished training it 47104/76743 of epoch 3, 88.01 ms/it, loss 0.468330
Finished training it 47104/76743 of epoch 3, 87.74 ms/it, loss 0.467281
Finished training it 48128/76743 of epoch 3, 87.75 ms/it, loss 0.470157
Finished training it 48128/76743 of epoch 3, 87.97 ms/it, loss 0.467675
Finished training it 48128/76743 of epoch 3, 87.65 ms/it, loss 0.467114
Finished training it 48128/76743 of epoch 3, 87.93 ms/it, loss 0.468549
Finished training it 49152/76743 of epoch 3, 89.09 ms/it, loss 0.468503
Finished training it 49152/76743 of epoch 3, 88.71 ms/it, loss 0.465729
Finished training it 49152/76743 of epoch 3, 88.95 ms/it, loss 0.471118
Finished training it 49152/76743 of epoch 3, 89.05 ms/it, loss 0.471619
Finished training it 50176/76743 of epoch 3, 88.08 ms/it, loss 0.467161
Finished training it 50176/76743 of epoch 3, 88.17 ms/it, loss 0.469529
Finished training it 50176/76743 of epoch 3, 88.14 ms/it, loss 0.470039
Finished training it 50176/76743 of epoch 3, 87.92 ms/it, loss 0.467683
Finished training it 51200/76743 of epoch 3, 88.69 ms/it, loss 0.468576
Finished training it 51200/76743 of epoch 3, 88.80 ms/it, loss 0.467253
Finished training it 51200/76743 of epoch 3, 88.58 ms/it, loss 0.468836
Finished training it 51200/76743 of epoch 3, 88.49 ms/it, loss 0.468556
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543813.0
get out
0 has test check 2543813.0 and sample count 3274240
 accuracy 77.692 %, best 78.704 %, roc auc score 0.7748, best 0.7993
Finished training it 52224/76743 of epoch 3, 87.98 ms/it, loss 0.467846
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543813.0
get out
1 has test check 2543813.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 88.01 ms/it, loss 0.466794
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543813.0
get out
2 has test check 2543813.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 87.93 ms/it, loss 0.466406
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543813.0
get out
3 has test check 2543813.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 88.10 ms/it, loss 0.468262
Finished training it 53248/76743 of epoch 3, 88.02 ms/it, loss 0.468297
Finished training it 53248/76743 of epoch 3, 88.18 ms/it, loss 0.466910
Finished training it 53248/76743 of epoch 3, 88.28 ms/it, loss 0.470779
Finished training it 53248/76743 of epoch 3, 88.31 ms/it, loss 0.466748
Finished training it 54272/76743 of epoch 3, 88.45 ms/it, loss 0.467842
Finished training it 54272/76743 of epoch 3, 88.21 ms/it, loss 0.469284
Finished training it 54272/76743 of epoch 3, 88.50 ms/it, loss 0.466361
Finished training it 54272/76743 of epoch 3, 88.26 ms/it, loss 0.465407
Finished training it 55296/76743 of epoch 3, 94.22 ms/it, loss 0.469160
Finished training it 55296/76743 of epoch 3, 94.84 ms/it, loss 0.466230
Finished training it 55296/76743 of epoch 3, 94.60 ms/it, loss 0.467366
Finished training it 55296/76743 of epoch 3, 94.97 ms/it, loss 0.465781
Finished training it 56320/76743 of epoch 3, 95.55 ms/it, loss 0.469890
Finished training it 56320/76743 of epoch 3, 95.02 ms/it, loss 0.469622
Finished training it 56320/76743 of epoch 3, 95.42 ms/it, loss 0.469481
Finished training it 56320/76743 of epoch 3, 95.66 ms/it, loss 0.467767
Finished training it 57344/76743 of epoch 3, 88.26 ms/it, loss 0.470150
Finished training it 57344/76743 of epoch 3, 88.61 ms/it, loss 0.467261
Finished training it 57344/76743 of epoch 3, 88.28 ms/it, loss 0.466628
Finished training it 57344/76743 of epoch 3, 88.49 ms/it, loss 0.467905
Finished training it 58368/76743 of epoch 3, 88.72 ms/it, loss 0.469111
Finished training it 58368/76743 of epoch 3, 88.39 ms/it, loss 0.468402
Finished training it 58368/76743 of epoch 3, 88.66 ms/it, loss 0.468432
Finished training it 58368/76743 of epoch 3, 88.43 ms/it, loss 0.471591
Finished training it 59392/76743 of epoch 3, 88.25 ms/it, loss 0.467775
Finished training it 59392/76743 of epoch 3, 88.37 ms/it, loss 0.467992
Finished training it 59392/76743 of epoch 3, 88.04 ms/it, loss 0.467178
Finished training it 59392/76743 of epoch 3, 88.34 ms/it, loss 0.467384
Finished training it 60416/76743 of epoch 3, 88.00 ms/it, loss 0.468626
Finished training it 60416/76743 of epoch 3, 88.16 ms/it, loss 0.467066
Finished training it 60416/76743 of epoch 3, 88.33 ms/it, loss 0.465678
Finished training it 60416/76743 of epoch 3, 88.41 ms/it, loss 0.467058
Finished training it 61440/76743 of epoch 3, 88.12 ms/it, loss 0.469503
Finished training it 61440/76743 of epoch 3, 88.07 ms/it, loss 0.466339
Finished training it 61440/76743 of epoch 3, 88.36 ms/it, loss 0.467198
Finished training it 61440/76743 of epoch 3, 88.21 ms/it, loss 0.469438
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543113.0
get out
0 has test check 2543113.0 and sample count 3274240
 accuracy 77.670 %, best 78.704 %, roc auc score 0.7748, best 0.7993
Finished training it 62464/76743 of epoch 3, 87.68 ms/it, loss 0.469463
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543113.0
get out
3 has test check 2543113.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 87.89 ms/it, loss 0.466500
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543113.0
get out
1 has test check 2543113.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 88.02 ms/it, loss 0.468909
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543113.0
get out
2 has test check 2543113.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 87.69 ms/it, loss 0.467698
Finished training it 63488/76743 of epoch 3, 88.44 ms/it, loss 0.471217
Finished training it 63488/76743 of epoch 3, 88.34 ms/it, loss 0.464708
Finished training it 63488/76743 of epoch 3, 88.59 ms/it, loss 0.469816
Finished training it 63488/76743 of epoch 3, 88.54 ms/it, loss 0.470605
Finished training it 64512/76743 of epoch 3, 88.22 ms/it, loss 0.471075
Finished training it 64512/76743 of epoch 3, 88.04 ms/it, loss 0.467390
Finished training it 64512/76743 of epoch 3, 88.08 ms/it, loss 0.468398
Finished training it 64512/76743 of epoch 3, 88.33 ms/it, loss 0.467978
Finished training it 65536/76743 of epoch 3, 88.10 ms/it, loss 0.468214
Finished training it 65536/76743 of epoch 3, 88.19 ms/it, loss 0.465106
Finished training it 65536/76743 of epoch 3, 88.10 ms/it, loss 0.468387
Finished training it 65536/76743 of epoch 3, 88.05 ms/it, loss 0.467095
Finished training it 66560/76743 of epoch 3, 88.44 ms/it, loss 0.467532
Finished training it 66560/76743 of epoch 3, 88.67 ms/it, loss 0.470342
Finished training it 66560/76743 of epoch 3, 88.48 ms/it, loss 0.467080
Finished training it 66560/76743 of epoch 3, 88.84 ms/it, loss 0.468309
Finished training it 67584/76743 of epoch 3, 87.94 ms/it, loss 0.464054
Finished training it 67584/76743 of epoch 3, 88.02 ms/it, loss 0.467565
Finished training it 67584/76743 of epoch 3, 88.38 ms/it, loss 0.467800
Finished training it 67584/76743 of epoch 3, 88.22 ms/it, loss 0.467844
Finished training it 68608/76743 of epoch 3, 87.86 ms/it, loss 0.465787
Finished training it 68608/76743 of epoch 3, 87.96 ms/it, loss 0.467272
Finished training it 68608/76743 of epoch 3, 88.27 ms/it, loss 0.467587
Finished training it 68608/76743 of epoch 3, 88.03 ms/it, loss 0.469861
Finished training it 69632/76743 of epoch 3, 87.90 ms/it, loss 0.464993
Finished training it 69632/76743 of epoch 3, 88.15 ms/it, loss 0.466715
Finished training it 69632/76743 of epoch 3, 87.99 ms/it, loss 0.468194
Finished training it 69632/76743 of epoch 3, 88.15 ms/it, loss 0.467481
Finished training it 70656/76743 of epoch 3, 88.14 ms/it, loss 0.469040
Finished training it 70656/76743 of epoch 3, 88.01 ms/it, loss 0.466939
Finished training it 70656/76743 of epoch 3, 88.06 ms/it, loss 0.466532
Finished training it 70656/76743 of epoch 3, 88.04 ms/it, loss 0.468319
Finished training it 71680/76743 of epoch 3, 87.84 ms/it, loss 0.465727
Finished training it 71680/76743 of epoch 3, 87.99 ms/it, loss 0.467955
Finished training it 71680/76743 of epoch 3, 87.84 ms/it, loss 0.467367
Finished training it 71680/76743 of epoch 3, 87.93 ms/it, loss 0.468788
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545339.0
get out
0 has test check 2545339.0 and sample count 3274240
 accuracy 77.738 %, best 78.704 %, roc auc score 0.7759, best 0.7993
Finished training it 72704/76743 of epoch 3, 87.86 ms/it, loss 0.466871
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545339.0
get out
2 has test check 2545339.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 87.61 ms/it, loss 0.465911
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545339.0
get out
1 has test check 2545339.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 87.90 ms/it, loss 0.467959
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545339.0
get out
3 has test check 2545339.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 87.84 ms/it, loss 0.466652
Finished training it 73728/76743 of epoch 3, 87.99 ms/it, loss 0.466806
Finished training it 73728/76743 of epoch 3, 88.04 ms/it, loss 0.467244
Finished training it 73728/76743 of epoch 3, 88.14 ms/it, loss 0.466531
Finished training it 73728/76743 of epoch 3, 87.78 ms/it, loss 0.467115
Finished training it 74752/76743 of epoch 3, 87.79 ms/it, loss 0.467647
Finished training it 74752/76743 of epoch 3, 87.86 ms/it, loss 0.467744
Finished training it 74752/76743 of epoch 3, 88.01 ms/it, loss 0.465363
Finished training it 74752/76743 of epoch 3, 88.00 ms/it, loss 0.469221
Finished training it 75776/76743 of epoch 3, 93.68 ms/it, loss 0.464839
Finished training it 75776/76743 of epoch 3, 93.60 ms/it, loss 0.465256
Finished training it 75776/76743 of epoch 3, 93.61 ms/it, loss 0.468756
Finished training it 75776/76743 of epoch 3, 93.42 ms/it, loss 0.467620
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.32 ms/it, loss 0.467170
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.77 ms/it, loss 0.466022
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 91.96 ms/it, loss 0.468242
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 92.19 ms/it, loss 0.466545
Finished training it 2048/76743 of epoch 4, 88.44 ms/it, loss 0.466563
Finished training it 2048/76743 of epoch 4, 88.17 ms/it, loss 0.468542
Finished training it 2048/76743 of epoch 4, 88.25 ms/it, loss 0.468246
Finished training it 2048/76743 of epoch 4, 88.07 ms/it, loss 0.467873
Finished training it 3072/76743 of epoch 4, 88.33 ms/it, loss 0.467197
Finished training it 3072/76743 of epoch 4, 88.56 ms/it, loss 0.463810
Finished training it 3072/76743 of epoch 4, 88.53 ms/it, loss 0.466220
Finished training it 3072/76743 of epoch 4, 88.26 ms/it, loss 0.469932
Finished training it 4096/76743 of epoch 4, 88.20 ms/it, loss 0.468958
Finished training it 4096/76743 of epoch 4, 88.08 ms/it, loss 0.468603
Finished training it 4096/76743 of epoch 4, 87.96 ms/it, loss 0.466045
Finished training it 4096/76743 of epoch 4, 88.31 ms/it, loss 0.466533
Finished training it 5120/76743 of epoch 4, 88.34 ms/it, loss 0.466381
Finished training it 5120/76743 of epoch 4, 88.34 ms/it, loss 0.464476
Finished training it 5120/76743 of epoch 4, 88.61 ms/it, loss 0.468035
Finished training it 5120/76743 of epoch 4, 88.56 ms/it, loss 0.470363
Finished training it 6144/76743 of epoch 4, 88.25 ms/it, loss 0.467467
Finished training it 6144/76743 of epoch 4, 87.91 ms/it, loss 0.466515
Finished training it 6144/76743 of epoch 4, 88.14 ms/it, loss 0.466264
Finished training it 6144/76743 of epoch 4, 87.93 ms/it, loss 0.468164
Finished training it 7168/76743 of epoch 4, 88.30 ms/it, loss 0.468489
Finished training it 7168/76743 of epoch 4, 88.25 ms/it, loss 0.464893
Finished training it 7168/76743 of epoch 4, 88.38 ms/it, loss 0.467440
Finished training it 7168/76743 of epoch 4, 88.11 ms/it, loss 0.467416
Finished training it 8192/76743 of epoch 4, 87.93 ms/it, loss 0.464285
Finished training it 8192/76743 of epoch 4, 87.69 ms/it, loss 0.468626
Finished training it 8192/76743 of epoch 4, 87.89 ms/it, loss 0.466585
Finished training it 8192/76743 of epoch 4, 88.05 ms/it, loss 0.466380
Finished training it 9216/76743 of epoch 4, 87.84 ms/it, loss 0.467092
Finished training it 9216/76743 of epoch 4, 87.86 ms/it, loss 0.466431
Finished training it 9216/76743 of epoch 4, 88.04 ms/it, loss 0.466406
Finished training it 9216/76743 of epoch 4, 88.06 ms/it, loss 0.467406
Finished training it 10240/76743 of epoch 4, 88.46 ms/it, loss 0.466551
Finished training it 10240/76743 of epoch 4, 88.30 ms/it, loss 0.470724
Finished training it 10240/76743 of epoch 4, 88.15 ms/it, loss 0.467028
Finished training it 10240/76743 of epoch 4, 88.44 ms/it, loss 0.467295
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544794.0
get out
0 has test check 2544794.0 and sample count 3274240
 accuracy 77.722 %, best 78.704 %, roc auc score 0.7761, best 0.7993
Finished training it 11264/76743 of epoch 4, 87.99 ms/it, loss 0.469255
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544794.0
get out
1 has test check 2544794.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.91 ms/it, loss 0.466574
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544794.0
get out
3 has test check 2544794.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.93 ms/it, loss 0.464679
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544794.0
get out
2 has test check 2544794.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.72 ms/it, loss 0.466313
Finished training it 12288/76743 of epoch 4, 88.10 ms/it, loss 0.468632
Finished training it 12288/76743 of epoch 4, 88.33 ms/it, loss 0.462615
Finished training it 12288/76743 of epoch 4, 88.13 ms/it, loss 0.467417
Finished training it 12288/76743 of epoch 4, 88.34 ms/it, loss 0.469443
Finished training it 13312/76743 of epoch 4, 88.44 ms/it, loss 0.466572
Finished training it 13312/76743 of epoch 4, 88.57 ms/it, loss 0.467886
Finished training it 13312/76743 of epoch 4, 88.08 ms/it, loss 0.468917
Finished training it 13312/76743 of epoch 4, 88.29 ms/it, loss 0.466044
Finished training it 14336/76743 of epoch 4, 87.85 ms/it, loss 0.469570
Finished training it 14336/76743 of epoch 4, 88.22 ms/it, loss 0.466748
Finished training it 14336/76743 of epoch 4, 88.15 ms/it, loss 0.466153
Finished training it 14336/76743 of epoch 4, 87.91 ms/it, loss 0.466847
Finished training it 15360/76743 of epoch 4, 88.49 ms/it, loss 0.468537
Finished training it 15360/76743 of epoch 4, 88.30 ms/it, loss 0.465479
Finished training it 15360/76743 of epoch 4, 88.61 ms/it, loss 0.465275
Finished training it 15360/76743 of epoch 4, 88.29 ms/it, loss 0.467492
Finished training it 16384/76743 of epoch 4, 87.80 ms/it, loss 0.468805
Finished training it 16384/76743 of epoch 4, 88.07 ms/it, loss 0.467252
Finished training it 16384/76743 of epoch 4, 87.74 ms/it, loss 0.465079
Finished training it 16384/76743 of epoch 4, 87.99 ms/it, loss 0.468512
Finished training it 17408/76743 of epoch 4, 88.26 ms/it, loss 0.465182
Finished training it 17408/76743 of epoch 4, 87.96 ms/it, loss 0.465244
Finished training it 17408/76743 of epoch 4, 88.29 ms/it, loss 0.468280
Finished training it 17408/76743 of epoch 4, 88.03 ms/it, loss 0.465185
Finished training it 18432/76743 of epoch 4, 87.89 ms/it, loss 0.464361
Finished training it 18432/76743 of epoch 4, 88.17 ms/it, loss 0.468255
Finished training it 18432/76743 of epoch 4, 88.35 ms/it, loss 0.469016
Finished training it 18432/76743 of epoch 4, 87.95 ms/it, loss 0.466941
Finished training it 19456/76743 of epoch 4, 87.96 ms/it, loss 0.468303
Finished training it 19456/76743 of epoch 4, 88.25 ms/it, loss 0.465793
Finished training it 19456/76743 of epoch 4, 88.08 ms/it, loss 0.467159
Finished training it 19456/76743 of epoch 4, 87.88 ms/it, loss 0.464397
Finished training it 20480/76743 of epoch 4, 88.43 ms/it, loss 0.466916
Finished training it 20480/76743 of epoch 4, 88.47 ms/it, loss 0.467580
Finished training it 20480/76743 of epoch 4, 88.16 ms/it, loss 0.467603
Finished training it 20480/76743 of epoch 4, 88.12 ms/it, loss 0.465875
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546755.0
get out
0 has test check 2546755.0 and sample count 3274240
 accuracy 77.782 %, best 78.704 %, roc auc score 0.7773, best 0.7993
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546755.0
get out
2 has test check 2546755.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.27 ms/it, loss 0.469591
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546755.0
get out
3 has test check 2546755.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.33 ms/it, loss 0.466258
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546755.0
get out
1 has test check 2546755.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.48 ms/it, loss 0.466457
Finished training it 21504/76743 of epoch 4, 88.20 ms/it, loss 0.467337
Finished training it 22528/76743 of epoch 4, 88.40 ms/it, loss 0.467596
Finished training it 22528/76743 of epoch 4, 88.52 ms/it, loss 0.468149
Finished training it 22528/76743 of epoch 4, 88.52 ms/it, loss 0.463880
Finished training it 22528/76743 of epoch 4, 88.16 ms/it, loss 0.467302
Finished training it 23552/76743 of epoch 4, 88.41 ms/it, loss 0.467195
Finished training it 23552/76743 of epoch 4, 88.38 ms/it, loss 0.467845
Finished training it 23552/76743 of epoch 4, 88.06 ms/it, loss 0.463567
Finished training it 23552/76743 of epoch 4, 88.23 ms/it, loss 0.466711
Finished training it 24576/76743 of epoch 4, 100.16 ms/it, loss 0.468163
Finished training it 24576/76743 of epoch 4, 100.62 ms/it, loss 0.464175
Finished training it 24576/76743 of epoch 4, 100.32 ms/it, loss 0.466460
Finished training it 24576/76743 of epoch 4, 100.40 ms/it, loss 0.466532
Finished training it 25600/76743 of epoch 4, 87.82 ms/it, loss 0.465092
Finished training it 25600/76743 of epoch 4, 88.17 ms/it, loss 0.464804
Finished training it 25600/76743 of epoch 4, 88.03 ms/it, loss 0.466079
Finished training it 25600/76743 of epoch 4, 87.98 ms/it, loss 0.466278
Finished training it 26624/76743 of epoch 4, 88.19 ms/it, loss 0.462347
Finished training it 26624/76743 of epoch 4, 88.47 ms/it, loss 0.466543
Finished training it 26624/76743 of epoch 4, 88.54 ms/it, loss 0.468651
Finished training it 26624/76743 of epoch 4, 88.32 ms/it, loss 0.465122
Finished training it 27648/76743 of epoch 4, 88.44 ms/it, loss 0.467989
Finished training it 27648/76743 of epoch 4, 88.57 ms/it, loss 0.467529
Finished training it 27648/76743 of epoch 4, 88.18 ms/it, loss 0.465353
Finished training it 27648/76743 of epoch 4, 88.44 ms/it, loss 0.467473
Finished training it 28672/76743 of epoch 4, 88.53 ms/it, loss 0.466788
Finished training it 28672/76743 of epoch 4, 88.64 ms/it, loss 0.466606
Finished training it 28672/76743 of epoch 4, 88.62 ms/it, loss 0.464756
Finished training it 28672/76743 of epoch 4, 88.43 ms/it, loss 0.467931
Finished training it 29696/76743 of epoch 4, 88.41 ms/it, loss 0.466991
Finished training it 29696/76743 of epoch 4, 88.41 ms/it, loss 0.462146
Finished training it 29696/76743 of epoch 4, 88.20 ms/it, loss 0.467522
Finished training it 29696/76743 of epoch 4, 88.26 ms/it, loss 0.468223
Finished training it 30720/76743 of epoch 4, 88.43 ms/it, loss 0.469078
Finished training it 30720/76743 of epoch 4, 88.61 ms/it, loss 0.464179
Finished training it 30720/76743 of epoch 4, 88.79 ms/it, loss 0.465383
Finished training it 30720/76743 of epoch 4, 88.55 ms/it, loss 0.469230
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547098.0
get out
0 has test check 2547098.0 and sample count 3274240
 accuracy 77.792 %, best 78.704 %, roc auc score 0.7771, best 0.7993
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547098.0
get out
3 has test check 2547098.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 87.85 ms/it, loss 0.464868
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547098.0
get out
1 has test check 2547098.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 87.96 ms/it, loss 0.464099
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547098.0
get out
2 has test check 2547098.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 87.68 ms/it, loss 0.467420
Finished training it 31744/76743 of epoch 4, 87.81 ms/it, loss 0.466197
Finished training it 32768/76743 of epoch 4, 88.53 ms/it, loss 0.468317
Finished training it 32768/76743 of epoch 4, 88.22 ms/it, loss 0.468242
Finished training it 32768/76743 of epoch 4, 88.44 ms/it, loss 0.469979
Finished training it 32768/76743 of epoch 4, 88.34 ms/it, loss 0.466154
Finished training it 33792/76743 of epoch 4, 88.56 ms/it, loss 0.466950
Finished training it 33792/76743 of epoch 4, 88.70 ms/it, loss 0.468093
Finished training it 33792/76743 of epoch 4, 88.34 ms/it, loss 0.464217
Finished training it 33792/76743 of epoch 4, 88.59 ms/it, loss 0.467527
Finished training it 34816/76743 of epoch 4, 88.33 ms/it, loss 0.466772
Finished training it 34816/76743 of epoch 4, 88.64 ms/it, loss 0.467728
Finished training it 34816/76743 of epoch 4, 88.66 ms/it, loss 0.465831
Finished training it 34816/76743 of epoch 4, 88.19 ms/it, loss 0.468269
Finished training it 35840/76743 of epoch 4, 88.13 ms/it, loss 0.466158
Finished training it 35840/76743 of epoch 4, 88.18 ms/it, loss 0.465269
Finished training it 35840/76743 of epoch 4, 88.25 ms/it, loss 0.466069
Finished training it 35840/76743 of epoch 4, 88.31 ms/it, loss 0.465379
Finished training it 36864/76743 of epoch 4, 87.88 ms/it, loss 0.465353
Finished training it 36864/76743 of epoch 4, 88.10 ms/it, loss 0.470545
Finished training it 36864/76743 of epoch 4, 87.92 ms/it, loss 0.466244
Finished training it 36864/76743 of epoch 4, 87.94 ms/it, loss 0.465014
Finished training it 37888/76743 of epoch 4, 87.41 ms/it, loss 0.467474
Finished training it 37888/76743 of epoch 4, 87.70 ms/it, loss 0.465040
Finished training it 37888/76743 of epoch 4, 87.82 ms/it, loss 0.464591
Finished training it 37888/76743 of epoch 4, 87.51 ms/it, loss 0.464569
Finished training it 38912/76743 of epoch 4, 88.09 ms/it, loss 0.464789
Finished training it 38912/76743 of epoch 4, 88.33 ms/it, loss 0.464572
Finished training it 38912/76743 of epoch 4, 87.94 ms/it, loss 0.463694
Finished training it 38912/76743 of epoch 4, 87.97 ms/it, loss 0.466171
Finished training it 39936/76743 of epoch 4, 88.50 ms/it, loss 0.464025
Finished training it 39936/76743 of epoch 4, 88.35 ms/it, loss 0.467068
Finished training it 39936/76743 of epoch 4, 88.17 ms/it, loss 0.463433
Finished training it 39936/76743 of epoch 4, 88.23 ms/it, loss 0.465790
Finished training it 40960/76743 of epoch 4, 88.47 ms/it, loss 0.467487
Finished training it 40960/76743 of epoch 4, 88.45 ms/it, loss 0.465956
Finished training it 40960/76743 of epoch 4, 88.64 ms/it, loss 0.466489
Finished training it 40960/76743 of epoch 4, 88.66 ms/it, loss 0.463853
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547590.0
get out
0 has test check 2547590.0 and sample count 3274240
 accuracy 77.807 %, best 78.704 %, roc auc score 0.7777, best 0.7993
Finished training it 41984/76743 of epoch 4, 87.44 ms/it, loss 0.466855
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547590.0
get out
3 has test check 2547590.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 87.67 ms/it, loss 0.465334
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547590.0
get out
2 has test check 2547590.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 87.44 ms/it, loss 0.469493
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547590.0
get out
1 has test check 2547590.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 87.56 ms/it, loss 0.464079
Finished training it 43008/76743 of epoch 4, 88.24 ms/it, loss 0.467495
Finished training it 43008/76743 of epoch 4, 88.40 ms/it, loss 0.469594
Finished training it 43008/76743 of epoch 4, 88.41 ms/it, loss 0.467130
Finished training it 43008/76743 of epoch 4, 88.15 ms/it, loss 0.466424
Finished training it 44032/76743 of epoch 4, 88.37 ms/it, loss 0.464835
Finished training it 44032/76743 of epoch 4, 88.28 ms/it, loss 0.464266
Finished training it 44032/76743 of epoch 4, 88.09 ms/it, loss 0.465133
Finished training it 44032/76743 of epoch 4, 87.96 ms/it, loss 0.466614
Finished training it 45056/76743 of epoch 4, 93.14 ms/it, loss 0.464529
Finished training it 45056/76743 of epoch 4, 93.58 ms/it, loss 0.464017
Finished training it 45056/76743 of epoch 4, 93.45 ms/it, loss 0.466673
Finished training it 45056/76743 of epoch 4, 93.30 ms/it, loss 0.466171
Finished training it 46080/76743 of epoch 4, 94.31 ms/it, loss 0.465651
Finished training it 46080/76743 of epoch 4, 93.75 ms/it, loss 0.467138
Finished training it 46080/76743 of epoch 4, 93.54 ms/it, loss 0.465937
Finished training it 46080/76743 of epoch 4, 93.95 ms/it, loss 0.464679
Finished training it 47104/76743 of epoch 4, 88.16 ms/it, loss 0.465630
Finished training it 47104/76743 of epoch 4, 88.10 ms/it, loss 0.464460
Finished training it 47104/76743 of epoch 4, 88.19 ms/it, loss 0.466237
Finished training it 47104/76743 of epoch 4, 88.24 ms/it, loss 0.465442
Finished training it 48128/76743 of epoch 4, 88.37 ms/it, loss 0.464889
Finished training it 48128/76743 of epoch 4, 88.58 ms/it, loss 0.467534
Finished training it 48128/76743 of epoch 4, 88.12 ms/it, loss 0.466243
Finished training it 48128/76743 of epoch 4, 88.15 ms/it, loss 0.464641
Finished training it 49152/76743 of epoch 4, 88.09 ms/it, loss 0.468842
Finished training it 49152/76743 of epoch 4, 88.29 ms/it, loss 0.468720
Finished training it 49152/76743 of epoch 4, 87.84 ms/it, loss 0.462869
Finished training it 49152/76743 of epoch 4, 88.23 ms/it, loss 0.465956
Finished training it 50176/76743 of epoch 4, 88.54 ms/it, loss 0.467139
Finished training it 50176/76743 of epoch 4, 88.16 ms/it, loss 0.465028
Finished training it 50176/76743 of epoch 4, 88.20 ms/it, loss 0.464503
Finished training it 50176/76743 of epoch 4, 88.34 ms/it, loss 0.467354
Finished training it 51200/76743 of epoch 4, 87.96 ms/it, loss 0.464586
Finished training it 51200/76743 of epoch 4, 88.02 ms/it, loss 0.465596
Finished training it 51200/76743 of epoch 4, 88.15 ms/it, loss 0.465749
Finished training it 51200/76743 of epoch 4, 87.83 ms/it, loss 0.466048
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548779.0
get out
0 has test check 2548779.0 and sample count 3274240
 accuracy 77.843 %, best 78.704 %, roc auc score 0.7783, best 0.7993
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548779.0
get out
3 has test check 2548779.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 87.71 ms/it, loss 0.465201
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548779.0
get out
1 has test check 2548779.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 87.77 ms/it, loss 0.464314
Finished training it 52224/76743 of epoch 4, 87.61 ms/it, loss 0.465355
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548779.0
get out
2 has test check 2548779.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 87.46 ms/it, loss 0.463865
Finished training it 53248/76743 of epoch 4, 88.09 ms/it, loss 0.465157
Finished training it 53248/76743 of epoch 4, 88.21 ms/it, loss 0.464279
Finished training it 53248/76743 of epoch 4, 88.38 ms/it, loss 0.467983
Finished training it 53248/76743 of epoch 4, 88.16 ms/it, loss 0.464592
Finished training it 54272/76743 of epoch 4, 88.00 ms/it, loss 0.466640
Finished training it 54272/76743 of epoch 4, 88.29 ms/it, loss 0.463791
Finished training it 54272/76743 of epoch 4, 88.04 ms/it, loss 0.462878
Finished training it 54272/76743 of epoch 4, 88.44 ms/it, loss 0.465761
Finished training it 55296/76743 of epoch 4, 87.91 ms/it, loss 0.466543
Finished training it 55296/76743 of epoch 4, 88.17 ms/it, loss 0.463347
Finished training it 55296/76743 of epoch 4, 87.79 ms/it, loss 0.464831
Finished training it 55296/76743 of epoch 4, 88.17 ms/it, loss 0.463588
Finished training it 56320/76743 of epoch 4, 87.93 ms/it, loss 0.467079
Finished training it 56320/76743 of epoch 4, 87.89 ms/it, loss 0.464976
Finished training it 56320/76743 of epoch 4, 87.89 ms/it, loss 0.467108
Finished training it 56320/76743 of epoch 4, 88.04 ms/it, loss 0.467468
Finished training it 57344/76743 of epoch 4, 88.43 ms/it, loss 0.463926
Finished training it 57344/76743 of epoch 4, 88.48 ms/it, loss 0.465405
Finished training it 57344/76743 of epoch 4, 88.68 ms/it, loss 0.464794
Finished training it 57344/76743 of epoch 4, 88.29 ms/it, loss 0.467179
Finished training it 58368/76743 of epoch 4, 88.30 ms/it, loss 0.466696
Finished training it 58368/76743 of epoch 4, 87.91 ms/it, loss 0.465632
Finished training it 58368/76743 of epoch 4, 88.13 ms/it, loss 0.465812
Finished training it 58368/76743 of epoch 4, 87.88 ms/it, loss 0.469342
Finished training it 59392/76743 of epoch 4, 88.12 ms/it, loss 0.465325
Finished training it 59392/76743 of epoch 4, 88.28 ms/it, loss 0.465101
Finished training it 59392/76743 of epoch 4, 88.20 ms/it, loss 0.465591
Finished training it 59392/76743 of epoch 4, 87.94 ms/it, loss 0.464262
Finished training it 60416/76743 of epoch 4, 88.16 ms/it, loss 0.464530
Finished training it 60416/76743 of epoch 4, 88.49 ms/it, loss 0.463310
Finished training it 60416/76743 of epoch 4, 88.47 ms/it, loss 0.464761
Finished training it 60416/76743 of epoch 4, 88.18 ms/it, loss 0.466117
Finished training it 61440/76743 of epoch 4, 87.67 ms/it, loss 0.466586
Finished training it 61440/76743 of epoch 4, 87.71 ms/it, loss 0.466481
Finished training it 61440/76743 of epoch 4, 87.87 ms/it, loss 0.464714
Finished training it 61440/76743 of epoch 4, 87.53 ms/it, loss 0.463999
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548082.0
get out
0 has test check 2548082.0 and sample count 3274240
 accuracy 77.822 %, best 78.704 %, roc auc score 0.7783, best 0.7993
Finished training it 62464/76743 of epoch 4, 88.16 ms/it, loss 0.466588
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548082.0
get out
3 has test check 2548082.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 88.59 ms/it, loss 0.463836
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548082.0
get out
1 has test check 2548082.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 88.56 ms/it, loss 0.466056
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548082.0
get out
2 has test check 2548082.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 88.28 ms/it, loss 0.464549
Finished training it 63488/76743 of epoch 4, 88.41 ms/it, loss 0.467919
Finished training it 63488/76743 of epoch 4, 88.07 ms/it, loss 0.462326
Finished training it 63488/76743 of epoch 4, 88.14 ms/it, loss 0.468659
Finished training it 63488/76743 of epoch 4, 88.32 ms/it, loss 0.466710
Finished training it 64512/76743 of epoch 4, 88.08 ms/it, loss 0.468797
Finished training it 64512/76743 of epoch 4, 87.79 ms/it, loss 0.465298
Finished training it 64512/76743 of epoch 4, 87.71 ms/it, loss 0.466001
Finished training it 64512/76743 of epoch 4, 87.90 ms/it, loss 0.464770
Finished training it 65536/76743 of epoch 4, 99.25 ms/it, loss 0.465643
Finished training it 65536/76743 of epoch 4, 99.60 ms/it, loss 0.465634
Finished training it 65536/76743 of epoch 4, 99.60 ms/it, loss 0.462753
Finished training it 65536/76743 of epoch 4, 98.96 ms/it, loss 0.464507
Finished training it 66560/76743 of epoch 4, 89.06 ms/it, loss 0.465140
Finished training it 66560/76743 of epoch 4, 88.63 ms/it, loss 0.464951
Finished training it 66560/76743 of epoch 4, 89.55 ms/it, loss 0.465752
Finished training it 66560/76743 of epoch 4, 89.42 ms/it, loss 0.467915
Finished training it 67584/76743 of epoch 4, 88.56 ms/it, loss 0.465110
Finished training it 67584/76743 of epoch 4, 88.30 ms/it, loss 0.465367
Finished training it 67584/76743 of epoch 4, 88.76 ms/it, loss 0.465646
Finished training it 67584/76743 of epoch 4, 88.24 ms/it, loss 0.462046
Finished training it 68608/76743 of epoch 4, 87.85 ms/it, loss 0.465118
Finished training it 68608/76743 of epoch 4, 88.26 ms/it, loss 0.467479
Finished training it 68608/76743 of epoch 4, 87.93 ms/it, loss 0.463077
Finished training it 68608/76743 of epoch 4, 88.17 ms/it, loss 0.465758
Finished training it 69632/76743 of epoch 4, 87.99 ms/it, loss 0.465373
Finished training it 69632/76743 of epoch 4, 88.24 ms/it, loss 0.465629
Finished training it 69632/76743 of epoch 4, 88.18 ms/it, loss 0.464458
Finished training it 69632/76743 of epoch 4, 87.98 ms/it, loss 0.463156
Finished training it 70656/76743 of epoch 4, 88.04 ms/it, loss 0.467393
Finished training it 70656/76743 of epoch 4, 88.05 ms/it, loss 0.464664
Finished training it 70656/76743 of epoch 4, 87.88 ms/it, loss 0.466049
Finished training it 70656/76743 of epoch 4, 87.75 ms/it, loss 0.464745
Finished training it 71680/76743 of epoch 4, 88.55 ms/it, loss 0.465716
Finished training it 71680/76743 of epoch 4, 88.63 ms/it, loss 0.464528
Finished training it 71680/76743 of epoch 4, 89.21 ms/it, loss 0.466108
Finished training it 71680/76743 of epoch 4, 89.01 ms/it, loss 0.467078
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548771.0
get out
0 has test check 2548771.0 and sample count 3274240
 accuracy 77.843 %, best 78.704 %, roc auc score 0.7781, best 0.7993
Finished training it 72704/76743 of epoch 4, 87.74 ms/it, loss 0.465273
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548771.0
get out
1 has test check 2548771.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 88.10 ms/it, loss 0.466103
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548771.0
get out
3 has test check 2548771.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 87.96 ms/it, loss 0.464632
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548771.0
get out
2 has test check 2548771.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 87.73 ms/it, loss 0.464213
Finished training it 73728/76743 of epoch 4, 88.22 ms/it, loss 0.464982
Finished training it 73728/76743 of epoch 4, 88.21 ms/it, loss 0.464729
Finished training it 73728/76743 of epoch 4, 88.65 ms/it, loss 0.465366
Finished training it 73728/76743 of epoch 4, 88.45 ms/it, loss 0.464995
Finished training it 74752/76743 of epoch 4, 88.40 ms/it, loss 0.463371
Finished training it 74752/76743 of epoch 4, 88.26 ms/it, loss 0.467114
Finished training it 74752/76743 of epoch 4, 87.99 ms/it, loss 0.465471
Finished training it 74752/76743 of epoch 4, 87.98 ms/it, loss 0.465577
Finished training it 75776/76743 of epoch 4, 88.03 ms/it, loss 0.463143
Finished training it 75776/76743 of epoch 4, 88.02 ms/it, loss 0.462359
Finished training it 75776/76743 of epoch 4, 87.80 ms/it, loss 0.466078
Finished training it 75776/76743 of epoch 4, 87.77 ms/it, loss 0.466724
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 89.22 ms/it, loss 0.466127
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 90.07 ms/it, loss 0.463940
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 89.65 ms/it, loss 0.465208
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 89.67 ms/it, loss 0.464157
Finished training it 2048/76743 of epoch 5, 88.49 ms/it, loss 0.466378
Finished training it 2048/76743 of epoch 5, 88.68 ms/it, loss 0.464631
Finished training it 2048/76743 of epoch 5, 88.77 ms/it, loss 0.466513
Finished training it 2048/76743 of epoch 5, 88.38 ms/it, loss 0.465599
Finished training it 3072/76743 of epoch 5, 88.29 ms/it, loss 0.462150
Finished training it 3072/76743 of epoch 5, 88.31 ms/it, loss 0.464455
Finished training it 3072/76743 of epoch 5, 88.00 ms/it, loss 0.467942
Finished training it 3072/76743 of epoch 5, 87.95 ms/it, loss 0.464556
Finished training it 4096/76743 of epoch 5, 87.88 ms/it, loss 0.464322
Finished training it 4096/76743 of epoch 5, 88.20 ms/it, loss 0.464278
Finished training it 4096/76743 of epoch 5, 87.80 ms/it, loss 0.466442
Finished training it 4096/76743 of epoch 5, 88.01 ms/it, loss 0.467224
Finished training it 5120/76743 of epoch 5, 88.47 ms/it, loss 0.464311
Finished training it 5120/76743 of epoch 5, 88.44 ms/it, loss 0.461963
Finished training it 5120/76743 of epoch 5, 88.64 ms/it, loss 0.468245
Finished training it 5120/76743 of epoch 5, 88.70 ms/it, loss 0.465972
Finished training it 6144/76743 of epoch 5, 88.49 ms/it, loss 0.466358
Finished training it 6144/76743 of epoch 5, 88.67 ms/it, loss 0.464318
Finished training it 6144/76743 of epoch 5, 88.43 ms/it, loss 0.465048
Finished training it 6144/76743 of epoch 5, 88.71 ms/it, loss 0.465514
Finished training it 7168/76743 of epoch 5, 87.80 ms/it, loss 0.463305
Finished training it 7168/76743 of epoch 5, 88.03 ms/it, loss 0.466471
Finished training it 7168/76743 of epoch 5, 87.96 ms/it, loss 0.465703
Finished training it 7168/76743 of epoch 5, 87.76 ms/it, loss 0.465364
Finished training it 8192/76743 of epoch 5, 88.49 ms/it, loss 0.464155
Finished training it 8192/76743 of epoch 5, 88.29 ms/it, loss 0.466814
Finished training it 8192/76743 of epoch 5, 88.19 ms/it, loss 0.465209
Finished training it 8192/76743 of epoch 5, 88.56 ms/it, loss 0.462169
Finished training it 9216/76743 of epoch 5, 93.34 ms/it, loss 0.464481
Finished training it 9216/76743 of epoch 5, 94.14 ms/it, loss 0.465511
Finished training it 9216/76743 of epoch 5, 93.72 ms/it, loss 0.465239
Finished training it 9216/76743 of epoch 5, 94.03 ms/it, loss 0.464705
Finished training it 10240/76743 of epoch 5, 88.15 ms/it, loss 0.468684
Finished training it 10240/76743 of epoch 5, 88.50 ms/it, loss 0.465316
Finished training it 10240/76743 of epoch 5, 88.45 ms/it, loss 0.464781
Finished training it 10240/76743 of epoch 5, 88.26 ms/it, loss 0.465081
Testing at - 10240/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548533.0
get out
0 has test check 2548533.0 and sample count 3274240
 accuracy 77.836 %, best 78.704 %, roc auc score 0.7782, best 0.7993
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548533.0
get out
2 has test check 2548533.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.78 ms/it, loss 0.464174
Finished training it 11264/76743 of epoch 5, 87.82 ms/it, loss 0.467683
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548533.0
get out
1 has test check 2548533.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.86 ms/it, loss 0.464680
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548533.0
get out
3 has test check 2548533.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 88.01 ms/it, loss 0.462532
Finished training it 12288/76743 of epoch 5, 95.03 ms/it, loss 0.467014
Finished training it 12288/76743 of epoch 5, 94.79 ms/it, loss 0.465610
Finished training it 12288/76743 of epoch 5, 95.24 ms/it, loss 0.461030
Finished training it 12288/76743 of epoch 5, 95.60 ms/it, loss 0.467210
Finished training it 13312/76743 of epoch 5, 88.31 ms/it, loss 0.463987
Finished training it 13312/76743 of epoch 5, 88.32 ms/it, loss 0.465928
Finished training it 13312/76743 of epoch 5, 88.03 ms/it, loss 0.466760
Finished training it 13312/76743 of epoch 5, 88.33 ms/it, loss 0.464849
Finished training it 14336/76743 of epoch 5, 88.87 ms/it, loss 0.465116
Finished training it 14336/76743 of epoch 5, 88.67 ms/it, loss 0.467688
Finished training it 14336/76743 of epoch 5, 88.75 ms/it, loss 0.465085
Finished training it 14336/76743 of epoch 5, 88.96 ms/it, loss 0.464383
Finished training it 15360/76743 of epoch 5, 96.01 ms/it, loss 0.463317
Finished training it 15360/76743 of epoch 5, 96.29 ms/it, loss 0.466634
Finished training it 15360/76743 of epoch 5, 96.20 ms/it, loss 0.465630
Finished training it 15360/76743 of epoch 5, 95.73 ms/it, loss 0.463582
Finished training it 16384/76743 of epoch 5, 88.28 ms/it, loss 0.465279
Finished training it 16384/76743 of epoch 5, 88.33 ms/it, loss 0.466937
Finished training it 16384/76743 of epoch 5, 88.01 ms/it, loss 0.463118
Finished training it 16384/76743 of epoch 5, 88.00 ms/it, loss 0.467195
Finished training it 17408/76743 of epoch 5, 88.37 ms/it, loss 0.463301
Finished training it 17408/76743 of epoch 5, 88.67 ms/it, loss 0.465635
Finished training it 17408/76743 of epoch 5, 88.57 ms/it, loss 0.462991
Finished training it 17408/76743 of epoch 5, 88.40 ms/it, loss 0.463194
Finished training it 18432/76743 of epoch 5, 88.53 ms/it, loss 0.466480
Finished training it 18432/76743 of epoch 5, 88.44 ms/it, loss 0.462442
Finished training it 18432/76743 of epoch 5, 88.35 ms/it, loss 0.464665
Finished training it 18432/76743 of epoch 5, 88.44 ms/it, loss 0.466422
Finished training it 19456/76743 of epoch 5, 88.47 ms/it, loss 0.461955
Finished training it 19456/76743 of epoch 5, 88.67 ms/it, loss 0.465023
Finished training it 19456/76743 of epoch 5, 88.33 ms/it, loss 0.466403
Finished training it 19456/76743 of epoch 5, 88.70 ms/it, loss 0.463564
Finished training it 20480/76743 of epoch 5, 88.44 ms/it, loss 0.465532
Finished training it 20480/76743 of epoch 5, 88.49 ms/it, loss 0.465561
Finished training it 20480/76743 of epoch 5, 88.22 ms/it, loss 0.465868
Finished training it 20480/76743 of epoch 5, 88.23 ms/it, loss 0.463792
