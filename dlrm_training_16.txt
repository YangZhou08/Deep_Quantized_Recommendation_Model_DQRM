Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.00 ms/it, loss 0.516702
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 51.37 ms/it, loss 0.513825
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 50.20 ms/it, loss 0.514677
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 16
---------- Embedding Table 1, quantization used, quantization bit set to 16
---------- Embedding Table 2, quantization used, quantization bit set to 16
---------- Embedding Table 3, quantization used, quantization bit set to 16
---------- Embedding Table 4, quantization used, quantization bit set to 16
---------- Embedding Table 5, quantization used, quantization bit set to 16
---------- Embedding Table 6, quantization used, quantization bit set to 16
---------- Embedding Table 7, quantization used, quantization bit set to 16
---------- Embedding Table 8, quantization used, quantization bit set to 16
---------- Embedding Table 9, quantization used, quantization bit set to 16
---------- Embedding Table 10, quantization used, quantization bit set to 16
---------- Embedding Table 11, quantization used, quantization bit set to 16
---------- Embedding Table 12, quantization used, quantization bit set to 16
---------- Embedding Table 13, quantization used, quantization bit set to 16
---------- Embedding Table 14, quantization used, quantization bit set to 16
---------- Embedding Table 15, quantization used, quantization bit set to 16
---------- Embedding Table 16, quantization used, quantization bit set to 16
---------- Embedding Table 17, quantization used, quantization bit set to 16
---------- Embedding Table 18, quantization used, quantization bit set to 16
---------- Embedding Table 19, quantization used, quantization bit set to 16
---------- Embedding Table 20, quantization used, quantization bit set to 16
---------- Embedding Table 21, quantization used, quantization bit set to 16
---------- Embedding Table 22, quantization used, quantization bit set to 16
---------- Embedding Table 23, quantization used, quantization bit set to 16
---------- Embedding Table 24, quantization used, quantization bit set to 16
---------- Embedding Table 25, quantization used, quantization bit set to 16
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 50.94 ms/it, loss 0.514511
Finished training it 2048/76743 of epoch 0, 48.09 ms/it, loss 0.501566
Finished training it 2048/76743 of epoch 0, 48.33 ms/it, loss 0.499272
Finished training it 2048/76743 of epoch 0, 48.06 ms/it, loss 0.501884
Finished training it 2048/76743 of epoch 0, 48.18 ms/it, loss 0.500736
Finished training it 3072/76743 of epoch 0, 48.29 ms/it, loss 0.490610
Finished training it 3072/76743 of epoch 0, 48.56 ms/it, loss 0.488415
Finished training it 3072/76743 of epoch 0, 48.24 ms/it, loss 0.492719
Finished training it 3072/76743 of epoch 0, 48.42 ms/it, loss 0.487366
Finished training it 4096/76743 of epoch 0, 48.05 ms/it, loss 0.482558
Finished training it 4096/76743 of epoch 0, 48.30 ms/it, loss 0.484233
Finished training it 4096/76743 of epoch 0, 47.96 ms/it, loss 0.481851
Finished training it 4096/76743 of epoch 0, 47.93 ms/it, loss 0.484565
Finished training it 5120/76743 of epoch 0, 48.41 ms/it, loss 0.476415
Finished training it 5120/76743 of epoch 0, 48.07 ms/it, loss 0.474909
Finished training it 5120/76743 of epoch 0, 48.70 ms/it, loss 0.478779
Finished training it 5120/76743 of epoch 0, 48.42 ms/it, loss 0.480180
Finished training it 6144/76743 of epoch 0, 47.02 ms/it, loss 0.475484
Finished training it 6144/76743 of epoch 0, 47.45 ms/it, loss 0.474379
Finished training it 6144/76743 of epoch 0, 47.13 ms/it, loss 0.473378
Finished training it 6144/76743 of epoch 0, 47.98 ms/it, loss 0.473141
Finished training it 7168/76743 of epoch 0, 47.92 ms/it, loss 0.471760
Finished training it 7168/76743 of epoch 0, 48.36 ms/it, loss 0.472133
Finished training it 7168/76743 of epoch 0, 48.36 ms/it, loss 0.471414
Finished training it 7168/76743 of epoch 0, 47.88 ms/it, loss 0.468577
Finished training it 8192/76743 of epoch 0, 47.60 ms/it, loss 0.467696
Finished training it 8192/76743 of epoch 0, 47.29 ms/it, loss 0.469971
Finished training it 8192/76743 of epoch 0, 47.26 ms/it, loss 0.468344
Finished training it 8192/76743 of epoch 0, 47.49 ms/it, loss 0.466164
Finished training it 9216/76743 of epoch 0, 47.98 ms/it, loss 0.466513
Finished training it 9216/76743 of epoch 0, 47.49 ms/it, loss 0.466562
Finished training it 9216/76743 of epoch 0, 47.52 ms/it, loss 0.466731
Finished training it 9216/76743 of epoch 0, 48.31 ms/it, loss 0.467677
Finished training it 10240/76743 of epoch 0, 47.72 ms/it, loss 0.465191
Finished training it 10240/76743 of epoch 0, 48.06 ms/it, loss 0.465384
Finished training it 10240/76743 of epoch 0, 47.95 ms/it, loss 0.466347
Finished training it 10240/76743 of epoch 0, 47.68 ms/it, loss 0.469050
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549684.0
get out
0 has test check 2549684.0 and sample count 3274240
 accuracy 77.871 %, best 77.871 %, roc auc score 0.7831, best 0.7831
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549684.0
get out
3 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.36 ms/it, loss 0.461716
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549684.0
get out
2 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.38 ms/it, loss 0.463816
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549684.0
get out
1 has test check 2549684.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 48.62 ms/it, loss 0.464142
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 48.25 ms/it, loss 0.466849
Finished training it 12288/76743 of epoch 0, 48.62 ms/it, loss 0.466195
Finished training it 12288/76743 of epoch 0, 48.55 ms/it, loss 0.465625
Finished training it 12288/76743 of epoch 0, 48.24 ms/it, loss 0.459643
Finished training it 12288/76743 of epoch 0, 48.27 ms/it, loss 0.463956
Finished training it 13312/76743 of epoch 0, 48.14 ms/it, loss 0.462094
Finished training it 13312/76743 of epoch 0, 48.03 ms/it, loss 0.463700
Finished training it 13312/76743 of epoch 0, 48.53 ms/it, loss 0.462694
Finished training it 13312/76743 of epoch 0, 48.27 ms/it, loss 0.464322
Finished training it 14336/76743 of epoch 0, 48.15 ms/it, loss 0.461709
Finished training it 14336/76743 of epoch 0, 48.16 ms/it, loss 0.464586
Finished training it 14336/76743 of epoch 0, 47.71 ms/it, loss 0.461618
Finished training it 14336/76743 of epoch 0, 47.91 ms/it, loss 0.461938
Finished training it 15360/76743 of epoch 0, 54.04 ms/it, loss 0.462586
Finished training it 15360/76743 of epoch 0, 54.42 ms/it, loss 0.459805
Finished training it 15360/76743 of epoch 0, 53.72 ms/it, loss 0.462678
Finished training it 15360/76743 of epoch 0, 54.42 ms/it, loss 0.460717
Finished training it 16384/76743 of epoch 0, 48.79 ms/it, loss 0.462612
Finished training it 16384/76743 of epoch 0, 49.13 ms/it, loss 0.462164
Finished training it 16384/76743 of epoch 0, 48.71 ms/it, loss 0.461386
Finished training it 16384/76743 of epoch 0, 48.56 ms/it, loss 0.458125
Finished training it 17408/76743 of epoch 0, 47.93 ms/it, loss 0.458936
Finished training it 17408/76743 of epoch 0, 47.83 ms/it, loss 0.459083
Finished training it 17408/76743 of epoch 0, 48.12 ms/it, loss 0.461089
Finished training it 17408/76743 of epoch 0, 47.79 ms/it, loss 0.459490
Finished training it 18432/76743 of epoch 0, 49.08 ms/it, loss 0.458103
Finished training it 18432/76743 of epoch 0, 49.21 ms/it, loss 0.461185
Finished training it 18432/76743 of epoch 0, 48.78 ms/it, loss 0.456773
Finished training it 18432/76743 of epoch 0, 48.94 ms/it, loss 0.460314
Finished training it 19456/76743 of epoch 0, 48.74 ms/it, loss 0.459921
Finished training it 19456/76743 of epoch 0, 48.38 ms/it, loss 0.456601
Finished training it 19456/76743 of epoch 0, 48.68 ms/it, loss 0.459978
Finished training it 19456/76743 of epoch 0, 48.72 ms/it, loss 0.457812
Finished training it 20480/76743 of epoch 0, 49.14 ms/it, loss 0.459376
Finished training it 20480/76743 of epoch 0, 48.91 ms/it, loss 0.458472
Finished training it 20480/76743 of epoch 0, 49.10 ms/it, loss 0.459975
Finished training it 20480/76743 of epoch 0, 48.66 ms/it, loss 0.460096
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2563544.0
get out
0 has test check 2563544.0 and sample count 3274240
 accuracy 78.294 %, best 78.294 %, roc auc score 0.7900, best 0.7900
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2563544.0
get out
2 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.02 ms/it, loss 0.459840
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2563544.0
get out
1 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 47.97 ms/it, loss 0.458065
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 47.96 ms/it, loss 0.458370
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2563544.0
get out
3 has test check 2563544.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 48.20 ms/it, loss 0.456938
Finished training it 22528/76743 of epoch 0, 47.61 ms/it, loss 0.458241
Finished training it 22528/76743 of epoch 0, 48.02 ms/it, loss 0.459465
Finished training it 22528/76743 of epoch 0, 47.80 ms/it, loss 0.457982
Finished training it 22528/76743 of epoch 0, 48.14 ms/it, loss 0.455198
Finished training it 23552/76743 of epoch 0, 48.07 ms/it, loss 0.456892
Finished training it 23552/76743 of epoch 0, 48.08 ms/it, loss 0.457524
Finished training it 23552/76743 of epoch 0, 47.95 ms/it, loss 0.457775
Finished training it 23552/76743 of epoch 0, 47.73 ms/it, loss 0.454250
Finished training it 24576/76743 of epoch 0, 47.08 ms/it, loss 0.456124
Finished training it 24576/76743 of epoch 0, 47.14 ms/it, loss 0.454025
Finished training it 24576/76743 of epoch 0, 46.88 ms/it, loss 0.457948
Finished training it 24576/76743 of epoch 0, 47.54 ms/it, loss 0.457084
Finished training it 25600/76743 of epoch 0, 47.64 ms/it, loss 0.456683
Finished training it 25600/76743 of epoch 0, 48.42 ms/it, loss 0.455833
Finished training it 25600/76743 of epoch 0, 47.36 ms/it, loss 0.454512
Finished training it 25600/76743 of epoch 0, 47.85 ms/it, loss 0.454230
Finished training it 26624/76743 of epoch 0, 48.84 ms/it, loss 0.456683
Finished training it 26624/76743 of epoch 0, 48.36 ms/it, loss 0.451934
Finished training it 26624/76743 of epoch 0, 48.83 ms/it, loss 0.454859
Finished training it 26624/76743 of epoch 0, 48.49 ms/it, loss 0.455893
Finished training it 27648/76743 of epoch 0, 48.00 ms/it, loss 0.456670
Finished training it 27648/76743 of epoch 0, 48.09 ms/it, loss 0.457671
Finished training it 27648/76743 of epoch 0, 47.78 ms/it, loss 0.457426
Finished training it 27648/76743 of epoch 0, 47.41 ms/it, loss 0.454501
Finished training it 28672/76743 of epoch 0, 48.04 ms/it, loss 0.455609
Finished training it 28672/76743 of epoch 0, 48.73 ms/it, loss 0.453449
Finished training it 28672/76743 of epoch 0, 48.75 ms/it, loss 0.456664
Finished training it 28672/76743 of epoch 0, 48.09 ms/it, loss 0.457040
Finished training it 29696/76743 of epoch 0, 48.09 ms/it, loss 0.452302
Finished training it 29696/76743 of epoch 0, 48.14 ms/it, loss 0.456050
Finished training it 29696/76743 of epoch 0, 48.10 ms/it, loss 0.455268
Finished training it 29696/76743 of epoch 0, 48.29 ms/it, loss 0.456925
Finished training it 30720/76743 of epoch 0, 47.65 ms/it, loss 0.457169
Finished training it 30720/76743 of epoch 0, 47.55 ms/it, loss 0.453619
Finished training it 30720/76743 of epoch 0, 47.23 ms/it, loss 0.458074
Finished training it 30720/76743 of epoch 0, 47.87 ms/it, loss 0.451933
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2567182.0
get out
0 has test check 2567182.0 and sample count 3274240
 accuracy 78.405 %, best 78.405 %, roc auc score 0.7931, best 0.7931
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 49.02 ms/it, loss 0.454278
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2567182.0
get out
2 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 48.77 ms/it, loss 0.456337
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2567182.0
get out
3 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 48.90 ms/it, loss 0.453122
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2567182.0
get out
1 has test check 2567182.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 49.07 ms/it, loss 0.452170
Finished training it 32768/76743 of epoch 0, 48.34 ms/it, loss 0.456932
Finished training it 32768/76743 of epoch 0, 48.32 ms/it, loss 0.455261
Finished training it 32768/76743 of epoch 0, 48.52 ms/it, loss 0.457966
Finished training it 32768/76743 of epoch 0, 48.47 ms/it, loss 0.454398
Finished training it 33792/76743 of epoch 0, 48.31 ms/it, loss 0.454994
Finished training it 33792/76743 of epoch 0, 47.91 ms/it, loss 0.456479
Finished training it 33792/76743 of epoch 0, 47.88 ms/it, loss 0.452749
Finished training it 33792/76743 of epoch 0, 48.08 ms/it, loss 0.454992
Finished training it 34816/76743 of epoch 0, 47.88 ms/it, loss 0.454144
Finished training it 34816/76743 of epoch 0, 47.97 ms/it, loss 0.455914
Finished training it 34816/76743 of epoch 0, 47.83 ms/it, loss 0.456577
Finished training it 34816/76743 of epoch 0, 47.96 ms/it, loss 0.453789
Finished training it 35840/76743 of epoch 0, 47.96 ms/it, loss 0.452597
Finished training it 35840/76743 of epoch 0, 52.71 ms/it, loss 0.453834
Finished training it 35840/76743 of epoch 0, 53.16 ms/it, loss 0.452831
Finished training it 35840/76743 of epoch 0, 52.67 ms/it, loss 0.453189
Finished training it 36864/76743 of epoch 0, 49.05 ms/it, loss 0.454233
Finished training it 36864/76743 of epoch 0, 54.10 ms/it, loss 0.452190
Finished training it 36864/76743 of epoch 0, 51.20 ms/it, loss 0.458764
Finished training it 36864/76743 of epoch 0, 49.18 ms/it, loss 0.451942
Finished training it 37888/76743 of epoch 0, 48.58 ms/it, loss 0.455170
Finished training it 37888/76743 of epoch 0, 48.72 ms/it, loss 0.452137
Finished training it 37888/76743 of epoch 0, 49.02 ms/it, loss 0.452008
Finished training it 37888/76743 of epoch 0, 48.42 ms/it, loss 0.450566
Finished training it 38912/76743 of epoch 0, 47.87 ms/it, loss 0.450676
Finished training it 38912/76743 of epoch 0, 47.80 ms/it, loss 0.451677
Finished training it 38912/76743 of epoch 0, 47.58 ms/it, loss 0.453735
Finished training it 38912/76743 of epoch 0, 47.94 ms/it, loss 0.451404
Finished training it 39936/76743 of epoch 0, 48.12 ms/it, loss 0.453750
Finished training it 39936/76743 of epoch 0, 48.46 ms/it, loss 0.451172
Finished training it 39936/76743 of epoch 0, 48.04 ms/it, loss 0.450423
Finished training it 39936/76743 of epoch 0, 48.03 ms/it, loss 0.452518
Finished training it 40960/76743 of epoch 0, 48.68 ms/it, loss 0.453365
Finished training it 40960/76743 of epoch 0, 48.41 ms/it, loss 0.452239
Finished training it 40960/76743 of epoch 0, 48.72 ms/it, loss 0.450162
Finished training it 40960/76743 of epoch 0, 48.57 ms/it, loss 0.453485
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2570506.0
get out
0 has test check 2570506.0 and sample count 3274240
 accuracy 78.507 %, best 78.507 %, roc auc score 0.7957, best 0.7957
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2570506.0
get out
2 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.41 ms/it, loss 0.455943
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2570506.0
get out
3 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.23 ms/it, loss 0.451408
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2570506.0
get out
1 has test check 2570506.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 48.22 ms/it, loss 0.450535
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 48.03 ms/it, loss 0.452744
Finished training it 43008/76743 of epoch 0, 48.98 ms/it, loss 0.453662
Finished training it 43008/76743 of epoch 0, 48.71 ms/it, loss 0.452099
Finished training it 43008/76743 of epoch 0, 49.04 ms/it, loss 0.453108
Finished training it 43008/76743 of epoch 0, 49.16 ms/it, loss 0.456309
Finished training it 44032/76743 of epoch 0, 47.68 ms/it, loss 0.451356
Finished training it 44032/76743 of epoch 0, 47.60 ms/it, loss 0.452624
Finished training it 44032/76743 of epoch 0, 47.99 ms/it, loss 0.450840
Finished training it 44032/76743 of epoch 0, 47.90 ms/it, loss 0.450190
Finished training it 45056/76743 of epoch 0, 48.53 ms/it, loss 0.451714
Finished training it 45056/76743 of epoch 0, 48.79 ms/it, loss 0.452669
Finished training it 45056/76743 of epoch 0, 49.20 ms/it, loss 0.453146
Finished training it 45056/76743 of epoch 0, 48.92 ms/it, loss 0.449284
Finished training it 46080/76743 of epoch 0, 48.31 ms/it, loss 0.453401
Finished training it 46080/76743 of epoch 0, 48.12 ms/it, loss 0.452237
Finished training it 46080/76743 of epoch 0, 48.04 ms/it, loss 0.452386
Finished training it 46080/76743 of epoch 0, 47.85 ms/it, loss 0.450302
Finished training it 47104/76743 of epoch 0, 48.06 ms/it, loss 0.451179
Finished training it 47104/76743 of epoch 0, 47.54 ms/it, loss 0.451909
Finished training it 47104/76743 of epoch 0, 47.38 ms/it, loss 0.450972
Finished training it 47104/76743 of epoch 0, 47.98 ms/it, loss 0.451733
Finished training it 48128/76743 of epoch 0, 48.58 ms/it, loss 0.449727
Finished training it 48128/76743 of epoch 0, 47.71 ms/it, loss 0.450537
Finished training it 48128/76743 of epoch 0, 48.36 ms/it, loss 0.452259
Finished training it 48128/76743 of epoch 0, 48.03 ms/it, loss 0.451165
Finished training it 49152/76743 of epoch 0, 48.12 ms/it, loss 0.451423
Finished training it 49152/76743 of epoch 0, 47.47 ms/it, loss 0.454000
Finished training it 49152/76743 of epoch 0, 47.69 ms/it, loss 0.447497
Finished training it 49152/76743 of epoch 0, 47.55 ms/it, loss 0.453314
Finished training it 50176/76743 of epoch 0, 47.44 ms/it, loss 0.452101
Finished training it 50176/76743 of epoch 0, 47.74 ms/it, loss 0.452839
Finished training it 50176/76743 of epoch 0, 47.42 ms/it, loss 0.450412
Finished training it 50176/76743 of epoch 0, 47.63 ms/it, loss 0.450544
Finished training it 51200/76743 of epoch 0, 48.00 ms/it, loss 0.450810
Finished training it 51200/76743 of epoch 0, 47.91 ms/it, loss 0.450537
Finished training it 51200/76743 of epoch 0, 47.75 ms/it, loss 0.448926
Finished training it 51200/76743 of epoch 0, 47.66 ms/it, loss 0.451426
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2572931.0
get out
0 has test check 2572931.0 and sample count 3274240
 accuracy 78.581 %, best 78.581 %, roc auc score 0.7972, best 0.7972
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 47.57 ms/it, loss 0.449577
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2572931.0
get out
1 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.73 ms/it, loss 0.449948
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2572931.0
get out
3 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.78 ms/it, loss 0.451299
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2572931.0
get out
2 has test check 2572931.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 47.75 ms/it, loss 0.449063
Finished training it 53248/76743 of epoch 0, 48.76 ms/it, loss 0.448759
Finished training it 53248/76743 of epoch 0, 48.48 ms/it, loss 0.449518
Finished training it 53248/76743 of epoch 0, 48.01 ms/it, loss 0.452576
Finished training it 53248/76743 of epoch 0, 48.15 ms/it, loss 0.449495
Finished training it 54272/76743 of epoch 0, 48.08 ms/it, loss 0.447998
Finished training it 54272/76743 of epoch 0, 47.82 ms/it, loss 0.448124
Finished training it 54272/76743 of epoch 0, 47.66 ms/it, loss 0.451287
Finished training it 54272/76743 of epoch 0, 47.84 ms/it, loss 0.450736
Finished training it 55296/76743 of epoch 0, 47.80 ms/it, loss 0.447134
Finished training it 55296/76743 of epoch 0, 47.42 ms/it, loss 0.449762
Finished training it 55296/76743 of epoch 0, 47.46 ms/it, loss 0.449230
Finished training it 55296/76743 of epoch 0, 47.82 ms/it, loss 0.450467
Finished training it 56320/76743 of epoch 0, 47.52 ms/it, loss 0.451954
Finished training it 56320/76743 of epoch 0, 47.58 ms/it, loss 0.450524
Finished training it 56320/76743 of epoch 0, 47.60 ms/it, loss 0.452406
Finished training it 56320/76743 of epoch 0, 47.78 ms/it, loss 0.448759
Finished training it 57344/76743 of epoch 0, 47.83 ms/it, loss 0.449886
Finished training it 57344/76743 of epoch 0, 47.66 ms/it, loss 0.447740
Finished training it 57344/76743 of epoch 0, 47.49 ms/it, loss 0.451849
Finished training it 57344/76743 of epoch 0, 47.76 ms/it, loss 0.449816
Finished training it 58368/76743 of epoch 0, 48.39 ms/it, loss 0.450219
Finished training it 58368/76743 of epoch 0, 48.53 ms/it, loss 0.450902
Finished training it 58368/76743 of epoch 0, 48.17 ms/it, loss 0.450212
Finished training it 58368/76743 of epoch 0, 48.30 ms/it, loss 0.453564
Finished training it 59392/76743 of epoch 0, 48.29 ms/it, loss 0.448366
Finished training it 59392/76743 of epoch 0, 48.52 ms/it, loss 0.449740
Finished training it 59392/76743 of epoch 0, 48.04 ms/it, loss 0.448196
Finished training it 59392/76743 of epoch 0, 48.60 ms/it, loss 0.448999
Finished training it 60416/76743 of epoch 0, 47.90 ms/it, loss 0.448910
Finished training it 60416/76743 of epoch 0, 47.72 ms/it, loss 0.448607
Finished training it 60416/76743 of epoch 0, 48.16 ms/it, loss 0.448228
Finished training it 60416/76743 of epoch 0, 47.54 ms/it, loss 0.450813
Finished training it 61440/76743 of epoch 0, 47.69 ms/it, loss 0.449368
Finished training it 61440/76743 of epoch 0, 47.76 ms/it, loss 0.450739
Finished training it 61440/76743 of epoch 0, 48.35 ms/it, loss 0.448064
Finished training it 61440/76743 of epoch 0, 47.39 ms/it, loss 0.446945
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571885.0
get out
0 has test check 2571885.0 and sample count 3274240
 accuracy 78.549 %, best 78.581 %, roc auc score 0.7979, best 0.7979
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571885.0
get out
3 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 48.16 ms/it, loss 0.448282
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571885.0
get out
2 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 47.81 ms/it, loss 0.448746
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571885.0
get out
1 has test check 2571885.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 47.94 ms/it, loss 0.450529
Finished training it 62464/76743 of epoch 0, 47.59 ms/it, loss 0.449810
Finished training it 63488/76743 of epoch 0, 48.19 ms/it, loss 0.451776
Finished training it 63488/76743 of epoch 0, 47.96 ms/it, loss 0.451880
Finished training it 63488/76743 of epoch 0, 48.02 ms/it, loss 0.451507
Finished training it 63488/76743 of epoch 0, 47.70 ms/it, loss 0.446173
Finished training it 64512/76743 of epoch 0, 47.90 ms/it, loss 0.449187
Finished training it 64512/76743 of epoch 0, 48.46 ms/it, loss 0.451185
Finished training it 64512/76743 of epoch 0, 47.99 ms/it, loss 0.449693
Finished training it 64512/76743 of epoch 0, 47.66 ms/it, loss 0.448755
Finished training it 65536/76743 of epoch 0, 47.97 ms/it, loss 0.448244
Finished training it 65536/76743 of epoch 0, 52.63 ms/it, loss 0.449794
Finished training it 65536/76743 of epoch 0, 52.98 ms/it, loss 0.446315
Finished training it 65536/76743 of epoch 0, 52.19 ms/it, loss 0.449632
Finished training it 66560/76743 of epoch 0, 47.78 ms/it, loss 0.448398
Finished training it 66560/76743 of epoch 0, 48.33 ms/it, loss 0.448673
Finished training it 66560/76743 of epoch 0, 53.03 ms/it, loss 0.447972
Finished training it 66560/76743 of epoch 0, 49.28 ms/it, loss 0.450361
Finished training it 67584/76743 of epoch 0, 47.48 ms/it, loss 0.448834
Finished training it 67584/76743 of epoch 0, 48.02 ms/it, loss 0.449102
Finished training it 67584/76743 of epoch 0, 47.65 ms/it, loss 0.448287
Finished training it 67584/76743 of epoch 0, 47.40 ms/it, loss 0.445058
Finished training it 68608/76743 of epoch 0, 47.47 ms/it, loss 0.445898
Finished training it 68608/76743 of epoch 0, 47.98 ms/it, loss 0.448245
Finished training it 68608/76743 of epoch 0, 47.83 ms/it, loss 0.447740
Finished training it 68608/76743 of epoch 0, 47.93 ms/it, loss 0.450626
Finished training it 69632/76743 of epoch 0, 48.32 ms/it, loss 0.447932
Finished training it 69632/76743 of epoch 0, 48.27 ms/it, loss 0.448163
Finished training it 69632/76743 of epoch 0, 48.16 ms/it, loss 0.444949
Finished training it 69632/76743 of epoch 0, 48.31 ms/it, loss 0.448331
Finished training it 70656/76743 of epoch 0, 46.99 ms/it, loss 0.447669
Finished training it 70656/76743 of epoch 0, 47.28 ms/it, loss 0.447329
Finished training it 70656/76743 of epoch 0, 47.23 ms/it, loss 0.449160
Finished training it 70656/76743 of epoch 0, 47.16 ms/it, loss 0.449886
Finished training it 71680/76743 of epoch 0, 48.01 ms/it, loss 0.446542
Finished training it 71680/76743 of epoch 0, 47.82 ms/it, loss 0.448325
Finished training it 71680/76743 of epoch 0, 48.14 ms/it, loss 0.449354
Finished training it 71680/76743 of epoch 0, 48.08 ms/it, loss 0.448979
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576942.0
get out
0 has test check 2576942.0 and sample count 3274240
 accuracy 78.704 %, best 78.704 %, roc auc score 0.7993, best 0.7993
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576942.0
get out
2 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 48.42 ms/it, loss 0.446340
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576942.0
get out
1 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 47.98 ms/it, loss 0.448149
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576942.0
get out
3 has test check 2576942.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 47.70 ms/it, loss 0.446646
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 47.92 ms/it, loss 0.447217
Finished training it 73728/76743 of epoch 0, 47.69 ms/it, loss 0.447677
Finished training it 73728/76743 of epoch 0, 47.76 ms/it, loss 0.447925
Finished training it 73728/76743 of epoch 0, 47.41 ms/it, loss 0.447366
Finished training it 73728/76743 of epoch 0, 47.88 ms/it, loss 0.447332
Finished training it 74752/76743 of epoch 0, 47.48 ms/it, loss 0.445855
Finished training it 74752/76743 of epoch 0, 47.45 ms/it, loss 0.448510
Finished training it 74752/76743 of epoch 0, 47.74 ms/it, loss 0.448739
Finished training it 74752/76743 of epoch 0, 48.07 ms/it, loss 0.447953
Finished training it 75776/76743 of epoch 0, 48.25 ms/it, loss 0.444716
Finished training it 75776/76743 of epoch 0, 48.32 ms/it, loss 0.448732
Finished training it 75776/76743 of epoch 0, 48.25 ms/it, loss 0.445917
Finished training it 75776/76743 of epoch 0, 47.76 ms/it, loss 0.448973
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.01 ms/it, loss 0.476744
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 69.74 ms/it, loss 0.476866
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 70.45 ms/it, loss 0.476371
Warning: Skipping the batch 76742 with size 14
Using 16-bit precision
Finished training it 1024/76743 of epoch 1, 69.53 ms/it, loss 0.477531
Finished training it 2048/76743 of epoch 1, 87.26 ms/it, loss 0.500668
Finished training it 2048/76743 of epoch 1, 87.41 ms/it, loss 0.502980
Finished training it 2048/76743 of epoch 1, 87.38 ms/it, loss 0.502547
Finished training it 2048/76743 of epoch 1, 87.05 ms/it, loss 0.503358
Finished training it 3072/76743 of epoch 1, 88.82 ms/it, loss 0.496104
Finished training it 3072/76743 of epoch 1, 88.86 ms/it, loss 0.496704
Finished training it 3072/76743 of epoch 1, 88.30 ms/it, loss 0.501644
Finished training it 3072/76743 of epoch 1, 88.07 ms/it, loss 0.498453
Finished training it 4096/76743 of epoch 1, 88.16 ms/it, loss 0.496113
Finished training it 4096/76743 of epoch 1, 87.57 ms/it, loss 0.498255
Finished training it 4096/76743 of epoch 1, 88.16 ms/it, loss 0.498128
Finished training it 4096/76743 of epoch 1, 87.40 ms/it, loss 0.495785
Finished training it 5120/76743 of epoch 1, 87.18 ms/it, loss 0.491684
Finished training it 5120/76743 of epoch 1, 86.88 ms/it, loss 0.496175
Finished training it 5120/76743 of epoch 1, 86.86 ms/it, loss 0.495462
Finished training it 5120/76743 of epoch 1, 86.77 ms/it, loss 0.493357
Finished training it 6144/76743 of epoch 1, 88.02 ms/it, loss 0.493378
Finished training it 6144/76743 of epoch 1, 87.90 ms/it, loss 0.492526
Finished training it 6144/76743 of epoch 1, 87.51 ms/it, loss 0.494200
Finished training it 6144/76743 of epoch 1, 88.40 ms/it, loss 0.492107
Finished training it 7168/76743 of epoch 1, 88.78 ms/it, loss 0.492626
Finished training it 7168/76743 of epoch 1, 88.22 ms/it, loss 0.492245
Finished training it 7168/76743 of epoch 1, 88.76 ms/it, loss 0.489791
Finished training it 7168/76743 of epoch 1, 88.45 ms/it, loss 0.492852
Finished training it 8192/76743 of epoch 1, 88.22 ms/it, loss 0.489808
Finished training it 8192/76743 of epoch 1, 88.61 ms/it, loss 0.491103
Finished training it 8192/76743 of epoch 1, 88.05 ms/it, loss 0.492997
Finished training it 8192/76743 of epoch 1, 88.12 ms/it, loss 0.490739
Finished training it 9216/76743 of epoch 1, 88.10 ms/it, loss 0.491960
Finished training it 9216/76743 of epoch 1, 88.07 ms/it, loss 0.489144
Finished training it 9216/76743 of epoch 1, 88.16 ms/it, loss 0.490514
Finished training it 9216/76743 of epoch 1, 87.78 ms/it, loss 0.491486
Finished training it 10240/76743 of epoch 1, 88.02 ms/it, loss 0.489313
Finished training it 10240/76743 of epoch 1, 87.54 ms/it, loss 0.490693
Finished training it 10240/76743 of epoch 1, 88.04 ms/it, loss 0.492856
Finished training it 10240/76743 of epoch 1, 87.97 ms/it, loss 0.489619
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2510407.0
get out
0 has test check 2510407.0 and sample count 3274240
 accuracy 76.671 %, best 78.704 %, roc auc score 0.7481, best 0.7993
Finished training it 11264/76743 of epoch 1, 86.79 ms/it, loss 0.492037
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2510407.0
get out
2 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.88 ms/it, loss 0.489123
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2510407.0
get out
3 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.48 ms/it, loss 0.486768
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2510407.0
get out
1 has test check 2510407.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.60 ms/it, loss 0.490108
Finished training it 12288/76743 of epoch 1, 88.26 ms/it, loss 0.490738
Finished training it 12288/76743 of epoch 1, 88.15 ms/it, loss 0.485474
Finished training it 12288/76743 of epoch 1, 88.60 ms/it, loss 0.491502
Finished training it 12288/76743 of epoch 1, 88.13 ms/it, loss 0.489378
Finished training it 13312/76743 of epoch 1, 88.34 ms/it, loss 0.487112
Finished training it 13312/76743 of epoch 1, 88.20 ms/it, loss 0.488987
Finished training it 13312/76743 of epoch 1, 87.52 ms/it, loss 0.487694
Finished training it 13312/76743 of epoch 1, 87.73 ms/it, loss 0.489990
Finished training it 14336/76743 of epoch 1, 93.98 ms/it, loss 0.487668
Finished training it 14336/76743 of epoch 1, 93.66 ms/it, loss 0.489624
Finished training it 14336/76743 of epoch 1, 93.93 ms/it, loss 0.487749
Finished training it 14336/76743 of epoch 1, 93.66 ms/it, loss 0.488375
Finished training it 15360/76743 of epoch 1, 94.22 ms/it, loss 0.486481
Finished training it 15360/76743 of epoch 1, 94.32 ms/it, loss 0.487590
Finished training it 15360/76743 of epoch 1, 94.13 ms/it, loss 0.485702
Finished training it 15360/76743 of epoch 1, 94.53 ms/it, loss 0.488900
Finished training it 16384/76743 of epoch 1, 87.91 ms/it, loss 0.485999
Finished training it 16384/76743 of epoch 1, 87.69 ms/it, loss 0.486791
Finished training it 16384/76743 of epoch 1, 87.69 ms/it, loss 0.488814
Finished training it 16384/76743 of epoch 1, 88.00 ms/it, loss 0.488679
Finished training it 17408/76743 of epoch 1, 88.22 ms/it, loss 0.486185
Finished training it 17408/76743 of epoch 1, 88.18 ms/it, loss 0.486286
Finished training it 17408/76743 of epoch 1, 88.03 ms/it, loss 0.487614
Finished training it 17408/76743 of epoch 1, 88.01 ms/it, loss 0.484907
Finished training it 18432/76743 of epoch 1, 87.63 ms/it, loss 0.483557
Finished training it 18432/76743 of epoch 1, 87.83 ms/it, loss 0.486053
Finished training it 18432/76743 of epoch 1, 88.20 ms/it, loss 0.488406
Finished training it 18432/76743 of epoch 1, 87.20 ms/it, loss 0.487713
Finished training it 19456/76743 of epoch 1, 87.63 ms/it, loss 0.487415
Finished training it 19456/76743 of epoch 1, 88.09 ms/it, loss 0.486516
Finished training it 19456/76743 of epoch 1, 87.78 ms/it, loss 0.484773
Finished training it 19456/76743 of epoch 1, 87.88 ms/it, loss 0.484874
Finished training it 20480/76743 of epoch 1, 88.52 ms/it, loss 0.487061
Finished training it 20480/76743 of epoch 1, 88.90 ms/it, loss 0.486463
Finished training it 20480/76743 of epoch 1, 88.68 ms/it, loss 0.486882
Finished training it 20480/76743 of epoch 1, 88.16 ms/it, loss 0.484607
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2516253.0
get out
0 has test check 2516253.0 and sample count 3274240
 accuracy 76.850 %, best 78.704 %, roc auc score 0.7534, best 0.7993
Finished training it 21504/76743 of epoch 1, 87.93 ms/it, loss 0.485554
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2516253.0
get out
1 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 88.07 ms/it, loss 0.484749
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2516253.0
get out
3 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.69 ms/it, loss 0.485001
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2516253.0
get out
2 has test check 2516253.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.77 ms/it, loss 0.488156
Finished training it 22528/76743 of epoch 1, 88.36 ms/it, loss 0.486558
Finished training it 22528/76743 of epoch 1, 88.23 ms/it, loss 0.486885
Finished training it 22528/76743 of epoch 1, 87.91 ms/it, loss 0.483254
Finished training it 22528/76743 of epoch 1, 87.81 ms/it, loss 0.486356
Finished training it 23552/76743 of epoch 1, 88.08 ms/it, loss 0.484726
Finished training it 23552/76743 of epoch 1, 87.76 ms/it, loss 0.482556
Finished training it 23552/76743 of epoch 1, 88.00 ms/it, loss 0.485984
Finished training it 23552/76743 of epoch 1, 87.89 ms/it, loss 0.485435
Finished training it 24576/76743 of epoch 1, 88.22 ms/it, loss 0.482051
Finished training it 24576/76743 of epoch 1, 87.58 ms/it, loss 0.484726
Finished training it 24576/76743 of epoch 1, 87.71 ms/it, loss 0.486725
Finished training it 24576/76743 of epoch 1, 88.44 ms/it, loss 0.484045
Finished training it 25600/76743 of epoch 1, 87.65 ms/it, loss 0.482757
Finished training it 25600/76743 of epoch 1, 87.88 ms/it, loss 0.483939
Finished training it 25600/76743 of epoch 1, 87.74 ms/it, loss 0.483172
Finished training it 25600/76743 of epoch 1, 88.31 ms/it, loss 0.482741
Finished training it 26624/76743 of epoch 1, 87.49 ms/it, loss 0.479914
Finished training it 26624/76743 of epoch 1, 87.87 ms/it, loss 0.485007
Finished training it 26624/76743 of epoch 1, 88.19 ms/it, loss 0.485866
Finished training it 26624/76743 of epoch 1, 87.74 ms/it, loss 0.481898
Finished training it 27648/76743 of epoch 1, 87.98 ms/it, loss 0.484078
Finished training it 27648/76743 of epoch 1, 87.88 ms/it, loss 0.485588
Finished training it 27648/76743 of epoch 1, 87.93 ms/it, loss 0.486189
Finished training it 27648/76743 of epoch 1, 87.56 ms/it, loss 0.482653
Finished training it 28672/76743 of epoch 1, 88.20 ms/it, loss 0.481463
Finished training it 28672/76743 of epoch 1, 87.45 ms/it, loss 0.484874
Finished training it 28672/76743 of epoch 1, 87.85 ms/it, loss 0.483315
Finished training it 28672/76743 of epoch 1, 87.48 ms/it, loss 0.484869
Finished training it 29696/76743 of epoch 1, 87.90 ms/it, loss 0.485562
Finished training it 29696/76743 of epoch 1, 88.05 ms/it, loss 0.484192
Finished training it 29696/76743 of epoch 1, 88.10 ms/it, loss 0.479023
Finished training it 29696/76743 of epoch 1, 88.09 ms/it, loss 0.482860
Finished training it 30720/76743 of epoch 1, 87.22 ms/it, loss 0.485767
Finished training it 30720/76743 of epoch 1, 87.95 ms/it, loss 0.481471
Finished training it 30720/76743 of epoch 1, 87.39 ms/it, loss 0.485413
Finished training it 30720/76743 of epoch 1, 87.62 ms/it, loss 0.481972
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2519434.0
get out
0 has test check 2519434.0 and sample count 3274240
 accuracy 76.947 %, best 78.704 %, roc auc score 0.7554, best 0.7993
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2519434.0
get out
1 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.11 ms/it, loss 0.480955
Finished training it 31744/76743 of epoch 1, 86.95 ms/it, loss 0.481703
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2519434.0
get out
3 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.23 ms/it, loss 0.481879
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2519434.0
get out
2 has test check 2519434.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.96 ms/it, loss 0.484642
Finished training it 32768/76743 of epoch 1, 89.13 ms/it, loss 0.487365
Finished training it 32768/76743 of epoch 1, 88.21 ms/it, loss 0.484045
Finished training it 32768/76743 of epoch 1, 88.66 ms/it, loss 0.484762
Finished training it 32768/76743 of epoch 1, 88.93 ms/it, loss 0.482243
Finished training it 33792/76743 of epoch 1, 87.80 ms/it, loss 0.482858
Finished training it 33792/76743 of epoch 1, 87.68 ms/it, loss 0.482825
Finished training it 33792/76743 of epoch 1, 87.96 ms/it, loss 0.484377
Finished training it 33792/76743 of epoch 1, 87.83 ms/it, loss 0.480118
Finished training it 34816/76743 of epoch 1, 98.60 ms/it, loss 0.482637
Finished training it 34816/76743 of epoch 1, 98.47 ms/it, loss 0.483563
Finished training it 34816/76743 of epoch 1, 98.13 ms/it, loss 0.481547
Finished training it 34816/76743 of epoch 1, 98.01 ms/it, loss 0.484524
Finished training it 35840/76743 of epoch 1, 87.64 ms/it, loss 0.481940
Finished training it 35840/76743 of epoch 1, 88.03 ms/it, loss 0.481962
Finished training it 35840/76743 of epoch 1, 88.07 ms/it, loss 0.481203
Finished training it 35840/76743 of epoch 1, 88.21 ms/it, loss 0.481007
Finished training it 36864/76743 of epoch 1, 87.45 ms/it, loss 0.480275
Finished training it 36864/76743 of epoch 1, 87.87 ms/it, loss 0.482424
Finished training it 36864/76743 of epoch 1, 88.38 ms/it, loss 0.486623
Finished training it 36864/76743 of epoch 1, 88.23 ms/it, loss 0.480883
Finished training it 37888/76743 of epoch 1, 88.09 ms/it, loss 0.483051
Finished training it 37888/76743 of epoch 1, 88.64 ms/it, loss 0.479779
Finished training it 37888/76743 of epoch 1, 88.20 ms/it, loss 0.480577
Finished training it 37888/76743 of epoch 1, 87.96 ms/it, loss 0.480079
Finished training it 38912/76743 of epoch 1, 87.69 ms/it, loss 0.479566
Finished training it 38912/76743 of epoch 1, 88.04 ms/it, loss 0.479919
Finished training it 38912/76743 of epoch 1, 87.65 ms/it, loss 0.481910
Finished training it 38912/76743 of epoch 1, 87.72 ms/it, loss 0.478146
Finished training it 39936/76743 of epoch 1, 87.93 ms/it, loss 0.478717
Finished training it 39936/76743 of epoch 1, 87.93 ms/it, loss 0.481487
Finished training it 39936/76743 of epoch 1, 87.53 ms/it, loss 0.479583
Finished training it 39936/76743 of epoch 1, 87.90 ms/it, loss 0.481891
Finished training it 40960/76743 of epoch 1, 88.06 ms/it, loss 0.479310
Finished training it 40960/76743 of epoch 1, 87.51 ms/it, loss 0.482568
Finished training it 40960/76743 of epoch 1, 87.92 ms/it, loss 0.480382
Finished training it 40960/76743 of epoch 1, 87.65 ms/it, loss 0.481380
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2522346.0
get out
0 has test check 2522346.0 and sample count 3274240
 accuracy 77.036 %, best 78.704 %, roc auc score 0.7585, best 0.7993
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2522346.0
get out
1 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.51 ms/it, loss 0.478080
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2522346.0
get out
3 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.25 ms/it, loss 0.480475
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2522346.0
get out
2 has test check 2522346.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 89.11 ms/it, loss 0.485047
Finished training it 41984/76743 of epoch 1, 89.07 ms/it, loss 0.482181
Finished training it 43008/76743 of epoch 1, 88.30 ms/it, loss 0.481530
Finished training it 43008/76743 of epoch 1, 88.66 ms/it, loss 0.482755
Finished training it 43008/76743 of epoch 1, 88.36 ms/it, loss 0.481712
Finished training it 43008/76743 of epoch 1, 88.64 ms/it, loss 0.484103
Finished training it 44032/76743 of epoch 1, 87.35 ms/it, loss 0.480952
Finished training it 44032/76743 of epoch 1, 87.70 ms/it, loss 0.479428
Finished training it 44032/76743 of epoch 1, 87.80 ms/it, loss 0.480779
Finished training it 44032/76743 of epoch 1, 87.48 ms/it, loss 0.479874
Finished training it 45056/76743 of epoch 1, 88.24 ms/it, loss 0.479948
Finished training it 45056/76743 of epoch 1, 87.94 ms/it, loss 0.480746
Finished training it 45056/76743 of epoch 1, 88.02 ms/it, loss 0.481502
Finished training it 45056/76743 of epoch 1, 88.08 ms/it, loss 0.479408
Finished training it 46080/76743 of epoch 1, 87.68 ms/it, loss 0.480805
Finished training it 46080/76743 of epoch 1, 87.79 ms/it, loss 0.481201
Finished training it 46080/76743 of epoch 1, 87.55 ms/it, loss 0.479085
Finished training it 46080/76743 of epoch 1, 87.31 ms/it, loss 0.480154
Finished training it 47104/76743 of epoch 1, 88.39 ms/it, loss 0.480461
Finished training it 47104/76743 of epoch 1, 87.76 ms/it, loss 0.479963
Finished training it 47104/76743 of epoch 1, 87.75 ms/it, loss 0.479599
Finished training it 47104/76743 of epoch 1, 88.10 ms/it, loss 0.479998
Finished training it 48128/76743 of epoch 1, 89.06 ms/it, loss 0.479839
Finished training it 48128/76743 of epoch 1, 89.03 ms/it, loss 0.478693
Finished training it 48128/76743 of epoch 1, 89.11 ms/it, loss 0.478783
Finished training it 48128/76743 of epoch 1, 88.99 ms/it, loss 0.482049
Finished training it 49152/76743 of epoch 1, 88.05 ms/it, loss 0.482810
Finished training it 49152/76743 of epoch 1, 87.85 ms/it, loss 0.482564
Finished training it 49152/76743 of epoch 1, 88.38 ms/it, loss 0.479114
Finished training it 49152/76743 of epoch 1, 87.60 ms/it, loss 0.476939
Finished training it 50176/76743 of epoch 1, 88.38 ms/it, loss 0.479073
Finished training it 50176/76743 of epoch 1, 89.20 ms/it, loss 0.480833
Finished training it 50176/76743 of epoch 1, 88.78 ms/it, loss 0.478890
Finished training it 50176/76743 of epoch 1, 88.75 ms/it, loss 0.480178
Finished training it 51200/76743 of epoch 1, 87.69 ms/it, loss 0.479020
Finished training it 51200/76743 of epoch 1, 87.52 ms/it, loss 0.479799
Finished training it 51200/76743 of epoch 1, 87.93 ms/it, loss 0.479597
Finished training it 51200/76743 of epoch 1, 87.60 ms/it, loss 0.479644
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2525202.0
get out
0 has test check 2525202.0 and sample count 3274240
 accuracy 77.123 %, best 78.704 %, roc auc score 0.7605, best 0.7993
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2525202.0
get out
1 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.36 ms/it, loss 0.478441
Finished training it 52224/76743 of epoch 1, 88.12 ms/it, loss 0.479141
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2525202.0
get out
3 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.43 ms/it, loss 0.479095
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2525202.0
get out
2 has test check 2525202.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 88.35 ms/it, loss 0.477308
Finished training it 53248/76743 of epoch 1, 88.00 ms/it, loss 0.477567
Finished training it 53248/76743 of epoch 1, 87.73 ms/it, loss 0.479475
Finished training it 53248/76743 of epoch 1, 87.94 ms/it, loss 0.477555
Finished training it 53248/76743 of epoch 1, 87.94 ms/it, loss 0.481361
Finished training it 54272/76743 of epoch 1, 88.00 ms/it, loss 0.476612
Finished training it 54272/76743 of epoch 1, 87.87 ms/it, loss 0.480235
Finished training it 54272/76743 of epoch 1, 87.69 ms/it, loss 0.476583
Finished training it 54272/76743 of epoch 1, 87.30 ms/it, loss 0.478982
Finished training it 55296/76743 of epoch 1, 93.45 ms/it, loss 0.478187
Finished training it 55296/76743 of epoch 1, 93.81 ms/it, loss 0.477381
Finished training it 55296/76743 of epoch 1, 92.90 ms/it, loss 0.476472
Finished training it 55296/76743 of epoch 1, 93.05 ms/it, loss 0.480263
Finished training it 56320/76743 of epoch 1, 93.17 ms/it, loss 0.478249
Finished training it 56320/76743 of epoch 1, 92.77 ms/it, loss 0.480550
Finished training it 56320/76743 of epoch 1, 93.01 ms/it, loss 0.479755
Finished training it 56320/76743 of epoch 1, 92.62 ms/it, loss 0.480162
Finished training it 57344/76743 of epoch 1, 87.45 ms/it, loss 0.477751
Finished training it 57344/76743 of epoch 1, 87.50 ms/it, loss 0.480612
Finished training it 57344/76743 of epoch 1, 88.18 ms/it, loss 0.478054
Finished training it 57344/76743 of epoch 1, 87.75 ms/it, loss 0.479600
Finished training it 58368/76743 of epoch 1, 88.78 ms/it, loss 0.479331
Finished training it 58368/76743 of epoch 1, 88.81 ms/it, loss 0.478659
Finished training it 58368/76743 of epoch 1, 88.26 ms/it, loss 0.481855
Finished training it 58368/76743 of epoch 1, 88.30 ms/it, loss 0.478205
Finished training it 59392/76743 of epoch 1, 88.44 ms/it, loss 0.478459
Finished training it 59392/76743 of epoch 1, 88.45 ms/it, loss 0.477618
Finished training it 59392/76743 of epoch 1, 87.97 ms/it, loss 0.478250
Finished training it 59392/76743 of epoch 1, 88.09 ms/it, loss 0.477477
Finished training it 60416/76743 of epoch 1, 88.49 ms/it, loss 0.476815
Finished training it 60416/76743 of epoch 1, 88.06 ms/it, loss 0.476961
Finished training it 60416/76743 of epoch 1, 88.25 ms/it, loss 0.478518
Finished training it 60416/76743 of epoch 1, 87.70 ms/it, loss 0.479036
Finished training it 61440/76743 of epoch 1, 88.26 ms/it, loss 0.476642
Finished training it 61440/76743 of epoch 1, 88.24 ms/it, loss 0.477849
Finished training it 61440/76743 of epoch 1, 88.43 ms/it, loss 0.479464
Finished training it 61440/76743 of epoch 1, 87.86 ms/it, loss 0.480364
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2526115.0
get out
0 has test check 2526115.0 and sample count 3274240
 accuracy 77.151 %, best 78.704 %, roc auc score 0.7617, best 0.7993
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2526115.0
get out
3 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 87.95 ms/it, loss 0.476667
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2526115.0
get out
1 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 88.36 ms/it, loss 0.479368
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2526115.0
get out
2 has test check 2526115.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 87.76 ms/it, loss 0.477677
Finished training it 62464/76743 of epoch 1, 87.95 ms/it, loss 0.479592
Finished training it 63488/76743 of epoch 1, 88.57 ms/it, loss 0.479896
Finished training it 63488/76743 of epoch 1, 88.28 ms/it, loss 0.474226
Finished training it 63488/76743 of epoch 1, 88.70 ms/it, loss 0.481390
Finished training it 63488/76743 of epoch 1, 88.04 ms/it, loss 0.480865
Finished training it 64512/76743 of epoch 1, 87.75 ms/it, loss 0.480227
Finished training it 64512/76743 of epoch 1, 87.45 ms/it, loss 0.477628
Finished training it 64512/76743 of epoch 1, 87.56 ms/it, loss 0.477494
Finished training it 64512/76743 of epoch 1, 87.42 ms/it, loss 0.478573
Finished training it 65536/76743 of epoch 1, 87.70 ms/it, loss 0.478486
Finished training it 65536/76743 of epoch 1, 87.98 ms/it, loss 0.474954
Finished training it 65536/76743 of epoch 1, 87.39 ms/it, loss 0.477289
Finished training it 65536/76743 of epoch 1, 87.47 ms/it, loss 0.476502
Finished training it 66560/76743 of epoch 1, 87.74 ms/it, loss 0.477557
Finished training it 66560/76743 of epoch 1, 88.12 ms/it, loss 0.477075
Finished training it 66560/76743 of epoch 1, 88.16 ms/it, loss 0.477397
Finished training it 66560/76743 of epoch 1, 88.22 ms/it, loss 0.479923
Finished training it 67584/76743 of epoch 1, 88.34 ms/it, loss 0.477107
Finished training it 67584/76743 of epoch 1, 88.18 ms/it, loss 0.474307
Finished training it 67584/76743 of epoch 1, 88.19 ms/it, loss 0.476936
Finished training it 67584/76743 of epoch 1, 88.42 ms/it, loss 0.476977
Finished training it 68608/76743 of epoch 1, 88.49 ms/it, loss 0.476342
Finished training it 68608/76743 of epoch 1, 88.62 ms/it, loss 0.478706
Finished training it 68608/76743 of epoch 1, 88.48 ms/it, loss 0.476777
Finished training it 68608/76743 of epoch 1, 88.39 ms/it, loss 0.474963
Finished training it 69632/76743 of epoch 1, 88.94 ms/it, loss 0.476321
Finished training it 69632/76743 of epoch 1, 88.32 ms/it, loss 0.474262
Finished training it 69632/76743 of epoch 1, 88.85 ms/it, loss 0.475717
Finished training it 69632/76743 of epoch 1, 88.33 ms/it, loss 0.477470
Finished training it 70656/76743 of epoch 1, 89.73 ms/it, loss 0.476265
Finished training it 70656/76743 of epoch 1, 89.76 ms/it, loss 0.476834
Finished training it 70656/76743 of epoch 1, 89.51 ms/it, loss 0.478328
Finished training it 70656/76743 of epoch 1, 89.41 ms/it, loss 0.478120
Finished training it 71680/76743 of epoch 1, 89.24 ms/it, loss 0.476943
Finished training it 71680/76743 of epoch 1, 89.36 ms/it, loss 0.475283
Finished training it 71680/76743 of epoch 1, 89.08 ms/it, loss 0.477260
Finished training it 71680/76743 of epoch 1, 88.97 ms/it, loss 0.478507
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2529471.0
get out
0 has test check 2529471.0 and sample count 3274240
 accuracy 77.254 %, best 78.704 %, roc auc score 0.7638, best 0.7993
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2529471.0
get out
1 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.09 ms/it, loss 0.477651
Finished training it 72704/76743 of epoch 1, 87.76 ms/it, loss 0.476826
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2529471.0
get out
2 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 87.77 ms/it, loss 0.475263
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2529471.0
get out
3 has test check 2529471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 88.07 ms/it, loss 0.475771
Finished training it 73728/76743 of epoch 1, 87.96 ms/it, loss 0.476415
Finished training it 73728/76743 of epoch 1, 87.79 ms/it, loss 0.476026
Finished training it 73728/76743 of epoch 1, 87.84 ms/it, loss 0.475823
Finished training it 73728/76743 of epoch 1, 87.55 ms/it, loss 0.476500
Finished training it 74752/76743 of epoch 1, 88.35 ms/it, loss 0.477357
Finished training it 74752/76743 of epoch 1, 88.36 ms/it, loss 0.475124
Finished training it 74752/76743 of epoch 1, 88.33 ms/it, loss 0.478557
Finished training it 74752/76743 of epoch 1, 88.01 ms/it, loss 0.476648
Finished training it 75776/76743 of epoch 1, 93.54 ms/it, loss 0.477980
Finished training it 75776/76743 of epoch 1, 93.41 ms/it, loss 0.476613
Finished training it 75776/76743 of epoch 1, 94.08 ms/it, loss 0.474237
Finished training it 75776/76743 of epoch 1, 94.17 ms/it, loss 0.474562
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.11 ms/it, loss 0.475236
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.72 ms/it, loss 0.476394
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 91.35 ms/it, loss 0.477190
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 92.09 ms/it, loss 0.475193
Finished training it 2048/76743 of epoch 2, 88.09 ms/it, loss 0.477360
Finished training it 2048/76743 of epoch 2, 88.06 ms/it, loss 0.475392
Finished training it 2048/76743 of epoch 2, 87.99 ms/it, loss 0.477521
Finished training it 2048/76743 of epoch 2, 87.91 ms/it, loss 0.477324
Finished training it 3072/76743 of epoch 2, 87.78 ms/it, loss 0.475776
Finished training it 3072/76743 of epoch 2, 87.72 ms/it, loss 0.478640
Finished training it 3072/76743 of epoch 2, 87.93 ms/it, loss 0.472468
Finished training it 3072/76743 of epoch 2, 87.98 ms/it, loss 0.474872
Finished training it 4096/76743 of epoch 2, 87.76 ms/it, loss 0.475188
Finished training it 4096/76743 of epoch 2, 87.98 ms/it, loss 0.474764
Finished training it 4096/76743 of epoch 2, 87.62 ms/it, loss 0.477921
Finished training it 4096/76743 of epoch 2, 87.95 ms/it, loss 0.477637
Finished training it 5120/76743 of epoch 2, 87.30 ms/it, loss 0.474859
Finished training it 5120/76743 of epoch 2, 87.58 ms/it, loss 0.477217
Finished training it 5120/76743 of epoch 2, 87.56 ms/it, loss 0.478462
Finished training it 5120/76743 of epoch 2, 87.45 ms/it, loss 0.472761
Finished training it 6144/76743 of epoch 2, 88.02 ms/it, loss 0.476638
Finished training it 6144/76743 of epoch 2, 88.27 ms/it, loss 0.475160
Finished training it 6144/76743 of epoch 2, 88.31 ms/it, loss 0.475240
Finished training it 6144/76743 of epoch 2, 88.23 ms/it, loss 0.476473
Finished training it 7168/76743 of epoch 2, 87.72 ms/it, loss 0.476499
Finished training it 7168/76743 of epoch 2, 87.64 ms/it, loss 0.473584
Finished training it 7168/76743 of epoch 2, 87.39 ms/it, loss 0.475574
Finished training it 7168/76743 of epoch 2, 87.67 ms/it, loss 0.476791
Finished training it 8192/76743 of epoch 2, 88.53 ms/it, loss 0.472882
Finished training it 8192/76743 of epoch 2, 88.28 ms/it, loss 0.474742
Finished training it 8192/76743 of epoch 2, 88.13 ms/it, loss 0.477009
Finished training it 8192/76743 of epoch 2, 88.44 ms/it, loss 0.474598
Finished training it 9216/76743 of epoch 2, 88.02 ms/it, loss 0.474618
Finished training it 9216/76743 of epoch 2, 87.96 ms/it, loss 0.476648
Finished training it 9216/76743 of epoch 2, 87.61 ms/it, loss 0.475849
Finished training it 9216/76743 of epoch 2, 87.72 ms/it, loss 0.474602
Finished training it 10240/76743 of epoch 2, 88.26 ms/it, loss 0.478308
Finished training it 10240/76743 of epoch 2, 88.27 ms/it, loss 0.475900
Finished training it 10240/76743 of epoch 2, 88.60 ms/it, loss 0.475591
Finished training it 10240/76743 of epoch 2, 88.45 ms/it, loss 0.474836
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2530982.0
get out
0 has test check 2530982.0 and sample count 3274240
 accuracy 77.300 %, best 78.704 %, roc auc score 0.7655, best 0.7993
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2530982.0
get out
1 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 88.06 ms/it, loss 0.475026
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2530982.0
get out
2 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.81 ms/it, loss 0.474754
Finished training it 11264/76743 of epoch 2, 87.90 ms/it, loss 0.477843
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2530982.0
get out
3 has test check 2530982.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 87.89 ms/it, loss 0.472637
Finished training it 12288/76743 of epoch 2, 87.85 ms/it, loss 0.477253
Finished training it 12288/76743 of epoch 2, 87.59 ms/it, loss 0.475798
Finished training it 12288/76743 of epoch 2, 87.58 ms/it, loss 0.471276
Finished training it 12288/76743 of epoch 2, 87.37 ms/it, loss 0.477149
Finished training it 13312/76743 of epoch 2, 88.10 ms/it, loss 0.474495
Finished training it 13312/76743 of epoch 2, 87.96 ms/it, loss 0.476647
Finished training it 13312/76743 of epoch 2, 88.13 ms/it, loss 0.474769
Finished training it 13312/76743 of epoch 2, 88.12 ms/it, loss 0.476262
Finished training it 14336/76743 of epoch 2, 87.89 ms/it, loss 0.474144
Finished training it 14336/76743 of epoch 2, 87.96 ms/it, loss 0.473611
Finished training it 14336/76743 of epoch 2, 88.10 ms/it, loss 0.474695
Finished training it 14336/76743 of epoch 2, 87.88 ms/it, loss 0.476864
Finished training it 15360/76743 of epoch 2, 88.49 ms/it, loss 0.473047
Finished training it 15360/76743 of epoch 2, 88.25 ms/it, loss 0.473926
Finished training it 15360/76743 of epoch 2, 88.57 ms/it, loss 0.475973
Finished training it 15360/76743 of epoch 2, 88.53 ms/it, loss 0.475011
Finished training it 16384/76743 of epoch 2, 88.61 ms/it, loss 0.475323
Finished training it 16384/76743 of epoch 2, 88.27 ms/it, loss 0.472684
Finished training it 16384/76743 of epoch 2, 88.14 ms/it, loss 0.476597
Finished training it 16384/76743 of epoch 2, 88.34 ms/it, loss 0.476801
Finished training it 17408/76743 of epoch 2, 88.08 ms/it, loss 0.472875
Finished training it 17408/76743 of epoch 2, 87.82 ms/it, loss 0.472435
Finished training it 17408/76743 of epoch 2, 87.96 ms/it, loss 0.474078
Finished training it 17408/76743 of epoch 2, 88.00 ms/it, loss 0.475640
Finished training it 18432/76743 of epoch 2, 88.37 ms/it, loss 0.475658
Finished training it 18432/76743 of epoch 2, 88.21 ms/it, loss 0.476605
Finished training it 18432/76743 of epoch 2, 87.99 ms/it, loss 0.474099
Finished training it 18432/76743 of epoch 2, 88.16 ms/it, loss 0.471871
Finished training it 19456/76743 of epoch 2, 87.92 ms/it, loss 0.472421
Finished training it 19456/76743 of epoch 2, 88.13 ms/it, loss 0.473286
Finished training it 19456/76743 of epoch 2, 88.12 ms/it, loss 0.475046
Finished training it 19456/76743 of epoch 2, 87.93 ms/it, loss 0.475285
Finished training it 20480/76743 of epoch 2, 88.00 ms/it, loss 0.475274
Finished training it 20480/76743 of epoch 2, 87.71 ms/it, loss 0.474770
Finished training it 20480/76743 of epoch 2, 88.03 ms/it, loss 0.474858
Finished training it 20480/76743 of epoch 2, 87.78 ms/it, loss 0.473372
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533419.0
get out
0 has test check 2533419.0 and sample count 3274240
 accuracy 77.374 %, best 78.704 %, roc auc score 0.7673, best 0.7993
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533419.0
get out
1 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.15 ms/it, loss 0.473876
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533419.0
get out
3 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 88.11 ms/it, loss 0.473643
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533419.0
get out
2 has test check 2533419.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 87.75 ms/it, loss 0.476905
Finished training it 21504/76743 of epoch 2, 87.81 ms/it, loss 0.474365
