Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
Finished training it 1024/76743 of epoch 0, 60.50 ms/it, loss 0.515499
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
Finished training it 1024/76743 of epoch 0, 60.48 ms/it, loss 0.515225
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
Finished training it 1024/76743 of epoch 0, 62.35 ms/it, loss 0.514771
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
bound increasing to 500
Finished training it 1024/76743 of epoch 0, 62.14 ms/it, loss 0.515448
Finished training it 2048/76743 of epoch 0, 58.89 ms/it, loss 0.499841
Finished training it 2048/76743 of epoch 0, 58.73 ms/it, loss 0.498798
Finished training it 2048/76743 of epoch 0, 58.80 ms/it, loss 0.497948
Finished training it 2048/76743 of epoch 0, 59.12 ms/it, loss 0.498498
Finished training it 3072/76743 of epoch 0, 59.09 ms/it, loss 0.490772
Finished training it 3072/76743 of epoch 0, 58.70 ms/it, loss 0.486504
Finished training it 3072/76743 of epoch 0, 59.11 ms/it, loss 0.490880
Finished training it 3072/76743 of epoch 0, 58.83 ms/it, loss 0.492076
Finished training it 4096/76743 of epoch 0, 58.64 ms/it, loss 0.483664
Finished training it 4096/76743 of epoch 0, 59.01 ms/it, loss 0.481732
Finished training it 4096/76743 of epoch 0, 58.88 ms/it, loss 0.483916
Finished training it 4096/76743 of epoch 0, 58.55 ms/it, loss 0.482973
Finished training it 5120/76743 of epoch 0, 58.66 ms/it, loss 0.477219
Finished training it 5120/76743 of epoch 0, 58.83 ms/it, loss 0.477124
Finished training it 5120/76743 of epoch 0, 58.60 ms/it, loss 0.477771
Finished training it 5120/76743 of epoch 0, 58.85 ms/it, loss 0.478657
Finished training it 6144/76743 of epoch 0, 58.30 ms/it, loss 0.476426
Finished training it 6144/76743 of epoch 0, 58.33 ms/it, loss 0.477076
Finished training it 6144/76743 of epoch 0, 57.91 ms/it, loss 0.474231
Finished training it 6144/76743 of epoch 0, 57.76 ms/it, loss 0.477790
Finished training it 7168/76743 of epoch 0, 59.35 ms/it, loss 0.470127
Finished training it 7168/76743 of epoch 0, 59.03 ms/it, loss 0.470860
Finished training it 7168/76743 of epoch 0, 58.82 ms/it, loss 0.472120
Finished training it 7168/76743 of epoch 0, 59.38 ms/it, loss 0.471306
Finished training it 8192/76743 of epoch 0, 58.50 ms/it, loss 0.471635
Finished training it 8192/76743 of epoch 0, 58.56 ms/it, loss 0.471087
Finished training it 8192/76743 of epoch 0, 58.20 ms/it, loss 0.470078
Finished training it 8192/76743 of epoch 0, 58.06 ms/it, loss 0.472846
Finished training it 9216/76743 of epoch 0, 58.37 ms/it, loss 0.470269
Finished training it 9216/76743 of epoch 0, 57.96 ms/it, loss 0.470494
Finished training it 9216/76743 of epoch 0, 57.95 ms/it, loss 0.469084
Finished training it 9216/76743 of epoch 0, 58.25 ms/it, loss 0.468896
Finished training it 10240/76743 of epoch 0, 57.72 ms/it, loss 0.468572
Finished training it 10240/76743 of epoch 0, 58.18 ms/it, loss 0.468459
Finished training it 10240/76743 of epoch 0, 57.95 ms/it, loss 0.469791
Finished training it 10240/76743 of epoch 0, 57.66 ms/it, loss 0.466887
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547776.0
get out
0 has test check 2547776.0 and sample count 3274240
 accuracy 77.813 %, best 77.813 %, roc auc score 0.7795, best 0.7795
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547776.0
get out
3 has test check 2547776.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 58.69 ms/it, loss 0.468837
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547776.0
get out
1 has test check 2547776.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 58.86 ms/it, loss 0.467858
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547776.0
get out
2 has test check 2547776.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 58.66 ms/it, loss 0.466744
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 58.68 ms/it, loss 0.468651
Finished training it 12288/76743 of epoch 0, 58.58 ms/it, loss 0.464786
Finished training it 12288/76743 of epoch 0, 58.71 ms/it, loss 0.467190
Finished training it 12288/76743 of epoch 0, 58.44 ms/it, loss 0.465419
Finished training it 12288/76743 of epoch 0, 58.82 ms/it, loss 0.464210
Finished training it 13312/76743 of epoch 0, 64.56 ms/it, loss 0.464657
Finished training it 13312/76743 of epoch 0, 64.19 ms/it, loss 0.463884
Finished training it 13312/76743 of epoch 0, 65.24 ms/it, loss 0.464900
Finished training it 13312/76743 of epoch 0, 64.24 ms/it, loss 0.464758
Finished training it 14336/76743 of epoch 0, 59.20 ms/it, loss 0.463671
Finished training it 14336/76743 of epoch 0, 59.63 ms/it, loss 0.464372
Finished training it 14336/76743 of epoch 0, 59.25 ms/it, loss 0.462284
Finished training it 14336/76743 of epoch 0, 58.99 ms/it, loss 0.463821
Finished training it 15360/76743 of epoch 0, 59.26 ms/it, loss 0.465807
Finished training it 15360/76743 of epoch 0, 59.19 ms/it, loss 0.463939
Finished training it 15360/76743 of epoch 0, 58.92 ms/it, loss 0.465661
Finished training it 15360/76743 of epoch 0, 58.94 ms/it, loss 0.464348
Finished training it 16384/76743 of epoch 0, 58.86 ms/it, loss 0.464138
Finished training it 16384/76743 of epoch 0, 59.16 ms/it, loss 0.461896
Finished training it 16384/76743 of epoch 0, 58.84 ms/it, loss 0.463765
Finished training it 16384/76743 of epoch 0, 58.85 ms/it, loss 0.462754
Finished training it 17408/76743 of epoch 0, 59.14 ms/it, loss 0.461299
Finished training it 17408/76743 of epoch 0, 58.98 ms/it, loss 0.465039
Finished training it 17408/76743 of epoch 0, 59.28 ms/it, loss 0.461715
Finished training it 17408/76743 of epoch 0, 59.26 ms/it, loss 0.459978
Finished training it 18432/76743 of epoch 0, 59.18 ms/it, loss 0.460275
Finished training it 18432/76743 of epoch 0, 59.45 ms/it, loss 0.464694
Finished training it 18432/76743 of epoch 0, 59.25 ms/it, loss 0.462155
Finished training it 18432/76743 of epoch 0, 59.44 ms/it, loss 0.464549
Finished training it 19456/76743 of epoch 0, 59.07 ms/it, loss 0.460501
Finished training it 19456/76743 of epoch 0, 59.05 ms/it, loss 0.464066
Finished training it 19456/76743 of epoch 0, 59.20 ms/it, loss 0.464640
Finished training it 19456/76743 of epoch 0, 58.90 ms/it, loss 0.463481
Finished training it 20480/76743 of epoch 0, 59.03 ms/it, loss 0.457863
Finished training it 20480/76743 of epoch 0, 59.27 ms/it, loss 0.461350
Finished training it 20480/76743 of epoch 0, 59.02 ms/it, loss 0.461284
Finished training it 20480/76743 of epoch 0, 59.43 ms/it, loss 0.462682
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2558460.0
get out
0 has test check 2558460.0 and sample count 3274240
 accuracy 78.139 %, best 78.139 %, roc auc score 0.7867, best 0.7867
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2558460.0
get out
3 has test check 2558460.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 59.46 ms/it, loss 0.456949
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2558460.0
get out
1 has test check 2558460.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 59.53 ms/it, loss 0.464233
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 59.19 ms/it, loss 0.462017
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2558460.0
get out
2 has test check 2558460.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 59.37 ms/it, loss 0.461702
Finished training it 22528/76743 of epoch 0, 59.95 ms/it, loss 0.461242
Finished training it 22528/76743 of epoch 0, 60.05 ms/it, loss 0.460529
Finished training it 22528/76743 of epoch 0, 59.87 ms/it, loss 0.460673
Finished training it 22528/76743 of epoch 0, 59.78 ms/it, loss 0.459975
Finished training it 23552/76743 of epoch 0, 59.28 ms/it, loss 0.462934
Finished training it 23552/76743 of epoch 0, 59.22 ms/it, loss 0.458577
Finished training it 23552/76743 of epoch 0, 59.27 ms/it, loss 0.463076
Finished training it 23552/76743 of epoch 0, 59.29 ms/it, loss 0.459884
Finished training it 24576/76743 of epoch 0, 59.17 ms/it, loss 0.459944
Finished training it 24576/76743 of epoch 0, 59.36 ms/it, loss 0.460749
Finished training it 24576/76743 of epoch 0, 59.15 ms/it, loss 0.459930
Finished training it 24576/76743 of epoch 0, 59.18 ms/it, loss 0.459016
Finished training it 25600/76743 of epoch 0, 59.67 ms/it, loss 0.458151
Finished training it 25600/76743 of epoch 0, 59.62 ms/it, loss 0.458672
Finished training it 25600/76743 of epoch 0, 59.62 ms/it, loss 0.459919
Finished training it 25600/76743 of epoch 0, 59.53 ms/it, loss 0.457437
Finished training it 26624/76743 of epoch 0, 59.34 ms/it, loss 0.458418
Finished training it 26624/76743 of epoch 0, 59.35 ms/it, loss 0.459550
Finished training it 26624/76743 of epoch 0, 59.34 ms/it, loss 0.459961
Finished training it 26624/76743 of epoch 0, 59.30 ms/it, loss 0.459789
Finished training it 27648/76743 of epoch 0, 59.89 ms/it, loss 0.459224
Finished training it 27648/76743 of epoch 0, 59.65 ms/it, loss 0.459183
Finished training it 27648/76743 of epoch 0, 59.65 ms/it, loss 0.460074
Finished training it 27648/76743 of epoch 0, 59.64 ms/it, loss 0.457836
Finished training it 28672/76743 of epoch 0, 59.95 ms/it, loss 0.461994
Finished training it 28672/76743 of epoch 0, 59.86 ms/it, loss 0.455054
Finished training it 28672/76743 of epoch 0, 59.69 ms/it, loss 0.457938
Finished training it 28672/76743 of epoch 0, 59.61 ms/it, loss 0.457460
Finished training it 29696/76743 of epoch 0, 133.28 ms/it, loss 0.455970
Finished training it 29696/76743 of epoch 0, 131.69 ms/it, loss 0.459818
Finished training it 29696/76743 of epoch 0, 135.57 ms/it, loss 0.459424
Finished training it 29696/76743 of epoch 0, 136.54 ms/it, loss 0.459598
Finished training it 30720/76743 of epoch 0, 164.85 ms/it, loss 0.457855
Finished training it 30720/76743 of epoch 0, 165.33 ms/it, loss 0.458793
Finished training it 30720/76743 of epoch 0, 160.83 ms/it, loss 0.457984
Finished training it 30720/76743 of epoch 0, 160.06 ms/it, loss 0.461407
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562150.0
get out
0 has test check 2562150.0 and sample count 3274240
 accuracy 78.252 %, best 78.252 %, roc auc score 0.7891, best 0.7891
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562150.0
get out
2 has test check 2562150.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 59.42 ms/it, loss 0.459356
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 59.40 ms/it, loss 0.457183
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562150.0
get out
3 has test check 2562150.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 59.48 ms/it, loss 0.459195
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562150.0
get out
1 has test check 2562150.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 59.35 ms/it, loss 0.460701
Finished training it 32768/76743 of epoch 0, 59.74 ms/it, loss 0.456465
Finished training it 32768/76743 of epoch 0, 59.68 ms/it, loss 0.458306
Finished training it 32768/76743 of epoch 0, 59.60 ms/it, loss 0.458641
Finished training it 32768/76743 of epoch 0, 59.44 ms/it, loss 0.459034
Finished training it 33792/76743 of epoch 0, 64.41 ms/it, loss 0.457250
Finished training it 33792/76743 of epoch 0, 65.13 ms/it, loss 0.461474
Finished training it 33792/76743 of epoch 0, 65.58 ms/it, loss 0.459517
Finished training it 33792/76743 of epoch 0, 65.29 ms/it, loss 0.457190
Finished training it 34816/76743 of epoch 0, 60.02 ms/it, loss 0.455303
Finished training it 34816/76743 of epoch 0, 59.95 ms/it, loss 0.454205
Finished training it 34816/76743 of epoch 0, 59.86 ms/it, loss 0.458270
Finished training it 34816/76743 of epoch 0, 59.82 ms/it, loss 0.457791
Finished training it 35840/76743 of epoch 0, 59.36 ms/it, loss 0.455215
Finished training it 35840/76743 of epoch 0, 59.61 ms/it, loss 0.456458
Finished training it 35840/76743 of epoch 0, 59.58 ms/it, loss 0.459701
Finished training it 35840/76743 of epoch 0, 59.57 ms/it, loss 0.457903
Finished training it 36864/76743 of epoch 0, 59.92 ms/it, loss 0.457752
Finished training it 36864/76743 of epoch 0, 59.82 ms/it, loss 0.459739
Finished training it 36864/76743 of epoch 0, 59.87 ms/it, loss 0.457036
Finished training it 36864/76743 of epoch 0, 59.78 ms/it, loss 0.457820
Finished training it 37888/76743 of epoch 0, 59.99 ms/it, loss 0.457323
Finished training it 37888/76743 of epoch 0, 59.88 ms/it, loss 0.457151
Finished training it 37888/76743 of epoch 0, 59.96 ms/it, loss 0.456344
Finished training it 37888/76743 of epoch 0, 59.90 ms/it, loss 0.458417
Finished training it 38912/76743 of epoch 0, 59.18 ms/it, loss 0.456444
Finished training it 38912/76743 of epoch 0, 59.35 ms/it, loss 0.455990
Finished training it 38912/76743 of epoch 0, 59.31 ms/it, loss 0.454637
Finished training it 38912/76743 of epoch 0, 59.32 ms/it, loss 0.458639
Finished training it 39936/76743 of epoch 0, 59.61 ms/it, loss 0.455779
Finished training it 39936/76743 of epoch 0, 59.56 ms/it, loss 0.457591
Finished training it 39936/76743 of epoch 0, 59.58 ms/it, loss 0.457296
Finished training it 39936/76743 of epoch 0, 59.50 ms/it, loss 0.455621
Finished training it 40960/76743 of epoch 0, 59.62 ms/it, loss 0.457859
Finished training it 40960/76743 of epoch 0, 59.66 ms/it, loss 0.456529
Finished training it 40960/76743 of epoch 0, 59.77 ms/it, loss 0.455812
Finished training it 40960/76743 of epoch 0, 59.79 ms/it, loss 0.457791
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2565222.0
get out
0 has test check 2565222.0 and sample count 3274240
 accuracy 78.346 %, best 78.346 %, roc auc score 0.7910, best 0.7910
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 59.56 ms/it, loss 0.455430
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2565222.0
get out
1 has test check 2565222.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 59.65 ms/it, loss 0.456351
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2565222.0
get out
2 has test check 2565222.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 59.44 ms/it, loss 0.456611
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2565222.0
get out
3 has test check 2565222.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 59.62 ms/it, loss 0.457943
Finished training it 43008/76743 of epoch 0, 59.64 ms/it, loss 0.457067
Finished training it 43008/76743 of epoch 0, 59.32 ms/it, loss 0.455247
Finished training it 43008/76743 of epoch 0, 59.52 ms/it, loss 0.455903
Finished training it 43008/76743 of epoch 0, 59.42 ms/it, loss 0.458868
Finished training it 44032/76743 of epoch 0, 59.34 ms/it, loss 0.456055
Finished training it 44032/76743 of epoch 0, 59.43 ms/it, loss 0.456305
Finished training it 44032/76743 of epoch 0, 59.34 ms/it, loss 0.457849
Finished training it 44032/76743 of epoch 0, 59.43 ms/it, loss 0.456380
Finished training it 45056/76743 of epoch 0, 59.68 ms/it, loss 0.454301
Finished training it 45056/76743 of epoch 0, 59.34 ms/it, loss 0.455523
Finished training it 45056/76743 of epoch 0, 59.49 ms/it, loss 0.456320
Finished training it 45056/76743 of epoch 0, 59.79 ms/it, loss 0.455221
Finished training it 46080/76743 of epoch 0, 59.41 ms/it, loss 0.456884
Finished training it 46080/76743 of epoch 0, 59.46 ms/it, loss 0.455689
Finished training it 46080/76743 of epoch 0, 59.41 ms/it, loss 0.454905
Finished training it 46080/76743 of epoch 0, 59.49 ms/it, loss 0.455624
Finished training it 47104/76743 of epoch 0, 59.46 ms/it, loss 0.456568
Finished training it 47104/76743 of epoch 0, 59.21 ms/it, loss 0.456368
Finished training it 47104/76743 of epoch 0, 59.28 ms/it, loss 0.454409
Finished training it 47104/76743 of epoch 0, 59.23 ms/it, loss 0.452534
Finished training it 48128/76743 of epoch 0, 59.50 ms/it, loss 0.456276
Finished training it 48128/76743 of epoch 0, 59.48 ms/it, loss 0.453742
Finished training it 48128/76743 of epoch 0, 59.47 ms/it, loss 0.454573
Finished training it 48128/76743 of epoch 0, 59.49 ms/it, loss 0.455085
Finished training it 49152/76743 of epoch 0, 59.19 ms/it, loss 0.459768
Finished training it 49152/76743 of epoch 0, 59.13 ms/it, loss 0.456075
Finished training it 49152/76743 of epoch 0, 59.23 ms/it, loss 0.454441
Finished training it 49152/76743 of epoch 0, 59.24 ms/it, loss 0.454749
Finished training it 50176/76743 of epoch 0, 59.47 ms/it, loss 0.453951
Finished training it 50176/76743 of epoch 0, 59.49 ms/it, loss 0.453328
Finished training it 50176/76743 of epoch 0, 59.55 ms/it, loss 0.455041
Finished training it 50176/76743 of epoch 0, 59.63 ms/it, loss 0.453651
Finished training it 51200/76743 of epoch 0, 59.64 ms/it, loss 0.455841
Finished training it 51200/76743 of epoch 0, 59.73 ms/it, loss 0.457749
Finished training it 51200/76743 of epoch 0, 59.63 ms/it, loss 0.457478
Finished training it 51200/76743 of epoch 0, 59.81 ms/it, loss 0.451587
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568803.0
get out
0 has test check 2568803.0 and sample count 3274240
 accuracy 78.455 %, best 78.455 %, roc auc score 0.7934, best 0.7934
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568803.0
get out
1 has test check 2568803.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 59.10 ms/it, loss 0.453720
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568803.0
get out
2 has test check 2568803.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 59.24 ms/it, loss 0.452347
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568803.0
get out
3 has test check 2568803.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 59.34 ms/it, loss 0.453766
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 59.14 ms/it, loss 0.455465
Finished training it 53248/76743 of epoch 0, 58.93 ms/it, loss 0.455355
Finished training it 53248/76743 of epoch 0, 58.91 ms/it, loss 0.454052
Finished training it 53248/76743 of epoch 0, 58.94 ms/it, loss 0.454007
Finished training it 53248/76743 of epoch 0, 58.97 ms/it, loss 0.456351
Finished training it 54272/76743 of epoch 0, 65.25 ms/it, loss 0.451939
Finished training it 54272/76743 of epoch 0, 65.01 ms/it, loss 0.453065
Finished training it 54272/76743 of epoch 0, 64.82 ms/it, loss 0.455941
Finished training it 54272/76743 of epoch 0, 64.82 ms/it, loss 0.454895
Finished training it 55296/76743 of epoch 0, 59.35 ms/it, loss 0.453372
Finished training it 55296/76743 of epoch 0, 59.39 ms/it, loss 0.453291
Finished training it 55296/76743 of epoch 0, 59.44 ms/it, loss 0.454163
Finished training it 55296/76743 of epoch 0, 59.40 ms/it, loss 0.452643
Finished training it 56320/76743 of epoch 0, 59.48 ms/it, loss 0.453083
Finished training it 56320/76743 of epoch 0, 59.50 ms/it, loss 0.455487
Finished training it 56320/76743 of epoch 0, 59.46 ms/it, loss 0.452428
Finished training it 56320/76743 of epoch 0, 59.36 ms/it, loss 0.454243
Finished training it 57344/76743 of epoch 0, 59.22 ms/it, loss 0.455713
Finished training it 57344/76743 of epoch 0, 59.18 ms/it, loss 0.455837
Finished training it 57344/76743 of epoch 0, 59.11 ms/it, loss 0.454218
Finished training it 57344/76743 of epoch 0, 59.05 ms/it, loss 0.456082
Finished training it 58368/76743 of epoch 0, 59.88 ms/it, loss 0.454311
Finished training it 58368/76743 of epoch 0, 59.93 ms/it, loss 0.456019
Finished training it 58368/76743 of epoch 0, 59.71 ms/it, loss 0.454054
Finished training it 58368/76743 of epoch 0, 59.78 ms/it, loss 0.456121
Finished training it 59392/76743 of epoch 0, 59.37 ms/it, loss 0.454665
Finished training it 59392/76743 of epoch 0, 59.25 ms/it, loss 0.454254
Finished training it 59392/76743 of epoch 0, 59.35 ms/it, loss 0.451523
Finished training it 59392/76743 of epoch 0, 59.21 ms/it, loss 0.454351
Finished training it 60416/76743 of epoch 0, 59.45 ms/it, loss 0.455237
Finished training it 60416/76743 of epoch 0, 59.25 ms/it, loss 0.455126
Finished training it 60416/76743 of epoch 0, 59.18 ms/it, loss 0.453744
Finished training it 60416/76743 of epoch 0, 59.36 ms/it, loss 0.454068
Finished training it 61440/76743 of epoch 0, 59.92 ms/it, loss 0.453013
Finished training it 61440/76743 of epoch 0, 59.93 ms/it, loss 0.453023
Finished training it 61440/76743 of epoch 0, 59.92 ms/it, loss 0.454203
Finished training it 61440/76743 of epoch 0, 60.21 ms/it, loss 0.454817
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568502.0
get out
0 has test check 2568502.0 and sample count 3274240
 accuracy 78.446 %, best 78.455 %, roc auc score 0.7945, best 0.7945
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568502.0
get out
2 has test check 2568502.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 59.89 ms/it, loss 0.451266
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568502.0
get out
3 has test check 2568502.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 59.93 ms/it, loss 0.455017
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 59.78 ms/it, loss 0.451864
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568502.0
get out
1 has test check 2568502.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 59.95 ms/it, loss 0.455193
Finished training it 63488/76743 of epoch 0, 59.51 ms/it, loss 0.455453
Finished training it 63488/76743 of epoch 0, 59.43 ms/it, loss 0.457129
Finished training it 63488/76743 of epoch 0, 59.30 ms/it, loss 0.451135
Finished training it 63488/76743 of epoch 0, 59.26 ms/it, loss 0.455318
Finished training it 64512/76743 of epoch 0, 59.27 ms/it, loss 0.453493
Finished training it 64512/76743 of epoch 0, 59.32 ms/it, loss 0.451714
Finished training it 64512/76743 of epoch 0, 59.45 ms/it, loss 0.452349
Finished training it 64512/76743 of epoch 0, 59.28 ms/it, loss 0.454208
Finished training it 65536/76743 of epoch 0, 59.08 ms/it, loss 0.449797
Finished training it 65536/76743 of epoch 0, 59.40 ms/it, loss 0.452123
Finished training it 65536/76743 of epoch 0, 58.99 ms/it, loss 0.454797
Finished training it 65536/76743 of epoch 0, 59.03 ms/it, loss 0.454911
Finished training it 66560/76743 of epoch 0, 59.37 ms/it, loss 0.453583
Finished training it 66560/76743 of epoch 0, 59.32 ms/it, loss 0.452169
Finished training it 66560/76743 of epoch 0, 59.30 ms/it, loss 0.452642
Finished training it 66560/76743 of epoch 0, 59.18 ms/it, loss 0.453490
Finished training it 67584/76743 of epoch 0, 59.03 ms/it, loss 0.453173
Finished training it 67584/76743 of epoch 0, 59.16 ms/it, loss 0.451970
Finished training it 67584/76743 of epoch 0, 59.36 ms/it, loss 0.455560
Finished training it 67584/76743 of epoch 0, 59.02 ms/it, loss 0.450511
Finished training it 68608/76743 of epoch 0, 59.10 ms/it, loss 0.451998
Finished training it 68608/76743 of epoch 0, 59.03 ms/it, loss 0.455197
Finished training it 68608/76743 of epoch 0, 58.96 ms/it, loss 0.452122
Finished training it 68608/76743 of epoch 0, 58.94 ms/it, loss 0.452422
Finished training it 69632/76743 of epoch 0, 64.59 ms/it, loss 0.449344
Finished training it 69632/76743 of epoch 0, 64.42 ms/it, loss 0.454879
Finished training it 69632/76743 of epoch 0, 64.48 ms/it, loss 0.454173
Finished training it 69632/76743 of epoch 0, 60.16 ms/it, loss 0.449779
Finished training it 70656/76743 of epoch 0, 59.19 ms/it, loss 0.450773
Finished training it 70656/76743 of epoch 0, 59.14 ms/it, loss 0.454591
Finished training it 70656/76743 of epoch 0, 59.24 ms/it, loss 0.454933
Finished training it 70656/76743 of epoch 0, 59.28 ms/it, loss 0.452679
Finished training it 71680/76743 of epoch 0, 59.31 ms/it, loss 0.453097
Finished training it 71680/76743 of epoch 0, 59.19 ms/it, loss 0.453936
Finished training it 71680/76743 of epoch 0, 59.28 ms/it, loss 0.454244
Finished training it 71680/76743 of epoch 0, 59.44 ms/it, loss 0.453459
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571828.0
get out
0 has test check 2571828.0 and sample count 3274240
 accuracy 78.547 %, best 78.547 %, roc auc score 0.7957, best 0.7957
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 59.31 ms/it, loss 0.453163
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571828.0
get out
1 has test check 2571828.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 59.46 ms/it, loss 0.453037
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571828.0
get out
2 has test check 2571828.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 59.43 ms/it, loss 0.452986
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571828.0
get out
3 has test check 2571828.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 59.53 ms/it, loss 0.453348
Finished training it 73728/76743 of epoch 0, 58.66 ms/it, loss 0.453824
Finished training it 73728/76743 of epoch 0, 58.64 ms/it, loss 0.452508
Finished training it 73728/76743 of epoch 0, 58.68 ms/it, loss 0.450594
Finished training it 73728/76743 of epoch 0, 58.65 ms/it, loss 0.452139
Finished training it 74752/76743 of epoch 0, 60.18 ms/it, loss 0.450713
Finished training it 74752/76743 of epoch 0, 64.93 ms/it, loss 0.450549
Finished training it 74752/76743 of epoch 0, 60.07 ms/it, loss 0.454348
Finished training it 74752/76743 of epoch 0, 60.45 ms/it, loss 0.450853
Finished training it 75776/76743 of epoch 0, 59.45 ms/it, loss 0.452578
Finished training it 75776/76743 of epoch 0, 59.41 ms/it, loss 0.452429
Finished training it 75776/76743 of epoch 0, 59.34 ms/it, loss 0.451885
Finished training it 75776/76743 of epoch 0, 59.41 ms/it, loss 0.453612
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 60.55 ms/it, loss 0.452391
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 60.18 ms/it, loss 0.452862
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 60.54 ms/it, loss 0.452951
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 60.04 ms/it, loss 0.450517
Finished training it 2048/76743 of epoch 1, 59.70 ms/it, loss 0.450661
Finished training it 2048/76743 of epoch 1, 59.54 ms/it, loss 0.452374
Finished training it 2048/76743 of epoch 1, 59.60 ms/it, loss 0.451214
Finished training it 2048/76743 of epoch 1, 59.69 ms/it, loss 0.450462
Finished training it 3072/76743 of epoch 1, 60.01 ms/it, loss 0.452621
Finished training it 3072/76743 of epoch 1, 60.03 ms/it, loss 0.448035
Finished training it 3072/76743 of epoch 1, 60.05 ms/it, loss 0.453211
Finished training it 3072/76743 of epoch 1, 60.00 ms/it, loss 0.452019
Finished training it 4096/76743 of epoch 1, 59.26 ms/it, loss 0.450434
Finished training it 4096/76743 of epoch 1, 59.35 ms/it, loss 0.452584
Finished training it 4096/76743 of epoch 1, 59.32 ms/it, loss 0.453160
Finished training it 4096/76743 of epoch 1, 59.37 ms/it, loss 0.451763
Finished training it 5120/76743 of epoch 1, 59.48 ms/it, loss 0.451224
Finished training it 5120/76743 of epoch 1, 59.19 ms/it, loss 0.451089
Finished training it 5120/76743 of epoch 1, 59.08 ms/it, loss 0.451877
Finished training it 5120/76743 of epoch 1, 59.20 ms/it, loss 0.451112
Finished training it 6144/76743 of epoch 1, 59.90 ms/it, loss 0.452739
Finished training it 6144/76743 of epoch 1, 59.85 ms/it, loss 0.452132
Finished training it 6144/76743 of epoch 1, 59.91 ms/it, loss 0.450578
Finished training it 6144/76743 of epoch 1, 59.81 ms/it, loss 0.454047
Finished training it 7168/76743 of epoch 1, 59.39 ms/it, loss 0.450040
Finished training it 7168/76743 of epoch 1, 59.44 ms/it, loss 0.449962
Finished training it 7168/76743 of epoch 1, 59.52 ms/it, loss 0.451722
Finished training it 7168/76743 of epoch 1, 59.58 ms/it, loss 0.448778
Finished training it 8192/76743 of epoch 1, 59.49 ms/it, loss 0.451838
Finished training it 8192/76743 of epoch 1, 59.54 ms/it, loss 0.451851
Finished training it 8192/76743 of epoch 1, 59.54 ms/it, loss 0.450328
Finished training it 8192/76743 of epoch 1, 59.64 ms/it, loss 0.453217
Finished training it 9216/76743 of epoch 1, 59.54 ms/it, loss 0.451576
Finished training it 9216/76743 of epoch 1, 59.50 ms/it, loss 0.450436
Finished training it 9216/76743 of epoch 1, 59.39 ms/it, loss 0.452192
Finished training it 9216/76743 of epoch 1, 59.48 ms/it, loss 0.450515
Finished training it 10240/76743 of epoch 1, 59.46 ms/it, loss 0.451546
Finished training it 10240/76743 of epoch 1, 59.29 ms/it, loss 0.452076
Finished training it 10240/76743 of epoch 1, 59.31 ms/it, loss 0.449677
Finished training it 10240/76743 of epoch 1, 59.37 ms/it, loss 0.452199
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2573678.0
get out
0 has test check 2573678.0 and sample count 3274240
 accuracy 78.604 %, best 78.604 %, roc auc score 0.7972, best 0.7972
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 59.80 ms/it, loss 0.452161
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2573678.0
get out
2 has test check 2573678.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 59.65 ms/it, loss 0.450565
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2573678.0
get out
3 has test check 2573678.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 59.71 ms/it, loss 0.452621
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2573678.0
get out
1 has test check 2573678.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 59.89 ms/it, loss 0.452130
Finished training it 12288/76743 of epoch 1, 59.39 ms/it, loss 0.451253
Finished training it 12288/76743 of epoch 1, 59.36 ms/it, loss 0.449063
Finished training it 12288/76743 of epoch 1, 59.36 ms/it, loss 0.448856
Finished training it 12288/76743 of epoch 1, 59.26 ms/it, loss 0.448922
Finished training it 13312/76743 of epoch 1, 59.59 ms/it, loss 0.448589
Finished training it 13312/76743 of epoch 1, 59.72 ms/it, loss 0.450725
Finished training it 13312/76743 of epoch 1, 59.81 ms/it, loss 0.449891
Finished training it 13312/76743 of epoch 1, 59.75 ms/it, loss 0.450358
Finished training it 14336/76743 of epoch 1, 59.08 ms/it, loss 0.451435
Finished training it 14336/76743 of epoch 1, 59.26 ms/it, loss 0.448456
Finished training it 14336/76743 of epoch 1, 59.23 ms/it, loss 0.450410
Finished training it 14336/76743 of epoch 1, 59.09 ms/it, loss 0.449227
Finished training it 15360/76743 of epoch 1, 59.61 ms/it, loss 0.450681
Finished training it 15360/76743 of epoch 1, 59.35 ms/it, loss 0.451528
Finished training it 15360/76743 of epoch 1, 59.56 ms/it, loss 0.452074
Finished training it 15360/76743 of epoch 1, 59.67 ms/it, loss 0.452247
Finished training it 16384/76743 of epoch 1, 59.76 ms/it, loss 0.449314
Finished training it 16384/76743 of epoch 1, 59.92 ms/it, loss 0.449600
Finished training it 16384/76743 of epoch 1, 59.75 ms/it, loss 0.450747
Finished training it 16384/76743 of epoch 1, 59.83 ms/it, loss 0.450604
Finished training it 17408/76743 of epoch 1, 59.24 ms/it, loss 0.449065
Finished training it 17408/76743 of epoch 1, 59.38 ms/it, loss 0.447609
Finished training it 17408/76743 of epoch 1, 59.27 ms/it, loss 0.451948
Finished training it 17408/76743 of epoch 1, 59.36 ms/it, loss 0.449167
Finished training it 18432/76743 of epoch 1, 59.65 ms/it, loss 0.452842
Finished training it 18432/76743 of epoch 1, 59.64 ms/it, loss 0.452239
Finished training it 18432/76743 of epoch 1, 64.03 ms/it, loss 0.450318
Finished training it 18432/76743 of epoch 1, 59.46 ms/it, loss 0.447692
Finished training it 19456/76743 of epoch 1, 61.74 ms/it, loss 0.452608
Finished training it 19456/76743 of epoch 1, 61.74 ms/it, loss 0.450668
Finished training it 19456/76743 of epoch 1, 61.70 ms/it, loss 0.448712
Finished training it 19456/76743 of epoch 1, 61.65 ms/it, loss 0.451637
Finished training it 20480/76743 of epoch 1, 56.62 ms/it, loss 0.449076
Finished training it 20480/76743 of epoch 1, 56.42 ms/it, loss 0.451426
Finished training it 20480/76743 of epoch 1, 56.18 ms/it, loss 0.446235
Finished training it 20480/76743 of epoch 1, 56.24 ms/it, loss 0.449216
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574934.0
get out
0 has test check 2574934.0 and sample count 3274240
 accuracy 78.642 %, best 78.642 %, roc auc score 0.7980, best 0.7980
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574934.0
get out
2 has test check 2574934.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 59.38 ms/it, loss 0.450274
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574934.0
get out
3 has test check 2574934.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 59.32 ms/it, loss 0.445781
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574934.0
get out
1 has test check 2574934.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 59.27 ms/it, loss 0.453140
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 59.32 ms/it, loss 0.451348
Finished training it 22528/76743 of epoch 1, 59.12 ms/it, loss 0.448669
Finished training it 22528/76743 of epoch 1, 59.01 ms/it, loss 0.449858
Finished training it 22528/76743 of epoch 1, 59.30 ms/it, loss 0.450605
Finished training it 22528/76743 of epoch 1, 59.25 ms/it, loss 0.449174
Finished training it 23552/76743 of epoch 1, 59.39 ms/it, loss 0.452777
Finished training it 23552/76743 of epoch 1, 59.19 ms/it, loss 0.448159
Finished training it 23552/76743 of epoch 1, 59.31 ms/it, loss 0.449001
Finished training it 23552/76743 of epoch 1, 59.29 ms/it, loss 0.453259
Finished training it 24576/76743 of epoch 1, 59.28 ms/it, loss 0.450535
Finished training it 24576/76743 of epoch 1, 59.34 ms/it, loss 0.449452
Finished training it 24576/76743 of epoch 1, 59.27 ms/it, loss 0.448470
Finished training it 24576/76743 of epoch 1, 59.30 ms/it, loss 0.450217
Finished training it 25600/76743 of epoch 1, 64.98 ms/it, loss 0.450235
Finished training it 25600/76743 of epoch 1, 65.10 ms/it, loss 0.448610
Finished training it 25600/76743 of epoch 1, 64.90 ms/it, loss 0.448687
Finished training it 25600/76743 of epoch 1, 60.11 ms/it, loss 0.447803
Finished training it 26624/76743 of epoch 1, 59.38 ms/it, loss 0.449006
Finished training it 26624/76743 of epoch 1, 59.26 ms/it, loss 0.450327
Finished training it 26624/76743 of epoch 1, 59.48 ms/it, loss 0.449943
Finished training it 26624/76743 of epoch 1, 59.30 ms/it, loss 0.449606
Finished training it 27648/76743 of epoch 1, 59.16 ms/it, loss 0.449189
Finished training it 27648/76743 of epoch 1, 59.17 ms/it, loss 0.449889
Finished training it 27648/76743 of epoch 1, 59.16 ms/it, loss 0.450752
Finished training it 27648/76743 of epoch 1, 59.02 ms/it, loss 0.448263
Finished training it 28672/76743 of epoch 1, 59.53 ms/it, loss 0.445321
Finished training it 28672/76743 of epoch 1, 59.53 ms/it, loss 0.452571
Finished training it 28672/76743 of epoch 1, 59.44 ms/it, loss 0.447742
Finished training it 28672/76743 of epoch 1, 59.32 ms/it, loss 0.448341
Finished training it 29696/76743 of epoch 1, 59.17 ms/it, loss 0.450833
Finished training it 29696/76743 of epoch 1, 59.17 ms/it, loss 0.447017
Finished training it 29696/76743 of epoch 1, 59.19 ms/it, loss 0.449629
Finished training it 29696/76743 of epoch 1, 59.18 ms/it, loss 0.450383
Finished training it 30720/76743 of epoch 1, 59.28 ms/it, loss 0.452654
Finished training it 30720/76743 of epoch 1, 59.39 ms/it, loss 0.448884
Finished training it 30720/76743 of epoch 1, 59.40 ms/it, loss 0.449576
Finished training it 30720/76743 of epoch 1, 59.34 ms/it, loss 0.448447
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574976.0
get out
0 has test check 2574976.0 and sample count 3274240
 accuracy 78.643 %, best 78.643 %, roc auc score 0.7981, best 0.7981
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574976.0
get out
2 has test check 2574976.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 58.53 ms/it, loss 0.450557
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 58.45 ms/it, loss 0.448196
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574976.0
get out
1 has test check 2574976.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 58.49 ms/it, loss 0.451751
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574976.0
get out
3 has test check 2574976.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 58.54 ms/it, loss 0.450287
Finished training it 32768/76743 of epoch 1, 59.15 ms/it, loss 0.450098
Finished training it 32768/76743 of epoch 1, 59.22 ms/it, loss 0.450620
Finished training it 32768/76743 of epoch 1, 59.21 ms/it, loss 0.449878
Finished training it 32768/76743 of epoch 1, 59.24 ms/it, loss 0.448047
Finished training it 33792/76743 of epoch 1, 59.63 ms/it, loss 0.453058
Finished training it 33792/76743 of epoch 1, 59.80 ms/it, loss 0.451104
Finished training it 33792/76743 of epoch 1, 59.68 ms/it, loss 0.448832
Finished training it 33792/76743 of epoch 1, 59.66 ms/it, loss 0.448515
Finished training it 34816/76743 of epoch 1, 59.07 ms/it, loss 0.447231
Finished training it 34816/76743 of epoch 1, 59.19 ms/it, loss 0.445580
Finished training it 34816/76743 of epoch 1, 59.25 ms/it, loss 0.449335
Finished training it 34816/76743 of epoch 1, 59.09 ms/it, loss 0.449676
Finished training it 35840/76743 of epoch 1, 59.20 ms/it, loss 0.447579
Finished training it 35840/76743 of epoch 1, 59.23 ms/it, loss 0.449769
Finished training it 35840/76743 of epoch 1, 59.40 ms/it, loss 0.451661
Finished training it 35840/76743 of epoch 1, 59.27 ms/it, loss 0.448563
Finished training it 36864/76743 of epoch 1, 59.58 ms/it, loss 0.449390
Finished training it 36864/76743 of epoch 1, 59.53 ms/it, loss 0.449583
Finished training it 36864/76743 of epoch 1, 59.54 ms/it, loss 0.451729
Finished training it 36864/76743 of epoch 1, 59.46 ms/it, loss 0.449486
Finished training it 37888/76743 of epoch 1, 59.60 ms/it, loss 0.449473
Finished training it 37888/76743 of epoch 1, 59.51 ms/it, loss 0.450426
Finished training it 37888/76743 of epoch 1, 59.45 ms/it, loss 0.448981
Finished training it 37888/76743 of epoch 1, 59.35 ms/it, loss 0.448404
Finished training it 38912/76743 of epoch 1, 59.71 ms/it, loss 0.448629
Finished training it 38912/76743 of epoch 1, 59.79 ms/it, loss 0.449141
Finished training it 38912/76743 of epoch 1, 59.67 ms/it, loss 0.451156
Finished training it 38912/76743 of epoch 1, 59.81 ms/it, loss 0.446609
Finished training it 39936/76743 of epoch 1, 59.63 ms/it, loss 0.450094
Finished training it 39936/76743 of epoch 1, 59.51 ms/it, loss 0.449567
Finished training it 39936/76743 of epoch 1, 59.57 ms/it, loss 0.448493
Finished training it 39936/76743 of epoch 1, 59.57 ms/it, loss 0.448486
Finished training it 40960/76743 of epoch 1, 58.90 ms/it, loss 0.450586
Finished training it 40960/76743 of epoch 1, 58.96 ms/it, loss 0.449031
Finished training it 40960/76743 of epoch 1, 58.94 ms/it, loss 0.448485
Finished training it 40960/76743 of epoch 1, 58.90 ms/it, loss 0.450294
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576185.0
get out
0 has test check 2576185.0 and sample count 3274240
 accuracy 78.680 %, best 78.680 %, roc auc score 0.7981, best 0.7981
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576185.0
get out
2 has test check 2576185.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 59.99 ms/it, loss 0.449777
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576185.0
get out
3 has test check 2576185.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 60.23 ms/it, loss 0.450765
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 59.89 ms/it, loss 0.447982
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576185.0
get out
1 has test check 2576185.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 60.05 ms/it, loss 0.448802
Finished training it 43008/76743 of epoch 1, 59.39 ms/it, loss 0.447919
Finished training it 43008/76743 of epoch 1, 59.45 ms/it, loss 0.448387
Finished training it 43008/76743 of epoch 1, 59.32 ms/it, loss 0.452004
Finished training it 43008/76743 of epoch 1, 59.49 ms/it, loss 0.449669
Finished training it 44032/76743 of epoch 1, 59.95 ms/it, loss 0.449722
Finished training it 44032/76743 of epoch 1, 64.35 ms/it, loss 0.449292
Finished training it 44032/76743 of epoch 1, 64.54 ms/it, loss 0.449052
Finished training it 44032/76743 of epoch 1, 64.34 ms/it, loss 0.450711
Finished training it 45056/76743 of epoch 1, 59.65 ms/it, loss 0.446989
Finished training it 45056/76743 of epoch 1, 59.66 ms/it, loss 0.449326
Finished training it 45056/76743 of epoch 1, 59.69 ms/it, loss 0.448524
Finished training it 45056/76743 of epoch 1, 59.65 ms/it, loss 0.448453
Finished training it 46080/76743 of epoch 1, 59.51 ms/it, loss 0.448905
Finished training it 46080/76743 of epoch 1, 59.52 ms/it, loss 0.448957
Finished training it 46080/76743 of epoch 1, 59.51 ms/it, loss 0.450136
Finished training it 46080/76743 of epoch 1, 59.52 ms/it, loss 0.447985
Finished training it 47104/76743 of epoch 1, 60.28 ms/it, loss 0.445868
Finished training it 47104/76743 of epoch 1, 59.93 ms/it, loss 0.449964
Finished training it 47104/76743 of epoch 1, 60.22 ms/it, loss 0.447568
Finished training it 47104/76743 of epoch 1, 64.94 ms/it, loss 0.449934
Finished training it 48128/76743 of epoch 1, 59.51 ms/it, loss 0.449403
Finished training it 48128/76743 of epoch 1, 59.51 ms/it, loss 0.448354
Finished training it 48128/76743 of epoch 1, 59.42 ms/it, loss 0.447724
Finished training it 48128/76743 of epoch 1, 59.47 ms/it, loss 0.447007
Finished training it 49152/76743 of epoch 1, 59.34 ms/it, loss 0.449378
Finished training it 49152/76743 of epoch 1, 59.35 ms/it, loss 0.447781
Finished training it 49152/76743 of epoch 1, 59.26 ms/it, loss 0.447396
Finished training it 49152/76743 of epoch 1, 59.32 ms/it, loss 0.453476
Finished training it 50176/76743 of epoch 1, 59.09 ms/it, loss 0.447132
Finished training it 50176/76743 of epoch 1, 59.06 ms/it, loss 0.448128
Finished training it 50176/76743 of epoch 1, 59.09 ms/it, loss 0.446617
Finished training it 50176/76743 of epoch 1, 59.03 ms/it, loss 0.447171
Finished training it 51200/76743 of epoch 1, 58.97 ms/it, loss 0.450898
Finished training it 51200/76743 of epoch 1, 59.14 ms/it, loss 0.444999
Finished training it 51200/76743 of epoch 1, 58.99 ms/it, loss 0.450974
Finished training it 51200/76743 of epoch 1, 59.12 ms/it, loss 0.449034
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577490.0
get out
0 has test check 2577490.0 and sample count 3274240
 accuracy 78.720 %, best 78.720 %, roc auc score 0.7993, best 0.7993
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 59.11 ms/it, loss 0.448541
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577490.0
get out
1 has test check 2577490.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 59.07 ms/it, loss 0.447732
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577490.0
get out
3 has test check 2577490.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 59.10 ms/it, loss 0.447218
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577490.0
get out
2 has test check 2577490.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 59.04 ms/it, loss 0.446099
Finished training it 53248/76743 of epoch 1, 59.23 ms/it, loss 0.447814
Finished training it 53248/76743 of epoch 1, 59.19 ms/it, loss 0.449860
Finished training it 53248/76743 of epoch 1, 59.09 ms/it, loss 0.447321
Finished training it 53248/76743 of epoch 1, 59.13 ms/it, loss 0.448836
Finished training it 54272/76743 of epoch 1, 59.57 ms/it, loss 0.445899
Finished training it 54272/76743 of epoch 1, 59.59 ms/it, loss 0.446812
Finished training it 54272/76743 of epoch 1, 59.44 ms/it, loss 0.448593
Finished training it 54272/76743 of epoch 1, 59.43 ms/it, loss 0.449862
Finished training it 55296/76743 of epoch 1, 59.25 ms/it, loss 0.447408
Finished training it 55296/76743 of epoch 1, 59.23 ms/it, loss 0.446720
Finished training it 55296/76743 of epoch 1, 59.16 ms/it, loss 0.446901
Finished training it 55296/76743 of epoch 1, 59.25 ms/it, loss 0.447836
Finished training it 56320/76743 of epoch 1, 59.45 ms/it, loss 0.446679
Finished training it 56320/76743 of epoch 1, 59.39 ms/it, loss 0.448223
Finished training it 56320/76743 of epoch 1, 59.38 ms/it, loss 0.447000
Finished training it 56320/76743 of epoch 1, 59.39 ms/it, loss 0.449903
Finished training it 57344/76743 of epoch 1, 59.14 ms/it, loss 0.449868
Finished training it 57344/76743 of epoch 1, 59.11 ms/it, loss 0.448296
Finished training it 57344/76743 of epoch 1, 59.16 ms/it, loss 0.449630
Finished training it 57344/76743 of epoch 1, 59.06 ms/it, loss 0.450407
Finished training it 58368/76743 of epoch 1, 59.19 ms/it, loss 0.448090
Finished training it 58368/76743 of epoch 1, 59.43 ms/it, loss 0.448329
Finished training it 58368/76743 of epoch 1, 59.34 ms/it, loss 0.450408
Finished training it 58368/76743 of epoch 1, 59.39 ms/it, loss 0.449850
Finished training it 59392/76743 of epoch 1, 59.06 ms/it, loss 0.448344
Finished training it 59392/76743 of epoch 1, 59.09 ms/it, loss 0.445446
Finished training it 59392/76743 of epoch 1, 59.06 ms/it, loss 0.448656
Finished training it 59392/76743 of epoch 1, 59.22 ms/it, loss 0.448991
Finished training it 60416/76743 of epoch 1, 59.39 ms/it, loss 0.447794
Finished training it 60416/76743 of epoch 1, 59.34 ms/it, loss 0.449123
Finished training it 60416/76743 of epoch 1, 59.52 ms/it, loss 0.448779
Finished training it 60416/76743 of epoch 1, 59.34 ms/it, loss 0.448017
Finished training it 61440/76743 of epoch 1, 59.63 ms/it, loss 0.448649
Finished training it 61440/76743 of epoch 1, 59.72 ms/it, loss 0.447347
Finished training it 61440/76743 of epoch 1, 59.69 ms/it, loss 0.447732
Finished training it 61440/76743 of epoch 1, 59.72 ms/it, loss 0.449224
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577631.0
get out
0 has test check 2577631.0 and sample count 3274240
 accuracy 78.725 %, best 78.725 %, roc auc score 0.7996, best 0.7996
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 59.04 ms/it, loss 0.446207
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577631.0
get out
2 has test check 2577631.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 59.24 ms/it, loss 0.445658
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577631.0
get out
1 has test check 2577631.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 59.06 ms/it, loss 0.449594
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577631.0
get out
3 has test check 2577631.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 59.11 ms/it, loss 0.449228
Finished training it 63488/76743 of epoch 1, 59.08 ms/it, loss 0.451639
Finished training it 63488/76743 of epoch 1, 59.07 ms/it, loss 0.449405
Finished training it 63488/76743 of epoch 1, 59.10 ms/it, loss 0.450079
Finished training it 63488/76743 of epoch 1, 59.07 ms/it, loss 0.445498
Finished training it 64512/76743 of epoch 1, 64.66 ms/it, loss 0.446787
Finished training it 64512/76743 of epoch 1, 59.97 ms/it, loss 0.446518
Finished training it 64512/76743 of epoch 1, 64.57 ms/it, loss 0.448049
Finished training it 64512/76743 of epoch 1, 64.74 ms/it, loss 0.448937
Finished training it 65536/76743 of epoch 1, 60.29 ms/it, loss 0.444255
Finished training it 65536/76743 of epoch 1, 59.93 ms/it, loss 0.446696
Finished training it 65536/76743 of epoch 1, 59.75 ms/it, loss 0.449240
Finished training it 65536/76743 of epoch 1, 64.55 ms/it, loss 0.449361
Finished training it 66560/76743 of epoch 1, 59.15 ms/it, loss 0.448457
Finished training it 66560/76743 of epoch 1, 59.26 ms/it, loss 0.447443
Finished training it 66560/76743 of epoch 1, 59.13 ms/it, loss 0.448090
Finished training it 66560/76743 of epoch 1, 59.20 ms/it, loss 0.446472
Finished training it 67584/76743 of epoch 1, 58.65 ms/it, loss 0.445255
Finished training it 67584/76743 of epoch 1, 58.77 ms/it, loss 0.446447
Finished training it 67584/76743 of epoch 1, 58.77 ms/it, loss 0.447831
Finished training it 67584/76743 of epoch 1, 58.81 ms/it, loss 0.449963
Finished training it 68608/76743 of epoch 1, 59.41 ms/it, loss 0.449625
Finished training it 68608/76743 of epoch 1, 59.30 ms/it, loss 0.447135
Finished training it 68608/76743 of epoch 1, 59.39 ms/it, loss 0.446721
Finished training it 68608/76743 of epoch 1, 59.44 ms/it, loss 0.447257
Finished training it 69632/76743 of epoch 1, 59.62 ms/it, loss 0.450080
Finished training it 69632/76743 of epoch 1, 59.58 ms/it, loss 0.444607
Finished training it 69632/76743 of epoch 1, 59.48 ms/it, loss 0.443678
Finished training it 69632/76743 of epoch 1, 59.47 ms/it, loss 0.448992
Finished training it 70656/76743 of epoch 1, 60.25 ms/it, loss 0.449825
Finished training it 70656/76743 of epoch 1, 60.17 ms/it, loss 0.445623
Finished training it 70656/76743 of epoch 1, 60.16 ms/it, loss 0.447536
Finished training it 70656/76743 of epoch 1, 60.06 ms/it, loss 0.449737
Finished training it 71680/76743 of epoch 1, 59.51 ms/it, loss 0.448064
Finished training it 71680/76743 of epoch 1, 59.48 ms/it, loss 0.449015
Finished training it 71680/76743 of epoch 1, 59.51 ms/it, loss 0.448360
Finished training it 71680/76743 of epoch 1, 59.34 ms/it, loss 0.449051
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578379.0
get out
0 has test check 2578379.0 and sample count 3274240
 accuracy 78.747 %, best 78.747 %, roc auc score 0.8000, best 0.8000
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578379.0
get out
3 has test check 2578379.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 59.90 ms/it, loss 0.448539
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578379.0
get out
2 has test check 2578379.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 59.81 ms/it, loss 0.447924
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 59.83 ms/it, loss 0.448211
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578379.0
get out
1 has test check 2578379.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 59.87 ms/it, loss 0.447828
Finished training it 73728/76743 of epoch 1, 59.72 ms/it, loss 0.448736
Finished training it 73728/76743 of epoch 1, 59.74 ms/it, loss 0.447552
Finished training it 73728/76743 of epoch 1, 59.80 ms/it, loss 0.446859
Finished training it 73728/76743 of epoch 1, 59.86 ms/it, loss 0.446164
Finished training it 74752/76743 of epoch 1, 58.92 ms/it, loss 0.449286
Finished training it 74752/76743 of epoch 1, 58.95 ms/it, loss 0.445844
Finished training it 74752/76743 of epoch 1, 58.88 ms/it, loss 0.445956
Finished training it 74752/76743 of epoch 1, 58.88 ms/it, loss 0.445641
Finished training it 75776/76743 of epoch 1, 58.61 ms/it, loss 0.447247
Finished training it 75776/76743 of epoch 1, 58.96 ms/it, loss 0.447522
Finished training it 75776/76743 of epoch 1, 58.77 ms/it, loss 0.447899
Finished training it 75776/76743 of epoch 1, 58.86 ms/it, loss 0.448227
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 60.39 ms/it, loss 0.447237
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 60.35 ms/it, loss 0.447689
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 60.56 ms/it, loss 0.448283
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 60.50 ms/it, loss 0.445540
Finished training it 2048/76743 of epoch 2, 59.33 ms/it, loss 0.446020
Finished training it 2048/76743 of epoch 2, 59.44 ms/it, loss 0.445330
Finished training it 2048/76743 of epoch 2, 59.51 ms/it, loss 0.446297
Finished training it 2048/76743 of epoch 2, 59.42 ms/it, loss 0.447679
Finished training it 3072/76743 of epoch 2, 60.32 ms/it, loss 0.443469
Finished training it 3072/76743 of epoch 2, 60.10 ms/it, loss 0.447974
Finished training it 3072/76743 of epoch 2, 60.37 ms/it, loss 0.446866
Finished training it 3072/76743 of epoch 2, 64.72 ms/it, loss 0.448477
Finished training it 4096/76743 of epoch 2, 59.05 ms/it, loss 0.448512
Finished training it 4096/76743 of epoch 2, 59.00 ms/it, loss 0.446746
Finished training it 4096/76743 of epoch 2, 59.12 ms/it, loss 0.445727
Finished training it 4096/76743 of epoch 2, 59.00 ms/it, loss 0.447767
Finished training it 5120/76743 of epoch 2, 59.32 ms/it, loss 0.446356
Finished training it 5120/76743 of epoch 2, 59.40 ms/it, loss 0.446413
Finished training it 5120/76743 of epoch 2, 59.45 ms/it, loss 0.446595
Finished training it 5120/76743 of epoch 2, 59.42 ms/it, loss 0.447337
Finished training it 6144/76743 of epoch 2, 59.42 ms/it, loss 0.449232
Finished training it 6144/76743 of epoch 2, 59.41 ms/it, loss 0.445889
Finished training it 6144/76743 of epoch 2, 59.55 ms/it, loss 0.447416
Finished training it 6144/76743 of epoch 2, 59.52 ms/it, loss 0.448257
Finished training it 7168/76743 of epoch 2, 59.63 ms/it, loss 0.447466
Finished training it 7168/76743 of epoch 2, 59.63 ms/it, loss 0.443991
Finished training it 7168/76743 of epoch 2, 59.50 ms/it, loss 0.445278
Finished training it 7168/76743 of epoch 2, 59.64 ms/it, loss 0.445534
Finished training it 8192/76743 of epoch 2, 59.45 ms/it, loss 0.448437
Finished training it 8192/76743 of epoch 2, 59.40 ms/it, loss 0.447226
Finished training it 8192/76743 of epoch 2, 59.63 ms/it, loss 0.447200
Finished training it 8192/76743 of epoch 2, 59.56 ms/it, loss 0.445633
Finished training it 9216/76743 of epoch 2, 59.71 ms/it, loss 0.445698
Finished training it 9216/76743 of epoch 2, 59.50 ms/it, loss 0.447450
Finished training it 9216/76743 of epoch 2, 59.58 ms/it, loss 0.446162
Finished training it 9216/76743 of epoch 2, 59.67 ms/it, loss 0.447246
Finished training it 10240/76743 of epoch 2, 59.29 ms/it, loss 0.447086
Finished training it 10240/76743 of epoch 2, 59.27 ms/it, loss 0.445380
Finished training it 10240/76743 of epoch 2, 59.41 ms/it, loss 0.447751
Finished training it 10240/76743 of epoch 2, 59.29 ms/it, loss 0.447485
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579275.0
get out
0 has test check 2579275.0 and sample count 3274240
 accuracy 78.775 %, best 78.775 %, roc auc score 0.8007, best 0.8007
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579275.0
get out
3 has test check 2579275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 59.55 ms/it, loss 0.448500
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579275.0
get out
1 has test check 2579275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 59.73 ms/it, loss 0.447722
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 59.58 ms/it, loss 0.447807
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579275.0
get out
2 has test check 2579275.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 59.48 ms/it, loss 0.446116
Finished training it 12288/76743 of epoch 2, 59.05 ms/it, loss 0.444553
Finished training it 12288/76743 of epoch 2, 58.91 ms/it, loss 0.446705
Finished training it 12288/76743 of epoch 2, 58.99 ms/it, loss 0.444685
Finished training it 12288/76743 of epoch 2, 58.95 ms/it, loss 0.444327
Finished training it 13312/76743 of epoch 2, 60.37 ms/it, loss 0.446545
Finished training it 13312/76743 of epoch 2, 64.74 ms/it, loss 0.446257
Finished training it 13312/76743 of epoch 2, 64.67 ms/it, loss 0.445361
Finished training it 13312/76743 of epoch 2, 64.74 ms/it, loss 0.444189
Finished training it 14336/76743 of epoch 2, 60.19 ms/it, loss 0.447141
Finished training it 14336/76743 of epoch 2, 60.05 ms/it, loss 0.444603
Finished training it 14336/76743 of epoch 2, 60.00 ms/it, loss 0.445733
Finished training it 14336/76743 of epoch 2, 60.08 ms/it, loss 0.444892
Finished training it 15360/76743 of epoch 2, 58.86 ms/it, loss 0.446264
Finished training it 15360/76743 of epoch 2, 58.85 ms/it, loss 0.447915
Finished training it 15360/76743 of epoch 2, 58.96 ms/it, loss 0.447185
Finished training it 15360/76743 of epoch 2, 59.09 ms/it, loss 0.447980
Finished training it 16384/76743 of epoch 2, 59.64 ms/it, loss 0.446713
Finished training it 16384/76743 of epoch 2, 59.50 ms/it, loss 0.445286
Finished training it 16384/76743 of epoch 2, 59.37 ms/it, loss 0.444790
Finished training it 16384/76743 of epoch 2, 59.46 ms/it, loss 0.446368
Finished training it 17408/76743 of epoch 2, 59.31 ms/it, loss 0.443480
Finished training it 17408/76743 of epoch 2, 59.31 ms/it, loss 0.444888
Finished training it 17408/76743 of epoch 2, 59.40 ms/it, loss 0.444759
Finished training it 17408/76743 of epoch 2, 59.38 ms/it, loss 0.447663
Finished training it 18432/76743 of epoch 2, 65.48 ms/it, loss 0.446478
Finished training it 18432/76743 of epoch 2, 60.56 ms/it, loss 0.448108
Finished training it 18432/76743 of epoch 2, 60.98 ms/it, loss 0.443699
Finished training it 18432/76743 of epoch 2, 60.63 ms/it, loss 0.448881
Finished training it 19456/76743 of epoch 2, 60.11 ms/it, loss 0.444752
Finished training it 19456/76743 of epoch 2, 60.03 ms/it, loss 0.448336
Finished training it 19456/76743 of epoch 2, 60.13 ms/it, loss 0.447559
Finished training it 19456/76743 of epoch 2, 59.93 ms/it, loss 0.446311
Finished training it 20480/76743 of epoch 2, 58.71 ms/it, loss 0.446947
Finished training it 20480/76743 of epoch 2, 58.70 ms/it, loss 0.441702
Finished training it 20480/76743 of epoch 2, 58.69 ms/it, loss 0.444942
Finished training it 20480/76743 of epoch 2, 58.62 ms/it, loss 0.444893
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579653.0
get out
0 has test check 2579653.0 and sample count 3274240
 accuracy 78.786 %, best 78.786 %, roc auc score 0.8010, best 0.8010
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579653.0
get out
3 has test check 2579653.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 59.22 ms/it, loss 0.441694
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579653.0
get out
1 has test check 2579653.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 59.04 ms/it, loss 0.448953
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579653.0
get out
2 has test check 2579653.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 59.13 ms/it, loss 0.446429
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 59.04 ms/it, loss 0.447264
Finished training it 22528/76743 of epoch 2, 59.02 ms/it, loss 0.444536
Finished training it 22528/76743 of epoch 2, 59.12 ms/it, loss 0.445001
Finished training it 22528/76743 of epoch 2, 58.96 ms/it, loss 0.446380
Finished training it 22528/76743 of epoch 2, 59.02 ms/it, loss 0.446087
Finished training it 23552/76743 of epoch 2, 59.30 ms/it, loss 0.444799
Finished training it 23552/76743 of epoch 2, 59.07 ms/it, loss 0.444374
Finished training it 23552/76743 of epoch 2, 59.14 ms/it, loss 0.449472
Finished training it 23552/76743 of epoch 2, 59.47 ms/it, loss 0.448732
Finished training it 24576/76743 of epoch 2, 59.55 ms/it, loss 0.445172
Finished training it 24576/76743 of epoch 2, 59.59 ms/it, loss 0.446472
Finished training it 24576/76743 of epoch 2, 59.49 ms/it, loss 0.444558
Finished training it 24576/76743 of epoch 2, 59.44 ms/it, loss 0.446072
Finished training it 25600/76743 of epoch 2, 59.22 ms/it, loss 0.444504
Finished training it 25600/76743 of epoch 2, 59.22 ms/it, loss 0.444625
Finished training it 25600/76743 of epoch 2, 59.11 ms/it, loss 0.443316
Finished training it 25600/76743 of epoch 2, 59.38 ms/it, loss 0.445696
Finished training it 26624/76743 of epoch 2, 59.02 ms/it, loss 0.444832
Finished training it 26624/76743 of epoch 2, 58.93 ms/it, loss 0.445633
Finished training it 26624/76743 of epoch 2, 58.85 ms/it, loss 0.446140
Finished training it 26624/76743 of epoch 2, 58.89 ms/it, loss 0.445623
Finished training it 27648/76743 of epoch 2, 59.04 ms/it, loss 0.444103
Finished training it 27648/76743 of epoch 2, 59.09 ms/it, loss 0.445993
Finished training it 27648/76743 of epoch 2, 59.18 ms/it, loss 0.445144
Finished training it 27648/76743 of epoch 2, 59.29 ms/it, loss 0.446367
Finished training it 28672/76743 of epoch 2, 59.26 ms/it, loss 0.441153
Finished training it 28672/76743 of epoch 2, 59.43 ms/it, loss 0.444247
Finished training it 28672/76743 of epoch 2, 59.46 ms/it, loss 0.448512
Finished training it 28672/76743 of epoch 2, 59.41 ms/it, loss 0.443827
Finished training it 29696/76743 of epoch 2, 59.86 ms/it, loss 0.445557
Finished training it 29696/76743 of epoch 2, 59.78 ms/it, loss 0.446754
Finished training it 29696/76743 of epoch 2, 59.74 ms/it, loss 0.443162
Finished training it 29696/76743 of epoch 2, 59.63 ms/it, loss 0.446333
Finished training it 30720/76743 of epoch 2, 59.39 ms/it, loss 0.444543
Finished training it 30720/76743 of epoch 2, 59.59 ms/it, loss 0.445801
Finished training it 30720/76743 of epoch 2, 59.57 ms/it, loss 0.444892
Finished training it 30720/76743 of epoch 2, 59.37 ms/it, loss 0.448643
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579260.0
get out
0 has test check 2579260.0 and sample count 3274240
 accuracy 78.774 %, best 78.786 %, roc auc score 0.8009, best 0.8010
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579260.0
get out
1 has test check 2579260.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 58.95 ms/it, loss 0.447829
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579260.0
get out
2 has test check 2579260.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 58.93 ms/it, loss 0.446237
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 59.04 ms/it, loss 0.444016
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579260.0
get out
3 has test check 2579260.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 58.98 ms/it, loss 0.446261
Finished training it 32768/76743 of epoch 2, 59.10 ms/it, loss 0.446681
Finished training it 32768/76743 of epoch 2, 59.08 ms/it, loss 0.445705
Finished training it 32768/76743 of epoch 2, 59.20 ms/it, loss 0.444019
Finished training it 32768/76743 of epoch 2, 59.18 ms/it, loss 0.445935
Finished training it 33792/76743 of epoch 2, 59.80 ms/it, loss 0.444471
Finished training it 33792/76743 of epoch 2, 64.53 ms/it, loss 0.444676
Finished training it 33792/76743 of epoch 2, 64.58 ms/it, loss 0.447321
Finished training it 33792/76743 of epoch 2, 64.48 ms/it, loss 0.449168
Finished training it 34816/76743 of epoch 2, 59.22 ms/it, loss 0.445664
Finished training it 34816/76743 of epoch 2, 58.99 ms/it, loss 0.445256
Finished training it 34816/76743 of epoch 2, 58.99 ms/it, loss 0.443367
Finished training it 34816/76743 of epoch 2, 59.17 ms/it, loss 0.441452
Finished training it 35840/76743 of epoch 2, 59.32 ms/it, loss 0.445685
Finished training it 35840/76743 of epoch 2, 59.39 ms/it, loss 0.444602
Finished training it 35840/76743 of epoch 2, 59.35 ms/it, loss 0.443714
Finished training it 35840/76743 of epoch 2, 59.54 ms/it, loss 0.447702
Finished training it 36864/76743 of epoch 2, 59.40 ms/it, loss 0.445330
Finished training it 36864/76743 of epoch 2, 59.27 ms/it, loss 0.445429
Finished training it 36864/76743 of epoch 2, 59.50 ms/it, loss 0.447567
Finished training it 36864/76743 of epoch 2, 59.23 ms/it, loss 0.445817
Finished training it 37888/76743 of epoch 2, 59.14 ms/it, loss 0.445753
Finished training it 37888/76743 of epoch 2, 58.92 ms/it, loss 0.445209
Finished training it 37888/76743 of epoch 2, 58.87 ms/it, loss 0.444599
Finished training it 37888/76743 of epoch 2, 59.01 ms/it, loss 0.446419
Finished training it 38912/76743 of epoch 2, 59.05 ms/it, loss 0.445059
Finished training it 38912/76743 of epoch 2, 59.05 ms/it, loss 0.446965
Finished training it 38912/76743 of epoch 2, 59.00 ms/it, loss 0.442756
Finished training it 38912/76743 of epoch 2, 59.20 ms/it, loss 0.444948
Finished training it 39936/76743 of epoch 2, 59.44 ms/it, loss 0.445584
Finished training it 39936/76743 of epoch 2, 59.57 ms/it, loss 0.444819
Finished training it 39936/76743 of epoch 2, 59.46 ms/it, loss 0.446697
Finished training it 39936/76743 of epoch 2, 59.58 ms/it, loss 0.444267
Finished training it 40960/76743 of epoch 2, 59.30 ms/it, loss 0.446569
Finished training it 40960/76743 of epoch 2, 59.30 ms/it, loss 0.444763
Finished training it 40960/76743 of epoch 2, 59.17 ms/it, loss 0.445347
Finished training it 40960/76743 of epoch 2, 59.32 ms/it, loss 0.446568
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580639.0
get out
0 has test check 2580639.0 and sample count 3274240
 accuracy 78.816 %, best 78.816 %, roc auc score 0.8012, best 0.8012
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580639.0
get out
2 has test check 2580639.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 59.16 ms/it, loss 0.445909
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580639.0
get out
1 has test check 2580639.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 59.06 ms/it, loss 0.445234
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 59.18 ms/it, loss 0.444071
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580639.0
get out
3 has test check 2580639.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 59.03 ms/it, loss 0.446943
Finished training it 43008/76743 of epoch 2, 59.37 ms/it, loss 0.446133
Finished training it 43008/76743 of epoch 2, 59.20 ms/it, loss 0.444180
Finished training it 43008/76743 of epoch 2, 59.16 ms/it, loss 0.444556
Finished training it 43008/76743 of epoch 2, 59.22 ms/it, loss 0.447992
Finished training it 44032/76743 of epoch 2, 58.86 ms/it, loss 0.445204
Finished training it 44032/76743 of epoch 2, 58.93 ms/it, loss 0.447083
Finished training it 44032/76743 of epoch 2, 58.94 ms/it, loss 0.445779
Finished training it 44032/76743 of epoch 2, 58.78 ms/it, loss 0.445389
Finished training it 45056/76743 of epoch 2, 59.34 ms/it, loss 0.444881
Finished training it 45056/76743 of epoch 2, 59.29 ms/it, loss 0.442801
Finished training it 45056/76743 of epoch 2, 59.36 ms/it, loss 0.444511
Finished training it 45056/76743 of epoch 2, 59.40 ms/it, loss 0.445509
Finished training it 46080/76743 of epoch 2, 59.45 ms/it, loss 0.446438
Finished training it 46080/76743 of epoch 2, 59.58 ms/it, loss 0.444268
Finished training it 46080/76743 of epoch 2, 59.41 ms/it, loss 0.445194
Finished training it 46080/76743 of epoch 2, 59.55 ms/it, loss 0.445026
Finished training it 47104/76743 of epoch 2, 64.61 ms/it, loss 0.446302
Finished training it 47104/76743 of epoch 2, 60.22 ms/it, loss 0.443842
Finished training it 47104/76743 of epoch 2, 60.02 ms/it, loss 0.442206
Finished training it 47104/76743 of epoch 2, 59.60 ms/it, loss 0.446288
Finished training it 48128/76743 of epoch 2, 59.87 ms/it, loss 0.443170
Finished training it 48128/76743 of epoch 2, 59.59 ms/it, loss 0.444042
Finished training it 48128/76743 of epoch 2, 59.87 ms/it, loss 0.444479
Finished training it 48128/76743 of epoch 2, 59.77 ms/it, loss 0.445363
Finished training it 49152/76743 of epoch 2, 59.53 ms/it, loss 0.449941
Finished training it 49152/76743 of epoch 2, 59.51 ms/it, loss 0.444190
Finished training it 49152/76743 of epoch 2, 59.48 ms/it, loss 0.443660
Finished training it 49152/76743 of epoch 2, 59.58 ms/it, loss 0.445994
Finished training it 50176/76743 of epoch 2, 58.89 ms/it, loss 0.444499
Finished training it 50176/76743 of epoch 2, 58.95 ms/it, loss 0.443352
Finished training it 50176/76743 of epoch 2, 59.03 ms/it, loss 0.443004
Finished training it 50176/76743 of epoch 2, 58.93 ms/it, loss 0.443255
Finished training it 51200/76743 of epoch 2, 58.93 ms/it, loss 0.445051
Finished training it 51200/76743 of epoch 2, 58.96 ms/it, loss 0.441161
Finished training it 51200/76743 of epoch 2, 59.06 ms/it, loss 0.446576
Finished training it 51200/76743 of epoch 2, 59.01 ms/it, loss 0.447125
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581213.0
get out
0 has test check 2581213.0 and sample count 3274240
 accuracy 78.834 %, best 78.834 %, roc auc score 0.8016, best 0.8016
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581213.0
get out
3 has test check 2581213.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 59.18 ms/it, loss 0.443313
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 59.18 ms/it, loss 0.444732
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581213.0
get out
2 has test check 2581213.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 59.19 ms/it, loss 0.442541
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581213.0
get out
1 has test check 2581213.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 59.03 ms/it, loss 0.443965
Finished training it 53248/76743 of epoch 2, 59.88 ms/it, loss 0.445280
Finished training it 53248/76743 of epoch 2, 59.72 ms/it, loss 0.443747
Finished training it 53248/76743 of epoch 2, 59.84 ms/it, loss 0.444106
Finished training it 53248/76743 of epoch 2, 60.01 ms/it, loss 0.446219
Finished training it 54272/76743 of epoch 2, 64.86 ms/it, loss 0.442634
Finished training it 54272/76743 of epoch 2, 64.67 ms/it, loss 0.446193
Finished training it 54272/76743 of epoch 2, 59.94 ms/it, loss 0.445103
Finished training it 54272/76743 of epoch 2, 64.61 ms/it, loss 0.443546
Finished training it 55296/76743 of epoch 2, 59.80 ms/it, loss 0.443093
Finished training it 55296/76743 of epoch 2, 59.65 ms/it, loss 0.442867
Finished training it 55296/76743 of epoch 2, 59.89 ms/it, loss 0.444004
Finished training it 55296/76743 of epoch 2, 60.11 ms/it, loss 0.443434
Finished training it 56320/76743 of epoch 2, 59.28 ms/it, loss 0.446546
Finished training it 56320/76743 of epoch 2, 59.30 ms/it, loss 0.443358
Finished training it 56320/76743 of epoch 2, 59.37 ms/it, loss 0.442964
Finished training it 56320/76743 of epoch 2, 59.17 ms/it, loss 0.444414
Finished training it 57344/76743 of epoch 2, 59.03 ms/it, loss 0.446123
Finished training it 57344/76743 of epoch 2, 59.24 ms/it, loss 0.446186
Finished training it 57344/76743 of epoch 2, 59.17 ms/it, loss 0.444329
Finished training it 57344/76743 of epoch 2, 59.05 ms/it, loss 0.446308
Finished training it 58368/76743 of epoch 2, 59.68 ms/it, loss 0.444685
Finished training it 58368/76743 of epoch 2, 59.57 ms/it, loss 0.444211
Finished training it 58368/76743 of epoch 2, 59.60 ms/it, loss 0.445942
Finished training it 58368/76743 of epoch 2, 59.56 ms/it, loss 0.446584
Finished training it 59392/76743 of epoch 2, 59.56 ms/it, loss 0.444842
Finished training it 59392/76743 of epoch 2, 59.72 ms/it, loss 0.441915
Finished training it 59392/76743 of epoch 2, 59.64 ms/it, loss 0.445519
Finished training it 59392/76743 of epoch 2, 59.54 ms/it, loss 0.445134
Finished training it 60416/76743 of epoch 2, 59.91 ms/it, loss 0.445196
Finished training it 60416/76743 of epoch 2, 59.74 ms/it, loss 0.445586
Finished training it 60416/76743 of epoch 2, 59.81 ms/it, loss 0.444245
Finished training it 60416/76743 of epoch 2, 59.84 ms/it, loss 0.444607
Finished training it 61440/76743 of epoch 2, 59.03 ms/it, loss 0.444012
Finished training it 61440/76743 of epoch 2, 59.27 ms/it, loss 0.444372
Finished training it 61440/76743 of epoch 2, 59.19 ms/it, loss 0.445170
Finished training it 61440/76743 of epoch 2, 59.30 ms/it, loss 0.445690
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580699.0
get out
0 has test check 2580699.0 and sample count 3274240
 accuracy 78.818 %, best 78.834 %, roc auc score 0.8018, best 0.8018
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580699.0
get out
2 has test check 2580699.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 58.88 ms/it, loss 0.442191
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580699.0
get out
1 has test check 2580699.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 58.91 ms/it, loss 0.446162
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 58.87 ms/it, loss 0.442877
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580699.0
get out
3 has test check 2580699.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 58.99 ms/it, loss 0.445664
Finished training it 63488/76743 of epoch 2, 59.81 ms/it, loss 0.446791
Finished training it 63488/76743 of epoch 2, 59.68 ms/it, loss 0.447871
Finished training it 63488/76743 of epoch 2, 59.45 ms/it, loss 0.442420
Finished training it 63488/76743 of epoch 2, 59.63 ms/it, loss 0.445941
Finished training it 64512/76743 of epoch 2, 59.77 ms/it, loss 0.444727
Finished training it 64512/76743 of epoch 2, 59.78 ms/it, loss 0.445325
Finished training it 64512/76743 of epoch 2, 59.82 ms/it, loss 0.442713
Finished training it 64512/76743 of epoch 2, 59.72 ms/it, loss 0.442963
Finished training it 65536/76743 of epoch 2, 60.19 ms/it, loss 0.440938
Finished training it 65536/76743 of epoch 2, 64.63 ms/it, loss 0.445979
Finished training it 65536/76743 of epoch 2, 60.46 ms/it, loss 0.443230
Finished training it 65536/76743 of epoch 2, 59.83 ms/it, loss 0.445715
Finished training it 66560/76743 of epoch 2, 59.40 ms/it, loss 0.444190
Finished training it 66560/76743 of epoch 2, 59.24 ms/it, loss 0.443266
Finished training it 66560/76743 of epoch 2, 59.28 ms/it, loss 0.444690
Finished training it 66560/76743 of epoch 2, 59.45 ms/it, loss 0.444755
Finished training it 67584/76743 of epoch 2, 72.62 ms/it, loss 0.444278
Finished training it 67584/76743 of epoch 2, 72.62 ms/it, loss 0.446097
Finished training it 67584/76743 of epoch 2, 72.58 ms/it, loss 0.442800
Finished training it 67584/76743 of epoch 2, 72.67 ms/it, loss 0.442047
Finished training it 68608/76743 of epoch 2, 59.36 ms/it, loss 0.443321
Finished training it 68608/76743 of epoch 2, 59.31 ms/it, loss 0.443704
Finished training it 68608/76743 of epoch 2, 59.43 ms/it, loss 0.443824
Finished training it 68608/76743 of epoch 2, 59.26 ms/it, loss 0.446053
Finished training it 69632/76743 of epoch 2, 59.68 ms/it, loss 0.446681
Finished training it 69632/76743 of epoch 2, 59.45 ms/it, loss 0.441072
Finished training it 69632/76743 of epoch 2, 59.58 ms/it, loss 0.445308
Finished training it 69632/76743 of epoch 2, 59.56 ms/it, loss 0.440087
Finished training it 70656/76743 of epoch 2, 59.37 ms/it, loss 0.446707
Finished training it 70656/76743 of epoch 2, 59.22 ms/it, loss 0.442431
Finished training it 70656/76743 of epoch 2, 59.50 ms/it, loss 0.444279
Finished training it 70656/76743 of epoch 2, 59.32 ms/it, loss 0.446162
Finished training it 71680/76743 of epoch 2, 59.24 ms/it, loss 0.444875
Finished training it 71680/76743 of epoch 2, 59.08 ms/it, loss 0.445835
Finished training it 71680/76743 of epoch 2, 59.19 ms/it, loss 0.445546
Finished training it 71680/76743 of epoch 2, 59.09 ms/it, loss 0.444712
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581063.0
get out
0 has test check 2581063.0 and sample count 3274240
 accuracy 78.829 %, best 78.834 %, roc auc score 0.8020, best 0.8020
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581063.0
get out
3 has test check 2581063.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 59.26 ms/it, loss 0.444765
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581063.0
get out
1 has test check 2581063.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 59.23 ms/it, loss 0.444528
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 59.17 ms/it, loss 0.444669
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581063.0
get out
2 has test check 2581063.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 59.29 ms/it, loss 0.444537
Finished training it 73728/76743 of epoch 2, 59.39 ms/it, loss 0.443514
Finished training it 73728/76743 of epoch 2, 59.50 ms/it, loss 0.442629
Finished training it 73728/76743 of epoch 2, 59.39 ms/it, loss 0.443995
Finished training it 73728/76743 of epoch 2, 59.50 ms/it, loss 0.445315
Finished training it 74752/76743 of epoch 2, 59.44 ms/it, loss 0.441938
Finished training it 74752/76743 of epoch 2, 64.72 ms/it, loss 0.442244
Finished training it 74752/76743 of epoch 2, 64.65 ms/it, loss 0.442823
Finished training it 74752/76743 of epoch 2, 64.68 ms/it, loss 0.445607
Finished training it 75776/76743 of epoch 2, 59.43 ms/it, loss 0.443970
Finished training it 75776/76743 of epoch 2, 59.47 ms/it, loss 0.444147
Finished training it 75776/76743 of epoch 2, 59.32 ms/it, loss 0.444942
Finished training it 75776/76743 of epoch 2, 59.68 ms/it, loss 0.443720
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 59.96 ms/it, loss 0.444904
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 59.67 ms/it, loss 0.442261
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 60.04 ms/it, loss 0.444317
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 60.44 ms/it, loss 0.444042
Finished training it 2048/76743 of epoch 3, 59.35 ms/it, loss 0.444307
Finished training it 2048/76743 of epoch 3, 59.41 ms/it, loss 0.442841
Finished training it 2048/76743 of epoch 3, 59.29 ms/it, loss 0.442008
Finished training it 2048/76743 of epoch 3, 59.16 ms/it, loss 0.443172
Finished training it 3072/76743 of epoch 3, 59.80 ms/it, loss 0.444582
Finished training it 3072/76743 of epoch 3, 64.79 ms/it, loss 0.445318
Finished training it 3072/76743 of epoch 3, 60.33 ms/it, loss 0.440176
Finished training it 3072/76743 of epoch 3, 60.33 ms/it, loss 0.443731
Finished training it 4096/76743 of epoch 3, 59.69 ms/it, loss 0.443238
Finished training it 4096/76743 of epoch 3, 59.60 ms/it, loss 0.444476
Finished training it 4096/76743 of epoch 3, 59.49 ms/it, loss 0.442453
Finished training it 4096/76743 of epoch 3, 59.67 ms/it, loss 0.445100
Finished training it 5120/76743 of epoch 3, 59.56 ms/it, loss 0.443097
Finished training it 5120/76743 of epoch 3, 59.41 ms/it, loss 0.443069
Finished training it 5120/76743 of epoch 3, 59.48 ms/it, loss 0.443993
Finished training it 5120/76743 of epoch 3, 59.53 ms/it, loss 0.443701
Finished training it 6144/76743 of epoch 3, 59.60 ms/it, loss 0.445229
Finished training it 6144/76743 of epoch 3, 59.64 ms/it, loss 0.442875
Finished training it 6144/76743 of epoch 3, 59.43 ms/it, loss 0.446032
Finished training it 6144/76743 of epoch 3, 59.74 ms/it, loss 0.444173
Finished training it 7168/76743 of epoch 3, 59.31 ms/it, loss 0.440749
Finished training it 7168/76743 of epoch 3, 59.33 ms/it, loss 0.442160
Finished training it 7168/76743 of epoch 3, 59.20 ms/it, loss 0.441761
Finished training it 7168/76743 of epoch 3, 59.21 ms/it, loss 0.444273
Finished training it 8192/76743 of epoch 3, 59.48 ms/it, loss 0.445301
Finished training it 8192/76743 of epoch 3, 59.48 ms/it, loss 0.444422
Finished training it 8192/76743 of epoch 3, 59.37 ms/it, loss 0.443985
Finished training it 8192/76743 of epoch 3, 59.29 ms/it, loss 0.442523
Finished training it 9216/76743 of epoch 3, 59.05 ms/it, loss 0.443347
Finished training it 9216/76743 of epoch 3, 58.99 ms/it, loss 0.444546
Finished training it 9216/76743 of epoch 3, 59.07 ms/it, loss 0.442439
Finished training it 9216/76743 of epoch 3, 59.15 ms/it, loss 0.444116
Finished training it 10240/76743 of epoch 3, 59.61 ms/it, loss 0.442075
Finished training it 10240/76743 of epoch 3, 59.52 ms/it, loss 0.444288
Finished training it 10240/76743 of epoch 3, 59.64 ms/it, loss 0.444482
Finished training it 10240/76743 of epoch 3, 59.65 ms/it, loss 0.443896
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580893.0
get out
0 has test check 2580893.0 and sample count 3274240
 accuracy 78.824 %, best 78.834 %, roc auc score 0.8024, best 0.8024
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580893.0
get out
2 has test check 2580893.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 59.07 ms/it, loss 0.443194
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 59.06 ms/it, loss 0.444267
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580893.0
get out
3 has test check 2580893.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 59.10 ms/it, loss 0.445624
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580893.0
get out
1 has test check 2580893.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 59.06 ms/it, loss 0.444756
Finished training it 12288/76743 of epoch 3, 59.12 ms/it, loss 0.443913
Finished training it 12288/76743 of epoch 3, 59.11 ms/it, loss 0.441619
Finished training it 12288/76743 of epoch 3, 59.03 ms/it, loss 0.441362
Finished training it 12288/76743 of epoch 3, 59.17 ms/it, loss 0.441777
Finished training it 13312/76743 of epoch 3, 59.29 ms/it, loss 0.443460
Finished training it 13312/76743 of epoch 3, 59.31 ms/it, loss 0.440845
Finished training it 13312/76743 of epoch 3, 59.06 ms/it, loss 0.441928
Finished training it 13312/76743 of epoch 3, 59.22 ms/it, loss 0.443209
Finished training it 14336/76743 of epoch 3, 59.62 ms/it, loss 0.443841
Finished training it 14336/76743 of epoch 3, 59.75 ms/it, loss 0.441280
Finished training it 14336/76743 of epoch 3, 59.53 ms/it, loss 0.442427
Finished training it 14336/76743 of epoch 3, 59.59 ms/it, loss 0.441601
Finished training it 15360/76743 of epoch 3, 59.12 ms/it, loss 0.444737
Finished training it 15360/76743 of epoch 3, 59.08 ms/it, loss 0.444904
Finished training it 15360/76743 of epoch 3, 59.27 ms/it, loss 0.443383
Finished training it 15360/76743 of epoch 3, 59.00 ms/it, loss 0.444186
Finished training it 16384/76743 of epoch 3, 59.01 ms/it, loss 0.442096
Finished training it 16384/76743 of epoch 3, 58.95 ms/it, loss 0.441207
Finished training it 16384/76743 of epoch 3, 58.92 ms/it, loss 0.443210
Finished training it 16384/76743 of epoch 3, 58.95 ms/it, loss 0.443237
Finished training it 17408/76743 of epoch 3, 59.48 ms/it, loss 0.443988
Finished training it 17408/76743 of epoch 3, 59.50 ms/it, loss 0.439984
Finished training it 17408/76743 of epoch 3, 59.61 ms/it, loss 0.441963
Finished training it 17408/76743 of epoch 3, 59.56 ms/it, loss 0.441605
Finished training it 18432/76743 of epoch 3, 65.27 ms/it, loss 0.443401
Finished training it 18432/76743 of epoch 3, 60.66 ms/it, loss 0.444912
Finished training it 18432/76743 of epoch 3, 60.14 ms/it, loss 0.440528
Finished training it 18432/76743 of epoch 3, 60.87 ms/it, loss 0.445365
Finished training it 19456/76743 of epoch 3, 58.89 ms/it, loss 0.441532
Finished training it 19456/76743 of epoch 3, 58.94 ms/it, loss 0.443963
Finished training it 19456/76743 of epoch 3, 58.95 ms/it, loss 0.445382
Finished training it 19456/76743 of epoch 3, 58.92 ms/it, loss 0.443032
Finished training it 20480/76743 of epoch 3, 59.61 ms/it, loss 0.444020
Finished training it 20480/76743 of epoch 3, 59.54 ms/it, loss 0.442240
Finished training it 20480/76743 of epoch 3, 59.39 ms/it, loss 0.441862
Finished training it 20480/76743 of epoch 3, 59.38 ms/it, loss 0.438408
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581693.0
get out
0 has test check 2581693.0 and sample count 3274240
 accuracy 78.849 %, best 78.849 %, roc auc score 0.8024, best 0.8024
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581693.0
get out
3 has test check 2581693.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 59.41 ms/it, loss 0.438606
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581693.0
get out
2 has test check 2581693.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 59.31 ms/it, loss 0.443633
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 59.16 ms/it, loss 0.444535
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581693.0
get out
1 has test check 2581693.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 59.36 ms/it, loss 0.445909
Finished training it 22528/76743 of epoch 3, 59.29 ms/it, loss 0.443592
Finished training it 22528/76743 of epoch 3, 59.28 ms/it, loss 0.441471
Finished training it 22528/76743 of epoch 3, 59.21 ms/it, loss 0.442858
Finished training it 22528/76743 of epoch 3, 59.14 ms/it, loss 0.441894
Finished training it 23552/76743 of epoch 3, 60.37 ms/it, loss 0.441191
Finished training it 23552/76743 of epoch 3, 64.72 ms/it, loss 0.441453
Finished training it 23552/76743 of epoch 3, 64.84 ms/it, loss 0.445675
Finished training it 23552/76743 of epoch 3, 64.93 ms/it, loss 0.446358
Finished training it 24576/76743 of epoch 3, 59.62 ms/it, loss 0.441842
Finished training it 24576/76743 of epoch 3, 59.40 ms/it, loss 0.442791
Finished training it 24576/76743 of epoch 3, 59.54 ms/it, loss 0.443285
Finished training it 24576/76743 of epoch 3, 59.30 ms/it, loss 0.441613
Finished training it 25600/76743 of epoch 3, 59.36 ms/it, loss 0.441534
Finished training it 25600/76743 of epoch 3, 59.34 ms/it, loss 0.439933
Finished training it 25600/76743 of epoch 3, 59.44 ms/it, loss 0.441306
Finished training it 25600/76743 of epoch 3, 59.34 ms/it, loss 0.442447
Finished training it 26624/76743 of epoch 3, 59.62 ms/it, loss 0.441459
Finished training it 26624/76743 of epoch 3, 59.65 ms/it, loss 0.442329
Finished training it 26624/76743 of epoch 3, 59.66 ms/it, loss 0.442684
Finished training it 26624/76743 of epoch 3, 59.54 ms/it, loss 0.442655
Finished training it 27648/76743 of epoch 3, 59.53 ms/it, loss 0.443058
Finished training it 27648/76743 of epoch 3, 59.54 ms/it, loss 0.443545
Finished training it 27648/76743 of epoch 3, 59.72 ms/it, loss 0.441942
Finished training it 27648/76743 of epoch 3, 59.60 ms/it, loss 0.441116
Finished training it 28672/76743 of epoch 3, 59.27 ms/it, loss 0.445449
Finished training it 28672/76743 of epoch 3, 59.19 ms/it, loss 0.437838
Finished training it 28672/76743 of epoch 3, 59.15 ms/it, loss 0.441198
Finished training it 28672/76743 of epoch 3, 59.30 ms/it, loss 0.440724
Finished training it 29696/76743 of epoch 3, 58.98 ms/it, loss 0.439754
Finished training it 29696/76743 of epoch 3, 59.02 ms/it, loss 0.442156
Finished training it 29696/76743 of epoch 3, 58.96 ms/it, loss 0.443318
Finished training it 29696/76743 of epoch 3, 59.07 ms/it, loss 0.443587
Finished training it 30720/76743 of epoch 3, 58.79 ms/it, loss 0.445558
Finished training it 30720/76743 of epoch 3, 58.78 ms/it, loss 0.441668
Finished training it 30720/76743 of epoch 3, 58.70 ms/it, loss 0.441491
Finished training it 30720/76743 of epoch 3, 58.85 ms/it, loss 0.442941
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582039.0
get out
0 has test check 2582039.0 and sample count 3274240
 accuracy 78.859 %, best 78.859 %, roc auc score 0.8026, best 0.8026
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582039.0
get out
3 has test check 2582039.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 59.65 ms/it, loss 0.443270
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582039.0
get out
2 has test check 2582039.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 59.59 ms/it, loss 0.443221
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 59.52 ms/it, loss 0.440904
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582039.0
get out
1 has test check 2582039.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 59.64 ms/it, loss 0.444632
Finished training it 32768/76743 of epoch 3, 59.17 ms/it, loss 0.442757
Finished training it 32768/76743 of epoch 3, 59.04 ms/it, loss 0.443550
Finished training it 32768/76743 of epoch 3, 59.20 ms/it, loss 0.441141
Finished training it 32768/76743 of epoch 3, 59.23 ms/it, loss 0.442413
Finished training it 33792/76743 of epoch 3, 59.34 ms/it, loss 0.441435
Finished training it 33792/76743 of epoch 3, 59.52 ms/it, loss 0.445869
Finished training it 33792/76743 of epoch 3, 59.23 ms/it, loss 0.441754
Finished training it 33792/76743 of epoch 3, 59.24 ms/it, loss 0.443984
Finished training it 34816/76743 of epoch 3, 58.65 ms/it, loss 0.442687
Finished training it 34816/76743 of epoch 3, 58.80 ms/it, loss 0.442161
Finished training it 34816/76743 of epoch 3, 58.94 ms/it, loss 0.438401
Finished training it 34816/76743 of epoch 3, 58.87 ms/it, loss 0.440291
Finished training it 35840/76743 of epoch 3, 60.12 ms/it, loss 0.444674
Finished training it 35840/76743 of epoch 3, 59.96 ms/it, loss 0.440324
Finished training it 35840/76743 of epoch 3, 60.04 ms/it, loss 0.443000
Finished training it 35840/76743 of epoch 3, 60.02 ms/it, loss 0.441517
Finished training it 36864/76743 of epoch 3, 59.10 ms/it, loss 0.441783
Finished training it 36864/76743 of epoch 3, 58.96 ms/it, loss 0.442873
Finished training it 36864/76743 of epoch 3, 59.07 ms/it, loss 0.444640
Finished training it 36864/76743 of epoch 3, 58.79 ms/it, loss 0.442126
Finished training it 37888/76743 of epoch 3, 58.81 ms/it, loss 0.443112
Finished training it 37888/76743 of epoch 3, 58.98 ms/it, loss 0.441998
Finished training it 37888/76743 of epoch 3, 58.91 ms/it, loss 0.441521
Finished training it 37888/76743 of epoch 3, 58.89 ms/it, loss 0.443133
Finished training it 38912/76743 of epoch 3, 64.31 ms/it, loss 0.441882
Finished training it 38912/76743 of epoch 3, 59.62 ms/it, loss 0.443708
Finished training it 38912/76743 of epoch 3, 64.25 ms/it, loss 0.439973
Finished training it 38912/76743 of epoch 3, 64.43 ms/it, loss 0.441783
Finished training it 39936/76743 of epoch 3, 59.59 ms/it, loss 0.442606
Finished training it 39936/76743 of epoch 3, 59.59 ms/it, loss 0.441826
Finished training it 39936/76743 of epoch 3, 59.47 ms/it, loss 0.443666
Finished training it 39936/76743 of epoch 3, 59.68 ms/it, loss 0.441435
Finished training it 40960/76743 of epoch 3, 59.47 ms/it, loss 0.441908
Finished training it 40960/76743 of epoch 3, 59.34 ms/it, loss 0.442316
Finished training it 40960/76743 of epoch 3, 59.45 ms/it, loss 0.444002
Finished training it 40960/76743 of epoch 3, 59.44 ms/it, loss 0.443527
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582105.0
get out
0 has test check 2582105.0 and sample count 3274240
 accuracy 78.861 %, best 78.861 %, roc auc score 0.8025, best 0.8026
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 59.02 ms/it, loss 0.440800
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582105.0
get out
2 has test check 2582105.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 59.03 ms/it, loss 0.442908
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582105.0
get out
1 has test check 2582105.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 58.99 ms/it, loss 0.441909
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582105.0
get out
3 has test check 2582105.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 59.01 ms/it, loss 0.444113
Finished training it 43008/76743 of epoch 3, 59.63 ms/it, loss 0.441743
Finished training it 43008/76743 of epoch 3, 59.57 ms/it, loss 0.445052
Finished training it 43008/76743 of epoch 3, 59.64 ms/it, loss 0.442949
Finished training it 43008/76743 of epoch 3, 59.45 ms/it, loss 0.441124
Finished training it 44032/76743 of epoch 3, 59.27 ms/it, loss 0.443966
Finished training it 44032/76743 of epoch 3, 59.32 ms/it, loss 0.442212
Finished training it 44032/76743 of epoch 3, 59.35 ms/it, loss 0.442393
Finished training it 44032/76743 of epoch 3, 59.37 ms/it, loss 0.442821
Finished training it 45056/76743 of epoch 3, 59.68 ms/it, loss 0.442630
Finished training it 45056/76743 of epoch 3, 59.66 ms/it, loss 0.439797
Finished training it 45056/76743 of epoch 3, 59.57 ms/it, loss 0.441471
Finished training it 45056/76743 of epoch 3, 59.88 ms/it, loss 0.442387
Finished training it 46080/76743 of epoch 3, 59.07 ms/it, loss 0.442088
Finished training it 46080/76743 of epoch 3, 59.09 ms/it, loss 0.441228
Finished training it 46080/76743 of epoch 3, 58.94 ms/it, loss 0.442244
Finished training it 46080/76743 of epoch 3, 59.01 ms/it, loss 0.443779
Finished training it 47104/76743 of epoch 3, 59.93 ms/it, loss 0.443575
Finished training it 47104/76743 of epoch 3, 59.84 ms/it, loss 0.440819
Finished training it 47104/76743 of epoch 3, 60.07 ms/it, loss 0.439373
Finished training it 47104/76743 of epoch 3, 64.43 ms/it, loss 0.443436
Finished training it 48128/76743 of epoch 3, 60.20 ms/it, loss 0.441338
Finished training it 48128/76743 of epoch 3, 59.88 ms/it, loss 0.441063
Finished training it 48128/76743 of epoch 3, 59.94 ms/it, loss 0.440030
Finished training it 48128/76743 of epoch 3, 59.99 ms/it, loss 0.442805
Finished training it 49152/76743 of epoch 3, 59.48 ms/it, loss 0.441365
Finished training it 49152/76743 of epoch 3, 59.40 ms/it, loss 0.442879
Finished training it 49152/76743 of epoch 3, 59.35 ms/it, loss 0.446797
Finished training it 49152/76743 of epoch 3, 59.32 ms/it, loss 0.441025
Finished training it 50176/76743 of epoch 3, 59.59 ms/it, loss 0.440280
Finished training it 50176/76743 of epoch 3, 59.48 ms/it, loss 0.441449
Finished training it 50176/76743 of epoch 3, 59.60 ms/it, loss 0.440563
Finished training it 50176/76743 of epoch 3, 59.50 ms/it, loss 0.439643
Finished training it 51200/76743 of epoch 3, 59.13 ms/it, loss 0.441971
Finished training it 51200/76743 of epoch 3, 59.35 ms/it, loss 0.438520
Finished training it 51200/76743 of epoch 3, 59.20 ms/it, loss 0.443525
Finished training it 51200/76743 of epoch 3, 59.05 ms/it, loss 0.444002
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582965.0
get out
0 has test check 2582965.0 and sample count 3274240
 accuracy 78.887 %, best 78.887 %, roc auc score 0.8030, best 0.8030
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582965.0
get out
3 has test check 2582965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 59.09 ms/it, loss 0.440616
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582965.0
get out
2 has test check 2582965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 59.13 ms/it, loss 0.439571
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 59.12 ms/it, loss 0.442072
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582965.0
get out
1 has test check 2582965.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 59.27 ms/it, loss 0.441225
Finished training it 53248/76743 of epoch 3, 59.17 ms/it, loss 0.443081
Finished training it 53248/76743 of epoch 3, 59.28 ms/it, loss 0.442344
Finished training it 53248/76743 of epoch 3, 59.30 ms/it, loss 0.441227
Finished training it 53248/76743 of epoch 3, 59.20 ms/it, loss 0.440656
Finished training it 54272/76743 of epoch 3, 58.79 ms/it, loss 0.443484
Finished training it 54272/76743 of epoch 3, 58.74 ms/it, loss 0.439294
Finished training it 54272/76743 of epoch 3, 58.86 ms/it, loss 0.440127
Finished training it 54272/76743 of epoch 3, 58.98 ms/it, loss 0.441937
Finished training it 55296/76743 of epoch 3, 59.39 ms/it, loss 0.441428
Finished training it 55296/76743 of epoch 3, 59.28 ms/it, loss 0.440607
Finished training it 55296/76743 of epoch 3, 59.27 ms/it, loss 0.440851
Finished training it 55296/76743 of epoch 3, 59.11 ms/it, loss 0.440018
Finished training it 56320/76743 of epoch 3, 59.06 ms/it, loss 0.440250
Finished training it 56320/76743 of epoch 3, 59.15 ms/it, loss 0.443520
Finished training it 56320/76743 of epoch 3, 59.33 ms/it, loss 0.439875
Finished training it 56320/76743 of epoch 3, 59.08 ms/it, loss 0.441686
Finished training it 57344/76743 of epoch 3, 59.24 ms/it, loss 0.441482
Finished training it 57344/76743 of epoch 3, 59.38 ms/it, loss 0.443300
Finished training it 57344/76743 of epoch 3, 59.33 ms/it, loss 0.443517
Finished training it 57344/76743 of epoch 3, 59.30 ms/it, loss 0.443481
Finished training it 58368/76743 of epoch 3, 59.35 ms/it, loss 0.441350
Finished training it 58368/76743 of epoch 3, 59.14 ms/it, loss 0.442744
Finished training it 58368/76743 of epoch 3, 59.00 ms/it, loss 0.441480
Finished training it 58368/76743 of epoch 3, 59.25 ms/it, loss 0.443689
Finished training it 59392/76743 of epoch 3, 58.99 ms/it, loss 0.438895
Finished training it 59392/76743 of epoch 3, 58.77 ms/it, loss 0.442515
Finished training it 59392/76743 of epoch 3, 58.83 ms/it, loss 0.442259
Finished training it 59392/76743 of epoch 3, 58.73 ms/it, loss 0.441782
Finished training it 60416/76743 of epoch 3, 59.52 ms/it, loss 0.441402
Finished training it 60416/76743 of epoch 3, 59.46 ms/it, loss 0.442767
Finished training it 60416/76743 of epoch 3, 59.56 ms/it, loss 0.442284
Finished training it 60416/76743 of epoch 3, 59.50 ms/it, loss 0.441912
Finished training it 61440/76743 of epoch 3, 60.11 ms/it, loss 0.440946
Finished training it 61440/76743 of epoch 3, 60.23 ms/it, loss 0.441542
Finished training it 61440/76743 of epoch 3, 60.32 ms/it, loss 0.442859
Finished training it 61440/76743 of epoch 3, 60.04 ms/it, loss 0.442319
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582434.0
get out
0 has test check 2582434.0 and sample count 3274240
 accuracy 78.871 %, best 78.887 %, roc auc score 0.8030, best 0.8030
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582434.0
get out
1 has test check 2582434.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 59.29 ms/it, loss 0.443440
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 59.19 ms/it, loss 0.440149
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582434.0
get out
3 has test check 2582434.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 59.28 ms/it, loss 0.442939
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582434.0
get out
2 has test check 2582434.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 59.22 ms/it, loss 0.439649
Finished training it 63488/76743 of epoch 3, 59.48 ms/it, loss 0.444157
Finished training it 63488/76743 of epoch 3, 59.32 ms/it, loss 0.443566
Finished training it 63488/76743 of epoch 3, 59.44 ms/it, loss 0.445480
Finished training it 63488/76743 of epoch 3, 59.33 ms/it, loss 0.439545
Finished training it 64512/76743 of epoch 3, 59.15 ms/it, loss 0.439901
Finished training it 64512/76743 of epoch 3, 59.12 ms/it, loss 0.442621
Finished training it 64512/76743 of epoch 3, 59.24 ms/it, loss 0.442029
Finished training it 64512/76743 of epoch 3, 59.12 ms/it, loss 0.440504
Finished training it 65536/76743 of epoch 3, 65.23 ms/it, loss 0.438193
Finished training it 65536/76743 of epoch 3, 65.35 ms/it, loss 0.440316
Finished training it 65536/76743 of epoch 3, 65.27 ms/it, loss 0.442990
Finished training it 65536/76743 of epoch 3, 64.94 ms/it, loss 0.442654
Finished training it 66560/76743 of epoch 3, 59.28 ms/it, loss 0.441130
Finished training it 66560/76743 of epoch 3, 59.15 ms/it, loss 0.441857
Finished training it 66560/76743 of epoch 3, 59.23 ms/it, loss 0.440518
Finished training it 66560/76743 of epoch 3, 59.21 ms/it, loss 0.442169
Finished training it 67584/76743 of epoch 3, 59.62 ms/it, loss 0.439635
Finished training it 67584/76743 of epoch 3, 59.62 ms/it, loss 0.438737
Finished training it 67584/76743 of epoch 3, 59.60 ms/it, loss 0.441304
Finished training it 67584/76743 of epoch 3, 59.80 ms/it, loss 0.443165
Finished training it 68608/76743 of epoch 3, 59.57 ms/it, loss 0.440867
Finished training it 68608/76743 of epoch 3, 59.53 ms/it, loss 0.440918
Finished training it 68608/76743 of epoch 3, 59.54 ms/it, loss 0.443214
Finished training it 68608/76743 of epoch 3, 59.49 ms/it, loss 0.440126
Finished training it 69632/76743 of epoch 3, 59.84 ms/it, loss 0.438133
Finished training it 69632/76743 of epoch 3, 59.95 ms/it, loss 0.442342
Finished training it 69632/76743 of epoch 3, 60.13 ms/it, loss 0.443495
Finished training it 69632/76743 of epoch 3, 60.04 ms/it, loss 0.436951
Finished training it 70656/76743 of epoch 3, 59.48 ms/it, loss 0.443658
Finished training it 70656/76743 of epoch 3, 59.48 ms/it, loss 0.439705
Finished training it 70656/76743 of epoch 3, 59.45 ms/it, loss 0.443272
Finished training it 70656/76743 of epoch 3, 59.49 ms/it, loss 0.441315
Finished training it 71680/76743 of epoch 3, 59.35 ms/it, loss 0.443034
Finished training it 71680/76743 of epoch 3, 59.56 ms/it, loss 0.441665
Finished training it 71680/76743 of epoch 3, 59.50 ms/it, loss 0.441930
Finished training it 71680/76743 of epoch 3, 59.55 ms/it, loss 0.442782
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582588.0
get out
0 has test check 2582588.0 and sample count 3274240
 accuracy 78.876 %, best 78.887 %, roc auc score 0.8032, best 0.8032
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582588.0
get out
3 has test check 2582588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 59.07 ms/it, loss 0.441879
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 59.11 ms/it, loss 0.441819
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582588.0
get out
2 has test check 2582588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 59.21 ms/it, loss 0.441826
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582588.0
get out
1 has test check 2582588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 59.01 ms/it, loss 0.441695
Finished training it 73728/76743 of epoch 3, 59.61 ms/it, loss 0.439806
Finished training it 73728/76743 of epoch 3, 59.63 ms/it, loss 0.442187
Finished training it 73728/76743 of epoch 3, 59.53 ms/it, loss 0.441134
Finished training it 73728/76743 of epoch 3, 59.55 ms/it, loss 0.440355
Finished training it 74752/76743 of epoch 3, 59.49 ms/it, loss 0.442675
Finished training it 74752/76743 of epoch 3, 59.33 ms/it, loss 0.439867
Finished training it 74752/76743 of epoch 3, 59.27 ms/it, loss 0.439167
Finished training it 74752/76743 of epoch 3, 59.15 ms/it, loss 0.438945
Finished training it 75776/76743 of epoch 3, 59.00 ms/it, loss 0.440656
Finished training it 75776/76743 of epoch 3, 59.14 ms/it, loss 0.440892
Finished training it 75776/76743 of epoch 3, 59.10 ms/it, loss 0.441156
Finished training it 75776/76743 of epoch 3, 58.97 ms/it, loss 0.441784
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 60.21 ms/it, loss 0.441799
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 60.25 ms/it, loss 0.441539
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 60.69 ms/it, loss 0.441201
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 59.89 ms/it, loss 0.439313
Finished training it 2048/76743 of epoch 4, 59.40 ms/it, loss 0.441635
Finished training it 2048/76743 of epoch 4, 59.33 ms/it, loss 0.439705
Finished training it 2048/76743 of epoch 4, 59.13 ms/it, loss 0.439065
Finished training it 2048/76743 of epoch 4, 59.32 ms/it, loss 0.440381
Finished training it 3072/76743 of epoch 4, 60.32 ms/it, loss 0.437075
Finished training it 3072/76743 of epoch 4, 65.07 ms/it, loss 0.442187
Finished training it 3072/76743 of epoch 4, 60.30 ms/it, loss 0.441542
Finished training it 3072/76743 of epoch 4, 60.52 ms/it, loss 0.440661
Finished training it 4096/76743 of epoch 4, 59.06 ms/it, loss 0.439303
Finished training it 4096/76743 of epoch 4, 59.19 ms/it, loss 0.441866
Finished training it 4096/76743 of epoch 4, 59.00 ms/it, loss 0.441102
Finished training it 4096/76743 of epoch 4, 59.05 ms/it, loss 0.440210
Finished training it 5120/76743 of epoch 4, 59.61 ms/it, loss 0.440513
Finished training it 5120/76743 of epoch 4, 59.59 ms/it, loss 0.441279
Finished training it 5120/76743 of epoch 4, 59.57 ms/it, loss 0.440230
Finished training it 5120/76743 of epoch 4, 59.59 ms/it, loss 0.439902
Finished training it 6144/76743 of epoch 4, 59.70 ms/it, loss 0.443024
Finished training it 6144/76743 of epoch 4, 59.67 ms/it, loss 0.441180
Finished training it 6144/76743 of epoch 4, 59.63 ms/it, loss 0.440037
Finished training it 6144/76743 of epoch 4, 59.94 ms/it, loss 0.442045
Finished training it 7168/76743 of epoch 4, 59.13 ms/it, loss 0.439028
Finished training it 7168/76743 of epoch 4, 58.96 ms/it, loss 0.438927
Finished training it 7168/76743 of epoch 4, 59.11 ms/it, loss 0.441392
Finished training it 7168/76743 of epoch 4, 59.02 ms/it, loss 0.437636
Finished training it 8192/76743 of epoch 4, 59.38 ms/it, loss 0.441252
Finished training it 8192/76743 of epoch 4, 59.22 ms/it, loss 0.439526
Finished training it 8192/76743 of epoch 4, 59.21 ms/it, loss 0.442268
Finished training it 8192/76743 of epoch 4, 59.30 ms/it, loss 0.440998
Finished training it 9216/76743 of epoch 4, 59.12 ms/it, loss 0.441070
Finished training it 9216/76743 of epoch 4, 59.14 ms/it, loss 0.439198
Finished training it 9216/76743 of epoch 4, 59.35 ms/it, loss 0.440492
Finished training it 9216/76743 of epoch 4, 59.17 ms/it, loss 0.441452
Finished training it 10240/76743 of epoch 4, 59.68 ms/it, loss 0.441445
Finished training it 10240/76743 of epoch 4, 59.66 ms/it, loss 0.441111
Finished training it 10240/76743 of epoch 4, 59.71 ms/it, loss 0.439150
Finished training it 10240/76743 of epoch 4, 59.76 ms/it, loss 0.441113
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582626.0
get out
0 has test check 2582626.0 and sample count 3274240
 accuracy 78.877 %, best 78.887 %, roc auc score 0.8032, best 0.8032
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582626.0
get out
3 has test check 2582626.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 59.24 ms/it, loss 0.442604
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 59.08 ms/it, loss 0.441144
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582626.0
get out
1 has test check 2582626.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 59.21 ms/it, loss 0.441397
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582626.0
get out
2 has test check 2582626.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 59.13 ms/it, loss 0.440414
Finished training it 12288/76743 of epoch 4, 59.22 ms/it, loss 0.440735
Finished training it 12288/76743 of epoch 4, 58.96 ms/it, loss 0.438231
Finished training it 12288/76743 of epoch 4, 59.12 ms/it, loss 0.438642
Finished training it 12288/76743 of epoch 4, 59.11 ms/it, loss 0.438640
Finished training it 13312/76743 of epoch 4, 64.68 ms/it, loss 0.437916
Finished training it 13312/76743 of epoch 4, 64.79 ms/it, loss 0.440030
Finished training it 13312/76743 of epoch 4, 64.68 ms/it, loss 0.439188
Finished training it 13312/76743 of epoch 4, 59.95 ms/it, loss 0.440328
Finished training it 14336/76743 of epoch 4, 59.61 ms/it, loss 0.439512
Finished training it 14336/76743 of epoch 4, 59.74 ms/it, loss 0.438916
Finished training it 14336/76743 of epoch 4, 59.69 ms/it, loss 0.441029
Finished training it 14336/76743 of epoch 4, 59.74 ms/it, loss 0.438128
Finished training it 15360/76743 of epoch 4, 59.61 ms/it, loss 0.441962
Finished training it 15360/76743 of epoch 4, 59.54 ms/it, loss 0.441797
Finished training it 15360/76743 of epoch 4, 59.52 ms/it, loss 0.440064
Finished training it 15360/76743 of epoch 4, 59.53 ms/it, loss 0.441232
Finished training it 16384/76743 of epoch 4, 59.53 ms/it, loss 0.440018
Finished training it 16384/76743 of epoch 4, 59.49 ms/it, loss 0.440003
Finished training it 16384/76743 of epoch 4, 59.56 ms/it, loss 0.438309
Finished training it 16384/76743 of epoch 4, 59.63 ms/it, loss 0.439120
Finished training it 17408/76743 of epoch 4, 59.30 ms/it, loss 0.438627
Finished training it 17408/76743 of epoch 4, 59.35 ms/it, loss 0.438690
Finished training it 17408/76743 of epoch 4, 59.24 ms/it, loss 0.441024
Finished training it 17408/76743 of epoch 4, 59.47 ms/it, loss 0.436636
Finished training it 18432/76743 of epoch 4, 59.88 ms/it, loss 0.437469
Finished training it 18432/76743 of epoch 4, 64.52 ms/it, loss 0.440381
Finished training it 18432/76743 of epoch 4, 59.87 ms/it, loss 0.442238
Finished training it 18432/76743 of epoch 4, 59.75 ms/it, loss 0.442059
Finished training it 19456/76743 of epoch 4, 59.08 ms/it, loss 0.440908
Finished training it 19456/76743 of epoch 4, 59.10 ms/it, loss 0.438237
Finished training it 19456/76743 of epoch 4, 59.10 ms/it, loss 0.439547
Finished training it 19456/76743 of epoch 4, 59.09 ms/it, loss 0.442344
Finished training it 20480/76743 of epoch 4, 59.69 ms/it, loss 0.439444
Finished training it 20480/76743 of epoch 4, 59.68 ms/it, loss 0.441021
Finished training it 20480/76743 of epoch 4, 59.62 ms/it, loss 0.438931
Finished training it 20480/76743 of epoch 4, 59.59 ms/it, loss 0.435142
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582476.0
get out
0 has test check 2582476.0 and sample count 3274240
 accuracy 78.873 %, best 78.887 %, roc auc score 0.8029, best 0.8032
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582476.0
get out
1 has test check 2582476.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 59.11 ms/it, loss 0.442951
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582476.0
get out
2 has test check 2582476.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 59.13 ms/it, loss 0.440501
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582476.0
get out
3 has test check 2582476.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 59.05 ms/it, loss 0.435940
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 59.10 ms/it, loss 0.441660
Finished training it 22528/76743 of epoch 4, 59.27 ms/it, loss 0.439639
Finished training it 22528/76743 of epoch 4, 59.36 ms/it, loss 0.440381
Finished training it 22528/76743 of epoch 4, 59.16 ms/it, loss 0.438478
Finished training it 22528/76743 of epoch 4, 59.25 ms/it, loss 0.438769
Finished training it 23552/76743 of epoch 4, 59.74 ms/it, loss 0.442907
Finished training it 23552/76743 of epoch 4, 59.33 ms/it, loss 0.438465
Finished training it 23552/76743 of epoch 4, 59.50 ms/it, loss 0.437968
Finished training it 23552/76743 of epoch 4, 59.56 ms/it, loss 0.443243
Finished training it 24576/76743 of epoch 4, 59.22 ms/it, loss 0.438918
Finished training it 24576/76743 of epoch 4, 59.37 ms/it, loss 0.440475
Finished training it 24576/76743 of epoch 4, 59.01 ms/it, loss 0.440048
Finished training it 24576/76743 of epoch 4, 58.99 ms/it, loss 0.438868
Finished training it 25600/76743 of epoch 4, 59.06 ms/it, loss 0.437720
Finished training it 25600/76743 of epoch 4, 59.04 ms/it, loss 0.438145
Finished training it 25600/76743 of epoch 4, 59.32 ms/it, loss 0.439104
Finished training it 25600/76743 of epoch 4, 59.11 ms/it, loss 0.437287
Finished training it 26624/76743 of epoch 4, 58.85 ms/it, loss 0.438171
Finished training it 26624/76743 of epoch 4, 58.75 ms/it, loss 0.438998
Finished training it 26624/76743 of epoch 4, 58.88 ms/it, loss 0.439629
Finished training it 26624/76743 of epoch 4, 58.77 ms/it, loss 0.439444
Finished training it 27648/76743 of epoch 4, 59.36 ms/it, loss 0.439862
Finished training it 27648/76743 of epoch 4, 59.19 ms/it, loss 0.437897
Finished training it 27648/76743 of epoch 4, 59.20 ms/it, loss 0.438800
Finished training it 27648/76743 of epoch 4, 59.24 ms/it, loss 0.440262
Finished training it 28672/76743 of epoch 4, 59.55 ms/it, loss 0.437580
Finished training it 28672/76743 of epoch 4, 59.59 ms/it, loss 0.442726
Finished training it 28672/76743 of epoch 4, 59.54 ms/it, loss 0.434644
Finished training it 28672/76743 of epoch 4, 59.52 ms/it, loss 0.438407
Finished training it 29696/76743 of epoch 4, 59.58 ms/it, loss 0.438806
Finished training it 29696/76743 of epoch 4, 59.85 ms/it, loss 0.440483
Finished training it 29696/76743 of epoch 4, 59.72 ms/it, loss 0.440265
Finished training it 29696/76743 of epoch 4, 59.78 ms/it, loss 0.436693
Finished training it 30720/76743 of epoch 4, 59.24 ms/it, loss 0.442134
Finished training it 30720/76743 of epoch 4, 59.48 ms/it, loss 0.439332
Finished training it 30720/76743 of epoch 4, 59.56 ms/it, loss 0.438337
Finished training it 30720/76743 of epoch 4, 59.50 ms/it, loss 0.438665
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582668.0
get out
0 has test check 2582668.0 and sample count 3274240
 accuracy 78.878 %, best 78.887 %, roc auc score 0.8031, best 0.8032
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 59.69 ms/it, loss 0.437854
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582668.0
get out
3 has test check 2582668.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 59.82 ms/it, loss 0.439634
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582668.0
get out
2 has test check 2582668.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 59.73 ms/it, loss 0.439866
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582668.0
get out
1 has test check 2582668.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 59.62 ms/it, loss 0.441435
Finished training it 32768/76743 of epoch 4, 59.08 ms/it, loss 0.439988
Finished training it 32768/76743 of epoch 4, 59.15 ms/it, loss 0.438955
Finished training it 32768/76743 of epoch 4, 59.18 ms/it, loss 0.439708
Finished training it 32768/76743 of epoch 4, 59.21 ms/it, loss 0.438162
Finished training it 33792/76743 of epoch 4, 65.13 ms/it, loss 0.440632
Finished training it 33792/76743 of epoch 4, 65.00 ms/it, loss 0.442597
Finished training it 33792/76743 of epoch 4, 60.08 ms/it, loss 0.438133
Finished training it 33792/76743 of epoch 4, 64.96 ms/it, loss 0.438757
Finished training it 34816/76743 of epoch 4, 59.33 ms/it, loss 0.435134
Finished training it 34816/76743 of epoch 4, 59.31 ms/it, loss 0.436864
Finished training it 34816/76743 of epoch 4, 59.27 ms/it, loss 0.438919
Finished training it 34816/76743 of epoch 4, 59.30 ms/it, loss 0.439595
Finished training it 35840/76743 of epoch 4, 59.24 ms/it, loss 0.438354
Finished training it 35840/76743 of epoch 4, 58.99 ms/it, loss 0.436699
Finished training it 35840/76743 of epoch 4, 59.11 ms/it, loss 0.439537
Finished training it 35840/76743 of epoch 4, 59.30 ms/it, loss 0.441008
Finished training it 36864/76743 of epoch 4, 59.26 ms/it, loss 0.438281
Finished training it 36864/76743 of epoch 4, 59.46 ms/it, loss 0.439883
Finished training it 36864/76743 of epoch 4, 59.17 ms/it, loss 0.439084
Finished training it 36864/76743 of epoch 4, 59.38 ms/it, loss 0.441156
Finished training it 37888/76743 of epoch 4, 59.57 ms/it, loss 0.438775
Finished training it 37888/76743 of epoch 4, 59.42 ms/it, loss 0.437939
Finished training it 37888/76743 of epoch 4, 59.63 ms/it, loss 0.439709
Finished training it 37888/76743 of epoch 4, 59.51 ms/it, loss 0.439868
Finished training it 38912/76743 of epoch 4, 59.03 ms/it, loss 0.438697
Finished training it 38912/76743 of epoch 4, 59.08 ms/it, loss 0.436643
Finished training it 38912/76743 of epoch 4, 59.05 ms/it, loss 0.440375
Finished training it 38912/76743 of epoch 4, 58.99 ms/it, loss 0.438498
Finished training it 39936/76743 of epoch 4, 59.20 ms/it, loss 0.438026
Finished training it 39936/76743 of epoch 4, 58.95 ms/it, loss 0.439469
Finished training it 39936/76743 of epoch 4, 59.20 ms/it, loss 0.438643
Finished training it 39936/76743 of epoch 4, 59.09 ms/it, loss 0.440732
Finished training it 40960/76743 of epoch 4, 59.33 ms/it, loss 0.438491
Finished training it 40960/76743 of epoch 4, 59.41 ms/it, loss 0.440777
Finished training it 40960/76743 of epoch 4, 59.41 ms/it, loss 0.438701
Finished training it 40960/76743 of epoch 4, 59.43 ms/it, loss 0.440149
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583133.0
get out
0 has test check 2583133.0 and sample count 3274240
 accuracy 78.893 %, best 78.893 %, roc auc score 0.8031, best 0.8032
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583133.0
get out
1 has test check 2583133.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 59.52 ms/it, loss 0.438649
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583133.0
get out
3 has test check 2583133.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 59.73 ms/it, loss 0.440991
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583133.0
get out
2 has test check 2583133.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 59.47 ms/it, loss 0.439430
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 59.34 ms/it, loss 0.437444
Finished training it 43008/76743 of epoch 4, 59.21 ms/it, loss 0.437753
Finished training it 43008/76743 of epoch 4, 59.17 ms/it, loss 0.441655
Finished training it 43008/76743 of epoch 4, 59.32 ms/it, loss 0.438484
Finished training it 43008/76743 of epoch 4, 59.13 ms/it, loss 0.439616
Finished training it 44032/76743 of epoch 4, 58.80 ms/it, loss 0.438876
Finished training it 44032/76743 of epoch 4, 58.97 ms/it, loss 0.438943
Finished training it 44032/76743 of epoch 4, 59.00 ms/it, loss 0.440933
Finished training it 44032/76743 of epoch 4, 58.96 ms/it, loss 0.439315
Finished training it 45056/76743 of epoch 4, 59.40 ms/it, loss 0.436449
Finished training it 45056/76743 of epoch 4, 59.29 ms/it, loss 0.438199
Finished training it 45056/76743 of epoch 4, 59.28 ms/it, loss 0.439406
Finished training it 45056/76743 of epoch 4, 59.45 ms/it, loss 0.438708
Finished training it 46080/76743 of epoch 4, 59.37 ms/it, loss 0.438209
Finished training it 46080/76743 of epoch 4, 59.45 ms/it, loss 0.439346
Finished training it 46080/76743 of epoch 4, 59.35 ms/it, loss 0.439138
Finished training it 46080/76743 of epoch 4, 59.37 ms/it, loss 0.440799
Finished training it 47104/76743 of epoch 4, 59.55 ms/it, loss 0.437495
Finished training it 47104/76743 of epoch 4, 59.77 ms/it, loss 0.435943
Finished training it 47104/76743 of epoch 4, 59.67 ms/it, loss 0.440511
Finished training it 47104/76743 of epoch 4, 64.42 ms/it, loss 0.440109
Finished training it 48128/76743 of epoch 4, 59.53 ms/it, loss 0.438085
Finished training it 48128/76743 of epoch 4, 59.55 ms/it, loss 0.437596
Finished training it 48128/76743 of epoch 4, 59.48 ms/it, loss 0.436614
Finished training it 48128/76743 of epoch 4, 59.86 ms/it, loss 0.439638
Finished training it 49152/76743 of epoch 4, 59.40 ms/it, loss 0.437761
Finished training it 49152/76743 of epoch 4, 59.37 ms/it, loss 0.443337
Finished training it 49152/76743 of epoch 4, 59.41 ms/it, loss 0.437336
Finished training it 49152/76743 of epoch 4, 59.46 ms/it, loss 0.439131
Finished training it 50176/76743 of epoch 4, 59.39 ms/it, loss 0.438051
Finished training it 50176/76743 of epoch 4, 59.41 ms/it, loss 0.436518
Finished training it 50176/76743 of epoch 4, 59.34 ms/it, loss 0.436439
Finished training it 50176/76743 of epoch 4, 59.43 ms/it, loss 0.436962
Finished training it 51200/76743 of epoch 4, 58.94 ms/it, loss 0.439991
Finished training it 51200/76743 of epoch 4, 58.92 ms/it, loss 0.438336
Finished training it 51200/76743 of epoch 4, 58.90 ms/it, loss 0.439737
Finished training it 51200/76743 of epoch 4, 58.78 ms/it, loss 0.435072
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583291.0
get out
0 has test check 2583291.0 and sample count 3274240
 accuracy 78.897 %, best 78.897 %, roc auc score 0.8031, best 0.8032
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583291.0
get out
2 has test check 2583291.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 59.81 ms/it, loss 0.436015
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583291.0
get out
3 has test check 2583291.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 59.65 ms/it, loss 0.437258
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583291.0
get out
1 has test check 2583291.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 59.74 ms/it, loss 0.437745
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 59.68 ms/it, loss 0.438649
Finished training it 53248/76743 of epoch 4, 59.41 ms/it, loss 0.437169
Finished training it 53248/76743 of epoch 4, 59.40 ms/it, loss 0.439473
Finished training it 53248/76743 of epoch 4, 59.37 ms/it, loss 0.437840
Finished training it 53248/76743 of epoch 4, 59.32 ms/it, loss 0.438995
Finished training it 54272/76743 of epoch 4, 65.15 ms/it, loss 0.435646
Finished training it 54272/76743 of epoch 4, 65.19 ms/it, loss 0.439487
Finished training it 54272/76743 of epoch 4, 60.32 ms/it, loss 0.438030
Finished training it 54272/76743 of epoch 4, 65.18 ms/it, loss 0.437002
Finished training it 55296/76743 of epoch 4, 59.16 ms/it, loss 0.437351
Finished training it 55296/76743 of epoch 4, 59.09 ms/it, loss 0.437146
Finished training it 55296/76743 of epoch 4, 59.07 ms/it, loss 0.436625
Finished training it 55296/76743 of epoch 4, 59.21 ms/it, loss 0.437905
Finished training it 56320/76743 of epoch 4, 59.86 ms/it, loss 0.435776
Finished training it 56320/76743 of epoch 4, 59.72 ms/it, loss 0.436230
Finished training it 56320/76743 of epoch 4, 59.80 ms/it, loss 0.437907
Finished training it 56320/76743 of epoch 4, 59.71 ms/it, loss 0.439550
Finished training it 57344/76743 of epoch 4, 59.59 ms/it, loss 0.439725
Finished training it 57344/76743 of epoch 4, 59.61 ms/it, loss 0.439781
Finished training it 57344/76743 of epoch 4, 59.61 ms/it, loss 0.437447
Finished training it 57344/76743 of epoch 4, 59.59 ms/it, loss 0.439231
Finished training it 58368/76743 of epoch 4, 59.41 ms/it, loss 0.437796
Finished training it 58368/76743 of epoch 4, 59.42 ms/it, loss 0.440045
Finished training it 58368/76743 of epoch 4, 59.28 ms/it, loss 0.438957
Finished training it 58368/76743 of epoch 4, 59.30 ms/it, loss 0.437615
Finished training it 59392/76743 of epoch 4, 58.87 ms/it, loss 0.438603
Finished training it 59392/76743 of epoch 4, 59.06 ms/it, loss 0.435005
Finished training it 59392/76743 of epoch 4, 59.01 ms/it, loss 0.438893
Finished training it 59392/76743 of epoch 4, 58.95 ms/it, loss 0.438180
Finished training it 60416/76743 of epoch 4, 59.77 ms/it, loss 0.438286
Finished training it 60416/76743 of epoch 4, 59.75 ms/it, loss 0.438217
Finished training it 60416/76743 of epoch 4, 59.68 ms/it, loss 0.439659
Finished training it 60416/76743 of epoch 4, 59.77 ms/it, loss 0.437565
Finished training it 61440/76743 of epoch 4, 58.98 ms/it, loss 0.437897
Finished training it 61440/76743 of epoch 4, 58.99 ms/it, loss 0.438397
Finished training it 61440/76743 of epoch 4, 59.07 ms/it, loss 0.437316
Finished training it 61440/76743 of epoch 4, 58.97 ms/it, loss 0.439333
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582583.0
get out
0 has test check 2582583.0 and sample count 3274240
 accuracy 78.876 %, best 78.897 %, roc auc score 0.8032, best 0.8032
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582583.0
get out
3 has test check 2582583.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 59.25 ms/it, loss 0.438882
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582583.0
get out
1 has test check 2582583.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 59.36 ms/it, loss 0.439531
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 59.19 ms/it, loss 0.436347
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582583.0
get out
2 has test check 2582583.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 59.33 ms/it, loss 0.435545
Finished training it 63488/76743 of epoch 4, 59.37 ms/it, loss 0.440455
Finished training it 63488/76743 of epoch 4, 59.09 ms/it, loss 0.435614
Finished training it 63488/76743 of epoch 4, 59.31 ms/it, loss 0.441926
Finished training it 63488/76743 of epoch 4, 59.12 ms/it, loss 0.439910
Finished training it 64512/76743 of epoch 4, 59.47 ms/it, loss 0.436394
Finished training it 64512/76743 of epoch 4, 59.36 ms/it, loss 0.438329
Finished training it 64512/76743 of epoch 4, 59.49 ms/it, loss 0.438152
Finished training it 64512/76743 of epoch 4, 59.51 ms/it, loss 0.436053
Finished training it 65536/76743 of epoch 4, 60.84 ms/it, loss 0.438647
Finished training it 65536/76743 of epoch 4, 60.61 ms/it, loss 0.434123
Finished training it 65536/76743 of epoch 4, 65.35 ms/it, loss 0.438889
Finished training it 65536/76743 of epoch 4, 60.26 ms/it, loss 0.436131
Finished training it 66560/76743 of epoch 4, 59.20 ms/it, loss 0.437279
Finished training it 66560/76743 of epoch 4, 59.27 ms/it, loss 0.437920
Finished training it 66560/76743 of epoch 4, 59.25 ms/it, loss 0.438563
Finished training it 66560/76743 of epoch 4, 59.35 ms/it, loss 0.436784
Finished training it 67584/76743 of epoch 4, 59.10 ms/it, loss 0.437612
Finished training it 67584/76743 of epoch 4, 59.03 ms/it, loss 0.439470
Finished training it 67584/76743 of epoch 4, 59.26 ms/it, loss 0.435596
Finished training it 67584/76743 of epoch 4, 58.91 ms/it, loss 0.434768
Finished training it 68608/76743 of epoch 4, 59.49 ms/it, loss 0.436620
Finished training it 68608/76743 of epoch 4, 59.54 ms/it, loss 0.436568
Finished training it 68608/76743 of epoch 4, 59.43 ms/it, loss 0.436404
Finished training it 68608/76743 of epoch 4, 59.51 ms/it, loss 0.438428
Finished training it 69632/76743 of epoch 4, 59.45 ms/it, loss 0.438113
Finished training it 69632/76743 of epoch 4, 59.72 ms/it, loss 0.433128
Finished training it 69632/76743 of epoch 4, 59.55 ms/it, loss 0.433965
Finished training it 69632/76743 of epoch 4, 59.62 ms/it, loss 0.439254
Finished training it 70656/76743 of epoch 4, 59.22 ms/it, loss 0.437985
Finished training it 70656/76743 of epoch 4, 59.31 ms/it, loss 0.439614
Finished training it 70656/76743 of epoch 4, 59.18 ms/it, loss 0.435825
Finished training it 70656/76743 of epoch 4, 59.18 ms/it, loss 0.439173
Finished training it 71680/76743 of epoch 4, 59.33 ms/it, loss 0.437962
Finished training it 71680/76743 of epoch 4, 59.26 ms/it, loss 0.437851
Finished training it 71680/76743 of epoch 4, 59.26 ms/it, loss 0.438874
Finished training it 71680/76743 of epoch 4, 59.33 ms/it, loss 0.439119
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582434.0
get out
0 has test check 2582434.0 and sample count 3274240
 accuracy 78.871 %, best 78.897 %, roc auc score 0.8031, best 0.8032
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582434.0
get out
2 has test check 2582434.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 59.18 ms/it, loss 0.438113
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 59.01 ms/it, loss 0.437426
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582434.0
get out
3 has test check 2582434.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 59.08 ms/it, loss 0.437873
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582434.0
get out
1 has test check 2582434.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 59.14 ms/it, loss 0.437586
Finished training it 73728/76743 of epoch 4, 59.33 ms/it, loss 0.437369
Finished training it 73728/76743 of epoch 4, 59.17 ms/it, loss 0.435781
Finished training it 73728/76743 of epoch 4, 59.31 ms/it, loss 0.438249
Finished training it 73728/76743 of epoch 4, 59.30 ms/it, loss 0.435933
Finished training it 74752/76743 of epoch 4, 64.87 ms/it, loss 0.435308
Finished training it 74752/76743 of epoch 4, 60.72 ms/it, loss 0.434980
Finished training it 74752/76743 of epoch 4, 64.75 ms/it, loss 0.435923
Finished training it 74752/76743 of epoch 4, 64.85 ms/it, loss 0.438598
Finished training it 75776/76743 of epoch 4, 59.49 ms/it, loss 0.436743
Finished training it 75776/76743 of epoch 4, 59.45 ms/it, loss 0.437769
Finished training it 75776/76743 of epoch 4, 59.34 ms/it, loss 0.437217
Finished training it 75776/76743 of epoch 4, 59.41 ms/it, loss 0.436687
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582774.0
get out
0 has test check 2582774.0 and sample count 3274240
 accuracy 78.882 %, best 78.897 %, roc auc score 0.8030, best 0.8032
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582774.0
get out
3 has test check 2582774.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582774.0
get out
2 has test check 2582774.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582774.0
get out
1 has test check 2582774.0 and sample count 3274240
