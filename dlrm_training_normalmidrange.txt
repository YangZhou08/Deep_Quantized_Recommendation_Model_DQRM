Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
8 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 31.64 ms/it, loss 0.518372
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
8 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 31.05 ms/it, loss 0.517877
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
8 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 31.67 ms/it, loss 0.520036
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
8 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 31.04 ms/it, loss 0.515134
Finished training it 2048/76743 of epoch 0, 30.43 ms/it, loss 0.500202
Finished training it 2048/76743 of epoch 0, 30.02 ms/it, loss 0.498622
Finished training it 2048/76743 of epoch 0, 30.53 ms/it, loss 0.501910
Finished training it 2048/76743 of epoch 0, 30.29 ms/it, loss 0.500658
Finished training it 3072/76743 of epoch 0, 29.69 ms/it, loss 0.492760
Finished training it 3072/76743 of epoch 0, 29.90 ms/it, loss 0.491574
Finished training it 3072/76743 of epoch 0, 29.55 ms/it, loss 0.494048
Finished training it 3072/76743 of epoch 0, 29.87 ms/it, loss 0.492341
Finished training it 4096/76743 of epoch 0, 29.83 ms/it, loss 0.487381
Finished training it 4096/76743 of epoch 0, 29.55 ms/it, loss 0.487797
Finished training it 4096/76743 of epoch 0, 29.74 ms/it, loss 0.486847
Finished training it 4096/76743 of epoch 0, 29.62 ms/it, loss 0.485927
Finished training it 5120/76743 of epoch 0, 29.98 ms/it, loss 0.480567
Finished training it 5120/76743 of epoch 0, 29.75 ms/it, loss 0.480535
Finished training it 5120/76743 of epoch 0, 29.60 ms/it, loss 0.480482
Finished training it 5120/76743 of epoch 0, 29.51 ms/it, loss 0.481831
Finished training it 6144/76743 of epoch 0, 29.98 ms/it, loss 0.478821
Finished training it 6144/76743 of epoch 0, 29.74 ms/it, loss 0.479157
Finished training it 6144/76743 of epoch 0, 29.96 ms/it, loss 0.478908
Finished training it 6144/76743 of epoch 0, 29.90 ms/it, loss 0.478351
Finished training it 7168/76743 of epoch 0, 30.02 ms/it, loss 0.474661
Finished training it 7168/76743 of epoch 0, 29.77 ms/it, loss 0.478873
Finished training it 7168/76743 of epoch 0, 29.84 ms/it, loss 0.478088
Finished training it 7168/76743 of epoch 0, 29.79 ms/it, loss 0.476304
Finished training it 8192/76743 of epoch 0, 29.47 ms/it, loss 0.474951
Finished training it 8192/76743 of epoch 0, 29.73 ms/it, loss 0.474497
Finished training it 8192/76743 of epoch 0, 29.64 ms/it, loss 0.473176
Finished training it 8192/76743 of epoch 0, 29.47 ms/it, loss 0.474412
Finished training it 9216/76743 of epoch 0, 29.36 ms/it, loss 0.474090
Finished training it 9216/76743 of epoch 0, 29.61 ms/it, loss 0.474239
Finished training it 9216/76743 of epoch 0, 29.51 ms/it, loss 0.471100
Finished training it 9216/76743 of epoch 0, 29.57 ms/it, loss 0.472482
Finished training it 10240/76743 of epoch 0, 29.57 ms/it, loss 0.472915
Finished training it 10240/76743 of epoch 0, 29.55 ms/it, loss 0.468636
Finished training it 10240/76743 of epoch 0, 29.81 ms/it, loss 0.471816
Finished training it 10240/76743 of epoch 0, 29.49 ms/it, loss 0.471157
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541161.0
get out
0 has test check 2541161.0 and sample count 3274240
 accuracy 77.611 %, best 77.611 %, roc auc score 0.7743, best 0.7743
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541161.0
get out
1 has test check 2541161.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 29.60 ms/it, loss 0.469575
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541161.0
get out
2 has test check 2541161.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 29.67 ms/it, loss 0.469690
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541161.0
get out
3 has test check 2541161.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 29.71 ms/it, loss 0.471427
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 29.69 ms/it, loss 0.468265
Finished training it 12288/76743 of epoch 0, 29.55 ms/it, loss 0.468323
Finished training it 12288/76743 of epoch 0, 29.53 ms/it, loss 0.471128
Finished training it 12288/76743 of epoch 0, 29.36 ms/it, loss 0.470216
Finished training it 12288/76743 of epoch 0, 29.97 ms/it, loss 0.471372
Finished training it 13312/76743 of epoch 0, 29.75 ms/it, loss 0.469480
Finished training it 13312/76743 of epoch 0, 29.69 ms/it, loss 0.469996
Finished training it 13312/76743 of epoch 0, 29.82 ms/it, loss 0.469106
Finished training it 13312/76743 of epoch 0, 29.93 ms/it, loss 0.470535
Finished training it 14336/76743 of epoch 0, 29.74 ms/it, loss 0.468102
Finished training it 14336/76743 of epoch 0, 29.69 ms/it, loss 0.467113
Finished training it 14336/76743 of epoch 0, 29.62 ms/it, loss 0.468548
Finished training it 14336/76743 of epoch 0, 29.63 ms/it, loss 0.466829
Finished training it 15360/76743 of epoch 0, 29.23 ms/it, loss 0.468476
Finished training it 15360/76743 of epoch 0, 29.59 ms/it, loss 0.467504
Finished training it 15360/76743 of epoch 0, 29.41 ms/it, loss 0.468696
Finished training it 15360/76743 of epoch 0, 29.50 ms/it, loss 0.465107
Finished training it 16384/76743 of epoch 0, 34.10 ms/it, loss 0.467022
Finished training it 16384/76743 of epoch 0, 34.22 ms/it, loss 0.465145
Finished training it 16384/76743 of epoch 0, 34.32 ms/it, loss 0.466287
Finished training it 16384/76743 of epoch 0, 34.37 ms/it, loss 0.466378
Finished training it 17408/76743 of epoch 0, 29.86 ms/it, loss 0.468394
Finished training it 17408/76743 of epoch 0, 29.64 ms/it, loss 0.461849
Finished training it 17408/76743 of epoch 0, 29.79 ms/it, loss 0.467916
Finished training it 17408/76743 of epoch 0, 29.78 ms/it, loss 0.465911
Finished training it 18432/76743 of epoch 0, 29.55 ms/it, loss 0.466270
Finished training it 18432/76743 of epoch 0, 29.74 ms/it, loss 0.465979
Finished training it 18432/76743 of epoch 0, 29.71 ms/it, loss 0.464703
Finished training it 18432/76743 of epoch 0, 29.79 ms/it, loss 0.466036
Finished training it 19456/76743 of epoch 0, 29.38 ms/it, loss 0.464697
Finished training it 19456/76743 of epoch 0, 29.36 ms/it, loss 0.467123
Finished training it 19456/76743 of epoch 0, 29.48 ms/it, loss 0.462997
Finished training it 19456/76743 of epoch 0, 29.73 ms/it, loss 0.467002
Finished training it 20480/76743 of epoch 0, 29.28 ms/it, loss 0.463793
Finished training it 20480/76743 of epoch 0, 29.58 ms/it, loss 0.463780
Finished training it 20480/76743 of epoch 0, 29.50 ms/it, loss 0.462665
Finished training it 20480/76743 of epoch 0, 29.57 ms/it, loss 0.464718
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555201.0
get out
0 has test check 2555201.0 and sample count 3274240
 accuracy 78.040 %, best 78.040 %, roc auc score 0.7843, best 0.7843
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555201.0
get out
3 has test check 2555201.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 29.86 ms/it, loss 0.463995
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555201.0
get out
2 has test check 2555201.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 29.66 ms/it, loss 0.460833
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555201.0
get out
1 has test check 2555201.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 29.71 ms/it, loss 0.464830
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 29.86 ms/it, loss 0.461405
Finished training it 22528/76743 of epoch 0, 29.32 ms/it, loss 0.463592
Finished training it 22528/76743 of epoch 0, 29.63 ms/it, loss 0.462184
Finished training it 22528/76743 of epoch 0, 29.28 ms/it, loss 0.460630
Finished training it 22528/76743 of epoch 0, 29.34 ms/it, loss 0.463235
Finished training it 23552/76743 of epoch 0, 29.64 ms/it, loss 0.462379
Finished training it 23552/76743 of epoch 0, 29.68 ms/it, loss 0.463495
Finished training it 23552/76743 of epoch 0, 29.85 ms/it, loss 0.463744
Finished training it 23552/76743 of epoch 0, 29.44 ms/it, loss 0.462045
Finished training it 24576/76743 of epoch 0, 29.47 ms/it, loss 0.462978
Finished training it 24576/76743 of epoch 0, 29.61 ms/it, loss 0.462942
Finished training it 24576/76743 of epoch 0, 29.52 ms/it, loss 0.464424
Finished training it 24576/76743 of epoch 0, 29.32 ms/it, loss 0.461401
Finished training it 25600/76743 of epoch 0, 29.54 ms/it, loss 0.463705
Finished training it 25600/76743 of epoch 0, 29.55 ms/it, loss 0.460409
Finished training it 25600/76743 of epoch 0, 29.57 ms/it, loss 0.460168
Finished training it 25600/76743 of epoch 0, 29.74 ms/it, loss 0.460564
Finished training it 26624/76743 of epoch 0, 29.59 ms/it, loss 0.461141
Finished training it 26624/76743 of epoch 0, 29.71 ms/it, loss 0.458797
Finished training it 26624/76743 of epoch 0, 29.40 ms/it, loss 0.460466
Finished training it 26624/76743 of epoch 0, 29.45 ms/it, loss 0.461005
Finished training it 27648/76743 of epoch 0, 29.71 ms/it, loss 0.460920
Finished training it 27648/76743 of epoch 0, 29.75 ms/it, loss 0.461706
Finished training it 27648/76743 of epoch 0, 29.69 ms/it, loss 0.459967
Finished training it 27648/76743 of epoch 0, 29.69 ms/it, loss 0.463367
Finished training it 28672/76743 of epoch 0, 29.60 ms/it, loss 0.460674
Finished training it 28672/76743 of epoch 0, 29.66 ms/it, loss 0.460750
Finished training it 28672/76743 of epoch 0, 29.42 ms/it, loss 0.460604
Finished training it 28672/76743 of epoch 0, 29.78 ms/it, loss 0.462100
Finished training it 29696/76743 of epoch 0, 30.04 ms/it, loss 0.461024
Finished training it 29696/76743 of epoch 0, 29.76 ms/it, loss 0.461094
Finished training it 29696/76743 of epoch 0, 29.88 ms/it, loss 0.461797
Finished training it 29696/76743 of epoch 0, 29.71 ms/it, loss 0.461607
Finished training it 30720/76743 of epoch 0, 29.71 ms/it, loss 0.459470
Finished training it 30720/76743 of epoch 0, 29.64 ms/it, loss 0.456964
Finished training it 30720/76743 of epoch 0, 29.95 ms/it, loss 0.461779
Finished training it 30720/76743 of epoch 0, 29.83 ms/it, loss 0.458236
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2559701.0
get out
0 has test check 2559701.0 and sample count 3274240
 accuracy 78.177 %, best 78.177 %, roc auc score 0.7880, best 0.7880
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 29.69 ms/it, loss 0.457648
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2559701.0
get out
2 has test check 2559701.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 29.45 ms/it, loss 0.459475
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2559701.0
get out
1 has test check 2559701.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 29.61 ms/it, loss 0.459270
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2559701.0
get out
3 has test check 2559701.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 29.65 ms/it, loss 0.461331
Finished training it 32768/76743 of epoch 0, 29.77 ms/it, loss 0.457970
Finished training it 32768/76743 of epoch 0, 29.22 ms/it, loss 0.459662
Finished training it 32768/76743 of epoch 0, 29.62 ms/it, loss 0.460222
Finished training it 32768/76743 of epoch 0, 29.27 ms/it, loss 0.457551
Finished training it 33792/76743 of epoch 0, 30.09 ms/it, loss 0.460162
Finished training it 33792/76743 of epoch 0, 29.90 ms/it, loss 0.460770
Finished training it 33792/76743 of epoch 0, 29.87 ms/it, loss 0.458527
Finished training it 33792/76743 of epoch 0, 29.84 ms/it, loss 0.459289
Finished training it 34816/76743 of epoch 0, 29.59 ms/it, loss 0.459598
Finished training it 34816/76743 of epoch 0, 29.61 ms/it, loss 0.457388
Finished training it 34816/76743 of epoch 0, 29.55 ms/it, loss 0.462239
Finished training it 34816/76743 of epoch 0, 29.58 ms/it, loss 0.460557
Finished training it 35840/76743 of epoch 0, 29.67 ms/it, loss 0.458752
Finished training it 35840/76743 of epoch 0, 30.22 ms/it, loss 0.455241
Finished training it 35840/76743 of epoch 0, 29.79 ms/it, loss 0.458430
Finished training it 35840/76743 of epoch 0, 29.81 ms/it, loss 0.459848
Finished training it 36864/76743 of epoch 0, 29.28 ms/it, loss 0.455500
Finished training it 36864/76743 of epoch 0, 29.15 ms/it, loss 0.459733
Finished training it 36864/76743 of epoch 0, 29.36 ms/it, loss 0.457465
Finished training it 36864/76743 of epoch 0, 29.34 ms/it, loss 0.458159
Finished training it 37888/76743 of epoch 0, 29.47 ms/it, loss 0.460925
Finished training it 37888/76743 of epoch 0, 29.59 ms/it, loss 0.459905
Finished training it 37888/76743 of epoch 0, 29.56 ms/it, loss 0.457613
Finished training it 37888/76743 of epoch 0, 29.38 ms/it, loss 0.457045
Finished training it 38912/76743 of epoch 0, 29.65 ms/it, loss 0.459588
Finished training it 38912/76743 of epoch 0, 29.73 ms/it, loss 0.459503
Finished training it 38912/76743 of epoch 0, 29.70 ms/it, loss 0.455221
Finished training it 38912/76743 of epoch 0, 29.85 ms/it, loss 0.457643
Finished training it 39936/76743 of epoch 0, 29.43 ms/it, loss 0.455669
Finished training it 39936/76743 of epoch 0, 29.74 ms/it, loss 0.457639
Finished training it 39936/76743 of epoch 0, 29.65 ms/it, loss 0.460188
Finished training it 39936/76743 of epoch 0, 29.53 ms/it, loss 0.455410
Finished training it 40960/76743 of epoch 0, 29.51 ms/it, loss 0.455344
Finished training it 40960/76743 of epoch 0, 29.46 ms/it, loss 0.457821
Finished training it 40960/76743 of epoch 0, 29.89 ms/it, loss 0.460079
Finished training it 40960/76743 of epoch 0, 29.73 ms/it, loss 0.459378
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2564048.0
get out
0 has test check 2564048.0 and sample count 3274240
 accuracy 78.310 %, best 78.310 %, roc auc score 0.7903, best 0.7903
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2564048.0
get out
1 has test check 2564048.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 29.43 ms/it, loss 0.456682
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 29.49 ms/it, loss 0.459977
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2564048.0
get out
3 has test check 2564048.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 29.44 ms/it, loss 0.459217
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2564048.0
get out
2 has test check 2564048.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 29.25 ms/it, loss 0.456427
Finished training it 43008/76743 of epoch 0, 28.99 ms/it, loss 0.457590
Finished training it 43008/76743 of epoch 0, 29.13 ms/it, loss 0.456412
Finished training it 43008/76743 of epoch 0, 28.99 ms/it, loss 0.455655
Finished training it 43008/76743 of epoch 0, 29.50 ms/it, loss 0.454342
Finished training it 44032/76743 of epoch 0, 29.63 ms/it, loss 0.458220
Finished training it 44032/76743 of epoch 0, 29.60 ms/it, loss 0.458182
Finished training it 44032/76743 of epoch 0, 29.76 ms/it, loss 0.457852
Finished training it 44032/76743 of epoch 0, 29.47 ms/it, loss 0.457221
Finished training it 45056/76743 of epoch 0, 29.58 ms/it, loss 0.457081
Finished training it 45056/76743 of epoch 0, 30.09 ms/it, loss 0.457848
Finished training it 45056/76743 of epoch 0, 29.89 ms/it, loss 0.455918
Finished training it 45056/76743 of epoch 0, 29.71 ms/it, loss 0.458631
Finished training it 46080/76743 of epoch 0, 29.65 ms/it, loss 0.457328
Finished training it 46080/76743 of epoch 0, 29.76 ms/it, loss 0.457232
Finished training it 46080/76743 of epoch 0, 29.97 ms/it, loss 0.454792
Finished training it 46080/76743 of epoch 0, 29.64 ms/it, loss 0.456248
Finished training it 47104/76743 of epoch 0, 33.85 ms/it, loss 0.458501
Finished training it 47104/76743 of epoch 0, 33.90 ms/it, loss 0.457627
Finished training it 47104/76743 of epoch 0, 34.21 ms/it, loss 0.456006
Finished training it 47104/76743 of epoch 0, 33.84 ms/it, loss 0.457764
Finished training it 48128/76743 of epoch 0, 30.22 ms/it, loss 0.455913
Finished training it 48128/76743 of epoch 0, 29.77 ms/it, loss 0.457699
Finished training it 48128/76743 of epoch 0, 29.95 ms/it, loss 0.457373
Finished training it 48128/76743 of epoch 0, 29.47 ms/it, loss 0.457469
Finished training it 49152/76743 of epoch 0, 29.85 ms/it, loss 0.455272
Finished training it 49152/76743 of epoch 0, 29.89 ms/it, loss 0.458412
Finished training it 49152/76743 of epoch 0, 29.48 ms/it, loss 0.457893
Finished training it 49152/76743 of epoch 0, 29.51 ms/it, loss 0.456160
Finished training it 50176/76743 of epoch 0, 29.35 ms/it, loss 0.456694
Finished training it 50176/76743 of epoch 0, 29.33 ms/it, loss 0.454267
Finished training it 50176/76743 of epoch 0, 29.21 ms/it, loss 0.455887
Finished training it 50176/76743 of epoch 0, 29.38 ms/it, loss 0.457323
Finished training it 51200/76743 of epoch 0, 29.20 ms/it, loss 0.457005
Finished training it 51200/76743 of epoch 0, 29.16 ms/it, loss 0.454282
Finished training it 51200/76743 of epoch 0, 29.21 ms/it, loss 0.453168
Finished training it 51200/76743 of epoch 0, 29.33 ms/it, loss 0.454752
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2566826.0
get out
0 has test check 2566826.0 and sample count 3274240
 accuracy 78.395 %, best 78.395 %, roc auc score 0.7923, best 0.7923
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2566826.0
get out
1 has test check 2566826.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 29.80 ms/it, loss 0.455967
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 29.90 ms/it, loss 0.457763
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2566826.0
get out
3 has test check 2566826.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 29.68 ms/it, loss 0.454114
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2566826.0
get out
2 has test check 2566826.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 29.76 ms/it, loss 0.453442
Finished training it 53248/76743 of epoch 0, 29.36 ms/it, loss 0.455900
Finished training it 53248/76743 of epoch 0, 29.63 ms/it, loss 0.454825
Finished training it 53248/76743 of epoch 0, 29.28 ms/it, loss 0.456441
Finished training it 53248/76743 of epoch 0, 29.35 ms/it, loss 0.454614
Finished training it 54272/76743 of epoch 0, 29.53 ms/it, loss 0.455963
Finished training it 54272/76743 of epoch 0, 29.42 ms/it, loss 0.456969
Finished training it 54272/76743 of epoch 0, 29.60 ms/it, loss 0.454096
Finished training it 54272/76743 of epoch 0, 29.70 ms/it, loss 0.455171
Finished training it 55296/76743 of epoch 0, 29.95 ms/it, loss 0.454076
Finished training it 55296/76743 of epoch 0, 30.02 ms/it, loss 0.456610
Finished training it 55296/76743 of epoch 0, 29.91 ms/it, loss 0.454500
Finished training it 55296/76743 of epoch 0, 29.85 ms/it, loss 0.456294
Finished training it 56320/76743 of epoch 0, 29.38 ms/it, loss 0.455235
Finished training it 56320/76743 of epoch 0, 29.91 ms/it, loss 0.454564
Finished training it 56320/76743 of epoch 0, 29.57 ms/it, loss 0.451862
Finished training it 56320/76743 of epoch 0, 29.60 ms/it, loss 0.456186
Finished training it 57344/76743 of epoch 0, 30.04 ms/it, loss 0.454482
Finished training it 57344/76743 of epoch 0, 29.48 ms/it, loss 0.456304
Finished training it 57344/76743 of epoch 0, 29.73 ms/it, loss 0.454421
Finished training it 57344/76743 of epoch 0, 29.48 ms/it, loss 0.454940
Finished training it 58368/76743 of epoch 0, 29.99 ms/it, loss 0.457234
Finished training it 58368/76743 of epoch 0, 29.90 ms/it, loss 0.456948
Finished training it 58368/76743 of epoch 0, 29.97 ms/it, loss 0.453295
Finished training it 58368/76743 of epoch 0, 29.91 ms/it, loss 0.453768
Finished training it 59392/76743 of epoch 0, 29.65 ms/it, loss 0.454950
Finished training it 59392/76743 of epoch 0, 29.70 ms/it, loss 0.455874
Finished training it 59392/76743 of epoch 0, 29.56 ms/it, loss 0.454772
Finished training it 59392/76743 of epoch 0, 29.84 ms/it, loss 0.456549
Finished training it 60416/76743 of epoch 0, 29.60 ms/it, loss 0.456221
Finished training it 60416/76743 of epoch 0, 29.40 ms/it, loss 0.454359
Finished training it 60416/76743 of epoch 0, 29.61 ms/it, loss 0.457280
Finished training it 60416/76743 of epoch 0, 29.57 ms/it, loss 0.456755
Finished training it 61440/76743 of epoch 0, 29.61 ms/it, loss 0.456084
Finished training it 61440/76743 of epoch 0, 29.68 ms/it, loss 0.453870
Finished training it 61440/76743 of epoch 0, 29.67 ms/it, loss 0.455550
Finished training it 61440/76743 of epoch 0, 29.60 ms/it, loss 0.456815
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568021.0
get out
0 has test check 2568021.0 and sample count 3274240
 accuracy 78.431 %, best 78.431 %, roc auc score 0.7933, best 0.7933
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568021.0
get out
1 has test check 2568021.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 29.29 ms/it, loss 0.451610
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 29.39 ms/it, loss 0.454285
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568021.0
get out
2 has test check 2568021.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 29.20 ms/it, loss 0.453078
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568021.0
get out
3 has test check 2568021.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 29.47 ms/it, loss 0.454608
Finished training it 63488/76743 of epoch 0, 29.52 ms/it, loss 0.451876
Finished training it 63488/76743 of epoch 0, 29.97 ms/it, loss 0.453582
Finished training it 63488/76743 of epoch 0, 29.71 ms/it, loss 0.455741
Finished training it 63488/76743 of epoch 0, 29.85 ms/it, loss 0.456262
Finished training it 64512/76743 of epoch 0, 29.32 ms/it, loss 0.454282
Finished training it 64512/76743 of epoch 0, 29.67 ms/it, loss 0.456389
Finished training it 64512/76743 of epoch 0, 29.31 ms/it, loss 0.452393
Finished training it 64512/76743 of epoch 0, 29.38 ms/it, loss 0.454039
Finished training it 65536/76743 of epoch 0, 29.19 ms/it, loss 0.452941
Finished training it 65536/76743 of epoch 0, 29.30 ms/it, loss 0.453587
Finished training it 65536/76743 of epoch 0, 29.25 ms/it, loss 0.454171
Finished training it 65536/76743 of epoch 0, 29.11 ms/it, loss 0.455170
Finished training it 66560/76743 of epoch 0, 29.71 ms/it, loss 0.454122
Finished training it 66560/76743 of epoch 0, 29.37 ms/it, loss 0.453441
Finished training it 66560/76743 of epoch 0, 29.34 ms/it, loss 0.455301
Finished training it 66560/76743 of epoch 0, 29.36 ms/it, loss 0.453627
Finished training it 67584/76743 of epoch 0, 29.92 ms/it, loss 0.454178
Finished training it 67584/76743 of epoch 0, 29.80 ms/it, loss 0.454979
Finished training it 67584/76743 of epoch 0, 29.59 ms/it, loss 0.451790
Finished training it 67584/76743 of epoch 0, 29.68 ms/it, loss 0.454190
Finished training it 68608/76743 of epoch 0, 29.24 ms/it, loss 0.454344
Finished training it 68608/76743 of epoch 0, 29.67 ms/it, loss 0.453247
Finished training it 68608/76743 of epoch 0, 29.43 ms/it, loss 0.452684
Finished training it 68608/76743 of epoch 0, 29.67 ms/it, loss 0.453670
Finished training it 69632/76743 of epoch 0, 29.78 ms/it, loss 0.453058
Finished training it 69632/76743 of epoch 0, 29.28 ms/it, loss 0.453648
Finished training it 69632/76743 of epoch 0, 29.55 ms/it, loss 0.452044
Finished training it 69632/76743 of epoch 0, 29.71 ms/it, loss 0.450345
Finished training it 70656/76743 of epoch 0, 30.19 ms/it, loss 0.455702
Finished training it 70656/76743 of epoch 0, 29.60 ms/it, loss 0.454268
Finished training it 70656/76743 of epoch 0, 29.65 ms/it, loss 0.455378
Finished training it 70656/76743 of epoch 0, 29.72 ms/it, loss 0.452296
Finished training it 71680/76743 of epoch 0, 29.96 ms/it, loss 0.454197
Finished training it 71680/76743 of epoch 0, 29.85 ms/it, loss 0.453912
Finished training it 71680/76743 of epoch 0, 29.87 ms/it, loss 0.451236
Finished training it 71680/76743 of epoch 0, 30.03 ms/it, loss 0.453168
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2569933.0
get out
0 has test check 2569933.0 and sample count 3274240
 accuracy 78.489 %, best 78.489 %, roc auc score 0.7946, best 0.7946
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2569933.0
get out
3 has test check 2569933.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 29.56 ms/it, loss 0.452037
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2569933.0
get out
2 has test check 2569933.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 29.38 ms/it, loss 0.452801
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 29.45 ms/it, loss 0.454885
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2569933.0
get out
1 has test check 2569933.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 29.50 ms/it, loss 0.453073
Finished training it 73728/76743 of epoch 0, 29.47 ms/it, loss 0.451828
Finished training it 73728/76743 of epoch 0, 29.47 ms/it, loss 0.454099
Finished training it 73728/76743 of epoch 0, 29.68 ms/it, loss 0.454501
Finished training it 73728/76743 of epoch 0, 29.88 ms/it, loss 0.452985
Finished training it 74752/76743 of epoch 0, 29.89 ms/it, loss 0.453507
Finished training it 74752/76743 of epoch 0, 29.86 ms/it, loss 0.453479
Finished training it 74752/76743 of epoch 0, 29.82 ms/it, loss 0.452632
Finished training it 74752/76743 of epoch 0, 29.84 ms/it, loss 0.452750
Finished training it 75776/76743 of epoch 0, 29.63 ms/it, loss 0.452252
Finished training it 75776/76743 of epoch 0, 29.65 ms/it, loss 0.452957
Finished training it 75776/76743 of epoch 0, 29.66 ms/it, loss 0.452399
Finished training it 75776/76743 of epoch 0, 29.79 ms/it, loss 0.451864
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 32.59 ms/it, loss 0.452015
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 32.89 ms/it, loss 0.450744
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 32.49 ms/it, loss 0.453452
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 32.53 ms/it, loss 0.451818
Finished training it 2048/76743 of epoch 1, 30.35 ms/it, loss 0.452145
Finished training it 2048/76743 of epoch 1, 30.04 ms/it, loss 0.451932
Finished training it 2048/76743 of epoch 1, 29.85 ms/it, loss 0.452010
Finished training it 2048/76743 of epoch 1, 29.67 ms/it, loss 0.449767
Finished training it 3072/76743 of epoch 1, 29.36 ms/it, loss 0.452896
Finished training it 3072/76743 of epoch 1, 29.53 ms/it, loss 0.454757
Finished training it 3072/76743 of epoch 1, 29.25 ms/it, loss 0.452981
Finished training it 3072/76743 of epoch 1, 29.41 ms/it, loss 0.451506
Finished training it 4096/76743 of epoch 1, 29.47 ms/it, loss 0.452927
Finished training it 4096/76743 of epoch 1, 29.48 ms/it, loss 0.454600
Finished training it 4096/76743 of epoch 1, 29.54 ms/it, loss 0.452183
Finished training it 4096/76743 of epoch 1, 29.53 ms/it, loss 0.452681
Finished training it 5120/76743 of epoch 1, 30.14 ms/it, loss 0.451289
Finished training it 5120/76743 of epoch 1, 29.90 ms/it, loss 0.452578
Finished training it 5120/76743 of epoch 1, 30.07 ms/it, loss 0.451031
Finished training it 5120/76743 of epoch 1, 30.01 ms/it, loss 0.451991
Finished training it 6144/76743 of epoch 1, 29.60 ms/it, loss 0.453752
Finished training it 6144/76743 of epoch 1, 29.33 ms/it, loss 0.452821
Finished training it 6144/76743 of epoch 1, 29.12 ms/it, loss 0.452006
Finished training it 6144/76743 of epoch 1, 29.25 ms/it, loss 0.450967
Finished training it 7168/76743 of epoch 1, 29.63 ms/it, loss 0.453109
Finished training it 7168/76743 of epoch 1, 29.85 ms/it, loss 0.450989
Finished training it 7168/76743 of epoch 1, 29.62 ms/it, loss 0.451385
Finished training it 7168/76743 of epoch 1, 29.67 ms/it, loss 0.453671
Finished training it 8192/76743 of epoch 1, 30.06 ms/it, loss 0.451366
Finished training it 8192/76743 of epoch 1, 29.61 ms/it, loss 0.449521
Finished training it 8192/76743 of epoch 1, 29.64 ms/it, loss 0.451915
Finished training it 8192/76743 of epoch 1, 29.67 ms/it, loss 0.452192
Finished training it 9216/76743 of epoch 1, 29.50 ms/it, loss 0.450836
Finished training it 9216/76743 of epoch 1, 29.60 ms/it, loss 0.453657
Finished training it 9216/76743 of epoch 1, 29.59 ms/it, loss 0.453125
Finished training it 9216/76743 of epoch 1, 29.52 ms/it, loss 0.450107
Finished training it 10240/76743 of epoch 1, 30.07 ms/it, loss 0.451882
Finished training it 10240/76743 of epoch 1, 29.57 ms/it, loss 0.450889
Finished training it 10240/76743 of epoch 1, 29.52 ms/it, loss 0.452965
Finished training it 10240/76743 of epoch 1, 29.70 ms/it, loss 0.448686
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571584.0
get out
0 has test check 2571584.0 and sample count 3274240
 accuracy 78.540 %, best 78.540 %, roc auc score 0.7953, best 0.7953
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571584.0
get out
2 has test check 2571584.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 29.79 ms/it, loss 0.450429
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 29.71 ms/it, loss 0.449256
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571584.0
get out
3 has test check 2571584.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 29.73 ms/it, loss 0.451932
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571584.0
get out
1 has test check 2571584.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 29.58 ms/it, loss 0.450555
Finished training it 12288/76743 of epoch 1, 29.46 ms/it, loss 0.450400
Finished training it 12288/76743 of epoch 1, 29.47 ms/it, loss 0.452982
Finished training it 12288/76743 of epoch 1, 29.87 ms/it, loss 0.453298
Finished training it 12288/76743 of epoch 1, 29.54 ms/it, loss 0.452013
Finished training it 13312/76743 of epoch 1, 29.62 ms/it, loss 0.453138
Finished training it 13312/76743 of epoch 1, 29.48 ms/it, loss 0.453250
Finished training it 13312/76743 of epoch 1, 29.59 ms/it, loss 0.451203
Finished training it 13312/76743 of epoch 1, 29.43 ms/it, loss 0.451999
Finished training it 14336/76743 of epoch 1, 29.87 ms/it, loss 0.451299
Finished training it 14336/76743 of epoch 1, 29.54 ms/it, loss 0.452085
Finished training it 14336/76743 of epoch 1, 29.74 ms/it, loss 0.450598
Finished training it 14336/76743 of epoch 1, 29.87 ms/it, loss 0.450386
Finished training it 15360/76743 of epoch 1, 29.69 ms/it, loss 0.448965
Finished training it 15360/76743 of epoch 1, 29.65 ms/it, loss 0.452914
Finished training it 15360/76743 of epoch 1, 29.44 ms/it, loss 0.452560
Finished training it 15360/76743 of epoch 1, 29.74 ms/it, loss 0.450970
Finished training it 16384/76743 of epoch 1, 29.36 ms/it, loss 0.451925
Finished training it 16384/76743 of epoch 1, 29.48 ms/it, loss 0.450873
Finished training it 16384/76743 of epoch 1, 29.39 ms/it, loss 0.449897
Finished training it 16384/76743 of epoch 1, 29.86 ms/it, loss 0.451063
Finished training it 17408/76743 of epoch 1, 29.40 ms/it, loss 0.452191
Finished training it 17408/76743 of epoch 1, 29.72 ms/it, loss 0.453046
Finished training it 17408/76743 of epoch 1, 29.47 ms/it, loss 0.453723
Finished training it 17408/76743 of epoch 1, 29.57 ms/it, loss 0.447026
Finished training it 18432/76743 of epoch 1, 29.52 ms/it, loss 0.452052
Finished training it 18432/76743 of epoch 1, 29.53 ms/it, loss 0.451918
Finished training it 18432/76743 of epoch 1, 29.68 ms/it, loss 0.450650
Finished training it 18432/76743 of epoch 1, 29.39 ms/it, loss 0.451147
Finished training it 19456/76743 of epoch 1, 29.66 ms/it, loss 0.449795
Finished training it 19456/76743 of epoch 1, 29.91 ms/it, loss 0.453195
Finished training it 19456/76743 of epoch 1, 29.54 ms/it, loss 0.451108
Finished training it 19456/76743 of epoch 1, 29.59 ms/it, loss 0.454071
Finished training it 20480/76743 of epoch 1, 29.77 ms/it, loss 0.451908
Finished training it 20480/76743 of epoch 1, 29.45 ms/it, loss 0.450744
Finished training it 20480/76743 of epoch 1, 29.54 ms/it, loss 0.449319
Finished training it 20480/76743 of epoch 1, 29.53 ms/it, loss 0.450784
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2573628.0
get out
0 has test check 2573628.0 and sample count 3274240
 accuracy 78.602 %, best 78.602 %, roc auc score 0.7968, best 0.7968
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 29.65 ms/it, loss 0.449045
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2573628.0
get out
3 has test check 2573628.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 29.64 ms/it, loss 0.451042
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2573628.0
get out
2 has test check 2573628.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 29.73 ms/it, loss 0.448340
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2573628.0
get out
1 has test check 2573628.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 29.69 ms/it, loss 0.452445
Finished training it 22528/76743 of epoch 1, 30.01 ms/it, loss 0.448851
Finished training it 22528/76743 of epoch 1, 30.11 ms/it, loss 0.450367
Finished training it 22528/76743 of epoch 1, 29.85 ms/it, loss 0.451946
Finished training it 22528/76743 of epoch 1, 29.82 ms/it, loss 0.450835
Finished training it 23552/76743 of epoch 1, 29.69 ms/it, loss 0.452076
Finished training it 23552/76743 of epoch 1, 29.68 ms/it, loss 0.450314
Finished training it 23552/76743 of epoch 1, 29.53 ms/it, loss 0.450773
Finished training it 23552/76743 of epoch 1, 29.50 ms/it, loss 0.452288
Finished training it 24576/76743 of epoch 1, 29.28 ms/it, loss 0.451298
Finished training it 24576/76743 of epoch 1, 29.49 ms/it, loss 0.451689
Finished training it 24576/76743 of epoch 1, 29.34 ms/it, loss 0.450325
Finished training it 24576/76743 of epoch 1, 29.23 ms/it, loss 0.452417
Finished training it 25600/76743 of epoch 1, 29.52 ms/it, loss 0.449461
Finished training it 25600/76743 of epoch 1, 29.73 ms/it, loss 0.450160
Finished training it 25600/76743 of epoch 1, 29.54 ms/it, loss 0.452697
Finished training it 25600/76743 of epoch 1, 29.55 ms/it, loss 0.449012
Finished training it 26624/76743 of epoch 1, 29.75 ms/it, loss 0.447844
Finished training it 26624/76743 of epoch 1, 29.48 ms/it, loss 0.449832
Finished training it 26624/76743 of epoch 1, 29.92 ms/it, loss 0.450322
Finished training it 26624/76743 of epoch 1, 29.40 ms/it, loss 0.450517
Finished training it 27648/76743 of epoch 1, 30.16 ms/it, loss 0.449543
Finished training it 27648/76743 of epoch 1, 29.54 ms/it, loss 0.450357
Finished training it 27648/76743 of epoch 1, 29.52 ms/it, loss 0.453087
Finished training it 27648/76743 of epoch 1, 29.45 ms/it, loss 0.451447
Finished training it 28672/76743 of epoch 1, 29.97 ms/it, loss 0.452065
Finished training it 28672/76743 of epoch 1, 29.92 ms/it, loss 0.450743
Finished training it 28672/76743 of epoch 1, 29.73 ms/it, loss 0.451044
Finished training it 28672/76743 of epoch 1, 29.76 ms/it, loss 0.450298
Finished training it 29696/76743 of epoch 1, 29.71 ms/it, loss 0.452005
Finished training it 29696/76743 of epoch 1, 30.13 ms/it, loss 0.451576
Finished training it 29696/76743 of epoch 1, 29.99 ms/it, loss 0.451186
Finished training it 29696/76743 of epoch 1, 29.54 ms/it, loss 0.451599
Finished training it 30720/76743 of epoch 1, 29.78 ms/it, loss 0.449419
Finished training it 30720/76743 of epoch 1, 30.10 ms/it, loss 0.452060
Finished training it 30720/76743 of epoch 1, 29.84 ms/it, loss 0.447517
Finished training it 30720/76743 of epoch 1, 29.79 ms/it, loss 0.448467
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574318.0
get out
0 has test check 2574318.0 and sample count 3274240
 accuracy 78.623 %, best 78.623 %, roc auc score 0.7972, best 0.7972
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574318.0
get out
2 has test check 2574318.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 30.83 ms/it, loss 0.450292
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574318.0
get out
1 has test check 2574318.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 30.82 ms/it, loss 0.449968
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 30.34 ms/it, loss 0.448223
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574318.0
get out
3 has test check 2574318.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 30.49 ms/it, loss 0.451655
Finished training it 32768/76743 of epoch 1, 34.98 ms/it, loss 0.448134
Finished training it 32768/76743 of epoch 1, 34.74 ms/it, loss 0.448656
Finished training it 32768/76743 of epoch 1, 35.12 ms/it, loss 0.451012
Finished training it 32768/76743 of epoch 1, 34.64 ms/it, loss 0.450376
Finished training it 33792/76743 of epoch 1, 29.71 ms/it, loss 0.451080
Finished training it 33792/76743 of epoch 1, 29.61 ms/it, loss 0.449632
Finished training it 33792/76743 of epoch 1, 29.80 ms/it, loss 0.450010
Finished training it 33792/76743 of epoch 1, 29.68 ms/it, loss 0.451488
Finished training it 34816/76743 of epoch 1, 29.56 ms/it, loss 0.448212
Finished training it 34816/76743 of epoch 1, 30.02 ms/it, loss 0.451717
Finished training it 34816/76743 of epoch 1, 29.88 ms/it, loss 0.450385
Finished training it 34816/76743 of epoch 1, 29.82 ms/it, loss 0.452796
Finished training it 35840/76743 of epoch 1, 34.61 ms/it, loss 0.449919
Finished training it 35840/76743 of epoch 1, 34.34 ms/it, loss 0.446495
Finished training it 35840/76743 of epoch 1, 34.26 ms/it, loss 0.450870
Finished training it 35840/76743 of epoch 1, 34.51 ms/it, loss 0.450985
Finished training it 36864/76743 of epoch 1, 29.82 ms/it, loss 0.449819
Finished training it 36864/76743 of epoch 1, 29.87 ms/it, loss 0.449312
Finished training it 36864/76743 of epoch 1, 29.55 ms/it, loss 0.451242
Finished training it 36864/76743 of epoch 1, 29.50 ms/it, loss 0.447008
Finished training it 37888/76743 of epoch 1, 29.77 ms/it, loss 0.451304
Finished training it 37888/76743 of epoch 1, 29.58 ms/it, loss 0.452641
Finished training it 37888/76743 of epoch 1, 29.48 ms/it, loss 0.448257
Finished training it 37888/76743 of epoch 1, 29.54 ms/it, loss 0.449095
Finished training it 38912/76743 of epoch 1, 29.52 ms/it, loss 0.449550
Finished training it 38912/76743 of epoch 1, 29.53 ms/it, loss 0.447013
Finished training it 38912/76743 of epoch 1, 29.29 ms/it, loss 0.450948
Finished training it 38912/76743 of epoch 1, 29.33 ms/it, loss 0.451651
Finished training it 39936/76743 of epoch 1, 29.19 ms/it, loss 0.447168
Finished training it 39936/76743 of epoch 1, 29.33 ms/it, loss 0.447614
Finished training it 39936/76743 of epoch 1, 29.32 ms/it, loss 0.452143
Finished training it 39936/76743 of epoch 1, 29.36 ms/it, loss 0.449210
Finished training it 40960/76743 of epoch 1, 29.57 ms/it, loss 0.447280
Finished training it 40960/76743 of epoch 1, 29.88 ms/it, loss 0.452284
Finished training it 40960/76743 of epoch 1, 29.69 ms/it, loss 0.450055
Finished training it 40960/76743 of epoch 1, 29.68 ms/it, loss 0.451299
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575056.0
get out
0 has test check 2575056.0 and sample count 3274240
 accuracy 78.646 %, best 78.646 %, roc auc score 0.7975, best 0.7975
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575056.0
get out
1 has test check 2575056.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 29.52 ms/it, loss 0.448976
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575056.0
get out
3 has test check 2575056.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 29.70 ms/it, loss 0.451698
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 29.48 ms/it, loss 0.452015
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575056.0
get out
2 has test check 2575056.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 29.57 ms/it, loss 0.448804
Finished training it 43008/76743 of epoch 1, 29.85 ms/it, loss 0.448371
Finished training it 43008/76743 of epoch 1, 29.75 ms/it, loss 0.449718
Finished training it 43008/76743 of epoch 1, 30.04 ms/it, loss 0.446495
Finished training it 43008/76743 of epoch 1, 29.97 ms/it, loss 0.448007
Finished training it 44032/76743 of epoch 1, 29.91 ms/it, loss 0.450783
Finished training it 44032/76743 of epoch 1, 30.01 ms/it, loss 0.450541
Finished training it 44032/76743 of epoch 1, 30.03 ms/it, loss 0.449713
Finished training it 44032/76743 of epoch 1, 29.97 ms/it, loss 0.450449
Finished training it 45056/76743 of epoch 1, 29.30 ms/it, loss 0.450109
Finished training it 45056/76743 of epoch 1, 29.13 ms/it, loss 0.449613
Finished training it 45056/76743 of epoch 1, 29.00 ms/it, loss 0.450744
Finished training it 45056/76743 of epoch 1, 29.26 ms/it, loss 0.448307
Finished training it 46080/76743 of epoch 1, 29.44 ms/it, loss 0.448973
Finished training it 46080/76743 of epoch 1, 29.40 ms/it, loss 0.449799
Finished training it 46080/76743 of epoch 1, 29.63 ms/it, loss 0.449802
Finished training it 46080/76743 of epoch 1, 29.60 ms/it, loss 0.447625
Finished training it 47104/76743 of epoch 1, 29.97 ms/it, loss 0.450733
Finished training it 47104/76743 of epoch 1, 29.53 ms/it, loss 0.449947
Finished training it 47104/76743 of epoch 1, 29.38 ms/it, loss 0.451466
Finished training it 47104/76743 of epoch 1, 29.12 ms/it, loss 0.448471
Finished training it 48128/76743 of epoch 1, 30.00 ms/it, loss 0.450192
Finished training it 48128/76743 of epoch 1, 29.56 ms/it, loss 0.450389
Finished training it 48128/76743 of epoch 1, 30.08 ms/it, loss 0.448572
Finished training it 48128/76743 of epoch 1, 29.70 ms/it, loss 0.450621
Finished training it 49152/76743 of epoch 1, 29.81 ms/it, loss 0.451204
Finished training it 49152/76743 of epoch 1, 29.76 ms/it, loss 0.447975
Finished training it 49152/76743 of epoch 1, 29.60 ms/it, loss 0.449246
Finished training it 49152/76743 of epoch 1, 29.74 ms/it, loss 0.450751
Finished training it 50176/76743 of epoch 1, 29.93 ms/it, loss 0.450378
Finished training it 50176/76743 of epoch 1, 29.60 ms/it, loss 0.449600
Finished training it 50176/76743 of epoch 1, 29.68 ms/it, loss 0.448523
Finished training it 50176/76743 of epoch 1, 29.96 ms/it, loss 0.447427
Finished training it 51200/76743 of epoch 1, 30.04 ms/it, loss 0.447560
Finished training it 51200/76743 of epoch 1, 29.68 ms/it, loss 0.447234
Finished training it 51200/76743 of epoch 1, 29.73 ms/it, loss 0.449655
Finished training it 51200/76743 of epoch 1, 29.74 ms/it, loss 0.446383
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575515.0
get out
0 has test check 2575515.0 and sample count 3274240
 accuracy 78.660 %, best 78.660 %, roc auc score 0.7982, best 0.7982
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575515.0
get out
2 has test check 2575515.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 29.56 ms/it, loss 0.446804
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575515.0
get out
1 has test check 2575515.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 29.71 ms/it, loss 0.449048
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575515.0
get out
3 has test check 2575515.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 29.48 ms/it, loss 0.447439
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 29.56 ms/it, loss 0.451253
Finished training it 53248/76743 of epoch 1, 29.89 ms/it, loss 0.447976
Finished training it 53248/76743 of epoch 1, 29.75 ms/it, loss 0.449187
Finished training it 53248/76743 of epoch 1, 29.58 ms/it, loss 0.448295
Finished training it 53248/76743 of epoch 1, 29.97 ms/it, loss 0.449941
Finished training it 54272/76743 of epoch 1, 29.66 ms/it, loss 0.447763
Finished training it 54272/76743 of epoch 1, 29.62 ms/it, loss 0.450691
Finished training it 54272/76743 of epoch 1, 29.61 ms/it, loss 0.449266
Finished training it 54272/76743 of epoch 1, 29.63 ms/it, loss 0.448498
Finished training it 55296/76743 of epoch 1, 29.46 ms/it, loss 0.447663
Finished training it 55296/76743 of epoch 1, 29.81 ms/it, loss 0.450172
Finished training it 55296/76743 of epoch 1, 29.56 ms/it, loss 0.447713
Finished training it 55296/76743 of epoch 1, 29.25 ms/it, loss 0.449767
Finished training it 56320/76743 of epoch 1, 29.51 ms/it, loss 0.445018
Finished training it 56320/76743 of epoch 1, 30.02 ms/it, loss 0.448011
Finished training it 56320/76743 of epoch 1, 29.41 ms/it, loss 0.448515
Finished training it 56320/76743 of epoch 1, 29.60 ms/it, loss 0.449768
Finished training it 57344/76743 of epoch 1, 29.75 ms/it, loss 0.449880
Finished training it 57344/76743 of epoch 1, 29.89 ms/it, loss 0.448851
Finished training it 57344/76743 of epoch 1, 29.80 ms/it, loss 0.447856
Finished training it 57344/76743 of epoch 1, 29.78 ms/it, loss 0.448391
Finished training it 58368/76743 of epoch 1, 29.44 ms/it, loss 0.447286
Finished training it 58368/76743 of epoch 1, 29.35 ms/it, loss 0.447507
Finished training it 58368/76743 of epoch 1, 29.80 ms/it, loss 0.450998
Finished training it 58368/76743 of epoch 1, 29.46 ms/it, loss 0.450620
Finished training it 59392/76743 of epoch 1, 29.80 ms/it, loss 0.450642
Finished training it 59392/76743 of epoch 1, 29.82 ms/it, loss 0.449536
Finished training it 59392/76743 of epoch 1, 29.67 ms/it, loss 0.448510
Finished training it 59392/76743 of epoch 1, 29.40 ms/it, loss 0.448635
Finished training it 60416/76743 of epoch 1, 29.56 ms/it, loss 0.450959
Finished training it 60416/76743 of epoch 1, 29.72 ms/it, loss 0.451232
Finished training it 60416/76743 of epoch 1, 29.20 ms/it, loss 0.448199
Finished training it 60416/76743 of epoch 1, 29.38 ms/it, loss 0.450168
Finished training it 61440/76743 of epoch 1, 29.37 ms/it, loss 0.450868
Finished training it 61440/76743 of epoch 1, 29.41 ms/it, loss 0.450256
Finished training it 61440/76743 of epoch 1, 29.43 ms/it, loss 0.448023
Finished training it 61440/76743 of epoch 1, 29.30 ms/it, loss 0.449312
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575473.0
get out
0 has test check 2575473.0 and sample count 3274240
 accuracy 78.659 %, best 78.660 %, roc auc score 0.7984, best 0.7984
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575473.0
get out
1 has test check 2575473.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 29.25 ms/it, loss 0.445411
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 29.28 ms/it, loss 0.448323
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575473.0
get out
2 has test check 2575473.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 29.38 ms/it, loss 0.447203
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575473.0
get out
3 has test check 2575473.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 29.31 ms/it, loss 0.448566
Finished training it 63488/76743 of epoch 1, 30.01 ms/it, loss 0.447782
Finished training it 63488/76743 of epoch 1, 29.60 ms/it, loss 0.449843
Finished training it 63488/76743 of epoch 1, 29.87 ms/it, loss 0.445833
Finished training it 63488/76743 of epoch 1, 29.63 ms/it, loss 0.449994
Finished training it 64512/76743 of epoch 1, 29.77 ms/it, loss 0.448076
Finished training it 64512/76743 of epoch 1, 29.81 ms/it, loss 0.446619
Finished training it 64512/76743 of epoch 1, 29.97 ms/it, loss 0.450729
Finished training it 64512/76743 of epoch 1, 29.78 ms/it, loss 0.448445
Finished training it 65536/76743 of epoch 1, 29.03 ms/it, loss 0.447808
Finished training it 65536/76743 of epoch 1, 29.17 ms/it, loss 0.447020
Finished training it 65536/76743 of epoch 1, 29.06 ms/it, loss 0.448221
Finished training it 65536/76743 of epoch 1, 29.38 ms/it, loss 0.449769
Finished training it 66560/76743 of epoch 1, 34.42 ms/it, loss 0.447935
Finished training it 66560/76743 of epoch 1, 34.76 ms/it, loss 0.447850
Finished training it 66560/76743 of epoch 1, 34.76 ms/it, loss 0.449410
Finished training it 66560/76743 of epoch 1, 35.35 ms/it, loss 0.448382
Finished training it 67584/76743 of epoch 1, 29.80 ms/it, loss 0.448453
Finished training it 67584/76743 of epoch 1, 30.02 ms/it, loss 0.448800
Finished training it 67584/76743 of epoch 1, 29.78 ms/it, loss 0.449260
Finished training it 67584/76743 of epoch 1, 29.62 ms/it, loss 0.446040
Finished training it 68608/76743 of epoch 1, 29.75 ms/it, loss 0.447680
Finished training it 68608/76743 of epoch 1, 29.94 ms/it, loss 0.448267
Finished training it 68608/76743 of epoch 1, 29.70 ms/it, loss 0.448978
Finished training it 68608/76743 of epoch 1, 29.74 ms/it, loss 0.446878
Finished training it 69632/76743 of epoch 1, 29.62 ms/it, loss 0.446251
Finished training it 69632/76743 of epoch 1, 29.56 ms/it, loss 0.444790
Finished training it 69632/76743 of epoch 1, 29.72 ms/it, loss 0.447575
Finished training it 69632/76743 of epoch 1, 29.65 ms/it, loss 0.447622
Finished training it 70656/76743 of epoch 1, 29.62 ms/it, loss 0.448777
Finished training it 70656/76743 of epoch 1, 29.59 ms/it, loss 0.449847
Finished training it 70656/76743 of epoch 1, 29.77 ms/it, loss 0.446763
Finished training it 70656/76743 of epoch 1, 29.66 ms/it, loss 0.450257
Finished training it 71680/76743 of epoch 1, 29.62 ms/it, loss 0.445735
Finished training it 71680/76743 of epoch 1, 29.78 ms/it, loss 0.448426
Finished training it 71680/76743 of epoch 1, 29.81 ms/it, loss 0.447819
Finished training it 71680/76743 of epoch 1, 29.71 ms/it, loss 0.448855
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576522.0
get out
0 has test check 2576522.0 and sample count 3274240
 accuracy 78.691 %, best 78.691 %, roc auc score 0.7990, best 0.7990
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576522.0
get out
1 has test check 2576522.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 29.70 ms/it, loss 0.447640
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576522.0
get out
3 has test check 2576522.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 29.78 ms/it, loss 0.446625
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 29.74 ms/it, loss 0.449364
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576522.0
get out
2 has test check 2576522.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 29.47 ms/it, loss 0.447336
Finished training it 73728/76743 of epoch 1, 29.41 ms/it, loss 0.446446
Finished training it 73728/76743 of epoch 1, 29.46 ms/it, loss 0.448911
Finished training it 73728/76743 of epoch 1, 29.83 ms/it, loss 0.447520
Finished training it 73728/76743 of epoch 1, 29.49 ms/it, loss 0.448728
Finished training it 74752/76743 of epoch 1, 29.55 ms/it, loss 0.447189
Finished training it 74752/76743 of epoch 1, 29.40 ms/it, loss 0.448233
Finished training it 74752/76743 of epoch 1, 29.44 ms/it, loss 0.448218
Finished training it 74752/76743 of epoch 1, 29.51 ms/it, loss 0.447373
Finished training it 75776/76743 of epoch 1, 29.66 ms/it, loss 0.446659
Finished training it 75776/76743 of epoch 1, 29.64 ms/it, loss 0.446921
Finished training it 75776/76743 of epoch 1, 30.00 ms/it, loss 0.447033
Finished training it 75776/76743 of epoch 1, 29.70 ms/it, loss 0.447675
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 30.00 ms/it, loss 0.446914
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 30.01 ms/it, loss 0.445441
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 29.90 ms/it, loss 0.446798
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 29.69 ms/it, loss 0.448412
Finished training it 2048/76743 of epoch 2, 29.60 ms/it, loss 0.444237
Finished training it 2048/76743 of epoch 2, 29.60 ms/it, loss 0.446382
Finished training it 2048/76743 of epoch 2, 29.64 ms/it, loss 0.447007
Finished training it 2048/76743 of epoch 2, 29.74 ms/it, loss 0.447033
Finished training it 3072/76743 of epoch 2, 29.47 ms/it, loss 0.449631
Finished training it 3072/76743 of epoch 2, 29.50 ms/it, loss 0.448012
Finished training it 3072/76743 of epoch 2, 29.71 ms/it, loss 0.446212
Finished training it 3072/76743 of epoch 2, 29.29 ms/it, loss 0.447733
Finished training it 4096/76743 of epoch 2, 29.67 ms/it, loss 0.447077
Finished training it 4096/76743 of epoch 2, 29.47 ms/it, loss 0.449572
Finished training it 4096/76743 of epoch 2, 29.68 ms/it, loss 0.447509
Finished training it 4096/76743 of epoch 2, 29.49 ms/it, loss 0.447892
Finished training it 5120/76743 of epoch 2, 29.53 ms/it, loss 0.445950
Finished training it 5120/76743 of epoch 2, 29.56 ms/it, loss 0.447069
Finished training it 5120/76743 of epoch 2, 29.93 ms/it, loss 0.446593
Finished training it 5120/76743 of epoch 2, 29.65 ms/it, loss 0.447707
Finished training it 6144/76743 of epoch 2, 29.41 ms/it, loss 0.448927
Finished training it 6144/76743 of epoch 2, 29.37 ms/it, loss 0.447851
Finished training it 6144/76743 of epoch 2, 29.49 ms/it, loss 0.446096
Finished training it 6144/76743 of epoch 2, 29.52 ms/it, loss 0.446937
Finished training it 7168/76743 of epoch 2, 29.26 ms/it, loss 0.448243
Finished training it 7168/76743 of epoch 2, 29.55 ms/it, loss 0.448681
Finished training it 7168/76743 of epoch 2, 29.90 ms/it, loss 0.446177
Finished training it 7168/76743 of epoch 2, 29.33 ms/it, loss 0.446444
Finished training it 8192/76743 of epoch 2, 30.02 ms/it, loss 0.446480
Finished training it 8192/76743 of epoch 2, 29.89 ms/it, loss 0.447295
Finished training it 8192/76743 of epoch 2, 29.65 ms/it, loss 0.447284
Finished training it 8192/76743 of epoch 2, 29.67 ms/it, loss 0.444336
Finished training it 9216/76743 of epoch 2, 30.11 ms/it, loss 0.449069
Finished training it 9216/76743 of epoch 2, 29.61 ms/it, loss 0.446180
Finished training it 9216/76743 of epoch 2, 29.51 ms/it, loss 0.445271
Finished training it 9216/76743 of epoch 2, 29.66 ms/it, loss 0.448262
Finished training it 10240/76743 of epoch 2, 29.53 ms/it, loss 0.448155
Finished training it 10240/76743 of epoch 2, 29.71 ms/it, loss 0.447054
Finished training it 10240/76743 of epoch 2, 29.43 ms/it, loss 0.444062
Finished training it 10240/76743 of epoch 2, 29.56 ms/it, loss 0.446237
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577232.0
get out
0 has test check 2577232.0 and sample count 3274240
 accuracy 78.712 %, best 78.712 %, roc auc score 0.7990, best 0.7990
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577232.0
get out
2 has test check 2577232.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 29.56 ms/it, loss 0.445469
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577232.0
get out
3 has test check 2577232.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 29.79 ms/it, loss 0.447227
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577232.0
get out
1 has test check 2577232.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 29.40 ms/it, loss 0.445713
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 29.59 ms/it, loss 0.444727
Finished training it 12288/76743 of epoch 2, 29.29 ms/it, loss 0.445774
Finished training it 12288/76743 of epoch 2, 29.88 ms/it, loss 0.448555
Finished training it 12288/76743 of epoch 2, 29.54 ms/it, loss 0.448242
Finished training it 12288/76743 of epoch 2, 29.45 ms/it, loss 0.447236
Finished training it 13312/76743 of epoch 2, 29.77 ms/it, loss 0.448597
Finished training it 13312/76743 of epoch 2, 29.88 ms/it, loss 0.447147
Finished training it 13312/76743 of epoch 2, 29.96 ms/it, loss 0.448157
Finished training it 13312/76743 of epoch 2, 29.67 ms/it, loss 0.446312
Finished training it 14336/76743 of epoch 2, 29.53 ms/it, loss 0.446087
Finished training it 14336/76743 of epoch 2, 29.62 ms/it, loss 0.447568
Finished training it 14336/76743 of epoch 2, 29.33 ms/it, loss 0.446696
Finished training it 14336/76743 of epoch 2, 29.44 ms/it, loss 0.445724
Finished training it 15360/76743 of epoch 2, 29.87 ms/it, loss 0.446153
Finished training it 15360/76743 of epoch 2, 29.76 ms/it, loss 0.444253
Finished training it 15360/76743 of epoch 2, 29.43 ms/it, loss 0.448179
Finished training it 15360/76743 of epoch 2, 29.46 ms/it, loss 0.448483
Finished training it 16384/76743 of epoch 2, 35.86 ms/it, loss 0.446584
Finished training it 16384/76743 of epoch 2, 35.28 ms/it, loss 0.447132
Finished training it 16384/76743 of epoch 2, 34.97 ms/it, loss 0.445388
Finished training it 16384/76743 of epoch 2, 35.07 ms/it, loss 0.446145
Finished training it 17408/76743 of epoch 2, 29.92 ms/it, loss 0.449144
Finished training it 17408/76743 of epoch 2, 30.08 ms/it, loss 0.448354
Finished training it 17408/76743 of epoch 2, 29.80 ms/it, loss 0.442489
Finished training it 17408/76743 of epoch 2, 30.10 ms/it, loss 0.447464
Finished training it 18432/76743 of epoch 2, 29.78 ms/it, loss 0.446101
Finished training it 18432/76743 of epoch 2, 29.71 ms/it, loss 0.445929
Finished training it 18432/76743 of epoch 2, 29.58 ms/it, loss 0.447494
Finished training it 18432/76743 of epoch 2, 29.57 ms/it, loss 0.447398
Finished training it 19456/76743 of epoch 2, 29.54 ms/it, loss 0.448761
Finished training it 19456/76743 of epoch 2, 29.55 ms/it, loss 0.449678
Finished training it 19456/76743 of epoch 2, 29.42 ms/it, loss 0.445068
Finished training it 19456/76743 of epoch 2, 29.43 ms/it, loss 0.446614
Finished training it 20480/76743 of epoch 2, 29.76 ms/it, loss 0.444585
Finished training it 20480/76743 of epoch 2, 30.00 ms/it, loss 0.447288
Finished training it 20480/76743 of epoch 2, 29.96 ms/it, loss 0.446144
Finished training it 20480/76743 of epoch 2, 29.77 ms/it, loss 0.446282
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578398.0
get out
0 has test check 2578398.0 and sample count 3274240
 accuracy 78.748 %, best 78.748 %, roc auc score 0.8001, best 0.8001
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578398.0
get out
3 has test check 2578398.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 29.73 ms/it, loss 0.446617
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578398.0
get out
2 has test check 2578398.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 29.79 ms/it, loss 0.443761
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578398.0
get out
1 has test check 2578398.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 29.68 ms/it, loss 0.447983
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 29.79 ms/it, loss 0.444717
Finished training it 22528/76743 of epoch 2, 29.91 ms/it, loss 0.446090
Finished training it 22528/76743 of epoch 2, 29.81 ms/it, loss 0.447382
Finished training it 22528/76743 of epoch 2, 29.81 ms/it, loss 0.444466
Finished training it 22528/76743 of epoch 2, 29.77 ms/it, loss 0.446401
Finished training it 23552/76743 of epoch 2, 29.35 ms/it, loss 0.447784
Finished training it 23552/76743 of epoch 2, 29.00 ms/it, loss 0.446312
Finished training it 23552/76743 of epoch 2, 29.14 ms/it, loss 0.447996
Finished training it 23552/76743 of epoch 2, 29.13 ms/it, loss 0.445896
Finished training it 24576/76743 of epoch 2, 29.21 ms/it, loss 0.447345
Finished training it 24576/76743 of epoch 2, 29.16 ms/it, loss 0.448015
Finished training it 24576/76743 of epoch 2, 29.12 ms/it, loss 0.446955
Finished training it 24576/76743 of epoch 2, 29.12 ms/it, loss 0.446034
Finished training it 25600/76743 of epoch 2, 29.91 ms/it, loss 0.445406
Finished training it 25600/76743 of epoch 2, 30.03 ms/it, loss 0.445802
Finished training it 25600/76743 of epoch 2, 30.03 ms/it, loss 0.444798
Finished training it 25600/76743 of epoch 2, 29.49 ms/it, loss 0.448366
Finished training it 26624/76743 of epoch 2, 29.16 ms/it, loss 0.445888
Finished training it 26624/76743 of epoch 2, 29.40 ms/it, loss 0.443371
Finished training it 26624/76743 of epoch 2, 29.19 ms/it, loss 0.445426
Finished training it 26624/76743 of epoch 2, 29.16 ms/it, loss 0.446283
Finished training it 27648/76743 of epoch 2, 29.90 ms/it, loss 0.445313
Finished training it 27648/76743 of epoch 2, 29.67 ms/it, loss 0.446020
Finished training it 27648/76743 of epoch 2, 29.62 ms/it, loss 0.447089
Finished training it 27648/76743 of epoch 2, 29.60 ms/it, loss 0.448954
Finished training it 28672/76743 of epoch 2, 29.61 ms/it, loss 0.447931
Finished training it 28672/76743 of epoch 2, 29.26 ms/it, loss 0.446527
Finished training it 28672/76743 of epoch 2, 29.27 ms/it, loss 0.446799
Finished training it 28672/76743 of epoch 2, 29.25 ms/it, loss 0.445774
Finished training it 29696/76743 of epoch 2, 29.42 ms/it, loss 0.448091
Finished training it 29696/76743 of epoch 2, 29.48 ms/it, loss 0.447152
Finished training it 29696/76743 of epoch 2, 29.55 ms/it, loss 0.446838
Finished training it 29696/76743 of epoch 2, 29.91 ms/it, loss 0.447287
Finished training it 30720/76743 of epoch 2, 29.84 ms/it, loss 0.444299
Finished training it 30720/76743 of epoch 2, 29.82 ms/it, loss 0.447574
Finished training it 30720/76743 of epoch 2, 29.93 ms/it, loss 0.443455
Finished training it 30720/76743 of epoch 2, 29.55 ms/it, loss 0.445137
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578639.0
get out
0 has test check 2578639.0 and sample count 3274240
 accuracy 78.755 %, best 78.755 %, roc auc score 0.8001, best 0.8001
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578639.0
get out
1 has test check 2578639.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 29.44 ms/it, loss 0.445826
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578639.0
get out
2 has test check 2578639.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 29.46 ms/it, loss 0.446376
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578639.0
get out
3 has test check 2578639.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 29.45 ms/it, loss 0.447437
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 29.60 ms/it, loss 0.443971
Finished training it 32768/76743 of epoch 2, 29.71 ms/it, loss 0.446253
Finished training it 32768/76743 of epoch 2, 29.68 ms/it, loss 0.443902
Finished training it 32768/76743 of epoch 2, 29.66 ms/it, loss 0.446887
Finished training it 32768/76743 of epoch 2, 30.04 ms/it, loss 0.444157
Finished training it 33792/76743 of epoch 2, 29.67 ms/it, loss 0.445746
Finished training it 33792/76743 of epoch 2, 29.49 ms/it, loss 0.447326
Finished training it 33792/76743 of epoch 2, 29.76 ms/it, loss 0.446971
Finished training it 33792/76743 of epoch 2, 29.52 ms/it, loss 0.445668
Finished training it 34816/76743 of epoch 2, 29.76 ms/it, loss 0.446402
Finished training it 34816/76743 of epoch 2, 29.40 ms/it, loss 0.447599
Finished training it 34816/76743 of epoch 2, 29.76 ms/it, loss 0.448500
Finished training it 34816/76743 of epoch 2, 29.30 ms/it, loss 0.443672
Finished training it 35840/76743 of epoch 2, 29.62 ms/it, loss 0.447073
Finished training it 35840/76743 of epoch 2, 29.79 ms/it, loss 0.445900
Finished training it 35840/76743 of epoch 2, 29.77 ms/it, loss 0.442281
Finished training it 35840/76743 of epoch 2, 29.62 ms/it, loss 0.446751
Finished training it 36864/76743 of epoch 2, 29.39 ms/it, loss 0.447353
Finished training it 36864/76743 of epoch 2, 29.90 ms/it, loss 0.445817
Finished training it 36864/76743 of epoch 2, 29.68 ms/it, loss 0.445254
Finished training it 36864/76743 of epoch 2, 29.32 ms/it, loss 0.442875
Finished training it 37888/76743 of epoch 2, 29.74 ms/it, loss 0.448413
Finished training it 37888/76743 of epoch 2, 29.68 ms/it, loss 0.443890
Finished training it 37888/76743 of epoch 2, 29.82 ms/it, loss 0.446891
Finished training it 37888/76743 of epoch 2, 29.65 ms/it, loss 0.444906
Finished training it 38912/76743 of epoch 2, 29.43 ms/it, loss 0.447600
Finished training it 38912/76743 of epoch 2, 29.64 ms/it, loss 0.442886
Finished training it 38912/76743 of epoch 2, 29.24 ms/it, loss 0.446728
Finished training it 38912/76743 of epoch 2, 29.56 ms/it, loss 0.445508
Finished training it 39936/76743 of epoch 2, 29.55 ms/it, loss 0.448145
Finished training it 39936/76743 of epoch 2, 29.76 ms/it, loss 0.445046
Finished training it 39936/76743 of epoch 2, 29.64 ms/it, loss 0.443054
Finished training it 39936/76743 of epoch 2, 29.67 ms/it, loss 0.443689
Finished training it 40960/76743 of epoch 2, 29.93 ms/it, loss 0.448249
Finished training it 40960/76743 of epoch 2, 29.79 ms/it, loss 0.447315
Finished training it 40960/76743 of epoch 2, 29.79 ms/it, loss 0.443299
Finished training it 40960/76743 of epoch 2, 29.66 ms/it, loss 0.446310
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578966.0
get out
0 has test check 2578966.0 and sample count 3274240
 accuracy 78.765 %, best 78.765 %, roc auc score 0.8002, best 0.8002
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578966.0
get out
2 has test check 2578966.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 29.29 ms/it, loss 0.444946
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578966.0
get out
3 has test check 2578966.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 29.64 ms/it, loss 0.447753
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578966.0
get out
1 has test check 2578966.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 29.30 ms/it, loss 0.445096
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 29.66 ms/it, loss 0.447971
Finished training it 43008/76743 of epoch 2, 29.42 ms/it, loss 0.444391
Finished training it 43008/76743 of epoch 2, 29.67 ms/it, loss 0.442349
Finished training it 43008/76743 of epoch 2, 29.33 ms/it, loss 0.444052
Finished training it 43008/76743 of epoch 2, 29.23 ms/it, loss 0.445760
Finished training it 44032/76743 of epoch 2, 29.71 ms/it, loss 0.446877
Finished training it 44032/76743 of epoch 2, 29.65 ms/it, loss 0.446714
Finished training it 44032/76743 of epoch 2, 29.69 ms/it, loss 0.445936
Finished training it 44032/76743 of epoch 2, 29.79 ms/it, loss 0.446730
Finished training it 45056/76743 of epoch 2, 29.95 ms/it, loss 0.444303
Finished training it 45056/76743 of epoch 2, 29.83 ms/it, loss 0.446545
Finished training it 45056/76743 of epoch 2, 29.84 ms/it, loss 0.445535
Finished training it 45056/76743 of epoch 2, 29.93 ms/it, loss 0.446019
Finished training it 46080/76743 of epoch 2, 29.38 ms/it, loss 0.443836
Finished training it 46080/76743 of epoch 2, 29.28 ms/it, loss 0.444825
Finished training it 46080/76743 of epoch 2, 29.74 ms/it, loss 0.445749
Finished training it 46080/76743 of epoch 2, 29.24 ms/it, loss 0.445767
Finished training it 47104/76743 of epoch 2, 34.76 ms/it, loss 0.447540
Finished training it 47104/76743 of epoch 2, 34.55 ms/it, loss 0.444465
Finished training it 47104/76743 of epoch 2, 35.20 ms/it, loss 0.446046
Finished training it 47104/76743 of epoch 2, 34.72 ms/it, loss 0.446942
Finished training it 48128/76743 of epoch 2, 29.62 ms/it, loss 0.446538
Finished training it 48128/76743 of epoch 2, 30.15 ms/it, loss 0.446866
Finished training it 48128/76743 of epoch 2, 30.21 ms/it, loss 0.444650
Finished training it 48128/76743 of epoch 2, 29.74 ms/it, loss 0.446350
Finished training it 49152/76743 of epoch 2, 29.73 ms/it, loss 0.446782
Finished training it 49152/76743 of epoch 2, 29.29 ms/it, loss 0.445306
Finished training it 49152/76743 of epoch 2, 29.65 ms/it, loss 0.444108
Finished training it 49152/76743 of epoch 2, 29.87 ms/it, loss 0.447295
Finished training it 50176/76743 of epoch 2, 29.92 ms/it, loss 0.446405
Finished training it 50176/76743 of epoch 2, 29.85 ms/it, loss 0.445824
Finished training it 50176/76743 of epoch 2, 29.95 ms/it, loss 0.444585
Finished training it 50176/76743 of epoch 2, 30.00 ms/it, loss 0.443583
Finished training it 51200/76743 of epoch 2, 29.63 ms/it, loss 0.443627
Finished training it 51200/76743 of epoch 2, 29.40 ms/it, loss 0.445448
Finished training it 51200/76743 of epoch 2, 29.46 ms/it, loss 0.442648
Finished training it 51200/76743 of epoch 2, 29.40 ms/it, loss 0.443283
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578458.0
get out
0 has test check 2578458.0 and sample count 3274240
 accuracy 78.750 %, best 78.765 %, roc auc score 0.8006, best 0.8006
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 29.57 ms/it, loss 0.447440
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578458.0
get out
3 has test check 2578458.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 29.63 ms/it, loss 0.443784
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578458.0
get out
2 has test check 2578458.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 29.47 ms/it, loss 0.443002
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578458.0
get out
1 has test check 2578458.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 29.46 ms/it, loss 0.445323
Finished training it 53248/76743 of epoch 2, 29.41 ms/it, loss 0.445440
Finished training it 53248/76743 of epoch 2, 29.48 ms/it, loss 0.444774
Finished training it 53248/76743 of epoch 2, 29.41 ms/it, loss 0.446239
Finished training it 53248/76743 of epoch 2, 29.71 ms/it, loss 0.444163
Finished training it 54272/76743 of epoch 2, 29.61 ms/it, loss 0.446906
Finished training it 54272/76743 of epoch 2, 29.71 ms/it, loss 0.443980
Finished training it 54272/76743 of epoch 2, 29.74 ms/it, loss 0.445508
Finished training it 54272/76743 of epoch 2, 29.64 ms/it, loss 0.444582
Finished training it 55296/76743 of epoch 2, 29.59 ms/it, loss 0.445733
Finished training it 55296/76743 of epoch 2, 29.55 ms/it, loss 0.443666
Finished training it 55296/76743 of epoch 2, 29.64 ms/it, loss 0.446509
Finished training it 55296/76743 of epoch 2, 29.58 ms/it, loss 0.443964
Finished training it 56320/76743 of epoch 2, 29.11 ms/it, loss 0.444711
Finished training it 56320/76743 of epoch 2, 29.23 ms/it, loss 0.440980
Finished training it 56320/76743 of epoch 2, 29.85 ms/it, loss 0.444067
Finished training it 56320/76743 of epoch 2, 29.47 ms/it, loss 0.445896
Finished training it 57344/76743 of epoch 2, 29.82 ms/it, loss 0.444723
Finished training it 57344/76743 of epoch 2, 29.64 ms/it, loss 0.445118
Finished training it 57344/76743 of epoch 2, 30.19 ms/it, loss 0.444101
Finished training it 57344/76743 of epoch 2, 29.77 ms/it, loss 0.446156
Finished training it 58368/76743 of epoch 2, 30.08 ms/it, loss 0.447196
Finished training it 58368/76743 of epoch 2, 30.02 ms/it, loss 0.443342
Finished training it 58368/76743 of epoch 2, 29.99 ms/it, loss 0.444038
Finished training it 58368/76743 of epoch 2, 29.78 ms/it, loss 0.446935
Finished training it 59392/76743 of epoch 2, 29.87 ms/it, loss 0.445693
Finished training it 59392/76743 of epoch 2, 29.49 ms/it, loss 0.444862
Finished training it 59392/76743 of epoch 2, 29.81 ms/it, loss 0.447064
Finished training it 59392/76743 of epoch 2, 29.60 ms/it, loss 0.444850
Finished training it 60416/76743 of epoch 2, 29.62 ms/it, loss 0.447551
Finished training it 60416/76743 of epoch 2, 29.68 ms/it, loss 0.446319
Finished training it 60416/76743 of epoch 2, 29.69 ms/it, loss 0.444676
Finished training it 60416/76743 of epoch 2, 29.58 ms/it, loss 0.447194
Finished training it 61440/76743 of epoch 2, 29.58 ms/it, loss 0.445643
Finished training it 61440/76743 of epoch 2, 29.47 ms/it, loss 0.444428
Finished training it 61440/76743 of epoch 2, 29.66 ms/it, loss 0.446553
Finished training it 61440/76743 of epoch 2, 29.67 ms/it, loss 0.447194
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578467.0
get out
0 has test check 2578467.0 and sample count 3274240
 accuracy 78.750 %, best 78.765 %, roc auc score 0.8007, best 0.8007
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578467.0
get out
1 has test check 2578467.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 29.54 ms/it, loss 0.441692
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 29.59 ms/it, loss 0.444676
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578467.0
get out
2 has test check 2578467.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 29.57 ms/it, loss 0.443586
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578467.0
get out
3 has test check 2578467.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 29.58 ms/it, loss 0.444746
Finished training it 63488/76743 of epoch 2, 30.09 ms/it, loss 0.446158
Finished training it 63488/76743 of epoch 2, 30.02 ms/it, loss 0.446264
Finished training it 63488/76743 of epoch 2, 29.77 ms/it, loss 0.442047
Finished training it 63488/76743 of epoch 2, 30.22 ms/it, loss 0.444198
Finished training it 64512/76743 of epoch 2, 29.57 ms/it, loss 0.447176
Finished training it 64512/76743 of epoch 2, 29.28 ms/it, loss 0.443084
Finished training it 64512/76743 of epoch 2, 29.41 ms/it, loss 0.444488
Finished training it 64512/76743 of epoch 2, 29.36 ms/it, loss 0.444780
Finished training it 65536/76743 of epoch 2, 30.09 ms/it, loss 0.443278
Finished training it 65536/76743 of epoch 2, 29.71 ms/it, loss 0.446223
Finished training it 65536/76743 of epoch 2, 29.68 ms/it, loss 0.444050
Finished training it 65536/76743 of epoch 2, 29.70 ms/it, loss 0.444464
Finished training it 66560/76743 of epoch 2, 29.77 ms/it, loss 0.444792
Finished training it 66560/76743 of epoch 2, 29.66 ms/it, loss 0.445690
Finished training it 66560/76743 of epoch 2, 29.94 ms/it, loss 0.444267
Finished training it 66560/76743 of epoch 2, 29.67 ms/it, loss 0.444463
Finished training it 67584/76743 of epoch 2, 29.57 ms/it, loss 0.444878
Finished training it 67584/76743 of epoch 2, 29.43 ms/it, loss 0.445517
Finished training it 67584/76743 of epoch 2, 29.16 ms/it, loss 0.442465
Finished training it 67584/76743 of epoch 2, 29.64 ms/it, loss 0.445311
Finished training it 68608/76743 of epoch 2, 29.43 ms/it, loss 0.444077
Finished training it 68608/76743 of epoch 2, 29.30 ms/it, loss 0.445398
Finished training it 68608/76743 of epoch 2, 29.57 ms/it, loss 0.444642
Finished training it 68608/76743 of epoch 2, 29.53 ms/it, loss 0.442970
Finished training it 69632/76743 of epoch 2, 29.39 ms/it, loss 0.441211
Finished training it 69632/76743 of epoch 2, 29.63 ms/it, loss 0.443839
Finished training it 69632/76743 of epoch 2, 29.16 ms/it, loss 0.442563
Finished training it 69632/76743 of epoch 2, 29.60 ms/it, loss 0.444057
Finished training it 70656/76743 of epoch 2, 29.75 ms/it, loss 0.446695
Finished training it 70656/76743 of epoch 2, 29.45 ms/it, loss 0.445339
Finished training it 70656/76743 of epoch 2, 29.69 ms/it, loss 0.443127
Finished training it 70656/76743 of epoch 2, 29.31 ms/it, loss 0.446125
Finished training it 71680/76743 of epoch 2, 29.94 ms/it, loss 0.442193
Finished training it 71680/76743 of epoch 2, 30.00 ms/it, loss 0.445414
Finished training it 71680/76743 of epoch 2, 29.98 ms/it, loss 0.444826
Finished training it 71680/76743 of epoch 2, 29.97 ms/it, loss 0.444215
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578918.0
get out
0 has test check 2578918.0 and sample count 3274240
 accuracy 78.764 %, best 78.765 %, roc auc score 0.8011, best 0.8011
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578918.0
get out
3 has test check 2578918.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 29.31 ms/it, loss 0.443244
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 29.19 ms/it, loss 0.445543
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578918.0
get out
2 has test check 2578918.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 28.97 ms/it, loss 0.443827
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578918.0
get out
1 has test check 2578918.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 29.00 ms/it, loss 0.444101
Finished training it 73728/76743 of epoch 2, 29.50 ms/it, loss 0.445222
Finished training it 73728/76743 of epoch 2, 29.83 ms/it, loss 0.443790
Finished training it 73728/76743 of epoch 2, 29.33 ms/it, loss 0.445372
Finished training it 73728/76743 of epoch 2, 29.74 ms/it, loss 0.442860
Finished training it 74752/76743 of epoch 2, 29.61 ms/it, loss 0.443605
Finished training it 74752/76743 of epoch 2, 29.41 ms/it, loss 0.443857
Finished training it 74752/76743 of epoch 2, 29.67 ms/it, loss 0.444752
Finished training it 74752/76743 of epoch 2, 29.43 ms/it, loss 0.444768
Finished training it 75776/76743 of epoch 2, 29.50 ms/it, loss 0.443504
Finished training it 75776/76743 of epoch 2, 29.18 ms/it, loss 0.443419
Finished training it 75776/76743 of epoch 2, 29.36 ms/it, loss 0.443043
Finished training it 75776/76743 of epoch 2, 29.39 ms/it, loss 0.444273
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 32.12 ms/it, loss 0.441835
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 31.99 ms/it, loss 0.443430
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 31.84 ms/it, loss 0.443387
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 32.04 ms/it, loss 0.444930
Finished training it 2048/76743 of epoch 3, 29.13 ms/it, loss 0.440449
Finished training it 2048/76743 of epoch 3, 30.16 ms/it, loss 0.442962
Finished training it 2048/76743 of epoch 3, 30.03 ms/it, loss 0.443439
Finished training it 2048/76743 of epoch 3, 29.33 ms/it, loss 0.443452
Finished training it 3072/76743 of epoch 3, 29.76 ms/it, loss 0.444495
Finished training it 3072/76743 of epoch 3, 29.89 ms/it, loss 0.442615
Finished training it 3072/76743 of epoch 3, 29.75 ms/it, loss 0.444136
Finished training it 3072/76743 of epoch 3, 29.87 ms/it, loss 0.446184
Finished training it 4096/76743 of epoch 3, 29.40 ms/it, loss 0.443935
Finished training it 4096/76743 of epoch 3, 29.29 ms/it, loss 0.444186
Finished training it 4096/76743 of epoch 3, 29.42 ms/it, loss 0.445980
Finished training it 4096/76743 of epoch 3, 29.42 ms/it, loss 0.443520
Finished training it 5120/76743 of epoch 3, 29.48 ms/it, loss 0.442271
Finished training it 5120/76743 of epoch 3, 29.51 ms/it, loss 0.443367
Finished training it 5120/76743 of epoch 3, 29.89 ms/it, loss 0.443154
Finished training it 5120/76743 of epoch 3, 29.65 ms/it, loss 0.444218
Finished training it 6144/76743 of epoch 3, 29.42 ms/it, loss 0.443581
Finished training it 6144/76743 of epoch 3, 29.52 ms/it, loss 0.442769
Finished training it 6144/76743 of epoch 3, 29.70 ms/it, loss 0.445431
Finished training it 6144/76743 of epoch 3, 29.57 ms/it, loss 0.444389
Finished training it 7168/76743 of epoch 3, 29.14 ms/it, loss 0.443101
Finished training it 7168/76743 of epoch 3, 29.35 ms/it, loss 0.442915
Finished training it 7168/76743 of epoch 3, 28.93 ms/it, loss 0.444822
Finished training it 7168/76743 of epoch 3, 29.43 ms/it, loss 0.445091
Finished training it 8192/76743 of epoch 3, 30.04 ms/it, loss 0.440717
Finished training it 8192/76743 of epoch 3, 29.92 ms/it, loss 0.443932
Finished training it 8192/76743 of epoch 3, 29.70 ms/it, loss 0.444061
Finished training it 8192/76743 of epoch 3, 29.90 ms/it, loss 0.443002
Finished training it 9216/76743 of epoch 3, 29.74 ms/it, loss 0.445412
Finished training it 9216/76743 of epoch 3, 29.73 ms/it, loss 0.441744
Finished training it 9216/76743 of epoch 3, 29.64 ms/it, loss 0.444921
Finished training it 9216/76743 of epoch 3, 29.62 ms/it, loss 0.442882
Finished training it 10240/76743 of epoch 3, 29.70 ms/it, loss 0.442908
Finished training it 10240/76743 of epoch 3, 30.00 ms/it, loss 0.443561
Finished training it 10240/76743 of epoch 3, 29.60 ms/it, loss 0.440783
Finished training it 10240/76743 of epoch 3, 29.68 ms/it, loss 0.444751
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579745.0
get out
0 has test check 2579745.0 and sample count 3274240
 accuracy 78.789 %, best 78.789 %, roc auc score 0.8008, best 0.8011
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579745.0
get out
3 has test check 2579745.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 29.69 ms/it, loss 0.443822
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 29.75 ms/it, loss 0.441322
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579745.0
get out
2 has test check 2579745.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 29.58 ms/it, loss 0.442010
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579745.0
get out
1 has test check 2579745.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 29.53 ms/it, loss 0.442359
Finished training it 12288/76743 of epoch 3, 29.73 ms/it, loss 0.442452
Finished training it 12288/76743 of epoch 3, 29.65 ms/it, loss 0.444776
Finished training it 12288/76743 of epoch 3, 30.11 ms/it, loss 0.445176
Finished training it 12288/76743 of epoch 3, 29.65 ms/it, loss 0.443828
Finished training it 13312/76743 of epoch 3, 29.71 ms/it, loss 0.442878
Finished training it 13312/76743 of epoch 3, 29.56 ms/it, loss 0.445287
Finished training it 13312/76743 of epoch 3, 29.46 ms/it, loss 0.443689
Finished training it 13312/76743 of epoch 3, 29.88 ms/it, loss 0.444505
Finished training it 14336/76743 of epoch 3, 29.78 ms/it, loss 0.444221
Finished training it 14336/76743 of epoch 3, 29.66 ms/it, loss 0.442303
Finished training it 14336/76743 of epoch 3, 29.93 ms/it, loss 0.443327
Finished training it 14336/76743 of epoch 3, 29.47 ms/it, loss 0.442726
Finished training it 15360/76743 of epoch 3, 30.26 ms/it, loss 0.442689
Finished training it 15360/76743 of epoch 3, 29.88 ms/it, loss 0.440784
Finished training it 15360/76743 of epoch 3, 29.88 ms/it, loss 0.444846
Finished training it 15360/76743 of epoch 3, 29.94 ms/it, loss 0.445037
Finished training it 16384/76743 of epoch 3, 29.02 ms/it, loss 0.442678
Finished training it 16384/76743 of epoch 3, 29.51 ms/it, loss 0.443232
Finished training it 16384/76743 of epoch 3, 29.14 ms/it, loss 0.443553
Finished training it 16384/76743 of epoch 3, 29.05 ms/it, loss 0.442093
Finished training it 17408/76743 of epoch 3, 29.49 ms/it, loss 0.443998
Finished training it 17408/76743 of epoch 3, 29.83 ms/it, loss 0.439103
Finished training it 17408/76743 of epoch 3, 29.91 ms/it, loss 0.445037
Finished training it 17408/76743 of epoch 3, 29.67 ms/it, loss 0.445867
Finished training it 18432/76743 of epoch 3, 29.74 ms/it, loss 0.442399
Finished training it 18432/76743 of epoch 3, 29.52 ms/it, loss 0.443977
Finished training it 18432/76743 of epoch 3, 29.67 ms/it, loss 0.444171
Finished training it 18432/76743 of epoch 3, 29.35 ms/it, loss 0.442652
Finished training it 19456/76743 of epoch 3, 29.59 ms/it, loss 0.445482
Finished training it 19456/76743 of epoch 3, 29.35 ms/it, loss 0.443284
Finished training it 19456/76743 of epoch 3, 29.44 ms/it, loss 0.446416
Finished training it 19456/76743 of epoch 3, 29.55 ms/it, loss 0.441775
Finished training it 20480/76743 of epoch 3, 30.02 ms/it, loss 0.443904
Finished training it 20480/76743 of epoch 3, 29.65 ms/it, loss 0.442858
Finished training it 20480/76743 of epoch 3, 29.54 ms/it, loss 0.442816
Finished training it 20480/76743 of epoch 3, 29.49 ms/it, loss 0.441127
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580884.0
get out
0 has test check 2580884.0 and sample count 3274240
 accuracy 78.824 %, best 78.824 %, roc auc score 0.8018, best 0.8018
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580884.0
get out
3 has test check 2580884.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 29.27 ms/it, loss 0.443209
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 29.45 ms/it, loss 0.441487
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580884.0
get out
1 has test check 2580884.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 29.39 ms/it, loss 0.444578
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580884.0
get out
2 has test check 2580884.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 29.29 ms/it, loss 0.440267
Finished training it 22528/76743 of epoch 3, 29.96 ms/it, loss 0.442863
Finished training it 22528/76743 of epoch 3, 29.57 ms/it, loss 0.443769
Finished training it 22528/76743 of epoch 3, 29.41 ms/it, loss 0.441122
Finished training it 22528/76743 of epoch 3, 29.44 ms/it, loss 0.443147
Finished training it 23552/76743 of epoch 3, 30.15 ms/it, loss 0.444636
Finished training it 23552/76743 of epoch 3, 29.69 ms/it, loss 0.444716
Finished training it 23552/76743 of epoch 3, 29.58 ms/it, loss 0.442875
Finished training it 23552/76743 of epoch 3, 29.73 ms/it, loss 0.442610
Finished training it 24576/76743 of epoch 3, 29.19 ms/it, loss 0.442780
Finished training it 24576/76743 of epoch 3, 29.53 ms/it, loss 0.444513
Finished training it 24576/76743 of epoch 3, 29.24 ms/it, loss 0.443684
Finished training it 24576/76743 of epoch 3, 29.62 ms/it, loss 0.444034
Finished training it 25600/76743 of epoch 3, 29.45 ms/it, loss 0.442256
Finished training it 25600/76743 of epoch 3, 29.40 ms/it, loss 0.442300
Finished training it 25600/76743 of epoch 3, 29.47 ms/it, loss 0.444965
Finished training it 25600/76743 of epoch 3, 29.29 ms/it, loss 0.441418
Finished training it 26624/76743 of epoch 3, 34.96 ms/it, loss 0.443077
Finished training it 26624/76743 of epoch 3, 35.17 ms/it, loss 0.440090
Finished training it 26624/76743 of epoch 3, 34.46 ms/it, loss 0.442105
Finished training it 26624/76743 of epoch 3, 33.72 ms/it, loss 0.442425
Finished training it 27648/76743 of epoch 3, 29.61 ms/it, loss 0.443747
Finished training it 27648/76743 of epoch 3, 30.47 ms/it, loss 0.441806
Finished training it 27648/76743 of epoch 3, 29.94 ms/it, loss 0.442743
Finished training it 27648/76743 of epoch 3, 29.74 ms/it, loss 0.445723
Finished training it 28672/76743 of epoch 3, 30.07 ms/it, loss 0.444741
Finished training it 28672/76743 of epoch 3, 29.68 ms/it, loss 0.443539
Finished training it 28672/76743 of epoch 3, 29.76 ms/it, loss 0.442488
Finished training it 28672/76743 of epoch 3, 29.50 ms/it, loss 0.443172
Finished training it 29696/76743 of epoch 3, 29.78 ms/it, loss 0.443788
Finished training it 29696/76743 of epoch 3, 29.69 ms/it, loss 0.444908
Finished training it 29696/76743 of epoch 3, 29.75 ms/it, loss 0.443616
Finished training it 29696/76743 of epoch 3, 30.18 ms/it, loss 0.444106
Finished training it 30720/76743 of epoch 3, 30.11 ms/it, loss 0.444173
Finished training it 30720/76743 of epoch 3, 29.64 ms/it, loss 0.441888
Finished training it 30720/76743 of epoch 3, 29.66 ms/it, loss 0.441100
Finished training it 30720/76743 of epoch 3, 29.73 ms/it, loss 0.440124
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580645.0
get out
0 has test check 2580645.0 and sample count 3274240
 accuracy 78.817 %, best 78.824 %, roc auc score 0.8016, best 0.8018
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580645.0
get out
3 has test check 2580645.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 29.86 ms/it, loss 0.444157
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 29.59 ms/it, loss 0.440549
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580645.0
get out
2 has test check 2580645.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 29.50 ms/it, loss 0.443189
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580645.0
get out
1 has test check 2580645.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 29.66 ms/it, loss 0.442548
Finished training it 32768/76743 of epoch 3, 29.94 ms/it, loss 0.440662
Finished training it 32768/76743 of epoch 3, 29.39 ms/it, loss 0.443075
Finished training it 32768/76743 of epoch 3, 29.34 ms/it, loss 0.443718
Finished training it 32768/76743 of epoch 3, 29.38 ms/it, loss 0.440607
Finished training it 33792/76743 of epoch 3, 29.88 ms/it, loss 0.443805
Finished training it 33792/76743 of epoch 3, 29.69 ms/it, loss 0.442640
Finished training it 33792/76743 of epoch 3, 29.89 ms/it, loss 0.444115
Finished training it 33792/76743 of epoch 3, 29.81 ms/it, loss 0.442359
Finished training it 34816/76743 of epoch 3, 29.92 ms/it, loss 0.440256
Finished training it 34816/76743 of epoch 3, 29.75 ms/it, loss 0.444297
Finished training it 34816/76743 of epoch 3, 29.84 ms/it, loss 0.443173
Finished training it 34816/76743 of epoch 3, 29.87 ms/it, loss 0.445119
Finished training it 35840/76743 of epoch 3, 29.56 ms/it, loss 0.443902
Finished training it 35840/76743 of epoch 3, 29.67 ms/it, loss 0.438895
Finished training it 35840/76743 of epoch 3, 29.33 ms/it, loss 0.443454
Finished training it 35840/76743 of epoch 3, 29.58 ms/it, loss 0.442774
Finished training it 36864/76743 of epoch 3, 29.98 ms/it, loss 0.442545
Finished training it 36864/76743 of epoch 3, 29.75 ms/it, loss 0.439675
Finished training it 36864/76743 of epoch 3, 29.61 ms/it, loss 0.441994
Finished training it 36864/76743 of epoch 3, 29.71 ms/it, loss 0.444241
Finished training it 37888/76743 of epoch 3, 29.61 ms/it, loss 0.445150
Finished training it 37888/76743 of epoch 3, 29.85 ms/it, loss 0.441560
Finished training it 37888/76743 of epoch 3, 29.96 ms/it, loss 0.443445
Finished training it 37888/76743 of epoch 3, 29.51 ms/it, loss 0.440619
Finished training it 38912/76743 of epoch 3, 30.00 ms/it, loss 0.439634
Finished training it 38912/76743 of epoch 3, 29.76 ms/it, loss 0.442230
Finished training it 38912/76743 of epoch 3, 29.78 ms/it, loss 0.443595
Finished training it 38912/76743 of epoch 3, 29.67 ms/it, loss 0.444314
Finished training it 39936/76743 of epoch 3, 30.39 ms/it, loss 0.441845
Finished training it 39936/76743 of epoch 3, 30.13 ms/it, loss 0.439643
Finished training it 39936/76743 of epoch 3, 30.18 ms/it, loss 0.440505
Finished training it 39936/76743 of epoch 3, 29.88 ms/it, loss 0.444970
Finished training it 40960/76743 of epoch 3, 29.72 ms/it, loss 0.443301
Finished training it 40960/76743 of epoch 3, 30.06 ms/it, loss 0.444953
Finished training it 40960/76743 of epoch 3, 29.82 ms/it, loss 0.444176
Finished training it 40960/76743 of epoch 3, 30.05 ms/it, loss 0.440123
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580633.0
get out
0 has test check 2580633.0 and sample count 3274240
 accuracy 78.816 %, best 78.824 %, roc auc score 0.8014, best 0.8018
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 29.79 ms/it, loss 0.444597
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580633.0
get out
2 has test check 2580633.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 29.68 ms/it, loss 0.441707
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580633.0
get out
3 has test check 2580633.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 29.77 ms/it, loss 0.444534
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580633.0
get out
1 has test check 2580633.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 29.61 ms/it, loss 0.442187
Finished training it 43008/76743 of epoch 3, 29.12 ms/it, loss 0.441176
Finished training it 43008/76743 of epoch 3, 29.56 ms/it, loss 0.442488
Finished training it 43008/76743 of epoch 3, 29.86 ms/it, loss 0.438960
Finished training it 43008/76743 of epoch 3, 29.31 ms/it, loss 0.441126
Finished training it 44032/76743 of epoch 3, 29.47 ms/it, loss 0.443387
Finished training it 44032/76743 of epoch 3, 29.81 ms/it, loss 0.443724
Finished training it 44032/76743 of epoch 3, 29.60 ms/it, loss 0.442800
Finished training it 44032/76743 of epoch 3, 29.59 ms/it, loss 0.443706
Finished training it 45056/76743 of epoch 3, 29.86 ms/it, loss 0.442883
Finished training it 45056/76743 of epoch 3, 29.81 ms/it, loss 0.443177
Finished training it 45056/76743 of epoch 3, 29.77 ms/it, loss 0.442139
Finished training it 45056/76743 of epoch 3, 29.64 ms/it, loss 0.441089
Finished training it 46080/76743 of epoch 3, 29.10 ms/it, loss 0.441503
Finished training it 46080/76743 of epoch 3, 29.86 ms/it, loss 0.442341
Finished training it 46080/76743 of epoch 3, 29.44 ms/it, loss 0.442536
Finished training it 46080/76743 of epoch 3, 29.23 ms/it, loss 0.440724
Finished training it 47104/76743 of epoch 3, 29.85 ms/it, loss 0.443809
Finished training it 47104/76743 of epoch 3, 29.71 ms/it, loss 0.441249
Finished training it 47104/76743 of epoch 3, 29.54 ms/it, loss 0.442669
Finished training it 47104/76743 of epoch 3, 29.80 ms/it, loss 0.444127
Finished training it 48128/76743 of epoch 3, 30.18 ms/it, loss 0.441491
Finished training it 48128/76743 of epoch 3, 29.98 ms/it, loss 0.443586
Finished training it 48128/76743 of epoch 3, 29.67 ms/it, loss 0.443426
Finished training it 48128/76743 of epoch 3, 29.78 ms/it, loss 0.443154
Finished training it 49152/76743 of epoch 3, 29.82 ms/it, loss 0.444013
Finished training it 49152/76743 of epoch 3, 29.55 ms/it, loss 0.442144
Finished training it 49152/76743 of epoch 3, 29.34 ms/it, loss 0.440900
Finished training it 49152/76743 of epoch 3, 29.69 ms/it, loss 0.443368
Finished training it 50176/76743 of epoch 3, 29.62 ms/it, loss 0.440414
Finished training it 50176/76743 of epoch 3, 29.52 ms/it, loss 0.441342
Finished training it 50176/76743 of epoch 3, 29.62 ms/it, loss 0.442759
Finished training it 50176/76743 of epoch 3, 29.81 ms/it, loss 0.443057
Finished training it 51200/76743 of epoch 3, 29.70 ms/it, loss 0.440080
Finished training it 51200/76743 of epoch 3, 29.92 ms/it, loss 0.440465
Finished training it 51200/76743 of epoch 3, 29.64 ms/it, loss 0.439655
Finished training it 51200/76743 of epoch 3, 29.46 ms/it, loss 0.441989
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580330.0
get out
0 has test check 2580330.0 and sample count 3274240
 accuracy 78.807 %, best 78.824 %, roc auc score 0.8018, best 0.8018
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580330.0
get out
1 has test check 2580330.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 29.56 ms/it, loss 0.442275
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580330.0
get out
3 has test check 2580330.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 29.58 ms/it, loss 0.440805
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 29.34 ms/it, loss 0.444226
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580330.0
get out
2 has test check 2580330.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 29.45 ms/it, loss 0.439920
Finished training it 53248/76743 of epoch 3, 29.80 ms/it, loss 0.443127
Finished training it 53248/76743 of epoch 3, 29.81 ms/it, loss 0.441714
Finished training it 53248/76743 of epoch 3, 29.89 ms/it, loss 0.442337
Finished training it 53248/76743 of epoch 3, 30.27 ms/it, loss 0.440992
Finished training it 54272/76743 of epoch 3, 30.02 ms/it, loss 0.440738
Finished training it 54272/76743 of epoch 3, 29.67 ms/it, loss 0.441415
Finished training it 54272/76743 of epoch 3, 29.73 ms/it, loss 0.443728
Finished training it 54272/76743 of epoch 3, 29.66 ms/it, loss 0.442410
Finished training it 55296/76743 of epoch 3, 29.63 ms/it, loss 0.442314
Finished training it 55296/76743 of epoch 3, 29.83 ms/it, loss 0.443432
Finished training it 55296/76743 of epoch 3, 29.73 ms/it, loss 0.440565
Finished training it 55296/76743 of epoch 3, 29.65 ms/it, loss 0.441029
Finished training it 56320/76743 of epoch 3, 29.27 ms/it, loss 0.442691
Finished training it 56320/76743 of epoch 3, 29.64 ms/it, loss 0.440834
Finished training it 56320/76743 of epoch 3, 29.05 ms/it, loss 0.437902
Finished training it 56320/76743 of epoch 3, 29.17 ms/it, loss 0.441462
Finished training it 57344/76743 of epoch 3, 34.16 ms/it, loss 0.440962
Finished training it 57344/76743 of epoch 3, 35.19 ms/it, loss 0.442976
Finished training it 57344/76743 of epoch 3, 34.87 ms/it, loss 0.441987
Finished training it 57344/76743 of epoch 3, 35.08 ms/it, loss 0.441537
Finished training it 58368/76743 of epoch 3, 30.02 ms/it, loss 0.444163
Finished training it 58368/76743 of epoch 3, 29.48 ms/it, loss 0.441097
Finished training it 58368/76743 of epoch 3, 29.46 ms/it, loss 0.440182
Finished training it 58368/76743 of epoch 3, 29.54 ms/it, loss 0.443775
Finished training it 59392/76743 of epoch 3, 29.95 ms/it, loss 0.441782
Finished training it 59392/76743 of epoch 3, 29.96 ms/it, loss 0.442437
Finished training it 59392/76743 of epoch 3, 29.87 ms/it, loss 0.441670
Finished training it 59392/76743 of epoch 3, 29.71 ms/it, loss 0.444052
Finished training it 60416/76743 of epoch 3, 29.48 ms/it, loss 0.443132
Finished training it 60416/76743 of epoch 3, 29.51 ms/it, loss 0.444111
Finished training it 60416/76743 of epoch 3, 29.61 ms/it, loss 0.441695
Finished training it 60416/76743 of epoch 3, 29.45 ms/it, loss 0.444452
Finished training it 61440/76743 of epoch 3, 29.83 ms/it, loss 0.443440
Finished training it 61440/76743 of epoch 3, 29.81 ms/it, loss 0.441297
Finished training it 61440/76743 of epoch 3, 29.55 ms/it, loss 0.442556
Finished training it 61440/76743 of epoch 3, 29.49 ms/it, loss 0.443966
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579567.0
get out
0 has test check 2579567.0 and sample count 3274240
 accuracy 78.784 %, best 78.824 %, roc auc score 0.8019, best 0.8019
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579567.0
get out
3 has test check 2579567.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 29.56 ms/it, loss 0.441577
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579567.0
get out
1 has test check 2579567.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 29.46 ms/it, loss 0.438496
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579567.0
get out
2 has test check 2579567.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 29.47 ms/it, loss 0.440437
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 29.36 ms/it, loss 0.441601
Finished training it 63488/76743 of epoch 3, 29.85 ms/it, loss 0.443273
Finished training it 63488/76743 of epoch 3, 29.81 ms/it, loss 0.438819
Finished training it 63488/76743 of epoch 3, 29.72 ms/it, loss 0.443017
Finished training it 63488/76743 of epoch 3, 30.09 ms/it, loss 0.441135
Finished training it 64512/76743 of epoch 3, 29.62 ms/it, loss 0.441593
Finished training it 64512/76743 of epoch 3, 29.72 ms/it, loss 0.440030
Finished training it 64512/76743 of epoch 3, 29.45 ms/it, loss 0.441385
Finished training it 64512/76743 of epoch 3, 29.74 ms/it, loss 0.444233
Finished training it 65536/76743 of epoch 3, 29.64 ms/it, loss 0.441283
Finished training it 65536/76743 of epoch 3, 29.54 ms/it, loss 0.441034
Finished training it 65536/76743 of epoch 3, 29.37 ms/it, loss 0.443264
Finished training it 65536/76743 of epoch 3, 29.56 ms/it, loss 0.440161
Finished training it 66560/76743 of epoch 3, 29.79 ms/it, loss 0.441521
Finished training it 66560/76743 of epoch 3, 29.88 ms/it, loss 0.442449
Finished training it 66560/76743 of epoch 3, 29.67 ms/it, loss 0.441183
Finished training it 66560/76743 of epoch 3, 30.13 ms/it, loss 0.441765
Finished training it 67584/76743 of epoch 3, 29.66 ms/it, loss 0.441778
Finished training it 67584/76743 of epoch 3, 29.94 ms/it, loss 0.442254
Finished training it 67584/76743 of epoch 3, 30.00 ms/it, loss 0.442500
Finished training it 67584/76743 of epoch 3, 29.74 ms/it, loss 0.439314
Finished training it 68608/76743 of epoch 3, 30.01 ms/it, loss 0.441274
Finished training it 68608/76743 of epoch 3, 30.01 ms/it, loss 0.439845
Finished training it 68608/76743 of epoch 3, 29.90 ms/it, loss 0.442312
Finished training it 68608/76743 of epoch 3, 29.57 ms/it, loss 0.441002
Finished training it 69632/76743 of epoch 3, 29.63 ms/it, loss 0.441061
Finished training it 69632/76743 of epoch 3, 29.49 ms/it, loss 0.439553
Finished training it 69632/76743 of epoch 3, 29.50 ms/it, loss 0.438103
Finished training it 69632/76743 of epoch 3, 29.67 ms/it, loss 0.440763
Finished training it 70656/76743 of epoch 3, 29.99 ms/it, loss 0.443564
Finished training it 70656/76743 of epoch 3, 29.63 ms/it, loss 0.439994
Finished training it 70656/76743 of epoch 3, 29.71 ms/it, loss 0.442897
Finished training it 70656/76743 of epoch 3, 29.95 ms/it, loss 0.442251
Finished training it 71680/76743 of epoch 3, 29.58 ms/it, loss 0.441573
Finished training it 71680/76743 of epoch 3, 29.73 ms/it, loss 0.441086
Finished training it 71680/76743 of epoch 3, 29.51 ms/it, loss 0.442429
Finished training it 71680/76743 of epoch 3, 29.31 ms/it, loss 0.439123
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580425.0
get out
0 has test check 2580425.0 and sample count 3274240
 accuracy 78.810 %, best 78.824 %, roc auc score 0.8022, best 0.8022
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580425.0
get out
1 has test check 2580425.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 29.57 ms/it, loss 0.440996
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580425.0
get out
3 has test check 2580425.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 29.49 ms/it, loss 0.440293
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580425.0
get out
2 has test check 2580425.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 29.45 ms/it, loss 0.440788
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 29.67 ms/it, loss 0.442191
Finished training it 73728/76743 of epoch 3, 29.75 ms/it, loss 0.440642
Finished training it 73728/76743 of epoch 3, 29.40 ms/it, loss 0.442157
Finished training it 73728/76743 of epoch 3, 29.42 ms/it, loss 0.442106
Finished training it 73728/76743 of epoch 3, 29.55 ms/it, loss 0.439808
Finished training it 74752/76743 of epoch 3, 29.70 ms/it, loss 0.440799
Finished training it 74752/76743 of epoch 3, 29.75 ms/it, loss 0.441976
Finished training it 74752/76743 of epoch 3, 29.64 ms/it, loss 0.440601
Finished training it 74752/76743 of epoch 3, 30.01 ms/it, loss 0.441676
Finished training it 75776/76743 of epoch 3, 29.72 ms/it, loss 0.440424
Finished training it 75776/76743 of epoch 3, 29.52 ms/it, loss 0.439856
Finished training it 75776/76743 of epoch 3, 29.78 ms/it, loss 0.440228
Finished training it 75776/76743 of epoch 3, 29.51 ms/it, loss 0.441393
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 30.06 ms/it, loss 0.438762
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 29.91 ms/it, loss 0.440323
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 29.82 ms/it, loss 0.440399
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 29.57 ms/it, loss 0.441926
Finished training it 2048/76743 of epoch 4, 29.59 ms/it, loss 0.440429
Finished training it 2048/76743 of epoch 4, 29.42 ms/it, loss 0.437366
Finished training it 2048/76743 of epoch 4, 29.37 ms/it, loss 0.439925
Finished training it 2048/76743 of epoch 4, 29.86 ms/it, loss 0.440316
Finished training it 3072/76743 of epoch 4, 29.59 ms/it, loss 0.440867
Finished training it 3072/76743 of epoch 4, 29.47 ms/it, loss 0.441341
Finished training it 3072/76743 of epoch 4, 29.55 ms/it, loss 0.443131
Finished training it 3072/76743 of epoch 4, 29.79 ms/it, loss 0.439480
Finished training it 4096/76743 of epoch 4, 30.05 ms/it, loss 0.441031
Finished training it 4096/76743 of epoch 4, 30.02 ms/it, loss 0.440842
Finished training it 4096/76743 of epoch 4, 29.68 ms/it, loss 0.440474
Finished training it 4096/76743 of epoch 4, 29.79 ms/it, loss 0.442722
Finished training it 5120/76743 of epoch 4, 29.58 ms/it, loss 0.440047
Finished training it 5120/76743 of epoch 4, 30.03 ms/it, loss 0.439939
Finished training it 5120/76743 of epoch 4, 29.48 ms/it, loss 0.441132
Finished training it 5120/76743 of epoch 4, 29.29 ms/it, loss 0.439148
Finished training it 6144/76743 of epoch 4, 29.73 ms/it, loss 0.441373
Finished training it 6144/76743 of epoch 4, 29.86 ms/it, loss 0.442306
Finished training it 6144/76743 of epoch 4, 29.65 ms/it, loss 0.439713
Finished training it 6144/76743 of epoch 4, 29.67 ms/it, loss 0.440544
Finished training it 7168/76743 of epoch 4, 29.63 ms/it, loss 0.441851
Finished training it 7168/76743 of epoch 4, 30.05 ms/it, loss 0.439864
Finished training it 7168/76743 of epoch 4, 29.65 ms/it, loss 0.441947
Finished training it 7168/76743 of epoch 4, 29.60 ms/it, loss 0.440031
Finished training it 8192/76743 of epoch 4, 29.54 ms/it, loss 0.441054
Finished training it 8192/76743 of epoch 4, 29.83 ms/it, loss 0.439937
Finished training it 8192/76743 of epoch 4, 29.54 ms/it, loss 0.440893
Finished training it 8192/76743 of epoch 4, 29.60 ms/it, loss 0.437530
Finished training it 9216/76743 of epoch 4, 29.80 ms/it, loss 0.442285
Finished training it 9216/76743 of epoch 4, 29.42 ms/it, loss 0.439864
Finished training it 9216/76743 of epoch 4, 29.47 ms/it, loss 0.441798
Finished training it 9216/76743 of epoch 4, 29.67 ms/it, loss 0.438784
Finished training it 10240/76743 of epoch 4, 29.65 ms/it, loss 0.437756
Finished training it 10240/76743 of epoch 4, 29.68 ms/it, loss 0.441771
Finished training it 10240/76743 of epoch 4, 29.29 ms/it, loss 0.440035
Finished training it 10240/76743 of epoch 4, 29.57 ms/it, loss 0.440431
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580924.0
get out
0 has test check 2580924.0 and sample count 3274240
 accuracy 78.825 %, best 78.825 %, roc auc score 0.8016, best 0.8022
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580924.0
get out
3 has test check 2580924.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 29.90 ms/it, loss 0.440782
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580924.0
get out
2 has test check 2580924.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 29.92 ms/it, loss 0.438851
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580924.0
get out
1 has test check 2580924.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 29.85 ms/it, loss 0.439324
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 29.98 ms/it, loss 0.438357
Finished training it 12288/76743 of epoch 4, 29.91 ms/it, loss 0.442024
Finished training it 12288/76743 of epoch 4, 29.67 ms/it, loss 0.439571
Finished training it 12288/76743 of epoch 4, 29.79 ms/it, loss 0.440696
Finished training it 12288/76743 of epoch 4, 29.68 ms/it, loss 0.441537
Finished training it 13312/76743 of epoch 4, 29.66 ms/it, loss 0.439775
Finished training it 13312/76743 of epoch 4, 29.54 ms/it, loss 0.442205
Finished training it 13312/76743 of epoch 4, 29.64 ms/it, loss 0.441376
Finished training it 13312/76743 of epoch 4, 29.64 ms/it, loss 0.440595
Finished training it 14336/76743 of epoch 4, 29.67 ms/it, loss 0.439277
Finished training it 14336/76743 of epoch 4, 30.04 ms/it, loss 0.440331
Finished training it 14336/76743 of epoch 4, 29.65 ms/it, loss 0.441111
Finished training it 14336/76743 of epoch 4, 29.64 ms/it, loss 0.439593
Finished training it 15360/76743 of epoch 4, 29.43 ms/it, loss 0.441829
Finished training it 15360/76743 of epoch 4, 29.45 ms/it, loss 0.442088
Finished training it 15360/76743 of epoch 4, 29.68 ms/it, loss 0.437731
Finished training it 15360/76743 of epoch 4, 29.89 ms/it, loss 0.439661
Finished training it 16384/76743 of epoch 4, 34.38 ms/it, loss 0.440329
Finished training it 16384/76743 of epoch 4, 35.15 ms/it, loss 0.440257
Finished training it 16384/76743 of epoch 4, 34.45 ms/it, loss 0.439227
Finished training it 16384/76743 of epoch 4, 34.64 ms/it, loss 0.439737
Finished training it 17408/76743 of epoch 4, 29.77 ms/it, loss 0.442824
Finished training it 17408/76743 of epoch 4, 30.18 ms/it, loss 0.441998
Finished training it 17408/76743 of epoch 4, 29.76 ms/it, loss 0.436040
Finished training it 17408/76743 of epoch 4, 29.75 ms/it, loss 0.440941
Finished training it 18432/76743 of epoch 4, 29.83 ms/it, loss 0.439131
Finished training it 18432/76743 of epoch 4, 29.44 ms/it, loss 0.440916
Finished training it 18432/76743 of epoch 4, 29.54 ms/it, loss 0.439562
Finished training it 18432/76743 of epoch 4, 29.45 ms/it, loss 0.441114
Finished training it 19456/76743 of epoch 4, 29.67 ms/it, loss 0.438637
Finished training it 19456/76743 of epoch 4, 29.39 ms/it, loss 0.443403
Finished training it 19456/76743 of epoch 4, 29.73 ms/it, loss 0.440151
Finished training it 19456/76743 of epoch 4, 30.24 ms/it, loss 0.442564
Finished training it 20480/76743 of epoch 4, 29.46 ms/it, loss 0.439834
Finished training it 20480/76743 of epoch 4, 29.64 ms/it, loss 0.440752
Finished training it 20480/76743 of epoch 4, 29.35 ms/it, loss 0.439640
Finished training it 20480/76743 of epoch 4, 29.32 ms/it, loss 0.438120
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581885.0
get out
0 has test check 2581885.0 and sample count 3274240
 accuracy 78.854 %, best 78.854 %, roc auc score 0.8027, best 0.8027
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 29.83 ms/it, loss 0.438412
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581885.0
get out
2 has test check 2581885.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 29.88 ms/it, loss 0.436906
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581885.0
get out
3 has test check 2581885.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 29.76 ms/it, loss 0.440150
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581885.0
get out
1 has test check 2581885.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 29.69 ms/it, loss 0.441516
Finished training it 22528/76743 of epoch 4, 30.03 ms/it, loss 0.439920
Finished training it 22528/76743 of epoch 4, 29.67 ms/it, loss 0.440055
Finished training it 22528/76743 of epoch 4, 29.52 ms/it, loss 0.440568
Finished training it 22528/76743 of epoch 4, 29.60 ms/it, loss 0.438051
Finished training it 23552/76743 of epoch 4, 29.96 ms/it, loss 0.441803
Finished training it 23552/76743 of epoch 4, 29.55 ms/it, loss 0.439618
Finished training it 23552/76743 of epoch 4, 29.54 ms/it, loss 0.439769
Finished training it 23552/76743 of epoch 4, 29.91 ms/it, loss 0.441664
Finished training it 24576/76743 of epoch 4, 29.31 ms/it, loss 0.440562
Finished training it 24576/76743 of epoch 4, 29.32 ms/it, loss 0.439724
Finished training it 24576/76743 of epoch 4, 29.70 ms/it, loss 0.441059
Finished training it 24576/76743 of epoch 4, 29.05 ms/it, loss 0.441352
Finished training it 25600/76743 of epoch 4, 29.47 ms/it, loss 0.439429
Finished training it 25600/76743 of epoch 4, 29.41 ms/it, loss 0.438468
Finished training it 25600/76743 of epoch 4, 29.42 ms/it, loss 0.441787
Finished training it 25600/76743 of epoch 4, 29.82 ms/it, loss 0.439055
Finished training it 26624/76743 of epoch 4, 29.81 ms/it, loss 0.439983
Finished training it 26624/76743 of epoch 4, 29.99 ms/it, loss 0.437164
Finished training it 26624/76743 of epoch 4, 29.63 ms/it, loss 0.439263
Finished training it 26624/76743 of epoch 4, 29.76 ms/it, loss 0.438927
Finished training it 27648/76743 of epoch 4, 29.70 ms/it, loss 0.440689
Finished training it 27648/76743 of epoch 4, 29.63 ms/it, loss 0.439733
Finished training it 27648/76743 of epoch 4, 29.45 ms/it, loss 0.442734
Finished training it 27648/76743 of epoch 4, 29.63 ms/it, loss 0.438669
Finished training it 28672/76743 of epoch 4, 29.51 ms/it, loss 0.439412
Finished training it 28672/76743 of epoch 4, 29.51 ms/it, loss 0.441743
Finished training it 28672/76743 of epoch 4, 29.52 ms/it, loss 0.440723
Finished training it 28672/76743 of epoch 4, 29.27 ms/it, loss 0.440163
Finished training it 29696/76743 of epoch 4, 29.83 ms/it, loss 0.440607
Finished training it 29696/76743 of epoch 4, 29.93 ms/it, loss 0.441952
Finished training it 29696/76743 of epoch 4, 29.89 ms/it, loss 0.440490
Finished training it 29696/76743 of epoch 4, 30.19 ms/it, loss 0.441163
Finished training it 30720/76743 of epoch 4, 29.61 ms/it, loss 0.438157
Finished training it 30720/76743 of epoch 4, 29.49 ms/it, loss 0.438815
Finished training it 30720/76743 of epoch 4, 29.24 ms/it, loss 0.437047
Finished training it 30720/76743 of epoch 4, 29.68 ms/it, loss 0.441064
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580824.0
get out
0 has test check 2580824.0 and sample count 3274240
 accuracy 78.822 %, best 78.854 %, roc auc score 0.8022, best 0.8027
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580824.0
get out
3 has test check 2580824.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 29.78 ms/it, loss 0.441095
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 29.91 ms/it, loss 0.437464
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580824.0
get out
1 has test check 2580824.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 29.75 ms/it, loss 0.439506
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580824.0
get out
2 has test check 2580824.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 29.75 ms/it, loss 0.440064
Finished training it 32768/76743 of epoch 4, 30.08 ms/it, loss 0.437474
Finished training it 32768/76743 of epoch 4, 29.80 ms/it, loss 0.440771
Finished training it 32768/76743 of epoch 4, 29.76 ms/it, loss 0.437663
Finished training it 32768/76743 of epoch 4, 29.65 ms/it, loss 0.440034
Finished training it 33792/76743 of epoch 4, 29.61 ms/it, loss 0.439711
Finished training it 33792/76743 of epoch 4, 29.77 ms/it, loss 0.440816
Finished training it 33792/76743 of epoch 4, 29.65 ms/it, loss 0.439311
Finished training it 33792/76743 of epoch 4, 29.58 ms/it, loss 0.441088
Finished training it 34816/76743 of epoch 4, 29.29 ms/it, loss 0.441265
Finished training it 34816/76743 of epoch 4, 29.48 ms/it, loss 0.437071
Finished training it 34816/76743 of epoch 4, 29.50 ms/it, loss 0.440255
Finished training it 34816/76743 of epoch 4, 29.91 ms/it, loss 0.442086
Finished training it 35840/76743 of epoch 4, 29.76 ms/it, loss 0.435727
Finished training it 35840/76743 of epoch 4, 29.48 ms/it, loss 0.440199
Finished training it 35840/76743 of epoch 4, 29.53 ms/it, loss 0.440778
Finished training it 35840/76743 of epoch 4, 29.78 ms/it, loss 0.439750
Finished training it 36864/76743 of epoch 4, 29.55 ms/it, loss 0.441162
Finished training it 36864/76743 of epoch 4, 29.58 ms/it, loss 0.439118
Finished training it 36864/76743 of epoch 4, 29.45 ms/it, loss 0.436700
Finished training it 36864/76743 of epoch 4, 29.94 ms/it, loss 0.439417
Finished training it 37888/76743 of epoch 4, 29.75 ms/it, loss 0.438433
Finished training it 37888/76743 of epoch 4, 29.96 ms/it, loss 0.440421
Finished training it 37888/76743 of epoch 4, 30.08 ms/it, loss 0.437605
Finished training it 37888/76743 of epoch 4, 29.98 ms/it, loss 0.441928
Finished training it 38912/76743 of epoch 4, 29.54 ms/it, loss 0.439277
Finished training it 38912/76743 of epoch 4, 29.84 ms/it, loss 0.441272
Finished training it 38912/76743 of epoch 4, 30.02 ms/it, loss 0.436654
Finished training it 38912/76743 of epoch 4, 29.67 ms/it, loss 0.440535
Finished training it 39936/76743 of epoch 4, 30.04 ms/it, loss 0.438882
Finished training it 39936/76743 of epoch 4, 29.76 ms/it, loss 0.437544
Finished training it 39936/76743 of epoch 4, 29.57 ms/it, loss 0.436536
Finished training it 39936/76743 of epoch 4, 29.70 ms/it, loss 0.442033
Finished training it 40960/76743 of epoch 4, 29.43 ms/it, loss 0.440428
Finished training it 40960/76743 of epoch 4, 29.49 ms/it, loss 0.437114
Finished training it 40960/76743 of epoch 4, 29.70 ms/it, loss 0.441154
Finished training it 40960/76743 of epoch 4, 29.97 ms/it, loss 0.442028
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581505.0
get out
0 has test check 2581505.0 and sample count 3274240
 accuracy 78.843 %, best 78.854 %, roc auc score 0.8020, best 0.8027
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 29.70 ms/it, loss 0.441598
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581505.0
get out
3 has test check 2581505.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 29.66 ms/it, loss 0.441461
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581505.0
get out
2 has test check 2581505.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 29.57 ms/it, loss 0.438687
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581505.0
get out
1 has test check 2581505.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 29.64 ms/it, loss 0.439338
Finished training it 43008/76743 of epoch 4, 29.82 ms/it, loss 0.435887
Finished training it 43008/76743 of epoch 4, 29.80 ms/it, loss 0.438305
Finished training it 43008/76743 of epoch 4, 29.57 ms/it, loss 0.437959
Finished training it 43008/76743 of epoch 4, 29.64 ms/it, loss 0.439447
Finished training it 44032/76743 of epoch 4, 30.07 ms/it, loss 0.440697
Finished training it 44032/76743 of epoch 4, 29.92 ms/it, loss 0.440825
Finished training it 44032/76743 of epoch 4, 29.78 ms/it, loss 0.439752
Finished training it 44032/76743 of epoch 4, 29.74 ms/it, loss 0.440262
Finished training it 45056/76743 of epoch 4, 30.05 ms/it, loss 0.439869
Finished training it 45056/76743 of epoch 4, 29.83 ms/it, loss 0.438956
Finished training it 45056/76743 of epoch 4, 29.98 ms/it, loss 0.439983
Finished training it 45056/76743 of epoch 4, 29.81 ms/it, loss 0.438109
Finished training it 46080/76743 of epoch 4, 30.00 ms/it, loss 0.439119
Finished training it 46080/76743 of epoch 4, 29.61 ms/it, loss 0.438340
Finished training it 46080/76743 of epoch 4, 29.78 ms/it, loss 0.439465
Finished training it 46080/76743 of epoch 4, 29.56 ms/it, loss 0.437621
Finished training it 47104/76743 of epoch 4, 34.62 ms/it, loss 0.439576
Finished training it 47104/76743 of epoch 4, 34.83 ms/it, loss 0.438201
Finished training it 47104/76743 of epoch 4, 35.52 ms/it, loss 0.440813
Finished training it 47104/76743 of epoch 4, 35.52 ms/it, loss 0.440962
Finished training it 48128/76743 of epoch 4, 30.51 ms/it, loss 0.438449
Finished training it 48128/76743 of epoch 4, 30.09 ms/it, loss 0.440533
Finished training it 48128/76743 of epoch 4, 29.99 ms/it, loss 0.440063
Finished training it 48128/76743 of epoch 4, 29.77 ms/it, loss 0.440520
Finished training it 49152/76743 of epoch 4, 29.54 ms/it, loss 0.439037
Finished training it 49152/76743 of epoch 4, 29.59 ms/it, loss 0.437836
Finished training it 49152/76743 of epoch 4, 30.01 ms/it, loss 0.441005
Finished training it 49152/76743 of epoch 4, 29.72 ms/it, loss 0.440221
Finished training it 50176/76743 of epoch 4, 29.79 ms/it, loss 0.439905
Finished training it 50176/76743 of epoch 4, 29.78 ms/it, loss 0.438228
Finished training it 50176/76743 of epoch 4, 29.57 ms/it, loss 0.437334
Finished training it 50176/76743 of epoch 4, 29.64 ms/it, loss 0.439729
Finished training it 51200/76743 of epoch 4, 29.73 ms/it, loss 0.436676
Finished training it 51200/76743 of epoch 4, 29.44 ms/it, loss 0.437035
Finished training it 51200/76743 of epoch 4, 29.76 ms/it, loss 0.437519
Finished training it 51200/76743 of epoch 4, 29.37 ms/it, loss 0.438815
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580798.0
get out
0 has test check 2580798.0 and sample count 3274240
 accuracy 78.821 %, best 78.854 %, roc auc score 0.8023, best 0.8027
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580798.0
get out
2 has test check 2580798.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 29.45 ms/it, loss 0.436985
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 29.56 ms/it, loss 0.441189
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580798.0
get out
3 has test check 2580798.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 29.66 ms/it, loss 0.437844
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580798.0
get out
1 has test check 2580798.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 29.76 ms/it, loss 0.439411
Finished training it 53248/76743 of epoch 4, 29.61 ms/it, loss 0.438852
Finished training it 53248/76743 of epoch 4, 29.48 ms/it, loss 0.440260
Finished training it 53248/76743 of epoch 4, 29.67 ms/it, loss 0.437872
Finished training it 53248/76743 of epoch 4, 29.66 ms/it, loss 0.439338
Finished training it 54272/76743 of epoch 4, 29.66 ms/it, loss 0.440460
Finished training it 54272/76743 of epoch 4, 29.67 ms/it, loss 0.439438
Finished training it 54272/76743 of epoch 4, 29.72 ms/it, loss 0.438378
Finished training it 54272/76743 of epoch 4, 29.93 ms/it, loss 0.437594
Finished training it 55296/76743 of epoch 4, 29.73 ms/it, loss 0.439178
Finished training it 55296/76743 of epoch 4, 29.63 ms/it, loss 0.438087
Finished training it 55296/76743 of epoch 4, 29.81 ms/it, loss 0.437576
Finished training it 55296/76743 of epoch 4, 29.82 ms/it, loss 0.440411
Finished training it 56320/76743 of epoch 4, 29.77 ms/it, loss 0.439681
Finished training it 56320/76743 of epoch 4, 30.19 ms/it, loss 0.437787
Finished training it 56320/76743 of epoch 4, 29.57 ms/it, loss 0.434945
Finished training it 56320/76743 of epoch 4, 29.85 ms/it, loss 0.438425
Finished training it 57344/76743 of epoch 4, 29.38 ms/it, loss 0.439887
Finished training it 57344/76743 of epoch 4, 29.38 ms/it, loss 0.438097
Finished training it 57344/76743 of epoch 4, 29.65 ms/it, loss 0.438849
Finished training it 57344/76743 of epoch 4, 29.80 ms/it, loss 0.438466
Finished training it 58368/76743 of epoch 4, 29.83 ms/it, loss 0.441245
Finished training it 58368/76743 of epoch 4, 29.88 ms/it, loss 0.438282
Finished training it 58368/76743 of epoch 4, 29.64 ms/it, loss 0.437111
Finished training it 58368/76743 of epoch 4, 29.78 ms/it, loss 0.440746
Finished training it 59392/76743 of epoch 4, 29.51 ms/it, loss 0.441031
Finished training it 59392/76743 of epoch 4, 29.60 ms/it, loss 0.438804
Finished training it 59392/76743 of epoch 4, 29.72 ms/it, loss 0.438737
Finished training it 59392/76743 of epoch 4, 29.92 ms/it, loss 0.439283
Finished training it 60416/76743 of epoch 4, 29.82 ms/it, loss 0.438828
Finished training it 60416/76743 of epoch 4, 29.71 ms/it, loss 0.441070
Finished training it 60416/76743 of epoch 4, 29.55 ms/it, loss 0.440220
Finished training it 60416/76743 of epoch 4, 30.05 ms/it, loss 0.441531
Finished training it 61440/76743 of epoch 4, 29.94 ms/it, loss 0.438257
Finished training it 61440/76743 of epoch 4, 29.90 ms/it, loss 0.441071
Finished training it 61440/76743 of epoch 4, 30.06 ms/it, loss 0.440354
Finished training it 61440/76743 of epoch 4, 29.76 ms/it, loss 0.439681
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579983.0
get out
0 has test check 2579983.0 and sample count 3274240
 accuracy 78.796 %, best 78.854 %, roc auc score 0.8025, best 0.8027
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579983.0
get out
3 has test check 2579983.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 29.89 ms/it, loss 0.438378
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579983.0
get out
1 has test check 2579983.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 29.81 ms/it, loss 0.435374
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 29.95 ms/it, loss 0.438533
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579983.0
get out
2 has test check 2579983.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 29.67 ms/it, loss 0.437452
Finished training it 63488/76743 of epoch 4, 30.25 ms/it, loss 0.438174
Finished training it 63488/76743 of epoch 4, 29.80 ms/it, loss 0.435794
Finished training it 63488/76743 of epoch 4, 29.82 ms/it, loss 0.440085
Finished training it 63488/76743 of epoch 4, 29.81 ms/it, loss 0.440331
Finished training it 64512/76743 of epoch 4, 29.76 ms/it, loss 0.441250
Finished training it 64512/76743 of epoch 4, 29.62 ms/it, loss 0.437010
Finished training it 64512/76743 of epoch 4, 29.50 ms/it, loss 0.438362
Finished training it 64512/76743 of epoch 4, 29.52 ms/it, loss 0.438657
Finished training it 65536/76743 of epoch 4, 29.75 ms/it, loss 0.438296
Finished training it 65536/76743 of epoch 4, 29.67 ms/it, loss 0.438073
Finished training it 65536/76743 of epoch 4, 29.96 ms/it, loss 0.437149
Finished training it 65536/76743 of epoch 4, 29.66 ms/it, loss 0.440271
Finished training it 66560/76743 of epoch 4, 29.23 ms/it, loss 0.438542
Finished training it 66560/76743 of epoch 4, 29.59 ms/it, loss 0.439251
Finished training it 66560/76743 of epoch 4, 29.61 ms/it, loss 0.438812
Finished training it 66560/76743 of epoch 4, 29.06 ms/it, loss 0.438206
Finished training it 67584/76743 of epoch 4, 29.68 ms/it, loss 0.439388
Finished training it 67584/76743 of epoch 4, 29.69 ms/it, loss 0.439236
Finished training it 67584/76743 of epoch 4, 29.61 ms/it, loss 0.436314
Finished training it 67584/76743 of epoch 4, 29.43 ms/it, loss 0.438708
Finished training it 68608/76743 of epoch 4, 29.25 ms/it, loss 0.439244
Finished training it 68608/76743 of epoch 4, 29.21 ms/it, loss 0.437926
Finished training it 68608/76743 of epoch 4, 29.40 ms/it, loss 0.436698
Finished training it 68608/76743 of epoch 4, 29.57 ms/it, loss 0.438059
Finished training it 69632/76743 of epoch 4, 30.02 ms/it, loss 0.438172
Finished training it 69632/76743 of epoch 4, 29.95 ms/it, loss 0.436454
Finished training it 69632/76743 of epoch 4, 30.06 ms/it, loss 0.437670
Finished training it 69632/76743 of epoch 4, 29.79 ms/it, loss 0.435039
Finished training it 70656/76743 of epoch 4, 29.43 ms/it, loss 0.439118
Finished training it 70656/76743 of epoch 4, 29.22 ms/it, loss 0.437084
Finished training it 70656/76743 of epoch 4, 29.92 ms/it, loss 0.440533
Finished training it 70656/76743 of epoch 4, 29.73 ms/it, loss 0.439811
Finished training it 71680/76743 of epoch 4, 29.35 ms/it, loss 0.436151
Finished training it 71680/76743 of epoch 4, 29.73 ms/it, loss 0.439446
Finished training it 71680/76743 of epoch 4, 29.78 ms/it, loss 0.438322
Finished training it 71680/76743 of epoch 4, 29.69 ms/it, loss 0.438054
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580926.0
get out
0 has test check 2580926.0 and sample count 3274240
 accuracy 78.825 %, best 78.854 %, roc auc score 0.8026, best 0.8027
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 29.91 ms/it, loss 0.438913
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580926.0
get out
3 has test check 2580926.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 29.86 ms/it, loss 0.437400
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580926.0
get out
1 has test check 2580926.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 29.71 ms/it, loss 0.437981
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580926.0
get out
2 has test check 2580926.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 29.75 ms/it, loss 0.437830
Finished training it 73728/76743 of epoch 4, 30.08 ms/it, loss 0.437590
Finished training it 73728/76743 of epoch 4, 29.52 ms/it, loss 0.436853
Finished training it 73728/76743 of epoch 4, 29.80 ms/it, loss 0.439001
Finished training it 73728/76743 of epoch 4, 29.69 ms/it, loss 0.439150
Finished training it 74752/76743 of epoch 4, 29.72 ms/it, loss 0.438790
Finished training it 74752/76743 of epoch 4, 29.49 ms/it, loss 0.437568
Finished training it 74752/76743 of epoch 4, 29.69 ms/it, loss 0.437574
Finished training it 74752/76743 of epoch 4, 29.78 ms/it, loss 0.439103
Finished training it 75776/76743 of epoch 4, 29.78 ms/it, loss 0.436851
Finished training it 75776/76743 of epoch 4, 30.19 ms/it, loss 0.437508
Finished training it 75776/76743 of epoch 4, 29.72 ms/it, loss 0.437120
Finished training it 75776/76743 of epoch 4, 29.65 ms/it, loss 0.438622
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578686.0
get out
0 has test check 2578686.0 and sample count 3274240
 accuracy 78.757 %, best 78.854 %, roc auc score 0.8023, best 0.8027
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578686.0
get out
1 has test check 2578686.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578686.0
get out
2 has test check 2578686.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578686.0
get out
3 has test check 2578686.0 and sample count 3274240
