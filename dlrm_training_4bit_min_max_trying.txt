Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 74.27 ms/it, loss 0.513714
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 76.83 ms/it, loss 0.512182
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 76.45 ms/it, loss 0.515019
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 74.77 ms/it, loss 0.516120
Finished training it 2048/76743 of epoch 0, 73.62 ms/it, loss 0.498907
Finished training it 2048/76743 of epoch 0, 73.62 ms/it, loss 0.499254
Finished training it 2048/76743 of epoch 0, 72.72 ms/it, loss 0.500507
Finished training it 2048/76743 of epoch 0, 72.84 ms/it, loss 0.498546
Finished training it 3072/76743 of epoch 0, 73.05 ms/it, loss 0.490708
Finished training it 3072/76743 of epoch 0, 72.35 ms/it, loss 0.490232
Finished training it 3072/76743 of epoch 0, 72.50 ms/it, loss 0.488126
Finished training it 3072/76743 of epoch 0, 73.06 ms/it, loss 0.490227
Finished training it 4096/76743 of epoch 0, 73.12 ms/it, loss 0.482358
Finished training it 4096/76743 of epoch 0, 73.18 ms/it, loss 0.485174
Finished training it 4096/76743 of epoch 0, 72.51 ms/it, loss 0.481694
Finished training it 4096/76743 of epoch 0, 72.56 ms/it, loss 0.484308
Finished training it 5120/76743 of epoch 0, 73.11 ms/it, loss 0.478462
Finished training it 5120/76743 of epoch 0, 72.38 ms/it, loss 0.479676
Finished training it 5120/76743 of epoch 0, 73.02 ms/it, loss 0.479542
Finished training it 5120/76743 of epoch 0, 72.54 ms/it, loss 0.481082
Finished training it 6144/76743 of epoch 0, 71.25 ms/it, loss 0.475374
Finished training it 6144/76743 of epoch 0, 71.63 ms/it, loss 0.474097
Finished training it 6144/76743 of epoch 0, 71.25 ms/it, loss 0.477803
Finished training it 6144/76743 of epoch 0, 71.69 ms/it, loss 0.475520
Finished training it 7168/76743 of epoch 0, 70.82 ms/it, loss 0.472840
Finished training it 7168/76743 of epoch 0, 70.87 ms/it, loss 0.472405
Finished training it 7168/76743 of epoch 0, 70.82 ms/it, loss 0.473495
Finished training it 7168/76743 of epoch 0, 71.11 ms/it, loss 0.473554
Finished training it 8192/76743 of epoch 0, 72.43 ms/it, loss 0.471405
Finished training it 8192/76743 of epoch 0, 72.35 ms/it, loss 0.472569
Finished training it 8192/76743 of epoch 0, 72.22 ms/it, loss 0.472208
Finished training it 8192/76743 of epoch 0, 72.22 ms/it, loss 0.471432
Finished training it 9216/76743 of epoch 0, 72.47 ms/it, loss 0.469539
Finished training it 9216/76743 of epoch 0, 72.19 ms/it, loss 0.467701
Finished training it 9216/76743 of epoch 0, 72.25 ms/it, loss 0.466244
Finished training it 9216/76743 of epoch 0, 72.51 ms/it, loss 0.471320
Finished training it 10240/76743 of epoch 0, 70.25 ms/it, loss 0.467274
Finished training it 10240/76743 of epoch 0, 70.32 ms/it, loss 0.470235
Finished training it 10240/76743 of epoch 0, 70.10 ms/it, loss 0.471749
Finished training it 10240/76743 of epoch 0, 70.04 ms/it, loss 0.468370
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548938.0
get out
0 has test check 2548938.0 and sample count 3274240
 accuracy 77.848 %, best 77.848 %, roc auc score 0.7800, best 0.7800
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548938.0
get out
1 has test check 2548938.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 68.67 ms/it, loss 0.468643
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 68.48 ms/it, loss 0.467561
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548938.0
get out
2 has test check 2548938.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 68.57 ms/it, loss 0.465080
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548938.0
get out
3 has test check 2548938.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 68.79 ms/it, loss 0.466737
Finished training it 12288/76743 of epoch 0, 69.10 ms/it, loss 0.465801
Finished training it 12288/76743 of epoch 0, 69.28 ms/it, loss 0.461758
Finished training it 12288/76743 of epoch 0, 69.35 ms/it, loss 0.466158
Finished training it 12288/76743 of epoch 0, 69.03 ms/it, loss 0.466645
Finished training it 13312/76743 of epoch 0, 69.33 ms/it, loss 0.466260
Finished training it 13312/76743 of epoch 0, 69.41 ms/it, loss 0.462598
Finished training it 13312/76743 of epoch 0, 69.42 ms/it, loss 0.464922
Finished training it 13312/76743 of epoch 0, 69.34 ms/it, loss 0.464553
Finished training it 14336/76743 of epoch 0, 68.87 ms/it, loss 0.463949
Finished training it 14336/76743 of epoch 0, 68.70 ms/it, loss 0.465375
Finished training it 14336/76743 of epoch 0, 68.85 ms/it, loss 0.463996
Finished training it 14336/76743 of epoch 0, 68.63 ms/it, loss 0.464483
Finished training it 15360/76743 of epoch 0, 74.65 ms/it, loss 0.462032
Finished training it 15360/76743 of epoch 0, 74.24 ms/it, loss 0.461340
Finished training it 15360/76743 of epoch 0, 75.03 ms/it, loss 0.463517
Finished training it 15360/76743 of epoch 0, 75.06 ms/it, loss 0.462011
Finished training it 16384/76743 of epoch 0, 70.20 ms/it, loss 0.462350
Finished training it 16384/76743 of epoch 0, 70.19 ms/it, loss 0.463088
Finished training it 16384/76743 of epoch 0, 70.12 ms/it, loss 0.464547
Finished training it 16384/76743 of epoch 0, 70.39 ms/it, loss 0.465638
Finished training it 17408/76743 of epoch 0, 70.73 ms/it, loss 0.463645
Finished training it 17408/76743 of epoch 0, 70.79 ms/it, loss 0.463966
Finished training it 17408/76743 of epoch 0, 70.58 ms/it, loss 0.462891
Finished training it 17408/76743 of epoch 0, 70.82 ms/it, loss 0.461311
Finished training it 18432/76743 of epoch 0, 70.69 ms/it, loss 0.461392
Finished training it 18432/76743 of epoch 0, 70.55 ms/it, loss 0.460734
Finished training it 18432/76743 of epoch 0, 70.80 ms/it, loss 0.463388
Finished training it 18432/76743 of epoch 0, 70.59 ms/it, loss 0.462292
Finished training it 19456/76743 of epoch 0, 70.95 ms/it, loss 0.463709
Finished training it 19456/76743 of epoch 0, 70.80 ms/it, loss 0.460231
Finished training it 19456/76743 of epoch 0, 70.77 ms/it, loss 0.461693
Finished training it 19456/76743 of epoch 0, 71.01 ms/it, loss 0.463456
Finished training it 20480/76743 of epoch 0, 70.13 ms/it, loss 0.460047
Finished training it 20480/76743 of epoch 0, 70.01 ms/it, loss 0.461305
Finished training it 20480/76743 of epoch 0, 70.14 ms/it, loss 0.461927
Finished training it 20480/76743 of epoch 0, 70.09 ms/it, loss 0.461914
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556864.0
get out
0 has test check 2556864.0 and sample count 3274240
 accuracy 78.090 %, best 78.090 %, roc auc score 0.7868, best 0.7868
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556864.0
get out
2 has test check 2556864.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 68.88 ms/it, loss 0.458758
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556864.0
get out
3 has test check 2556864.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 69.03 ms/it, loss 0.463802
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 68.75 ms/it, loss 0.460539
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556864.0
get out
1 has test check 2556864.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 68.81 ms/it, loss 0.461519
Finished training it 22528/76743 of epoch 0, 69.13 ms/it, loss 0.457738
Finished training it 22528/76743 of epoch 0, 68.98 ms/it, loss 0.459845
Finished training it 22528/76743 of epoch 0, 68.83 ms/it, loss 0.460276
Finished training it 22528/76743 of epoch 0, 69.18 ms/it, loss 0.461599
Finished training it 23552/76743 of epoch 0, 68.73 ms/it, loss 0.461324
Finished training it 23552/76743 of epoch 0, 68.86 ms/it, loss 0.460377
Finished training it 23552/76743 of epoch 0, 69.02 ms/it, loss 0.460198
Finished training it 23552/76743 of epoch 0, 68.93 ms/it, loss 0.461458
Finished training it 24576/76743 of epoch 0, 69.09 ms/it, loss 0.459009
Finished training it 24576/76743 of epoch 0, 68.81 ms/it, loss 0.459352
Finished training it 24576/76743 of epoch 0, 68.89 ms/it, loss 0.458914
Finished training it 24576/76743 of epoch 0, 68.97 ms/it, loss 0.461773
Finished training it 25600/76743 of epoch 0, 68.79 ms/it, loss 0.459849
Finished training it 25600/76743 of epoch 0, 69.02 ms/it, loss 0.460052
Finished training it 25600/76743 of epoch 0, 68.86 ms/it, loss 0.459186
Finished training it 25600/76743 of epoch 0, 68.91 ms/it, loss 0.462632
Finished training it 26624/76743 of epoch 0, 68.96 ms/it, loss 0.457604
Finished training it 26624/76743 of epoch 0, 68.75 ms/it, loss 0.458135
Finished training it 26624/76743 of epoch 0, 68.62 ms/it, loss 0.460597
Finished training it 26624/76743 of epoch 0, 68.49 ms/it, loss 0.458884
Finished training it 27648/76743 of epoch 0, 69.17 ms/it, loss 0.457790
Finished training it 27648/76743 of epoch 0, 69.56 ms/it, loss 0.457326
Finished training it 27648/76743 of epoch 0, 69.39 ms/it, loss 0.457731
Finished training it 27648/76743 of epoch 0, 69.21 ms/it, loss 0.458503
Finished training it 28672/76743 of epoch 0, 69.35 ms/it, loss 0.457549
Finished training it 28672/76743 of epoch 0, 69.67 ms/it, loss 0.456795
Finished training it 28672/76743 of epoch 0, 69.28 ms/it, loss 0.460035
Finished training it 28672/76743 of epoch 0, 69.64 ms/it, loss 0.458590
Finished training it 29696/76743 of epoch 0, 69.18 ms/it, loss 0.458235
Finished training it 29696/76743 of epoch 0, 69.17 ms/it, loss 0.459891
Finished training it 29696/76743 of epoch 0, 69.53 ms/it, loss 0.456785
Finished training it 29696/76743 of epoch 0, 69.22 ms/it, loss 0.457751
Finished training it 30720/76743 of epoch 0, 68.80 ms/it, loss 0.457492
Finished training it 30720/76743 of epoch 0, 68.98 ms/it, loss 0.458112
Finished training it 30720/76743 of epoch 0, 69.17 ms/it, loss 0.458826
Finished training it 30720/76743 of epoch 0, 68.92 ms/it, loss 0.456457
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2558058.0
get out
0 has test check 2558058.0 and sample count 3274240
 accuracy 78.127 %, best 78.127 %, roc auc score 0.7898, best 0.7898
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2558058.0
get out
1 has test check 2558058.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 68.63 ms/it, loss 0.454912
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 68.57 ms/it, loss 0.455935
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2558058.0
get out
2 has test check 2558058.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 68.69 ms/it, loss 0.458450
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2558058.0
get out
3 has test check 2558058.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 68.76 ms/it, loss 0.456480
Finished training it 32768/76743 of epoch 0, 69.05 ms/it, loss 0.459082
Finished training it 32768/76743 of epoch 0, 68.85 ms/it, loss 0.457429
Finished training it 32768/76743 of epoch 0, 69.03 ms/it, loss 0.457757
Finished training it 32768/76743 of epoch 0, 69.34 ms/it, loss 0.457027
Finished training it 33792/76743 of epoch 0, 69.45 ms/it, loss 0.456151
Finished training it 33792/76743 of epoch 0, 69.84 ms/it, loss 0.458688
Finished training it 33792/76743 of epoch 0, 69.37 ms/it, loss 0.458767
Finished training it 33792/76743 of epoch 0, 69.44 ms/it, loss 0.459590
Finished training it 34816/76743 of epoch 0, 69.42 ms/it, loss 0.456879
Finished training it 34816/76743 of epoch 0, 69.28 ms/it, loss 0.455093
Finished training it 34816/76743 of epoch 0, 69.58 ms/it, loss 0.458163
Finished training it 34816/76743 of epoch 0, 69.38 ms/it, loss 0.456244
Finished training it 35840/76743 of epoch 0, 73.88 ms/it, loss 0.455931
Finished training it 35840/76743 of epoch 0, 69.67 ms/it, loss 0.456799
Finished training it 35840/76743 of epoch 0, 73.83 ms/it, loss 0.458310
Finished training it 35840/76743 of epoch 0, 74.15 ms/it, loss 0.455000
Finished training it 36864/76743 of epoch 0, 70.44 ms/it, loss 0.457415
Finished training it 36864/76743 of epoch 0, 70.50 ms/it, loss 0.456166
Finished training it 36864/76743 of epoch 0, 69.95 ms/it, loss 0.455991
Finished training it 36864/76743 of epoch 0, 74.93 ms/it, loss 0.455499
Finished training it 37888/76743 of epoch 0, 69.12 ms/it, loss 0.458366
Finished training it 37888/76743 of epoch 0, 69.68 ms/it, loss 0.457349
Finished training it 37888/76743 of epoch 0, 69.21 ms/it, loss 0.457501
Finished training it 37888/76743 of epoch 0, 69.13 ms/it, loss 0.454464
Finished training it 38912/76743 of epoch 0, 68.89 ms/it, loss 0.455893
Finished training it 38912/76743 of epoch 0, 68.94 ms/it, loss 0.456848
Finished training it 38912/76743 of epoch 0, 69.22 ms/it, loss 0.454452
Finished training it 38912/76743 of epoch 0, 68.77 ms/it, loss 0.455502
Finished training it 39936/76743 of epoch 0, 68.95 ms/it, loss 0.456476
Finished training it 39936/76743 of epoch 0, 69.28 ms/it, loss 0.458109
Finished training it 39936/76743 of epoch 0, 69.33 ms/it, loss 0.456654
Finished training it 39936/76743 of epoch 0, 69.07 ms/it, loss 0.454992
Finished training it 40960/76743 of epoch 0, 69.50 ms/it, loss 0.456312
Finished training it 40960/76743 of epoch 0, 69.24 ms/it, loss 0.455329
Finished training it 40960/76743 of epoch 0, 69.13 ms/it, loss 0.457374
Finished training it 40960/76743 of epoch 0, 69.17 ms/it, loss 0.459389
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2565357.0
get out
0 has test check 2565357.0 and sample count 3274240
 accuracy 78.350 %, best 78.350 %, roc auc score 0.7915, best 0.7915
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2565357.0
get out
1 has test check 2565357.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 68.30 ms/it, loss 0.455597
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 68.07 ms/it, loss 0.455948
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2565357.0
get out
2 has test check 2565357.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 68.29 ms/it, loss 0.455964
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2565357.0
get out
3 has test check 2565357.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 68.20 ms/it, loss 0.453881
Finished training it 43008/76743 of epoch 0, 69.00 ms/it, loss 0.453279
Finished training it 43008/76743 of epoch 0, 68.93 ms/it, loss 0.454556
Finished training it 43008/76743 of epoch 0, 69.08 ms/it, loss 0.454659
Finished training it 43008/76743 of epoch 0, 69.17 ms/it, loss 0.452955
Finished training it 44032/76743 of epoch 0, 68.82 ms/it, loss 0.456663
Finished training it 44032/76743 of epoch 0, 69.02 ms/it, loss 0.455042
Finished training it 44032/76743 of epoch 0, 68.87 ms/it, loss 0.458172
Finished training it 44032/76743 of epoch 0, 68.99 ms/it, loss 0.455455
Finished training it 45056/76743 of epoch 0, 69.05 ms/it, loss 0.453121
Finished training it 45056/76743 of epoch 0, 68.95 ms/it, loss 0.453510
Finished training it 45056/76743 of epoch 0, 69.05 ms/it, loss 0.455873
Finished training it 45056/76743 of epoch 0, 68.95 ms/it, loss 0.455118
Finished training it 46080/76743 of epoch 0, 68.88 ms/it, loss 0.454295
Finished training it 46080/76743 of epoch 0, 69.06 ms/it, loss 0.456183
Finished training it 46080/76743 of epoch 0, 69.10 ms/it, loss 0.455341
Finished training it 46080/76743 of epoch 0, 68.97 ms/it, loss 0.456950
Finished training it 47104/76743 of epoch 0, 68.75 ms/it, loss 0.457371
Finished training it 47104/76743 of epoch 0, 68.89 ms/it, loss 0.455533
Finished training it 47104/76743 of epoch 0, 69.10 ms/it, loss 0.455236
Finished training it 47104/76743 of epoch 0, 68.82 ms/it, loss 0.457244
Finished training it 48128/76743 of epoch 0, 69.01 ms/it, loss 0.453346
Finished training it 48128/76743 of epoch 0, 68.91 ms/it, loss 0.456294
Finished training it 48128/76743 of epoch 0, 69.33 ms/it, loss 0.454839
Finished training it 48128/76743 of epoch 0, 69.30 ms/it, loss 0.453546
Finished training it 49152/76743 of epoch 0, 69.39 ms/it, loss 0.458307
Finished training it 49152/76743 of epoch 0, 69.09 ms/it, loss 0.454882
Finished training it 49152/76743 of epoch 0, 69.10 ms/it, loss 0.455466
Finished training it 49152/76743 of epoch 0, 68.97 ms/it, loss 0.455005
Finished training it 50176/76743 of epoch 0, 69.56 ms/it, loss 0.454800
Finished training it 50176/76743 of epoch 0, 69.09 ms/it, loss 0.454192
Finished training it 50176/76743 of epoch 0, 69.04 ms/it, loss 0.452629
Finished training it 50176/76743 of epoch 0, 69.09 ms/it, loss 0.455503
Finished training it 51200/76743 of epoch 0, 69.27 ms/it, loss 0.456469
Finished training it 51200/76743 of epoch 0, 69.03 ms/it, loss 0.453361
Finished training it 51200/76743 of epoch 0, 69.17 ms/it, loss 0.451963
Finished training it 51200/76743 of epoch 0, 69.55 ms/it, loss 0.455481
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2567452.0
get out
0 has test check 2567452.0 and sample count 3274240
 accuracy 78.414 %, best 78.414 %, roc auc score 0.7928, best 0.7928
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2567452.0
get out
1 has test check 2567452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 70.13 ms/it, loss 0.452746
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2567452.0
get out
2 has test check 2567452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 70.07 ms/it, loss 0.455333
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 69.73 ms/it, loss 0.455316
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2567452.0
get out
3 has test check 2567452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 69.89 ms/it, loss 0.453738
Finished training it 53248/76743 of epoch 0, 69.09 ms/it, loss 0.455347
Finished training it 53248/76743 of epoch 0, 68.66 ms/it, loss 0.453398
Finished training it 53248/76743 of epoch 0, 68.96 ms/it, loss 0.455457
Finished training it 53248/76743 of epoch 0, 68.93 ms/it, loss 0.453612
Finished training it 54272/76743 of epoch 0, 69.53 ms/it, loss 0.455980
Finished training it 54272/76743 of epoch 0, 69.36 ms/it, loss 0.454719
Finished training it 54272/76743 of epoch 0, 69.29 ms/it, loss 0.455832
Finished training it 54272/76743 of epoch 0, 69.40 ms/it, loss 0.453936
Finished training it 55296/76743 of epoch 0, 69.56 ms/it, loss 0.453857
Finished training it 55296/76743 of epoch 0, 69.76 ms/it, loss 0.457155
Finished training it 55296/76743 of epoch 0, 69.69 ms/it, loss 0.453751
Finished training it 55296/76743 of epoch 0, 69.59 ms/it, loss 0.453903
Finished training it 56320/76743 of epoch 0, 68.98 ms/it, loss 0.456996
Finished training it 56320/76743 of epoch 0, 68.73 ms/it, loss 0.455565
Finished training it 56320/76743 of epoch 0, 68.75 ms/it, loss 0.455837
Finished training it 56320/76743 of epoch 0, 68.69 ms/it, loss 0.455433
Finished training it 57344/76743 of epoch 0, 69.66 ms/it, loss 0.453675
Finished training it 57344/76743 of epoch 0, 74.45 ms/it, loss 0.456948
Finished training it 57344/76743 of epoch 0, 74.22 ms/it, loss 0.454244
Finished training it 57344/76743 of epoch 0, 74.20 ms/it, loss 0.452600
Finished training it 58368/76743 of epoch 0, 68.91 ms/it, loss 0.451968
Finished training it 58368/76743 of epoch 0, 68.74 ms/it, loss 0.454257
Finished training it 58368/76743 of epoch 0, 68.60 ms/it, loss 0.453624
Finished training it 58368/76743 of epoch 0, 68.60 ms/it, loss 0.456874
Finished training it 59392/76743 of epoch 0, 68.59 ms/it, loss 0.453147
Finished training it 59392/76743 of epoch 0, 69.10 ms/it, loss 0.452780
Finished training it 59392/76743 of epoch 0, 68.95 ms/it, loss 0.453747
Finished training it 59392/76743 of epoch 0, 68.93 ms/it, loss 0.455674
Finished training it 60416/76743 of epoch 0, 69.49 ms/it, loss 0.456152
Finished training it 60416/76743 of epoch 0, 69.36 ms/it, loss 0.453830
Finished training it 60416/76743 of epoch 0, 69.62 ms/it, loss 0.453388
Finished training it 60416/76743 of epoch 0, 69.33 ms/it, loss 0.453012
Finished training it 61440/76743 of epoch 0, 69.24 ms/it, loss 0.453528
Finished training it 61440/76743 of epoch 0, 69.42 ms/it, loss 0.454535
Finished training it 61440/76743 of epoch 0, 69.47 ms/it, loss 0.455483
Finished training it 61440/76743 of epoch 0, 69.55 ms/it, loss 0.453246
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568910.0
get out
0 has test check 2568910.0 and sample count 3274240
 accuracy 78.458 %, best 78.458 %, roc auc score 0.7950, best 0.7950
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 69.16 ms/it, loss 0.454291
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568910.0
get out
2 has test check 2568910.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 69.53 ms/it, loss 0.451602
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568910.0
get out
1 has test check 2568910.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 69.45 ms/it, loss 0.453159
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568910.0
get out
3 has test check 2568910.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 69.51 ms/it, loss 0.453551
Finished training it 63488/76743 of epoch 0, 69.06 ms/it, loss 0.453075
Finished training it 63488/76743 of epoch 0, 68.90 ms/it, loss 0.454790
Finished training it 63488/76743 of epoch 0, 69.09 ms/it, loss 0.452842
Finished training it 63488/76743 of epoch 0, 69.19 ms/it, loss 0.453981
Finished training it 64512/76743 of epoch 0, 69.69 ms/it, loss 0.455697
Finished training it 64512/76743 of epoch 0, 69.60 ms/it, loss 0.452371
Finished training it 64512/76743 of epoch 0, 69.92 ms/it, loss 0.453105
Finished training it 64512/76743 of epoch 0, 69.80 ms/it, loss 0.455301
Finished training it 65536/76743 of epoch 0, 70.98 ms/it, loss 0.453481
Finished training it 65536/76743 of epoch 0, 70.32 ms/it, loss 0.454546
Finished training it 65536/76743 of epoch 0, 74.73 ms/it, loss 0.453550
Finished training it 65536/76743 of epoch 0, 70.17 ms/it, loss 0.452691
Finished training it 66560/76743 of epoch 0, 68.78 ms/it, loss 0.456511
Finished training it 66560/76743 of epoch 0, 69.42 ms/it, loss 0.453306
Finished training it 66560/76743 of epoch 0, 68.87 ms/it, loss 0.451321
Finished training it 66560/76743 of epoch 0, 69.07 ms/it, loss 0.450670
Finished training it 67584/76743 of epoch 0, 69.29 ms/it, loss 0.455086
Finished training it 67584/76743 of epoch 0, 68.77 ms/it, loss 0.456988
Finished training it 67584/76743 of epoch 0, 68.87 ms/it, loss 0.453691
Finished training it 67584/76743 of epoch 0, 68.84 ms/it, loss 0.451223
Finished training it 68608/76743 of epoch 0, 68.80 ms/it, loss 0.451279
Finished training it 68608/76743 of epoch 0, 68.74 ms/it, loss 0.453529
Finished training it 68608/76743 of epoch 0, 69.04 ms/it, loss 0.452098
Finished training it 68608/76743 of epoch 0, 68.81 ms/it, loss 0.451953
Finished training it 69632/76743 of epoch 0, 68.52 ms/it, loss 0.450593
Finished training it 69632/76743 of epoch 0, 68.57 ms/it, loss 0.451052
Finished training it 69632/76743 of epoch 0, 68.46 ms/it, loss 0.453019
Finished training it 69632/76743 of epoch 0, 68.74 ms/it, loss 0.453081
Finished training it 70656/76743 of epoch 0, 69.22 ms/it, loss 0.452850
Finished training it 70656/76743 of epoch 0, 69.32 ms/it, loss 0.450965
Finished training it 70656/76743 of epoch 0, 69.11 ms/it, loss 0.453764
Finished training it 70656/76743 of epoch 0, 69.12 ms/it, loss 0.453630
Finished training it 71680/76743 of epoch 0, 69.47 ms/it, loss 0.451985
Finished training it 71680/76743 of epoch 0, 69.36 ms/it, loss 0.454140
Finished training it 71680/76743 of epoch 0, 69.35 ms/it, loss 0.454975
Finished training it 71680/76743 of epoch 0, 69.74 ms/it, loss 0.452493
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571170.0
get out
0 has test check 2571170.0 and sample count 3274240
 accuracy 78.527 %, best 78.527 %, roc auc score 0.7957, best 0.7957
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 68.91 ms/it, loss 0.453588
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571170.0
get out
3 has test check 2571170.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 69.03 ms/it, loss 0.451531
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571170.0
get out
1 has test check 2571170.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 69.11 ms/it, loss 0.452393
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571170.0
get out
2 has test check 2571170.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 69.12 ms/it, loss 0.452079
Finished training it 73728/76743 of epoch 0, 68.78 ms/it, loss 0.455012
Finished training it 73728/76743 of epoch 0, 68.57 ms/it, loss 0.451847
Finished training it 73728/76743 of epoch 0, 68.66 ms/it, loss 0.454469
Finished training it 73728/76743 of epoch 0, 68.76 ms/it, loss 0.452139
Finished training it 74752/76743 of epoch 0, 69.41 ms/it, loss 0.452320
Finished training it 74752/76743 of epoch 0, 69.25 ms/it, loss 0.452962
Finished training it 74752/76743 of epoch 0, 69.30 ms/it, loss 0.454581
Finished training it 74752/76743 of epoch 0, 69.19 ms/it, loss 0.452736
Finished training it 75776/76743 of epoch 0, 69.35 ms/it, loss 0.453993
Finished training it 75776/76743 of epoch 0, 68.98 ms/it, loss 0.453007
Finished training it 75776/76743 of epoch 0, 69.16 ms/it, loss 0.452817
Finished training it 75776/76743 of epoch 0, 69.32 ms/it, loss 0.452806
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 70.04 ms/it, loss 0.452595
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 69.63 ms/it, loss 0.453552
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 70.07 ms/it, loss 0.451541
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 69.78 ms/it, loss 0.451972
Finished training it 2048/76743 of epoch 1, 69.04 ms/it, loss 0.452189
Finished training it 2048/76743 of epoch 1, 68.99 ms/it, loss 0.449982
Finished training it 2048/76743 of epoch 1, 69.38 ms/it, loss 0.452837
Finished training it 2048/76743 of epoch 1, 69.03 ms/it, loss 0.450479
Finished training it 3072/76743 of epoch 1, 68.96 ms/it, loss 0.452903
Finished training it 3072/76743 of epoch 1, 69.19 ms/it, loss 0.452253
Finished training it 3072/76743 of epoch 1, 69.28 ms/it, loss 0.449643
Finished training it 3072/76743 of epoch 1, 68.80 ms/it, loss 0.451894
Finished training it 4096/76743 of epoch 1, 69.29 ms/it, loss 0.452250
Finished training it 4096/76743 of epoch 1, 68.86 ms/it, loss 0.451645
Finished training it 4096/76743 of epoch 1, 68.87 ms/it, loss 0.452786
Finished training it 4096/76743 of epoch 1, 68.93 ms/it, loss 0.450784
Finished training it 5120/76743 of epoch 1, 69.27 ms/it, loss 0.452252
Finished training it 5120/76743 of epoch 1, 69.63 ms/it, loss 0.453822
Finished training it 5120/76743 of epoch 1, 69.20 ms/it, loss 0.452995
Finished training it 5120/76743 of epoch 1, 69.17 ms/it, loss 0.452988
Finished training it 6144/76743 of epoch 1, 68.81 ms/it, loss 0.451757
Finished training it 6144/76743 of epoch 1, 68.70 ms/it, loss 0.450216
Finished training it 6144/76743 of epoch 1, 68.76 ms/it, loss 0.451459
Finished training it 6144/76743 of epoch 1, 68.97 ms/it, loss 0.453127
Finished training it 7168/76743 of epoch 1, 71.21 ms/it, loss 0.451450
Finished training it 7168/76743 of epoch 1, 71.64 ms/it, loss 0.451318
Finished training it 7168/76743 of epoch 1, 71.28 ms/it, loss 0.451491
Finished training it 7168/76743 of epoch 1, 71.29 ms/it, loss 0.451529
Finished training it 8192/76743 of epoch 1, 69.51 ms/it, loss 0.453145
Finished training it 8192/76743 of epoch 1, 69.36 ms/it, loss 0.451744
Finished training it 8192/76743 of epoch 1, 69.61 ms/it, loss 0.452987
Finished training it 8192/76743 of epoch 1, 69.47 ms/it, loss 0.453488
Finished training it 9216/76743 of epoch 1, 69.19 ms/it, loss 0.452851
Finished training it 9216/76743 of epoch 1, 69.41 ms/it, loss 0.448468
Finished training it 9216/76743 of epoch 1, 69.00 ms/it, loss 0.449996
Finished training it 9216/76743 of epoch 1, 69.16 ms/it, loss 0.451137
Finished training it 10240/76743 of epoch 1, 69.06 ms/it, loss 0.451212
Finished training it 10240/76743 of epoch 1, 69.55 ms/it, loss 0.451903
Finished training it 10240/76743 of epoch 1, 69.18 ms/it, loss 0.455289
Finished training it 10240/76743 of epoch 1, 69.28 ms/it, loss 0.453775
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2573388.0
get out
0 has test check 2573388.0 and sample count 3274240
 accuracy 78.595 %, best 78.595 %, roc auc score 0.7967, best 0.7967
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 68.88 ms/it, loss 0.451512
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2573388.0
get out
2 has test check 2573388.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 69.09 ms/it, loss 0.450056
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2573388.0
get out
3 has test check 2573388.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 69.15 ms/it, loss 0.451263
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2573388.0
get out
1 has test check 2573388.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 69.09 ms/it, loss 0.453126
Finished training it 12288/76743 of epoch 1, 68.82 ms/it, loss 0.452219
Finished training it 12288/76743 of epoch 1, 69.27 ms/it, loss 0.451491
Finished training it 12288/76743 of epoch 1, 68.87 ms/it, loss 0.450751
Finished training it 12288/76743 of epoch 1, 68.81 ms/it, loss 0.447062
Finished training it 13312/76743 of epoch 1, 69.73 ms/it, loss 0.449115
Finished training it 13312/76743 of epoch 1, 69.46 ms/it, loss 0.450064
Finished training it 13312/76743 of epoch 1, 69.46 ms/it, loss 0.451998
Finished training it 13312/76743 of epoch 1, 69.33 ms/it, loss 0.450718
Finished training it 14336/76743 of epoch 1, 74.19 ms/it, loss 0.450505
Finished training it 14336/76743 of epoch 1, 69.69 ms/it, loss 0.451293
Finished training it 14336/76743 of epoch 1, 74.40 ms/it, loss 0.449794
Finished training it 14336/76743 of epoch 1, 74.29 ms/it, loss 0.452091
Finished training it 15360/76743 of epoch 1, 70.69 ms/it, loss 0.448800
Finished training it 15360/76743 of epoch 1, 75.08 ms/it, loss 0.450224
Finished training it 15360/76743 of epoch 1, 70.85 ms/it, loss 0.449494
Finished training it 15360/76743 of epoch 1, 70.26 ms/it, loss 0.448834
Finished training it 16384/76743 of epoch 1, 68.81 ms/it, loss 0.452576
Finished training it 16384/76743 of epoch 1, 69.02 ms/it, loss 0.448819
Finished training it 16384/76743 of epoch 1, 68.68 ms/it, loss 0.450519
Finished training it 16384/76743 of epoch 1, 68.76 ms/it, loss 0.452014
Finished training it 17408/76743 of epoch 1, 69.69 ms/it, loss 0.451097
Finished training it 17408/76743 of epoch 1, 69.97 ms/it, loss 0.448797
Finished training it 17408/76743 of epoch 1, 69.70 ms/it, loss 0.451707
Finished training it 17408/76743 of epoch 1, 69.85 ms/it, loss 0.451915
Finished training it 18432/76743 of epoch 1, 69.21 ms/it, loss 0.449743
Finished training it 18432/76743 of epoch 1, 69.41 ms/it, loss 0.448979
Finished training it 18432/76743 of epoch 1, 69.60 ms/it, loss 0.451163
Finished training it 18432/76743 of epoch 1, 69.33 ms/it, loss 0.451243
Finished training it 19456/76743 of epoch 1, 69.45 ms/it, loss 0.449894
Finished training it 19456/76743 of epoch 1, 69.28 ms/it, loss 0.448976
Finished training it 19456/76743 of epoch 1, 69.41 ms/it, loss 0.452860
Finished training it 19456/76743 of epoch 1, 69.58 ms/it, loss 0.452492
Finished training it 20480/76743 of epoch 1, 69.36 ms/it, loss 0.448832
Finished training it 20480/76743 of epoch 1, 69.73 ms/it, loss 0.451061
Finished training it 20480/76743 of epoch 1, 69.33 ms/it, loss 0.449812
Finished training it 20480/76743 of epoch 1, 69.27 ms/it, loss 0.451455
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574315.0
get out
0 has test check 2574315.0 and sample count 3274240
 accuracy 78.623 %, best 78.623 %, roc auc score 0.7972, best 0.7972
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574315.0
get out
3 has test check 2574315.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 68.75 ms/it, loss 0.452955
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574315.0
get out
2 has test check 2574315.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 68.82 ms/it, loss 0.448952
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 68.69 ms/it, loss 0.449817
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574315.0
get out
1 has test check 2574315.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 68.76 ms/it, loss 0.451163
Finished training it 22528/76743 of epoch 1, 69.21 ms/it, loss 0.450294
Finished training it 22528/76743 of epoch 1, 69.23 ms/it, loss 0.450933
Finished training it 22528/76743 of epoch 1, 69.41 ms/it, loss 0.448776
Finished training it 22528/76743 of epoch 1, 69.25 ms/it, loss 0.447938
Finished training it 23552/76743 of epoch 1, 69.44 ms/it, loss 0.450299
Finished training it 23552/76743 of epoch 1, 69.40 ms/it, loss 0.450999
Finished training it 23552/76743 of epoch 1, 69.35 ms/it, loss 0.451393
Finished training it 23552/76743 of epoch 1, 69.39 ms/it, loss 0.450173
Finished training it 24576/76743 of epoch 1, 69.21 ms/it, loss 0.451314
Finished training it 24576/76743 of epoch 1, 69.31 ms/it, loss 0.448736
Finished training it 24576/76743 of epoch 1, 69.15 ms/it, loss 0.449271
Finished training it 24576/76743 of epoch 1, 69.22 ms/it, loss 0.449030
Finished training it 25600/76743 of epoch 1, 69.15 ms/it, loss 0.450407
Finished training it 25600/76743 of epoch 1, 69.32 ms/it, loss 0.453027
Finished training it 25600/76743 of epoch 1, 69.43 ms/it, loss 0.449436
Finished training it 25600/76743 of epoch 1, 69.22 ms/it, loss 0.450228
Finished training it 26624/76743 of epoch 1, 69.56 ms/it, loss 0.448515
Finished training it 26624/76743 of epoch 1, 69.49 ms/it, loss 0.451013
Finished training it 26624/76743 of epoch 1, 69.42 ms/it, loss 0.450054
Finished training it 26624/76743 of epoch 1, 69.67 ms/it, loss 0.449162
Finished training it 27648/76743 of epoch 1, 69.20 ms/it, loss 0.448972
Finished training it 27648/76743 of epoch 1, 69.35 ms/it, loss 0.448719
Finished training it 27648/76743 of epoch 1, 69.19 ms/it, loss 0.449509
Finished training it 27648/76743 of epoch 1, 69.26 ms/it, loss 0.448270
Finished training it 28672/76743 of epoch 1, 68.97 ms/it, loss 0.451050
Finished training it 28672/76743 of epoch 1, 68.95 ms/it, loss 0.449810
Finished training it 28672/76743 of epoch 1, 69.07 ms/it, loss 0.447865
Finished training it 28672/76743 of epoch 1, 69.12 ms/it, loss 0.448438
Finished training it 29696/76743 of epoch 1, 68.58 ms/it, loss 0.451610
Finished training it 29696/76743 of epoch 1, 68.66 ms/it, loss 0.448585
Finished training it 29696/76743 of epoch 1, 68.74 ms/it, loss 0.449201
Finished training it 29696/76743 of epoch 1, 68.65 ms/it, loss 0.449490
Finished training it 30720/76743 of epoch 1, 69.00 ms/it, loss 0.447741
Finished training it 30720/76743 of epoch 1, 68.97 ms/it, loss 0.450374
Finished training it 30720/76743 of epoch 1, 68.83 ms/it, loss 0.449988
Finished training it 30720/76743 of epoch 1, 69.01 ms/it, loss 0.449430
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2570540.0
get out
0 has test check 2570540.0 and sample count 3274240
 accuracy 78.508 %, best 78.623 %, roc auc score 0.7977, best 0.7977
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 68.77 ms/it, loss 0.447931
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2570540.0
get out
1 has test check 2570540.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 68.98 ms/it, loss 0.447052
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2570540.0
get out
3 has test check 2570540.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 68.88 ms/it, loss 0.448174
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2570540.0
get out
2 has test check 2570540.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 68.92 ms/it, loss 0.450544
Finished training it 32768/76743 of epoch 1, 68.98 ms/it, loss 0.449150
Finished training it 32768/76743 of epoch 1, 68.95 ms/it, loss 0.451050
Finished training it 32768/76743 of epoch 1, 68.96 ms/it, loss 0.449271
Finished training it 32768/76743 of epoch 1, 69.03 ms/it, loss 0.449055
Finished training it 33792/76743 of epoch 1, 69.00 ms/it, loss 0.450004
Finished training it 33792/76743 of epoch 1, 69.01 ms/it, loss 0.447600
Finished training it 33792/76743 of epoch 1, 68.93 ms/it, loss 0.450897
Finished training it 33792/76743 of epoch 1, 68.96 ms/it, loss 0.451938
Finished training it 34816/76743 of epoch 1, 69.03 ms/it, loss 0.447294
Finished training it 34816/76743 of epoch 1, 69.08 ms/it, loss 0.448949
Finished training it 34816/76743 of epoch 1, 69.11 ms/it, loss 0.448409
Finished training it 34816/76743 of epoch 1, 69.20 ms/it, loss 0.450069
Finished training it 35840/76743 of epoch 1, 69.73 ms/it, loss 0.450396
Finished training it 35840/76743 of epoch 1, 69.60 ms/it, loss 0.447341
Finished training it 35840/76743 of epoch 1, 69.68 ms/it, loss 0.448529
Finished training it 35840/76743 of epoch 1, 73.83 ms/it, loss 0.449431
Finished training it 36864/76743 of epoch 1, 74.77 ms/it, loss 0.448479
Finished training it 36864/76743 of epoch 1, 70.05 ms/it, loss 0.448226
Finished training it 36864/76743 of epoch 1, 74.90 ms/it, loss 0.449647
Finished training it 36864/76743 of epoch 1, 74.86 ms/it, loss 0.448683
Finished training it 37888/76743 of epoch 1, 69.42 ms/it, loss 0.449999
Finished training it 37888/76743 of epoch 1, 69.38 ms/it, loss 0.451133
Finished training it 37888/76743 of epoch 1, 69.29 ms/it, loss 0.450078
Finished training it 37888/76743 of epoch 1, 69.34 ms/it, loss 0.447107
Finished training it 38912/76743 of epoch 1, 69.46 ms/it, loss 0.448438
Finished training it 38912/76743 of epoch 1, 69.34 ms/it, loss 0.446785
Finished training it 38912/76743 of epoch 1, 69.33 ms/it, loss 0.449469
Finished training it 38912/76743 of epoch 1, 69.37 ms/it, loss 0.448135
Finished training it 39936/76743 of epoch 1, 69.00 ms/it, loss 0.450955
Finished training it 39936/76743 of epoch 1, 68.98 ms/it, loss 0.447578
Finished training it 39936/76743 of epoch 1, 69.14 ms/it, loss 0.449588
Finished training it 39936/76743 of epoch 1, 69.06 ms/it, loss 0.449622
Finished training it 40960/76743 of epoch 1, 69.23 ms/it, loss 0.452534
Finished training it 40960/76743 of epoch 1, 69.14 ms/it, loss 0.449866
Finished training it 40960/76743 of epoch 1, 69.13 ms/it, loss 0.449034
Finished training it 40960/76743 of epoch 1, 69.09 ms/it, loss 0.448111
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575390.0
get out
0 has test check 2575390.0 and sample count 3274240
 accuracy 78.656 %, best 78.656 %, roc auc score 0.7984, best 0.7984
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575390.0
get out
3 has test check 2575390.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 68.76 ms/it, loss 0.446544
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 68.59 ms/it, loss 0.448755
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575390.0
get out
1 has test check 2575390.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 68.57 ms/it, loss 0.448406
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575390.0
get out
2 has test check 2575390.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 68.80 ms/it, loss 0.449478
Finished training it 43008/76743 of epoch 1, 68.84 ms/it, loss 0.446318
Finished training it 43008/76743 of epoch 1, 69.07 ms/it, loss 0.445943
Finished training it 43008/76743 of epoch 1, 68.94 ms/it, loss 0.447744
Finished training it 43008/76743 of epoch 1, 68.82 ms/it, loss 0.447488
Finished training it 44032/76743 of epoch 1, 68.86 ms/it, loss 0.450115
Finished training it 44032/76743 of epoch 1, 68.72 ms/it, loss 0.448665
Finished training it 44032/76743 of epoch 1, 68.76 ms/it, loss 0.448573
Finished training it 44032/76743 of epoch 1, 68.68 ms/it, loss 0.451207
Finished training it 45056/76743 of epoch 1, 69.31 ms/it, loss 0.446401
Finished training it 45056/76743 of epoch 1, 69.43 ms/it, loss 0.447119
Finished training it 45056/76743 of epoch 1, 69.25 ms/it, loss 0.448292
Finished training it 45056/76743 of epoch 1, 69.40 ms/it, loss 0.448948
Finished training it 46080/76743 of epoch 1, 69.35 ms/it, loss 0.449955
Finished training it 46080/76743 of epoch 1, 69.06 ms/it, loss 0.450159
Finished training it 46080/76743 of epoch 1, 69.30 ms/it, loss 0.448016
Finished training it 46080/76743 of epoch 1, 69.16 ms/it, loss 0.448733
Finished training it 47104/76743 of epoch 1, 69.08 ms/it, loss 0.450756
Finished training it 47104/76743 of epoch 1, 68.96 ms/it, loss 0.448561
Finished training it 47104/76743 of epoch 1, 68.85 ms/it, loss 0.450487
Finished training it 47104/76743 of epoch 1, 69.04 ms/it, loss 0.448863
Finished training it 48128/76743 of epoch 1, 68.77 ms/it, loss 0.446961
Finished training it 48128/76743 of epoch 1, 68.80 ms/it, loss 0.449717
Finished training it 48128/76743 of epoch 1, 68.90 ms/it, loss 0.448155
Finished training it 48128/76743 of epoch 1, 68.99 ms/it, loss 0.447099
Finished training it 49152/76743 of epoch 1, 69.43 ms/it, loss 0.452603
Finished training it 49152/76743 of epoch 1, 69.15 ms/it, loss 0.448937
Finished training it 49152/76743 of epoch 1, 69.36 ms/it, loss 0.448841
Finished training it 49152/76743 of epoch 1, 69.20 ms/it, loss 0.449365
Finished training it 50176/76743 of epoch 1, 69.74 ms/it, loss 0.448768
Finished training it 50176/76743 of epoch 1, 69.88 ms/it, loss 0.449226
Finished training it 50176/76743 of epoch 1, 69.67 ms/it, loss 0.446601
Finished training it 50176/76743 of epoch 1, 69.62 ms/it, loss 0.447835
Finished training it 51200/76743 of epoch 1, 69.51 ms/it, loss 0.446148
Finished training it 51200/76743 of epoch 1, 69.56 ms/it, loss 0.447461
Finished training it 51200/76743 of epoch 1, 69.53 ms/it, loss 0.450741
Finished training it 51200/76743 of epoch 1, 69.70 ms/it, loss 0.449364
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575866.0
get out
0 has test check 2575866.0 and sample count 3274240
 accuracy 78.671 %, best 78.671 %, roc auc score 0.7985, best 0.7985
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 68.88 ms/it, loss 0.449319
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575866.0
get out
3 has test check 2575866.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 69.11 ms/it, loss 0.447529
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575866.0
get out
2 has test check 2575866.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 69.16 ms/it, loss 0.449107
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575866.0
get out
1 has test check 2575866.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 69.18 ms/it, loss 0.447193
Finished training it 53248/76743 of epoch 1, 69.34 ms/it, loss 0.447663
Finished training it 53248/76743 of epoch 1, 68.98 ms/it, loss 0.447619
Finished training it 53248/76743 of epoch 1, 69.22 ms/it, loss 0.449188
Finished training it 53248/76743 of epoch 1, 69.32 ms/it, loss 0.449281
Finished training it 54272/76743 of epoch 1, 68.88 ms/it, loss 0.448879
Finished training it 54272/76743 of epoch 1, 69.09 ms/it, loss 0.450060
Finished training it 54272/76743 of epoch 1, 69.16 ms/it, loss 0.449798
Finished training it 54272/76743 of epoch 1, 69.13 ms/it, loss 0.448399
Finished training it 55296/76743 of epoch 1, 69.21 ms/it, loss 0.451168
Finished training it 55296/76743 of epoch 1, 69.06 ms/it, loss 0.447597
Finished training it 55296/76743 of epoch 1, 69.26 ms/it, loss 0.448151
Finished training it 55296/76743 of epoch 1, 69.02 ms/it, loss 0.447831
Finished training it 56320/76743 of epoch 1, 69.16 ms/it, loss 0.449182
Finished training it 56320/76743 of epoch 1, 69.19 ms/it, loss 0.450531
Finished training it 56320/76743 of epoch 1, 69.11 ms/it, loss 0.451267
Finished training it 56320/76743 of epoch 1, 69.24 ms/it, loss 0.449882
Finished training it 57344/76743 of epoch 1, 69.08 ms/it, loss 0.451154
Finished training it 57344/76743 of epoch 1, 69.30 ms/it, loss 0.446688
Finished training it 57344/76743 of epoch 1, 68.98 ms/it, loss 0.448560
Finished training it 57344/76743 of epoch 1, 69.08 ms/it, loss 0.447680
Finished training it 58368/76743 of epoch 1, 69.19 ms/it, loss 0.448098
Finished training it 58368/76743 of epoch 1, 69.32 ms/it, loss 0.448569
Finished training it 58368/76743 of epoch 1, 69.33 ms/it, loss 0.446289
Finished training it 58368/76743 of epoch 1, 69.28 ms/it, loss 0.450914
Finished training it 59392/76743 of epoch 1, 69.08 ms/it, loss 0.447507
Finished training it 59392/76743 of epoch 1, 69.40 ms/it, loss 0.447653
Finished training it 59392/76743 of epoch 1, 69.35 ms/it, loss 0.449941
Finished training it 59392/76743 of epoch 1, 69.22 ms/it, loss 0.446917
Finished training it 60416/76743 of epoch 1, 68.94 ms/it, loss 0.447501
Finished training it 60416/76743 of epoch 1, 69.00 ms/it, loss 0.448322
Finished training it 60416/76743 of epoch 1, 68.88 ms/it, loss 0.449831
Finished training it 60416/76743 of epoch 1, 68.96 ms/it, loss 0.447747
Finished training it 61440/76743 of epoch 1, 68.72 ms/it, loss 0.448957
Finished training it 61440/76743 of epoch 1, 68.88 ms/it, loss 0.448116
Finished training it 61440/76743 of epoch 1, 68.87 ms/it, loss 0.447958
Finished training it 61440/76743 of epoch 1, 68.77 ms/it, loss 0.449852
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576308.0
get out
0 has test check 2576308.0 and sample count 3274240
 accuracy 78.684 %, best 78.684 %, roc auc score 0.7998, best 0.7998
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576308.0
get out
1 has test check 2576308.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 69.72 ms/it, loss 0.447771
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576308.0
get out
2 has test check 2576308.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 69.68 ms/it, loss 0.446117
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 69.63 ms/it, loss 0.448668
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576308.0
get out
3 has test check 2576308.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 69.89 ms/it, loss 0.448167
Finished training it 63488/76743 of epoch 1, 68.86 ms/it, loss 0.447605
Finished training it 63488/76743 of epoch 1, 68.98 ms/it, loss 0.449718
Finished training it 63488/76743 of epoch 1, 68.92 ms/it, loss 0.448823
Finished training it 63488/76743 of epoch 1, 69.01 ms/it, loss 0.447820
Finished training it 64512/76743 of epoch 1, 69.51 ms/it, loss 0.449801
Finished training it 64512/76743 of epoch 1, 69.46 ms/it, loss 0.447806
Finished training it 64512/76743 of epoch 1, 69.27 ms/it, loss 0.446713
Finished training it 64512/76743 of epoch 1, 69.34 ms/it, loss 0.450383
Finished training it 65536/76743 of epoch 1, 74.11 ms/it, loss 0.448105
Finished training it 65536/76743 of epoch 1, 74.66 ms/it, loss 0.447117
Finished training it 65536/76743 of epoch 1, 74.74 ms/it, loss 0.447860
Finished training it 65536/76743 of epoch 1, 74.20 ms/it, loss 0.449469
Finished training it 66560/76743 of epoch 1, 69.44 ms/it, loss 0.448314
Finished training it 66560/76743 of epoch 1, 69.46 ms/it, loss 0.445597
Finished training it 66560/76743 of epoch 1, 69.69 ms/it, loss 0.451370
Finished training it 66560/76743 of epoch 1, 69.73 ms/it, loss 0.444927
Finished training it 67584/76743 of epoch 1, 69.36 ms/it, loss 0.450299
Finished training it 67584/76743 of epoch 1, 69.33 ms/it, loss 0.451689
Finished training it 67584/76743 of epoch 1, 69.46 ms/it, loss 0.448572
Finished training it 67584/76743 of epoch 1, 69.29 ms/it, loss 0.445808
Finished training it 68608/76743 of epoch 1, 69.80 ms/it, loss 0.446512
Finished training it 68608/76743 of epoch 1, 69.66 ms/it, loss 0.446940
Finished training it 68608/76743 of epoch 1, 69.73 ms/it, loss 0.445798
Finished training it 68608/76743 of epoch 1, 69.70 ms/it, loss 0.448451
Finished training it 69632/76743 of epoch 1, 68.54 ms/it, loss 0.445963
Finished training it 69632/76743 of epoch 1, 68.62 ms/it, loss 0.447894
Finished training it 69632/76743 of epoch 1, 68.74 ms/it, loss 0.447860
Finished training it 69632/76743 of epoch 1, 68.60 ms/it, loss 0.445630
Finished training it 70656/76743 of epoch 1, 68.93 ms/it, loss 0.448819
Finished training it 70656/76743 of epoch 1, 69.04 ms/it, loss 0.448777
Finished training it 70656/76743 of epoch 1, 69.10 ms/it, loss 0.445708
Finished training it 70656/76743 of epoch 1, 68.97 ms/it, loss 0.447798
Finished training it 71680/76743 of epoch 1, 69.05 ms/it, loss 0.448741
Finished training it 71680/76743 of epoch 1, 69.17 ms/it, loss 0.449512
Finished training it 71680/76743 of epoch 1, 69.06 ms/it, loss 0.447304
Finished training it 71680/76743 of epoch 1, 69.21 ms/it, loss 0.447481
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578473.0
get out
0 has test check 2578473.0 and sample count 3274240
 accuracy 78.750 %, best 78.750 %, roc auc score 0.8000, best 0.8000
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578473.0
get out
3 has test check 2578473.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 69.56 ms/it, loss 0.446412
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 69.41 ms/it, loss 0.448523
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578473.0
get out
1 has test check 2578473.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 69.59 ms/it, loss 0.447389
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578473.0
get out
2 has test check 2578473.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 69.65 ms/it, loss 0.446609
Finished training it 73728/76743 of epoch 1, 69.36 ms/it, loss 0.446892
Finished training it 73728/76743 of epoch 1, 69.45 ms/it, loss 0.446966
Finished training it 73728/76743 of epoch 1, 69.38 ms/it, loss 0.450302
Finished training it 73728/76743 of epoch 1, 69.44 ms/it, loss 0.449739
Finished training it 74752/76743 of epoch 1, 68.75 ms/it, loss 0.447782
Finished training it 74752/76743 of epoch 1, 68.61 ms/it, loss 0.448214
Finished training it 74752/76743 of epoch 1, 68.69 ms/it, loss 0.447314
Finished training it 74752/76743 of epoch 1, 68.54 ms/it, loss 0.449806
Finished training it 75776/76743 of epoch 1, 68.89 ms/it, loss 0.449014
Finished training it 75776/76743 of epoch 1, 68.81 ms/it, loss 0.447888
Finished training it 75776/76743 of epoch 1, 68.81 ms/it, loss 0.448528
Finished training it 75776/76743 of epoch 1, 69.11 ms/it, loss 0.448359
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 69.73 ms/it, loss 0.447563
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 69.60 ms/it, loss 0.448746
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 69.97 ms/it, loss 0.446636
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 70.02 ms/it, loss 0.447880
Finished training it 2048/76743 of epoch 2, 69.36 ms/it, loss 0.445688
Finished training it 2048/76743 of epoch 2, 69.10 ms/it, loss 0.446154
Finished training it 2048/76743 of epoch 2, 69.25 ms/it, loss 0.448645
Finished training it 2048/76743 of epoch 2, 69.12 ms/it, loss 0.447720
Finished training it 3072/76743 of epoch 2, 69.66 ms/it, loss 0.447887
Finished training it 3072/76743 of epoch 2, 69.51 ms/it, loss 0.447470
Finished training it 3072/76743 of epoch 2, 69.69 ms/it, loss 0.447306
Finished training it 3072/76743 of epoch 2, 69.65 ms/it, loss 0.445261
Finished training it 4096/76743 of epoch 2, 69.38 ms/it, loss 0.447708
Finished training it 4096/76743 of epoch 2, 69.36 ms/it, loss 0.448063
Finished training it 4096/76743 of epoch 2, 69.35 ms/it, loss 0.447102
Finished training it 4096/76743 of epoch 2, 69.21 ms/it, loss 0.446310
Finished training it 5120/76743 of epoch 2, 69.33 ms/it, loss 0.448714
Finished training it 5120/76743 of epoch 2, 69.46 ms/it, loss 0.447501
Finished training it 5120/76743 of epoch 2, 69.59 ms/it, loss 0.449241
Finished training it 5120/76743 of epoch 2, 69.40 ms/it, loss 0.448538
Finished training it 6144/76743 of epoch 2, 69.29 ms/it, loss 0.445765
Finished training it 6144/76743 of epoch 2, 69.31 ms/it, loss 0.446430
Finished training it 6144/76743 of epoch 2, 69.42 ms/it, loss 0.448387
Finished training it 6144/76743 of epoch 2, 69.33 ms/it, loss 0.447206
Finished training it 7168/76743 of epoch 2, 69.09 ms/it, loss 0.447002
Finished training it 7168/76743 of epoch 2, 69.04 ms/it, loss 0.447256
Finished training it 7168/76743 of epoch 2, 69.33 ms/it, loss 0.446597
Finished training it 7168/76743 of epoch 2, 69.11 ms/it, loss 0.446969
Finished training it 8192/76743 of epoch 2, 69.19 ms/it, loss 0.447554
Finished training it 8192/76743 of epoch 2, 69.18 ms/it, loss 0.448509
Finished training it 8192/76743 of epoch 2, 69.22 ms/it, loss 0.448482
Finished training it 8192/76743 of epoch 2, 69.17 ms/it, loss 0.448907
Finished training it 9216/76743 of epoch 2, 69.24 ms/it, loss 0.445573
Finished training it 9216/76743 of epoch 2, 69.34 ms/it, loss 0.444048
Finished training it 9216/76743 of epoch 2, 69.34 ms/it, loss 0.448508
Finished training it 9216/76743 of epoch 2, 69.31 ms/it, loss 0.446972
Finished training it 10240/76743 of epoch 2, 68.73 ms/it, loss 0.447737
Finished training it 10240/76743 of epoch 2, 68.80 ms/it, loss 0.451301
Finished training it 10240/76743 of epoch 2, 68.56 ms/it, loss 0.447067
Finished training it 10240/76743 of epoch 2, 68.70 ms/it, loss 0.449247
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578664.0
get out
0 has test check 2578664.0 and sample count 3274240
 accuracy 78.756 %, best 78.756 %, roc auc score 0.8002, best 0.8002
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578664.0
get out
1 has test check 2578664.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 69.04 ms/it, loss 0.448693
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578664.0
get out
2 has test check 2578664.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 69.28 ms/it, loss 0.445703
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 69.11 ms/it, loss 0.447249
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578664.0
get out
3 has test check 2578664.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 69.19 ms/it, loss 0.446877
Finished training it 12288/76743 of epoch 2, 68.92 ms/it, loss 0.446728
Finished training it 12288/76743 of epoch 2, 68.86 ms/it, loss 0.442545
Finished training it 12288/76743 of epoch 2, 68.95 ms/it, loss 0.447187
Finished training it 12288/76743 of epoch 2, 68.78 ms/it, loss 0.448000
Finished training it 13312/76743 of epoch 2, 68.74 ms/it, loss 0.446057
Finished training it 13312/76743 of epoch 2, 68.76 ms/it, loss 0.446763
Finished training it 13312/76743 of epoch 2, 68.80 ms/it, loss 0.445079
Finished training it 13312/76743 of epoch 2, 68.70 ms/it, loss 0.447985
Finished training it 14336/76743 of epoch 2, 69.86 ms/it, loss 0.446013
Finished training it 14336/76743 of epoch 2, 69.36 ms/it, loss 0.445517
Finished training it 14336/76743 of epoch 2, 69.72 ms/it, loss 0.447861
Finished training it 14336/76743 of epoch 2, 73.94 ms/it, loss 0.447235
Finished training it 15360/76743 of epoch 2, 74.85 ms/it, loss 0.444517
Finished training it 15360/76743 of epoch 2, 74.84 ms/it, loss 0.444160
Finished training it 15360/76743 of epoch 2, 74.59 ms/it, loss 0.444978
Finished training it 15360/76743 of epoch 2, 70.23 ms/it, loss 0.445817
Finished training it 16384/76743 of epoch 2, 69.10 ms/it, loss 0.447852
Finished training it 16384/76743 of epoch 2, 69.04 ms/it, loss 0.448084
Finished training it 16384/76743 of epoch 2, 68.83 ms/it, loss 0.446837
Finished training it 16384/76743 of epoch 2, 68.95 ms/it, loss 0.444558
Finished training it 17408/76743 of epoch 2, 69.20 ms/it, loss 0.447148
Finished training it 17408/76743 of epoch 2, 69.26 ms/it, loss 0.444539
Finished training it 17408/76743 of epoch 2, 69.07 ms/it, loss 0.447302
Finished training it 17408/76743 of epoch 2, 69.01 ms/it, loss 0.447740
Finished training it 18432/76743 of epoch 2, 69.15 ms/it, loss 0.444699
Finished training it 18432/76743 of epoch 2, 69.12 ms/it, loss 0.446538
Finished training it 18432/76743 of epoch 2, 68.93 ms/it, loss 0.446898
Finished training it 18432/76743 of epoch 2, 68.95 ms/it, loss 0.445495
Finished training it 19456/76743 of epoch 2, 69.00 ms/it, loss 0.448442
Finished training it 19456/76743 of epoch 2, 69.12 ms/it, loss 0.444910
Finished training it 19456/76743 of epoch 2, 69.08 ms/it, loss 0.445481
Finished training it 19456/76743 of epoch 2, 69.16 ms/it, loss 0.448680
Finished training it 20480/76743 of epoch 2, 69.08 ms/it, loss 0.446027
Finished training it 20480/76743 of epoch 2, 69.02 ms/it, loss 0.447302
Finished training it 20480/76743 of epoch 2, 69.00 ms/it, loss 0.444573
Finished training it 20480/76743 of epoch 2, 68.92 ms/it, loss 0.446979
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580170.0
get out
0 has test check 2580170.0 and sample count 3274240
 accuracy 78.802 %, best 78.802 %, roc auc score 0.8009, best 0.8009
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580170.0
get out
3 has test check 2580170.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 69.18 ms/it, loss 0.448994
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 69.08 ms/it, loss 0.445508
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580170.0
get out
2 has test check 2580170.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 69.03 ms/it, loss 0.445350
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580170.0
get out
1 has test check 2580170.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 69.15 ms/it, loss 0.447135
Finished training it 22528/76743 of epoch 2, 69.66 ms/it, loss 0.446661
Finished training it 22528/76743 of epoch 2, 69.68 ms/it, loss 0.443929
Finished training it 22528/76743 of epoch 2, 69.60 ms/it, loss 0.446326
Finished training it 22528/76743 of epoch 2, 69.65 ms/it, loss 0.444911
Finished training it 23552/76743 of epoch 2, 69.10 ms/it, loss 0.446676
Finished training it 23552/76743 of epoch 2, 69.17 ms/it, loss 0.446337
Finished training it 23552/76743 of epoch 2, 69.16 ms/it, loss 0.446171
Finished training it 23552/76743 of epoch 2, 69.08 ms/it, loss 0.447443
Finished training it 24576/76743 of epoch 2, 69.37 ms/it, loss 0.444949
Finished training it 24576/76743 of epoch 2, 69.08 ms/it, loss 0.447367
Finished training it 24576/76743 of epoch 2, 69.16 ms/it, loss 0.445846
Finished training it 24576/76743 of epoch 2, 69.15 ms/it, loss 0.444547
Finished training it 25600/76743 of epoch 2, 69.45 ms/it, loss 0.448844
Finished training it 25600/76743 of epoch 2, 69.39 ms/it, loss 0.446225
Finished training it 25600/76743 of epoch 2, 69.19 ms/it, loss 0.446719
Finished training it 25600/76743 of epoch 2, 69.35 ms/it, loss 0.445163
Finished training it 26624/76743 of epoch 2, 69.36 ms/it, loss 0.447114
Finished training it 26624/76743 of epoch 2, 69.12 ms/it, loss 0.445754
Finished training it 26624/76743 of epoch 2, 69.37 ms/it, loss 0.445135
Finished training it 26624/76743 of epoch 2, 69.37 ms/it, loss 0.444579
Finished training it 27648/76743 of epoch 2, 69.22 ms/it, loss 0.444733
Finished training it 27648/76743 of epoch 2, 69.06 ms/it, loss 0.445650
Finished training it 27648/76743 of epoch 2, 69.04 ms/it, loss 0.444910
Finished training it 27648/76743 of epoch 2, 69.15 ms/it, loss 0.444186
Finished training it 28672/76743 of epoch 2, 69.54 ms/it, loss 0.444906
Finished training it 28672/76743 of epoch 2, 69.35 ms/it, loss 0.443735
Finished training it 28672/76743 of epoch 2, 69.46 ms/it, loss 0.447406
Finished training it 28672/76743 of epoch 2, 69.42 ms/it, loss 0.445907
Finished training it 29696/76743 of epoch 2, 69.40 ms/it, loss 0.445675
Finished training it 29696/76743 of epoch 2, 69.50 ms/it, loss 0.447988
Finished training it 29696/76743 of epoch 2, 69.49 ms/it, loss 0.444338
Finished training it 29696/76743 of epoch 2, 69.60 ms/it, loss 0.445300
Finished training it 30720/76743 of epoch 2, 69.48 ms/it, loss 0.446915
Finished training it 30720/76743 of epoch 2, 69.33 ms/it, loss 0.445370
Finished training it 30720/76743 of epoch 2, 69.32 ms/it, loss 0.444157
Finished training it 30720/76743 of epoch 2, 69.45 ms/it, loss 0.445715
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577453.0
get out
0 has test check 2577453.0 and sample count 3274240
 accuracy 78.719 %, best 78.802 %, roc auc score 0.8013, best 0.8013
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577453.0
get out
3 has test check 2577453.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 69.40 ms/it, loss 0.444354
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 69.19 ms/it, loss 0.444331
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577453.0
get out
1 has test check 2577453.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 69.33 ms/it, loss 0.443360
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577453.0
get out
2 has test check 2577453.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 69.28 ms/it, loss 0.446632
Finished training it 32768/76743 of epoch 2, 68.98 ms/it, loss 0.445253
Finished training it 32768/76743 of epoch 2, 69.19 ms/it, loss 0.445062
Finished training it 32768/76743 of epoch 2, 69.16 ms/it, loss 0.445430
Finished training it 32768/76743 of epoch 2, 69.23 ms/it, loss 0.447127
Finished training it 33792/76743 of epoch 2, 69.54 ms/it, loss 0.446902
Finished training it 33792/76743 of epoch 2, 69.44 ms/it, loss 0.448081
Finished training it 33792/76743 of epoch 2, 69.45 ms/it, loss 0.446090
Finished training it 33792/76743 of epoch 2, 69.27 ms/it, loss 0.443537
Finished training it 34816/76743 of epoch 2, 69.48 ms/it, loss 0.446086
Finished training it 34816/76743 of epoch 2, 69.55 ms/it, loss 0.445052
Finished training it 34816/76743 of epoch 2, 73.72 ms/it, loss 0.444504
Finished training it 34816/76743 of epoch 2, 69.61 ms/it, loss 0.443499
Finished training it 35840/76743 of epoch 2, 69.82 ms/it, loss 0.445617
Finished training it 35840/76743 of epoch 2, 74.32 ms/it, loss 0.444857
Finished training it 35840/76743 of epoch 2, 74.32 ms/it, loss 0.446325
Finished training it 35840/76743 of epoch 2, 74.39 ms/it, loss 0.443817
Finished training it 36864/76743 of epoch 2, 68.65 ms/it, loss 0.444628
Finished training it 36864/76743 of epoch 2, 68.60 ms/it, loss 0.445064
Finished training it 36864/76743 of epoch 2, 68.77 ms/it, loss 0.445923
Finished training it 36864/76743 of epoch 2, 68.62 ms/it, loss 0.444663
Finished training it 37888/76743 of epoch 2, 68.92 ms/it, loss 0.445846
Finished training it 37888/76743 of epoch 2, 68.80 ms/it, loss 0.447230
Finished training it 37888/76743 of epoch 2, 68.71 ms/it, loss 0.446365
Finished training it 37888/76743 of epoch 2, 68.95 ms/it, loss 0.443060
Finished training it 38912/76743 of epoch 2, 69.13 ms/it, loss 0.444470
Finished training it 38912/76743 of epoch 2, 69.25 ms/it, loss 0.442909
Finished training it 38912/76743 of epoch 2, 69.11 ms/it, loss 0.445770
Finished training it 38912/76743 of epoch 2, 69.13 ms/it, loss 0.444325
Finished training it 39936/76743 of epoch 2, 69.45 ms/it, loss 0.447517
Finished training it 39936/76743 of epoch 2, 69.57 ms/it, loss 0.443928
Finished training it 39936/76743 of epoch 2, 69.46 ms/it, loss 0.446128
Finished training it 39936/76743 of epoch 2, 69.39 ms/it, loss 0.445549
Finished training it 40960/76743 of epoch 2, 68.64 ms/it, loss 0.448887
Finished training it 40960/76743 of epoch 2, 68.52 ms/it, loss 0.444650
Finished training it 40960/76743 of epoch 2, 68.55 ms/it, loss 0.445489
Finished training it 40960/76743 of epoch 2, 68.41 ms/it, loss 0.445701
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579577.0
get out
0 has test check 2579577.0 and sample count 3274240
 accuracy 78.784 %, best 78.802 %, roc auc score 0.8009, best 0.8013
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579577.0
get out
1 has test check 2579577.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 70.16 ms/it, loss 0.444927
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 70.14 ms/it, loss 0.445073
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579577.0
get out
2 has test check 2579577.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 70.22 ms/it, loss 0.445701
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579577.0
get out
3 has test check 2579577.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 70.14 ms/it, loss 0.442666
Finished training it 43008/76743 of epoch 2, 69.27 ms/it, loss 0.442693
Finished training it 43008/76743 of epoch 2, 69.30 ms/it, loss 0.444247
Finished training it 43008/76743 of epoch 2, 69.18 ms/it, loss 0.442533
Finished training it 43008/76743 of epoch 2, 69.16 ms/it, loss 0.443878
Finished training it 44032/76743 of epoch 2, 69.22 ms/it, loss 0.444644
Finished training it 44032/76743 of epoch 2, 69.33 ms/it, loss 0.447320
Finished training it 44032/76743 of epoch 2, 69.24 ms/it, loss 0.446833
Finished training it 44032/76743 of epoch 2, 69.21 ms/it, loss 0.445357
Finished training it 45056/76743 of epoch 2, 69.33 ms/it, loss 0.442723
Finished training it 45056/76743 of epoch 2, 69.56 ms/it, loss 0.444994
Finished training it 45056/76743 of epoch 2, 69.38 ms/it, loss 0.443308
Finished training it 45056/76743 of epoch 2, 69.50 ms/it, loss 0.445320
Finished training it 46080/76743 of epoch 2, 69.59 ms/it, loss 0.444361
Finished training it 46080/76743 of epoch 2, 69.84 ms/it, loss 0.445151
Finished training it 46080/76743 of epoch 2, 69.69 ms/it, loss 0.446782
Finished training it 46080/76743 of epoch 2, 69.84 ms/it, loss 0.446384
Finished training it 47104/76743 of epoch 2, 69.49 ms/it, loss 0.445075
Finished training it 47104/76743 of epoch 2, 69.42 ms/it, loss 0.446843
Finished training it 47104/76743 of epoch 2, 69.35 ms/it, loss 0.447287
Finished training it 47104/76743 of epoch 2, 69.46 ms/it, loss 0.445216
Finished training it 48128/76743 of epoch 2, 69.46 ms/it, loss 0.443586
Finished training it 48128/76743 of epoch 2, 69.69 ms/it, loss 0.444411
Finished training it 48128/76743 of epoch 2, 69.56 ms/it, loss 0.446190
Finished training it 48128/76743 of epoch 2, 69.68 ms/it, loss 0.443697
Finished training it 49152/76743 of epoch 2, 69.57 ms/it, loss 0.445560
Finished training it 49152/76743 of epoch 2, 69.51 ms/it, loss 0.448770
Finished training it 49152/76743 of epoch 2, 69.55 ms/it, loss 0.445644
Finished training it 49152/76743 of epoch 2, 69.57 ms/it, loss 0.445460
Finished training it 50176/76743 of epoch 2, 69.14 ms/it, loss 0.443458
Finished training it 50176/76743 of epoch 2, 69.00 ms/it, loss 0.444526
Finished training it 50176/76743 of epoch 2, 69.03 ms/it, loss 0.445829
Finished training it 50176/76743 of epoch 2, 69.10 ms/it, loss 0.445380
Finished training it 51200/76743 of epoch 2, 69.07 ms/it, loss 0.442431
Finished training it 51200/76743 of epoch 2, 68.99 ms/it, loss 0.445931
Finished training it 51200/76743 of epoch 2, 69.02 ms/it, loss 0.443996
Finished training it 51200/76743 of epoch 2, 69.07 ms/it, loss 0.446923
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580241.0
get out
0 has test check 2580241.0 and sample count 3274240
 accuracy 78.804 %, best 78.804 %, roc auc score 0.8013, best 0.8013
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580241.0
get out
3 has test check 2580241.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 68.97 ms/it, loss 0.444281
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 68.66 ms/it, loss 0.445882
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580241.0
get out
1 has test check 2580241.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 68.91 ms/it, loss 0.443762
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580241.0
get out
2 has test check 2580241.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 69.00 ms/it, loss 0.445543
Finished training it 53248/76743 of epoch 2, 68.75 ms/it, loss 0.444339
Finished training it 53248/76743 of epoch 2, 68.74 ms/it, loss 0.445477
Finished training it 53248/76743 of epoch 2, 68.86 ms/it, loss 0.444095
Finished training it 53248/76743 of epoch 2, 68.68 ms/it, loss 0.445562
Finished training it 54272/76743 of epoch 2, 68.80 ms/it, loss 0.445436
Finished training it 54272/76743 of epoch 2, 68.81 ms/it, loss 0.446158
Finished training it 54272/76743 of epoch 2, 69.01 ms/it, loss 0.444699
Finished training it 54272/76743 of epoch 2, 68.92 ms/it, loss 0.447213
Finished training it 55296/76743 of epoch 2, 68.55 ms/it, loss 0.444224
Finished training it 55296/76743 of epoch 2, 68.74 ms/it, loss 0.447688
Finished training it 55296/76743 of epoch 2, 68.72 ms/it, loss 0.445149
Finished training it 55296/76743 of epoch 2, 68.66 ms/it, loss 0.444798
Finished training it 56320/76743 of epoch 2, 70.08 ms/it, loss 0.447943
Finished training it 56320/76743 of epoch 2, 69.55 ms/it, loss 0.447550
Finished training it 56320/76743 of epoch 2, 69.94 ms/it, loss 0.445827
Finished training it 56320/76743 of epoch 2, 74.56 ms/it, loss 0.446697
Finished training it 57344/76743 of epoch 2, 70.19 ms/it, loss 0.445024
Finished training it 57344/76743 of epoch 2, 69.63 ms/it, loss 0.443908
Finished training it 57344/76743 of epoch 2, 70.22 ms/it, loss 0.443344
Finished training it 57344/76743 of epoch 2, 70.41 ms/it, loss 0.447877
Finished training it 58368/76743 of epoch 2, 69.35 ms/it, loss 0.444999
Finished training it 58368/76743 of epoch 2, 69.27 ms/it, loss 0.444873
Finished training it 58368/76743 of epoch 2, 69.27 ms/it, loss 0.447605
Finished training it 58368/76743 of epoch 2, 69.39 ms/it, loss 0.443019
Finished training it 59392/76743 of epoch 2, 69.08 ms/it, loss 0.443727
Finished training it 59392/76743 of epoch 2, 69.20 ms/it, loss 0.444552
Finished training it 59392/76743 of epoch 2, 69.13 ms/it, loss 0.444194
Finished training it 59392/76743 of epoch 2, 69.16 ms/it, loss 0.446377
Finished training it 60416/76743 of epoch 2, 68.43 ms/it, loss 0.446589
Finished training it 60416/76743 of epoch 2, 68.55 ms/it, loss 0.445173
Finished training it 60416/76743 of epoch 2, 68.53 ms/it, loss 0.443911
Finished training it 60416/76743 of epoch 2, 68.56 ms/it, loss 0.444186
Finished training it 61440/76743 of epoch 2, 68.87 ms/it, loss 0.444951
Finished training it 61440/76743 of epoch 2, 68.59 ms/it, loss 0.444223
Finished training it 61440/76743 of epoch 2, 68.61 ms/it, loss 0.445560
Finished training it 61440/76743 of epoch 2, 68.63 ms/it, loss 0.446485
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580232.0
get out
0 has test check 2580232.0 and sample count 3274240
 accuracy 78.804 %, best 78.804 %, roc auc score 0.8022, best 0.8022
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580232.0
get out
3 has test check 2580232.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 68.82 ms/it, loss 0.445025
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580232.0
get out
1 has test check 2580232.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 69.03 ms/it, loss 0.444695
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 68.82 ms/it, loss 0.445155
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580232.0
get out
2 has test check 2580232.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 69.11 ms/it, loss 0.442589
Finished training it 63488/76743 of epoch 2, 68.73 ms/it, loss 0.446603
Finished training it 63488/76743 of epoch 2, 68.79 ms/it, loss 0.444136
Finished training it 63488/76743 of epoch 2, 68.87 ms/it, loss 0.445745
Finished training it 63488/76743 of epoch 2, 68.82 ms/it, loss 0.444837
Finished training it 64512/76743 of epoch 2, 69.07 ms/it, loss 0.443371
Finished training it 64512/76743 of epoch 2, 69.08 ms/it, loss 0.447192
Finished training it 64512/76743 of epoch 2, 69.09 ms/it, loss 0.446651
Finished training it 64512/76743 of epoch 2, 69.24 ms/it, loss 0.444853
Finished training it 65536/76743 of epoch 2, 73.46 ms/it, loss 0.445698
Finished training it 65536/76743 of epoch 2, 73.46 ms/it, loss 0.444663
Finished training it 65536/76743 of epoch 2, 73.35 ms/it, loss 0.444156
Finished training it 65536/76743 of epoch 2, 69.34 ms/it, loss 0.444973
Finished training it 66560/76743 of epoch 2, 68.11 ms/it, loss 0.442026
Finished training it 66560/76743 of epoch 2, 68.03 ms/it, loss 0.441504
Finished training it 66560/76743 of epoch 2, 68.08 ms/it, loss 0.447994
Finished training it 66560/76743 of epoch 2, 68.24 ms/it, loss 0.445175
Finished training it 67584/76743 of epoch 2, 68.54 ms/it, loss 0.445392
Finished training it 67584/76743 of epoch 2, 68.64 ms/it, loss 0.448548
Finished training it 67584/76743 of epoch 2, 68.79 ms/it, loss 0.447276
Finished training it 67584/76743 of epoch 2, 68.72 ms/it, loss 0.442650
Finished training it 68608/76743 of epoch 2, 69.07 ms/it, loss 0.443243
Finished training it 68608/76743 of epoch 2, 69.38 ms/it, loss 0.445327
Finished training it 68608/76743 of epoch 2, 69.27 ms/it, loss 0.443707
Finished training it 68608/76743 of epoch 2, 69.14 ms/it, loss 0.442590
Finished training it 69632/76743 of epoch 2, 68.93 ms/it, loss 0.444897
Finished training it 69632/76743 of epoch 2, 68.76 ms/it, loss 0.444488
Finished training it 69632/76743 of epoch 2, 68.68 ms/it, loss 0.442482
Finished training it 69632/76743 of epoch 2, 68.78 ms/it, loss 0.442694
Finished training it 70656/76743 of epoch 2, 68.80 ms/it, loss 0.442940
Finished training it 70656/76743 of epoch 2, 68.76 ms/it, loss 0.444428
Finished training it 70656/76743 of epoch 2, 68.73 ms/it, loss 0.445708
Finished training it 70656/76743 of epoch 2, 68.56 ms/it, loss 0.445554
Finished training it 71680/76743 of epoch 2, 70.75 ms/it, loss 0.445756
Finished training it 71680/76743 of epoch 2, 70.75 ms/it, loss 0.444030
Finished training it 71680/76743 of epoch 2, 70.80 ms/it, loss 0.446411
Finished training it 71680/76743 of epoch 2, 70.94 ms/it, loss 0.444288
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581635.0
get out
0 has test check 2581635.0 and sample count 3274240
 accuracy 78.847 %, best 78.847 %, roc auc score 0.8023, best 0.8023
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581635.0
get out
2 has test check 2581635.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 71.41 ms/it, loss 0.443295
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581635.0
get out
1 has test check 2581635.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 71.63 ms/it, loss 0.444176
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 71.50 ms/it, loss 0.445237
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581635.0
get out
3 has test check 2581635.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 71.31 ms/it, loss 0.443224
Finished training it 73728/76743 of epoch 2, 71.63 ms/it, loss 0.443574
Finished training it 73728/76743 of epoch 2, 71.57 ms/it, loss 0.443507
Finished training it 73728/76743 of epoch 2, 71.52 ms/it, loss 0.446485
Finished training it 73728/76743 of epoch 2, 71.49 ms/it, loss 0.447169
Finished training it 74752/76743 of epoch 2, 71.05 ms/it, loss 0.444663
Finished training it 74752/76743 of epoch 2, 71.16 ms/it, loss 0.445157
Finished training it 74752/76743 of epoch 2, 71.12 ms/it, loss 0.446507
Finished training it 74752/76743 of epoch 2, 71.04 ms/it, loss 0.444345
Finished training it 75776/76743 of epoch 2, 71.50 ms/it, loss 0.445004
Finished training it 75776/76743 of epoch 2, 71.53 ms/it, loss 0.444998
Finished training it 75776/76743 of epoch 2, 71.62 ms/it, loss 0.445403
Finished training it 75776/76743 of epoch 2, 71.55 ms/it, loss 0.445908
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.06 ms/it, loss 0.443511
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.27 ms/it, loss 0.444240
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 71.71 ms/it, loss 0.444359
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.35 ms/it, loss 0.445915
Finished training it 2048/76743 of epoch 3, 71.11 ms/it, loss 0.443060
Finished training it 2048/76743 of epoch 3, 71.13 ms/it, loss 0.444397
Finished training it 2048/76743 of epoch 3, 71.13 ms/it, loss 0.445537
Finished training it 2048/76743 of epoch 3, 71.13 ms/it, loss 0.442461
Finished training it 3072/76743 of epoch 3, 70.68 ms/it, loss 0.444600
Finished training it 3072/76743 of epoch 3, 70.67 ms/it, loss 0.444738
Finished training it 3072/76743 of epoch 3, 70.82 ms/it, loss 0.444229
Finished training it 3072/76743 of epoch 3, 70.91 ms/it, loss 0.442316
Finished training it 4096/76743 of epoch 3, 71.53 ms/it, loss 0.444003
Finished training it 4096/76743 of epoch 3, 71.47 ms/it, loss 0.443372
Finished training it 4096/76743 of epoch 3, 71.43 ms/it, loss 0.444126
Finished training it 4096/76743 of epoch 3, 71.36 ms/it, loss 0.444580
Finished training it 5120/76743 of epoch 3, 70.74 ms/it, loss 0.444495
Finished training it 5120/76743 of epoch 3, 70.83 ms/it, loss 0.445450
Finished training it 5120/76743 of epoch 3, 70.77 ms/it, loss 0.446192
Finished training it 5120/76743 of epoch 3, 70.75 ms/it, loss 0.445390
Finished training it 6144/76743 of epoch 3, 70.78 ms/it, loss 0.445270
Finished training it 6144/76743 of epoch 3, 70.86 ms/it, loss 0.442897
Finished training it 6144/76743 of epoch 3, 71.00 ms/it, loss 0.444005
Finished training it 6144/76743 of epoch 3, 70.80 ms/it, loss 0.442991
Finished training it 7168/76743 of epoch 3, 71.52 ms/it, loss 0.444268
Finished training it 7168/76743 of epoch 3, 71.63 ms/it, loss 0.444491
Finished training it 7168/76743 of epoch 3, 71.51 ms/it, loss 0.443473
Finished training it 7168/76743 of epoch 3, 71.56 ms/it, loss 0.443897
Finished training it 8192/76743 of epoch 3, 71.18 ms/it, loss 0.445730
Finished training it 8192/76743 of epoch 3, 71.16 ms/it, loss 0.445807
Finished training it 8192/76743 of epoch 3, 71.15 ms/it, loss 0.444353
Finished training it 8192/76743 of epoch 3, 71.25 ms/it, loss 0.445289
Finished training it 9216/76743 of epoch 3, 74.81 ms/it, loss 0.442500
Finished training it 9216/76743 of epoch 3, 74.91 ms/it, loss 0.445032
Finished training it 9216/76743 of epoch 3, 74.81 ms/it, loss 0.441083
Finished training it 9216/76743 of epoch 3, 74.82 ms/it, loss 0.444191
Finished training it 10240/76743 of epoch 3, 71.92 ms/it, loss 0.444026
Finished training it 10240/76743 of epoch 3, 73.68 ms/it, loss 0.444708
Finished training it 10240/76743 of epoch 3, 78.72 ms/it, loss 0.447948
Finished training it 10240/76743 of epoch 3, 72.99 ms/it, loss 0.446446
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581799.0
get out
0 has test check 2581799.0 and sample count 3274240
 accuracy 78.852 %, best 78.852 %, roc auc score 0.8023, best 0.8023
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581799.0
get out
1 has test check 2581799.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 70.97 ms/it, loss 0.445214
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 71.14 ms/it, loss 0.444440
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581799.0
get out
2 has test check 2581799.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 71.19 ms/it, loss 0.442508
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581799.0
get out
3 has test check 2581799.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 71.18 ms/it, loss 0.443597
Finished training it 12288/76743 of epoch 3, 71.19 ms/it, loss 0.444098
Finished training it 12288/76743 of epoch 3, 71.27 ms/it, loss 0.445012
Finished training it 12288/76743 of epoch 3, 71.35 ms/it, loss 0.443773
Finished training it 12288/76743 of epoch 3, 71.10 ms/it, loss 0.439391
Finished training it 13312/76743 of epoch 3, 71.93 ms/it, loss 0.442073
Finished training it 13312/76743 of epoch 3, 72.07 ms/it, loss 0.443303
Finished training it 13312/76743 of epoch 3, 72.44 ms/it, loss 0.443840
Finished training it 13312/76743 of epoch 3, 76.64 ms/it, loss 0.444967
Finished training it 14336/76743 of epoch 3, 75.74 ms/it, loss 0.443225
Finished training it 14336/76743 of epoch 3, 75.53 ms/it, loss 0.442794
Finished training it 14336/76743 of epoch 3, 71.30 ms/it, loss 0.444383
Finished training it 14336/76743 of epoch 3, 75.75 ms/it, loss 0.444861
Finished training it 15360/76743 of epoch 3, 71.25 ms/it, loss 0.442982
Finished training it 15360/76743 of epoch 3, 71.12 ms/it, loss 0.441576
Finished training it 15360/76743 of epoch 3, 71.20 ms/it, loss 0.442395
Finished training it 15360/76743 of epoch 3, 71.01 ms/it, loss 0.441584
Finished training it 16384/76743 of epoch 3, 71.47 ms/it, loss 0.441545
Finished training it 16384/76743 of epoch 3, 71.41 ms/it, loss 0.445264
Finished training it 16384/76743 of epoch 3, 71.18 ms/it, loss 0.443949
Finished training it 16384/76743 of epoch 3, 71.52 ms/it, loss 0.444887
Finished training it 17408/76743 of epoch 3, 71.26 ms/it, loss 0.444341
Finished training it 17408/76743 of epoch 3, 71.08 ms/it, loss 0.443988
Finished training it 17408/76743 of epoch 3, 70.92 ms/it, loss 0.444708
Finished training it 17408/76743 of epoch 3, 70.68 ms/it, loss 0.441443
Finished training it 18432/76743 of epoch 3, 71.07 ms/it, loss 0.443934
Finished training it 18432/76743 of epoch 3, 71.14 ms/it, loss 0.442884
Finished training it 18432/76743 of epoch 3, 71.16 ms/it, loss 0.441838
Finished training it 18432/76743 of epoch 3, 71.16 ms/it, loss 0.443621
Finished training it 19456/76743 of epoch 3, 71.52 ms/it, loss 0.445775
Finished training it 19456/76743 of epoch 3, 71.36 ms/it, loss 0.445886
Finished training it 19456/76743 of epoch 3, 71.17 ms/it, loss 0.442145
Finished training it 19456/76743 of epoch 3, 71.53 ms/it, loss 0.442552
Finished training it 20480/76743 of epoch 3, 71.81 ms/it, loss 0.443700
Finished training it 20480/76743 of epoch 3, 71.69 ms/it, loss 0.441899
Finished training it 20480/76743 of epoch 3, 71.38 ms/it, loss 0.442850
Finished training it 20480/76743 of epoch 3, 71.59 ms/it, loss 0.444465
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582365.0
get out
0 has test check 2582365.0 and sample count 3274240
 accuracy 78.869 %, best 78.869 %, roc auc score 0.8025, best 0.8025
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582365.0
get out
1 has test check 2582365.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.02 ms/it, loss 0.444177
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582365.0
get out
3 has test check 2582365.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.23 ms/it, loss 0.445931
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 71.03 ms/it, loss 0.442721
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582365.0
get out
2 has test check 2582365.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.13 ms/it, loss 0.442469
Finished training it 22528/76743 of epoch 3, 71.30 ms/it, loss 0.443357
Finished training it 22528/76743 of epoch 3, 71.24 ms/it, loss 0.442346
Finished training it 22528/76743 of epoch 3, 70.96 ms/it, loss 0.440994
Finished training it 22528/76743 of epoch 3, 71.12 ms/it, loss 0.444084
Finished training it 23552/76743 of epoch 3, 71.52 ms/it, loss 0.443879
Finished training it 23552/76743 of epoch 3, 71.48 ms/it, loss 0.444514
Finished training it 23552/76743 of epoch 3, 71.52 ms/it, loss 0.443005
Finished training it 23552/76743 of epoch 3, 71.26 ms/it, loss 0.443663
Finished training it 24576/76743 of epoch 3, 71.46 ms/it, loss 0.443128
Finished training it 24576/76743 of epoch 3, 71.50 ms/it, loss 0.441961
Finished training it 24576/76743 of epoch 3, 71.39 ms/it, loss 0.444291
Finished training it 24576/76743 of epoch 3, 71.19 ms/it, loss 0.441976
Finished training it 25600/76743 of epoch 3, 71.28 ms/it, loss 0.442248
Finished training it 25600/76743 of epoch 3, 71.30 ms/it, loss 0.443623
Finished training it 25600/76743 of epoch 3, 71.38 ms/it, loss 0.446502
Finished training it 25600/76743 of epoch 3, 71.29 ms/it, loss 0.443839
Finished training it 26624/76743 of epoch 3, 71.14 ms/it, loss 0.442376
Finished training it 26624/76743 of epoch 3, 71.07 ms/it, loss 0.443204
Finished training it 26624/76743 of epoch 3, 71.36 ms/it, loss 0.441790
Finished training it 26624/76743 of epoch 3, 71.15 ms/it, loss 0.444393
Finished training it 27648/76743 of epoch 3, 71.75 ms/it, loss 0.442107
Finished training it 27648/76743 of epoch 3, 71.51 ms/it, loss 0.441552
Finished training it 27648/76743 of epoch 3, 71.78 ms/it, loss 0.441792
Finished training it 27648/76743 of epoch 3, 71.68 ms/it, loss 0.442783
Finished training it 28672/76743 of epoch 3, 70.74 ms/it, loss 0.443149
Finished training it 28672/76743 of epoch 3, 70.82 ms/it, loss 0.442082
Finished training it 28672/76743 of epoch 3, 70.74 ms/it, loss 0.444492
Finished training it 28672/76743 of epoch 3, 70.95 ms/it, loss 0.440847
Finished training it 29696/76743 of epoch 3, 71.14 ms/it, loss 0.445630
Finished training it 29696/76743 of epoch 3, 70.94 ms/it, loss 0.441498
Finished training it 29696/76743 of epoch 3, 70.98 ms/it, loss 0.442049
Finished training it 29696/76743 of epoch 3, 71.13 ms/it, loss 0.442846
Finished training it 30720/76743 of epoch 3, 71.24 ms/it, loss 0.442618
Finished training it 30720/76743 of epoch 3, 71.26 ms/it, loss 0.441240
Finished training it 30720/76743 of epoch 3, 71.45 ms/it, loss 0.444204
Finished training it 30720/76743 of epoch 3, 71.32 ms/it, loss 0.442918
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579707.0
get out
0 has test check 2579707.0 and sample count 3274240
 accuracy 78.788 %, best 78.869 %, roc auc score 0.8029, best 0.8029
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579707.0
get out
1 has test check 2579707.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 71.01 ms/it, loss 0.440931
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579707.0
get out
3 has test check 2579707.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 71.12 ms/it, loss 0.441655
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579707.0
get out
2 has test check 2579707.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 71.09 ms/it, loss 0.443764
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 70.85 ms/it, loss 0.441351
Finished training it 32768/76743 of epoch 3, 71.30 ms/it, loss 0.444220
Finished training it 32768/76743 of epoch 3, 71.37 ms/it, loss 0.442146
Finished training it 32768/76743 of epoch 3, 71.26 ms/it, loss 0.442309
Finished training it 32768/76743 of epoch 3, 71.29 ms/it, loss 0.442648
Finished training it 33792/76743 of epoch 3, 71.09 ms/it, loss 0.440360
Finished training it 33792/76743 of epoch 3, 71.24 ms/it, loss 0.443482
Finished training it 33792/76743 of epoch 3, 71.13 ms/it, loss 0.444034
Finished training it 33792/76743 of epoch 3, 71.07 ms/it, loss 0.445448
Finished training it 34816/76743 of epoch 3, 77.85 ms/it, loss 0.442482
Finished training it 34816/76743 of epoch 3, 72.71 ms/it, loss 0.442075
Finished training it 34816/76743 of epoch 3, 77.94 ms/it, loss 0.440730
Finished training it 34816/76743 of epoch 3, 78.05 ms/it, loss 0.443608
Finished training it 35840/76743 of epoch 3, 72.82 ms/it, loss 0.441187
Finished training it 35840/76743 of epoch 3, 73.95 ms/it, loss 0.443448
Finished training it 35840/76743 of epoch 3, 71.85 ms/it, loss 0.441888
Finished training it 35840/76743 of epoch 3, 78.13 ms/it, loss 0.443051
Finished training it 36864/76743 of epoch 3, 70.83 ms/it, loss 0.442317
Finished training it 36864/76743 of epoch 3, 71.02 ms/it, loss 0.441861
Finished training it 36864/76743 of epoch 3, 71.03 ms/it, loss 0.441766
Finished training it 36864/76743 of epoch 3, 70.97 ms/it, loss 0.442971
Finished training it 37888/76743 of epoch 3, 71.56 ms/it, loss 0.442963
Finished training it 37888/76743 of epoch 3, 71.65 ms/it, loss 0.439988
Finished training it 37888/76743 of epoch 3, 71.61 ms/it, loss 0.444543
Finished training it 37888/76743 of epoch 3, 71.72 ms/it, loss 0.443645
Finished training it 38912/76743 of epoch 3, 71.95 ms/it, loss 0.441445
Finished training it 38912/76743 of epoch 3, 71.88 ms/it, loss 0.440169
Finished training it 38912/76743 of epoch 3, 71.96 ms/it, loss 0.441776
Finished training it 38912/76743 of epoch 3, 71.80 ms/it, loss 0.443095
Finished training it 39936/76743 of epoch 3, 70.75 ms/it, loss 0.442829
Finished training it 39936/76743 of epoch 3, 70.84 ms/it, loss 0.441235
Finished training it 39936/76743 of epoch 3, 70.97 ms/it, loss 0.443100
Finished training it 39936/76743 of epoch 3, 70.95 ms/it, loss 0.444752
Finished training it 40960/76743 of epoch 3, 71.51 ms/it, loss 0.441818
Finished training it 40960/76743 of epoch 3, 71.27 ms/it, loss 0.446193
Finished training it 40960/76743 of epoch 3, 71.25 ms/it, loss 0.442504
Finished training it 40960/76743 of epoch 3, 71.24 ms/it, loss 0.442892
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582710.0
get out
0 has test check 2582710.0 and sample count 3274240
 accuracy 78.880 %, best 78.880 %, roc auc score 0.8029, best 0.8029
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 70.81 ms/it, loss 0.442249
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582710.0
get out
1 has test check 2582710.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 70.93 ms/it, loss 0.442517
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582710.0
get out
3 has test check 2582710.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 70.89 ms/it, loss 0.439945
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582710.0
get out
2 has test check 2582710.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 70.93 ms/it, loss 0.443074
Finished training it 43008/76743 of epoch 3, 71.12 ms/it, loss 0.439830
Finished training it 43008/76743 of epoch 3, 71.08 ms/it, loss 0.441277
Finished training it 43008/76743 of epoch 3, 71.16 ms/it, loss 0.441280
Finished training it 43008/76743 of epoch 3, 71.32 ms/it, loss 0.440080
Finished training it 44032/76743 of epoch 3, 71.54 ms/it, loss 0.444001
Finished training it 44032/76743 of epoch 3, 71.50 ms/it, loss 0.444714
Finished training it 44032/76743 of epoch 3, 71.38 ms/it, loss 0.442608
Finished training it 44032/76743 of epoch 3, 71.51 ms/it, loss 0.442034
Finished training it 45056/76743 of epoch 3, 71.09 ms/it, loss 0.442733
Finished training it 45056/76743 of epoch 3, 71.13 ms/it, loss 0.440642
Finished training it 45056/76743 of epoch 3, 71.09 ms/it, loss 0.442115
Finished training it 45056/76743 of epoch 3, 71.30 ms/it, loss 0.440081
Finished training it 46080/76743 of epoch 3, 71.13 ms/it, loss 0.444172
Finished training it 46080/76743 of epoch 3, 71.13 ms/it, loss 0.441450
Finished training it 46080/76743 of epoch 3, 71.11 ms/it, loss 0.442503
Finished training it 46080/76743 of epoch 3, 71.39 ms/it, loss 0.443546
Finished training it 47104/76743 of epoch 3, 71.15 ms/it, loss 0.444407
Finished training it 47104/76743 of epoch 3, 71.23 ms/it, loss 0.444537
Finished training it 47104/76743 of epoch 3, 71.14 ms/it, loss 0.442606
Finished training it 47104/76743 of epoch 3, 71.35 ms/it, loss 0.442793
Finished training it 48128/76743 of epoch 3, 71.39 ms/it, loss 0.440972
Finished training it 48128/76743 of epoch 3, 71.25 ms/it, loss 0.443697
Finished training it 48128/76743 of epoch 3, 71.29 ms/it, loss 0.441467
Finished training it 48128/76743 of epoch 3, 71.43 ms/it, loss 0.440892
Finished training it 49152/76743 of epoch 3, 71.09 ms/it, loss 0.446077
Finished training it 49152/76743 of epoch 3, 71.13 ms/it, loss 0.443022
Finished training it 49152/76743 of epoch 3, 70.93 ms/it, loss 0.443141
Finished training it 49152/76743 of epoch 3, 71.02 ms/it, loss 0.443047
Finished training it 50176/76743 of epoch 3, 71.42 ms/it, loss 0.441937
Finished training it 50176/76743 of epoch 3, 71.27 ms/it, loss 0.443564
Finished training it 50176/76743 of epoch 3, 71.18 ms/it, loss 0.442532
Finished training it 50176/76743 of epoch 3, 71.22 ms/it, loss 0.440653
Finished training it 51200/76743 of epoch 3, 71.49 ms/it, loss 0.439836
Finished training it 51200/76743 of epoch 3, 71.32 ms/it, loss 0.443745
Finished training it 51200/76743 of epoch 3, 71.23 ms/it, loss 0.444043
Finished training it 51200/76743 of epoch 3, 71.36 ms/it, loss 0.441422
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581858.0
get out
0 has test check 2581858.0 and sample count 3274240
 accuracy 78.854 %, best 78.880 %, roc auc score 0.8026, best 0.8029
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581858.0
get out
3 has test check 2581858.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 70.64 ms/it, loss 0.441593
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581858.0
get out
2 has test check 2581858.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 70.68 ms/it, loss 0.443087
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 70.58 ms/it, loss 0.443609
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581858.0
get out
1 has test check 2581858.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 70.57 ms/it, loss 0.441285
Finished training it 53248/76743 of epoch 3, 70.85 ms/it, loss 0.442704
Finished training it 53248/76743 of epoch 3, 71.10 ms/it, loss 0.441321
Finished training it 53248/76743 of epoch 3, 70.94 ms/it, loss 0.441487
Finished training it 53248/76743 of epoch 3, 71.17 ms/it, loss 0.442911
Finished training it 54272/76743 of epoch 3, 71.57 ms/it, loss 0.442196
Finished training it 54272/76743 of epoch 3, 71.65 ms/it, loss 0.443544
Finished training it 54272/76743 of epoch 3, 71.65 ms/it, loss 0.444697
Finished training it 54272/76743 of epoch 3, 71.65 ms/it, loss 0.442906
Finished training it 55296/76743 of epoch 3, 76.25 ms/it, loss 0.442789
Finished training it 55296/76743 of epoch 3, 76.17 ms/it, loss 0.441570
Finished training it 55296/76743 of epoch 3, 72.37 ms/it, loss 0.442619
Finished training it 55296/76743 of epoch 3, 76.09 ms/it, loss 0.445014
Finished training it 56320/76743 of epoch 3, 71.75 ms/it, loss 0.445479
Finished training it 56320/76743 of epoch 3, 72.43 ms/it, loss 0.443046
Finished training it 56320/76743 of epoch 3, 77.70 ms/it, loss 0.444051
Finished training it 56320/76743 of epoch 3, 73.94 ms/it, loss 0.444917
Finished training it 57344/76743 of epoch 3, 70.83 ms/it, loss 0.441137
Finished training it 57344/76743 of epoch 3, 70.97 ms/it, loss 0.440690
Finished training it 57344/76743 of epoch 3, 70.77 ms/it, loss 0.445529
Finished training it 57344/76743 of epoch 3, 70.95 ms/it, loss 0.442303
Finished training it 58368/76743 of epoch 3, 72.27 ms/it, loss 0.442154
Finished training it 58368/76743 of epoch 3, 71.94 ms/it, loss 0.441848
Finished training it 58368/76743 of epoch 3, 72.00 ms/it, loss 0.445059
Finished training it 58368/76743 of epoch 3, 71.95 ms/it, loss 0.440496
Finished training it 59392/76743 of epoch 3, 71.63 ms/it, loss 0.443628
Finished training it 59392/76743 of epoch 3, 71.85 ms/it, loss 0.441593
Finished training it 59392/76743 of epoch 3, 71.74 ms/it, loss 0.441079
Finished training it 59392/76743 of epoch 3, 71.83 ms/it, loss 0.442243
Finished training it 60416/76743 of epoch 3, 70.96 ms/it, loss 0.442320
Finished training it 60416/76743 of epoch 3, 70.95 ms/it, loss 0.441877
Finished training it 60416/76743 of epoch 3, 70.99 ms/it, loss 0.441296
Finished training it 60416/76743 of epoch 3, 70.92 ms/it, loss 0.443650
Finished training it 61440/76743 of epoch 3, 71.04 ms/it, loss 0.441522
Finished training it 61440/76743 of epoch 3, 70.85 ms/it, loss 0.444019
Finished training it 61440/76743 of epoch 3, 70.88 ms/it, loss 0.442873
Finished training it 61440/76743 of epoch 3, 70.78 ms/it, loss 0.442760
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582138.0
get out
0 has test check 2582138.0 and sample count 3274240
 accuracy 78.862 %, best 78.880 %, roc auc score 0.8036, best 0.8036
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 71.00 ms/it, loss 0.442799
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582138.0
get out
2 has test check 2582138.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 71.18 ms/it, loss 0.439759
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582138.0
get out
3 has test check 2582138.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 70.99 ms/it, loss 0.442175
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582138.0
get out
1 has test check 2582138.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 70.97 ms/it, loss 0.442260
Finished training it 63488/76743 of epoch 3, 71.20 ms/it, loss 0.442995
Finished training it 63488/76743 of epoch 3, 71.17 ms/it, loss 0.442529
Finished training it 63488/76743 of epoch 3, 71.12 ms/it, loss 0.441592
Finished training it 63488/76743 of epoch 3, 71.24 ms/it, loss 0.443870
Finished training it 64512/76743 of epoch 3, 70.81 ms/it, loss 0.440895
Finished training it 64512/76743 of epoch 3, 70.74 ms/it, loss 0.442523
Finished training it 64512/76743 of epoch 3, 70.58 ms/it, loss 0.444709
Finished training it 64512/76743 of epoch 3, 70.70 ms/it, loss 0.444041
Finished training it 65536/76743 of epoch 3, 70.83 ms/it, loss 0.443043
Finished training it 65536/76743 of epoch 3, 70.78 ms/it, loss 0.442108
Finished training it 65536/76743 of epoch 3, 71.01 ms/it, loss 0.442634
Finished training it 65536/76743 of epoch 3, 71.09 ms/it, loss 0.441945
Finished training it 66560/76743 of epoch 3, 71.36 ms/it, loss 0.439747
Finished training it 66560/76743 of epoch 3, 71.14 ms/it, loss 0.442532
Finished training it 66560/76743 of epoch 3, 71.52 ms/it, loss 0.439139
Finished training it 66560/76743 of epoch 3, 71.47 ms/it, loss 0.445617
Finished training it 67584/76743 of epoch 3, 71.66 ms/it, loss 0.445980
Finished training it 67584/76743 of epoch 3, 71.66 ms/it, loss 0.444337
Finished training it 67584/76743 of epoch 3, 71.67 ms/it, loss 0.442755
Finished training it 67584/76743 of epoch 3, 71.78 ms/it, loss 0.440122
Finished training it 68608/76743 of epoch 3, 71.05 ms/it, loss 0.440728
Finished training it 68608/76743 of epoch 3, 71.13 ms/it, loss 0.442912
Finished training it 68608/76743 of epoch 3, 71.20 ms/it, loss 0.440053
Finished training it 68608/76743 of epoch 3, 71.23 ms/it, loss 0.441131
Finished training it 69632/76743 of epoch 3, 71.32 ms/it, loss 0.441927
Finished training it 69632/76743 of epoch 3, 71.40 ms/it, loss 0.439706
Finished training it 69632/76743 of epoch 3, 71.14 ms/it, loss 0.442442
Finished training it 69632/76743 of epoch 3, 71.10 ms/it, loss 0.439839
Finished training it 70656/76743 of epoch 3, 71.19 ms/it, loss 0.443063
Finished training it 70656/76743 of epoch 3, 71.00 ms/it, loss 0.439976
Finished training it 70656/76743 of epoch 3, 70.89 ms/it, loss 0.443078
Finished training it 70656/76743 of epoch 3, 71.15 ms/it, loss 0.441884
Finished training it 71680/76743 of epoch 3, 71.01 ms/it, loss 0.443416
Finished training it 71680/76743 of epoch 3, 71.09 ms/it, loss 0.443726
Finished training it 71680/76743 of epoch 3, 70.97 ms/it, loss 0.441361
Finished training it 71680/76743 of epoch 3, 71.04 ms/it, loss 0.442209
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584089.0
get out
0 has test check 2584089.0 and sample count 3274240
 accuracy 78.922 %, best 78.922 %, roc auc score 0.8035, best 0.8036
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584089.0
get out
3 has test check 2584089.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 71.46 ms/it, loss 0.440711
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 71.04 ms/it, loss 0.442753
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584089.0
get out
2 has test check 2584089.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 71.20 ms/it, loss 0.440937
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584089.0
get out
1 has test check 2584089.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 71.30 ms/it, loss 0.442245
Finished training it 73728/76743 of epoch 3, 71.45 ms/it, loss 0.441007
Finished training it 73728/76743 of epoch 3, 71.55 ms/it, loss 0.444681
Finished training it 73728/76743 of epoch 3, 71.48 ms/it, loss 0.441112
Finished training it 73728/76743 of epoch 3, 71.35 ms/it, loss 0.444079
Finished training it 74752/76743 of epoch 3, 71.00 ms/it, loss 0.441772
Finished training it 74752/76743 of epoch 3, 71.28 ms/it, loss 0.442101
Finished training it 74752/76743 of epoch 3, 71.31 ms/it, loss 0.443885
Finished training it 74752/76743 of epoch 3, 70.98 ms/it, loss 0.442807
Finished training it 75776/76743 of epoch 3, 71.09 ms/it, loss 0.442171
Finished training it 75776/76743 of epoch 3, 71.33 ms/it, loss 0.442792
Finished training it 75776/76743 of epoch 3, 71.35 ms/it, loss 0.443385
Finished training it 75776/76743 of epoch 3, 71.47 ms/it, loss 0.442434
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 74.50 ms/it, loss 0.443310
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 74.49 ms/it, loss 0.441733
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 75.02 ms/it, loss 0.440898
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 74.63 ms/it, loss 0.442130
Finished training it 2048/76743 of epoch 4, 71.38 ms/it, loss 0.440541
Finished training it 2048/76743 of epoch 4, 71.37 ms/it, loss 0.443336
Finished training it 2048/76743 of epoch 4, 71.45 ms/it, loss 0.440107
Finished training it 2048/76743 of epoch 4, 71.29 ms/it, loss 0.442131
Finished training it 3072/76743 of epoch 4, 71.30 ms/it, loss 0.441978
Finished training it 3072/76743 of epoch 4, 71.43 ms/it, loss 0.441798
Finished training it 3072/76743 of epoch 4, 71.31 ms/it, loss 0.440261
Finished training it 3072/76743 of epoch 4, 71.48 ms/it, loss 0.442193
Finished training it 4096/76743 of epoch 4, 71.02 ms/it, loss 0.441123
Finished training it 4096/76743 of epoch 4, 70.94 ms/it, loss 0.441627
Finished training it 4096/76743 of epoch 4, 70.95 ms/it, loss 0.441947
Finished training it 4096/76743 of epoch 4, 71.03 ms/it, loss 0.441481
Finished training it 5120/76743 of epoch 4, 71.68 ms/it, loss 0.441859
Finished training it 5120/76743 of epoch 4, 71.71 ms/it, loss 0.443716
Finished training it 5120/76743 of epoch 4, 71.74 ms/it, loss 0.442958
Finished training it 5120/76743 of epoch 4, 71.70 ms/it, loss 0.442608
Finished training it 6144/76743 of epoch 4, 70.97 ms/it, loss 0.441652
Finished training it 6144/76743 of epoch 4, 70.81 ms/it, loss 0.440477
Finished training it 6144/76743 of epoch 4, 70.68 ms/it, loss 0.443126
Finished training it 6144/76743 of epoch 4, 70.89 ms/it, loss 0.440293
Finished training it 7168/76743 of epoch 4, 71.81 ms/it, loss 0.440623
Finished training it 7168/76743 of epoch 4, 71.87 ms/it, loss 0.441600
Finished training it 7168/76743 of epoch 4, 71.71 ms/it, loss 0.441977
Finished training it 7168/76743 of epoch 4, 71.84 ms/it, loss 0.441247
Finished training it 8192/76743 of epoch 4, 71.24 ms/it, loss 0.443234
Finished training it 8192/76743 of epoch 4, 71.07 ms/it, loss 0.443331
Finished training it 8192/76743 of epoch 4, 71.28 ms/it, loss 0.442021
Finished training it 8192/76743 of epoch 4, 71.29 ms/it, loss 0.443105
Finished training it 9216/76743 of epoch 4, 71.10 ms/it, loss 0.440107
Finished training it 9216/76743 of epoch 4, 71.40 ms/it, loss 0.443005
Finished training it 9216/76743 of epoch 4, 71.18 ms/it, loss 0.439062
Finished training it 9216/76743 of epoch 4, 71.11 ms/it, loss 0.442056
Finished training it 10240/76743 of epoch 4, 71.43 ms/it, loss 0.445388
Finished training it 10240/76743 of epoch 4, 71.50 ms/it, loss 0.441568
Finished training it 10240/76743 of epoch 4, 71.64 ms/it, loss 0.442200
Finished training it 10240/76743 of epoch 4, 71.43 ms/it, loss 0.444064
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583386.0
get out
0 has test check 2583386.0 and sample count 3274240
 accuracy 78.900 %, best 78.922 %, roc auc score 0.8036, best 0.8036
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583386.0
get out
1 has test check 2583386.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 71.46 ms/it, loss 0.442876
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 71.40 ms/it, loss 0.442026
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583386.0
get out
2 has test check 2583386.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 71.59 ms/it, loss 0.440026
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583386.0
get out
3 has test check 2583386.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 71.41 ms/it, loss 0.441065
Finished training it 12288/76743 of epoch 4, 71.44 ms/it, loss 0.437287
Finished training it 12288/76743 of epoch 4, 71.50 ms/it, loss 0.441216
Finished training it 12288/76743 of epoch 4, 71.37 ms/it, loss 0.441453
Finished training it 12288/76743 of epoch 4, 71.47 ms/it, loss 0.442534
Finished training it 13312/76743 of epoch 4, 70.77 ms/it, loss 0.442280
Finished training it 13312/76743 of epoch 4, 70.78 ms/it, loss 0.441608
Finished training it 13312/76743 of epoch 4, 70.79 ms/it, loss 0.440732
Finished training it 13312/76743 of epoch 4, 70.86 ms/it, loss 0.439336
Finished training it 14336/76743 of epoch 4, 71.26 ms/it, loss 0.440077
Finished training it 14336/76743 of epoch 4, 71.25 ms/it, loss 0.442096
Finished training it 14336/76743 of epoch 4, 71.23 ms/it, loss 0.442415
Finished training it 14336/76743 of epoch 4, 71.46 ms/it, loss 0.440825
Finished training it 15360/76743 of epoch 4, 71.85 ms/it, loss 0.438960
Finished training it 15360/76743 of epoch 4, 71.86 ms/it, loss 0.440517
Finished training it 15360/76743 of epoch 4, 71.69 ms/it, loss 0.439130
Finished training it 15360/76743 of epoch 4, 71.95 ms/it, loss 0.440138
Finished training it 16384/76743 of epoch 4, 71.57 ms/it, loss 0.443125
Finished training it 16384/76743 of epoch 4, 71.30 ms/it, loss 0.441418
Finished training it 16384/76743 of epoch 4, 71.43 ms/it, loss 0.438862
Finished training it 16384/76743 of epoch 4, 71.23 ms/it, loss 0.442534
Finished training it 17408/76743 of epoch 4, 71.22 ms/it, loss 0.442003
Finished training it 17408/76743 of epoch 4, 71.21 ms/it, loss 0.438923
Finished training it 17408/76743 of epoch 4, 71.14 ms/it, loss 0.442547
Finished training it 17408/76743 of epoch 4, 71.31 ms/it, loss 0.441865
Finished training it 18432/76743 of epoch 4, 71.31 ms/it, loss 0.439452
Finished training it 18432/76743 of epoch 4, 71.50 ms/it, loss 0.441060
Finished training it 18432/76743 of epoch 4, 71.28 ms/it, loss 0.440508
Finished training it 18432/76743 of epoch 4, 71.58 ms/it, loss 0.441961
Finished training it 19456/76743 of epoch 4, 71.39 ms/it, loss 0.439633
Finished training it 19456/76743 of epoch 4, 71.29 ms/it, loss 0.440062
Finished training it 19456/76743 of epoch 4, 71.47 ms/it, loss 0.443673
Finished training it 19456/76743 of epoch 4, 71.40 ms/it, loss 0.443346
Finished training it 20480/76743 of epoch 4, 71.71 ms/it, loss 0.440542
Finished training it 20480/76743 of epoch 4, 71.56 ms/it, loss 0.439353
Finished training it 20480/76743 of epoch 4, 71.58 ms/it, loss 0.442463
Finished training it 20480/76743 of epoch 4, 71.72 ms/it, loss 0.440825
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583676.0
get out
0 has test check 2583676.0 and sample count 3274240
 accuracy 78.909 %, best 78.922 %, roc auc score 0.8035, best 0.8036
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583676.0
get out
1 has test check 2583676.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 71.35 ms/it, loss 0.442031
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 71.20 ms/it, loss 0.440487
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583676.0
get out
2 has test check 2583676.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 71.31 ms/it, loss 0.440103
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583676.0
get out
3 has test check 2583676.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 71.49 ms/it, loss 0.443350
Finished training it 22528/76743 of epoch 4, 71.09 ms/it, loss 0.440888
Finished training it 22528/76743 of epoch 4, 71.20 ms/it, loss 0.439989
Finished training it 22528/76743 of epoch 4, 71.32 ms/it, loss 0.441855
Finished training it 22528/76743 of epoch 4, 71.43 ms/it, loss 0.438808
Finished training it 23552/76743 of epoch 4, 70.83 ms/it, loss 0.441392
Finished training it 23552/76743 of epoch 4, 70.98 ms/it, loss 0.440528
Finished training it 23552/76743 of epoch 4, 70.83 ms/it, loss 0.441873
Finished training it 23552/76743 of epoch 4, 71.09 ms/it, loss 0.442006
Finished training it 24576/76743 of epoch 4, 77.41 ms/it, loss 0.439624
Finished training it 24576/76743 of epoch 4, 77.17 ms/it, loss 0.440690
Finished training it 24576/76743 of epoch 4, 77.69 ms/it, loss 0.441997
Finished training it 24576/76743 of epoch 4, 76.60 ms/it, loss 0.439436
Finished training it 25600/76743 of epoch 4, 71.24 ms/it, loss 0.444164
Finished training it 25600/76743 of epoch 4, 71.40 ms/it, loss 0.441400
Finished training it 25600/76743 of epoch 4, 71.42 ms/it, loss 0.439879
Finished training it 25600/76743 of epoch 4, 71.29 ms/it, loss 0.441307
Finished training it 26624/76743 of epoch 4, 71.56 ms/it, loss 0.440863
Finished training it 26624/76743 of epoch 4, 71.19 ms/it, loss 0.439513
Finished training it 26624/76743 of epoch 4, 71.46 ms/it, loss 0.442111
Finished training it 26624/76743 of epoch 4, 71.44 ms/it, loss 0.440225
Finished training it 27648/76743 of epoch 4, 71.51 ms/it, loss 0.439177
Finished training it 27648/76743 of epoch 4, 71.33 ms/it, loss 0.439601
Finished training it 27648/76743 of epoch 4, 71.39 ms/it, loss 0.440589
Finished training it 27648/76743 of epoch 4, 71.17 ms/it, loss 0.439991
Finished training it 28672/76743 of epoch 4, 71.24 ms/it, loss 0.441122
Finished training it 28672/76743 of epoch 4, 71.17 ms/it, loss 0.442276
Finished training it 28672/76743 of epoch 4, 71.37 ms/it, loss 0.438793
Finished training it 28672/76743 of epoch 4, 71.13 ms/it, loss 0.439885
Finished training it 29696/76743 of epoch 4, 71.82 ms/it, loss 0.439188
Finished training it 29696/76743 of epoch 4, 71.79 ms/it, loss 0.440533
Finished training it 29696/76743 of epoch 4, 71.78 ms/it, loss 0.439815
Finished training it 29696/76743 of epoch 4, 71.65 ms/it, loss 0.443267
Finished training it 30720/76743 of epoch 4, 71.39 ms/it, loss 0.440192
Finished training it 30720/76743 of epoch 4, 71.44 ms/it, loss 0.440285
Finished training it 30720/76743 of epoch 4, 71.34 ms/it, loss 0.441630
Finished training it 30720/76743 of epoch 4, 71.44 ms/it, loss 0.438894
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580777.0
get out
0 has test check 2580777.0 and sample count 3274240
 accuracy 78.821 %, best 78.922 %, roc auc score 0.8039, best 0.8039
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580777.0
get out
2 has test check 2580777.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 71.23 ms/it, loss 0.441412
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580777.0
get out
1 has test check 2580777.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 71.31 ms/it, loss 0.438659
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580777.0
get out
3 has test check 2580777.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 71.33 ms/it, loss 0.439150
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 71.27 ms/it, loss 0.439111
Finished training it 32768/76743 of epoch 4, 71.41 ms/it, loss 0.439798
Finished training it 32768/76743 of epoch 4, 71.28 ms/it, loss 0.439886
Finished training it 32768/76743 of epoch 4, 71.43 ms/it, loss 0.440366
Finished training it 32768/76743 of epoch 4, 71.49 ms/it, loss 0.442086
Finished training it 33792/76743 of epoch 4, 71.31 ms/it, loss 0.443142
Finished training it 33792/76743 of epoch 4, 71.17 ms/it, loss 0.437910
Finished training it 33792/76743 of epoch 4, 71.11 ms/it, loss 0.441009
Finished training it 33792/76743 of epoch 4, 71.30 ms/it, loss 0.441340
Finished training it 34816/76743 of epoch 4, 71.36 ms/it, loss 0.440001
Finished training it 34816/76743 of epoch 4, 71.32 ms/it, loss 0.439943
Finished training it 34816/76743 of epoch 4, 71.24 ms/it, loss 0.441264
Finished training it 34816/76743 of epoch 4, 71.24 ms/it, loss 0.438471
Finished training it 35840/76743 of epoch 4, 70.88 ms/it, loss 0.439149
Finished training it 35840/76743 of epoch 4, 70.75 ms/it, loss 0.440761
Finished training it 35840/76743 of epoch 4, 71.00 ms/it, loss 0.439700
Finished training it 35840/76743 of epoch 4, 70.91 ms/it, loss 0.441295
Finished training it 36864/76743 of epoch 4, 71.28 ms/it, loss 0.440089
Finished training it 36864/76743 of epoch 4, 71.24 ms/it, loss 0.439529
Finished training it 36864/76743 of epoch 4, 71.37 ms/it, loss 0.439353
Finished training it 36864/76743 of epoch 4, 71.43 ms/it, loss 0.440540
Finished training it 37888/76743 of epoch 4, 71.75 ms/it, loss 0.440413
Finished training it 37888/76743 of epoch 4, 71.76 ms/it, loss 0.442136
Finished training it 37888/76743 of epoch 4, 72.03 ms/it, loss 0.441261
Finished training it 37888/76743 of epoch 4, 71.81 ms/it, loss 0.437688
Finished training it 38912/76743 of epoch 4, 71.25 ms/it, loss 0.439205
Finished training it 38912/76743 of epoch 4, 71.08 ms/it, loss 0.437923
Finished training it 38912/76743 of epoch 4, 71.31 ms/it, loss 0.441001
Finished training it 38912/76743 of epoch 4, 71.22 ms/it, loss 0.439159
Finished training it 39936/76743 of epoch 4, 71.12 ms/it, loss 0.438755
Finished training it 39936/76743 of epoch 4, 71.16 ms/it, loss 0.440688
Finished training it 39936/76743 of epoch 4, 71.12 ms/it, loss 0.440869
Finished training it 39936/76743 of epoch 4, 71.26 ms/it, loss 0.442871
Finished training it 40960/76743 of epoch 4, 71.28 ms/it, loss 0.440377
Finished training it 40960/76743 of epoch 4, 71.14 ms/it, loss 0.439625
Finished training it 40960/76743 of epoch 4, 71.17 ms/it, loss 0.443988
Finished training it 40960/76743 of epoch 4, 71.33 ms/it, loss 0.440729
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583886.0
get out
0 has test check 2583886.0 and sample count 3274240
 accuracy 78.916 %, best 78.922 %, roc auc score 0.8037, best 0.8039
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583886.0
get out
1 has test check 2583886.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 71.35 ms/it, loss 0.440229
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583886.0
get out
2 has test check 2583886.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 71.27 ms/it, loss 0.441032
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583886.0
get out
3 has test check 2583886.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 71.22 ms/it, loss 0.437844
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 71.14 ms/it, loss 0.440020
Finished training it 43008/76743 of epoch 4, 71.34 ms/it, loss 0.437753
Finished training it 43008/76743 of epoch 4, 71.34 ms/it, loss 0.437994
Finished training it 43008/76743 of epoch 4, 71.17 ms/it, loss 0.439219
Finished training it 43008/76743 of epoch 4, 71.18 ms/it, loss 0.439154
Finished training it 44032/76743 of epoch 4, 71.31 ms/it, loss 0.442133
Finished training it 44032/76743 of epoch 4, 71.41 ms/it, loss 0.440439
Finished training it 44032/76743 of epoch 4, 71.27 ms/it, loss 0.441754
Finished training it 44032/76743 of epoch 4, 71.21 ms/it, loss 0.439656
Finished training it 45056/76743 of epoch 4, 71.57 ms/it, loss 0.438350
Finished training it 45056/76743 of epoch 4, 71.66 ms/it, loss 0.440337
Finished training it 45056/76743 of epoch 4, 71.60 ms/it, loss 0.439854
Finished training it 45056/76743 of epoch 4, 71.45 ms/it, loss 0.437817
Finished training it 46080/76743 of epoch 4, 77.46 ms/it, loss 0.442107
Finished training it 46080/76743 of epoch 4, 77.45 ms/it, loss 0.440237
Finished training it 46080/76743 of epoch 4, 76.87 ms/it, loss 0.441273
Finished training it 46080/76743 of epoch 4, 76.91 ms/it, loss 0.439306
Finished training it 47104/76743 of epoch 4, 71.32 ms/it, loss 0.442355
Finished training it 47104/76743 of epoch 4, 71.39 ms/it, loss 0.440297
Finished training it 47104/76743 of epoch 4, 71.64 ms/it, loss 0.442134
Finished training it 47104/76743 of epoch 4, 71.45 ms/it, loss 0.440643
Finished training it 48128/76743 of epoch 4, 71.52 ms/it, loss 0.438859
Finished training it 48128/76743 of epoch 4, 71.59 ms/it, loss 0.438450
Finished training it 48128/76743 of epoch 4, 71.69 ms/it, loss 0.439232
Finished training it 48128/76743 of epoch 4, 71.39 ms/it, loss 0.441364
Finished training it 49152/76743 of epoch 4, 71.40 ms/it, loss 0.440684
Finished training it 49152/76743 of epoch 4, 71.06 ms/it, loss 0.440639
Finished training it 49152/76743 of epoch 4, 71.05 ms/it, loss 0.443463
Finished training it 49152/76743 of epoch 4, 71.13 ms/it, loss 0.440864
Finished training it 50176/76743 of epoch 4, 70.79 ms/it, loss 0.440628
Finished training it 50176/76743 of epoch 4, 70.80 ms/it, loss 0.441221
Finished training it 50176/76743 of epoch 4, 70.68 ms/it, loss 0.439660
Finished training it 50176/76743 of epoch 4, 70.88 ms/it, loss 0.438313
Finished training it 51200/76743 of epoch 4, 71.02 ms/it, loss 0.439491
Finished training it 51200/76743 of epoch 4, 70.99 ms/it, loss 0.441707
Finished training it 51200/76743 of epoch 4, 70.90 ms/it, loss 0.441656
Finished training it 51200/76743 of epoch 4, 70.91 ms/it, loss 0.437861
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583552.0
get out
0 has test check 2583552.0 and sample count 3274240
 accuracy 78.905 %, best 78.922 %, roc auc score 0.8035, best 0.8039
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583552.0
get out
3 has test check 2583552.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 71.00 ms/it, loss 0.439690
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 70.81 ms/it, loss 0.441421
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583552.0
get out
2 has test check 2583552.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 70.85 ms/it, loss 0.440968
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583552.0
get out
1 has test check 2583552.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 70.83 ms/it, loss 0.439197
Finished training it 53248/76743 of epoch 4, 71.06 ms/it, loss 0.439146
Finished training it 53248/76743 of epoch 4, 71.17 ms/it, loss 0.440907
Finished training it 53248/76743 of epoch 4, 71.20 ms/it, loss 0.439449
Finished training it 53248/76743 of epoch 4, 71.22 ms/it, loss 0.440362
Finished training it 54272/76743 of epoch 4, 70.80 ms/it, loss 0.440781
Finished training it 54272/76743 of epoch 4, 71.23 ms/it, loss 0.441517
Finished training it 54272/76743 of epoch 4, 71.10 ms/it, loss 0.440064
Finished training it 54272/76743 of epoch 4, 71.11 ms/it, loss 0.442529
Finished training it 55296/76743 of epoch 4, 71.36 ms/it, loss 0.440504
Finished training it 55296/76743 of epoch 4, 71.31 ms/it, loss 0.439007
Finished training it 55296/76743 of epoch 4, 71.11 ms/it, loss 0.442936
Finished training it 55296/76743 of epoch 4, 71.49 ms/it, loss 0.440369
Finished training it 56320/76743 of epoch 4, 71.51 ms/it, loss 0.440862
Finished training it 56320/76743 of epoch 4, 71.55 ms/it, loss 0.443040
Finished training it 56320/76743 of epoch 4, 71.57 ms/it, loss 0.443461
Finished training it 56320/76743 of epoch 4, 71.39 ms/it, loss 0.441374
Finished training it 57344/76743 of epoch 4, 71.54 ms/it, loss 0.438369
Finished training it 57344/76743 of epoch 4, 71.56 ms/it, loss 0.443216
Finished training it 57344/76743 of epoch 4, 71.26 ms/it, loss 0.438840
Finished training it 57344/76743 of epoch 4, 71.57 ms/it, loss 0.440341
Finished training it 58368/76743 of epoch 4, 70.82 ms/it, loss 0.439862
Finished training it 58368/76743 of epoch 4, 70.98 ms/it, loss 0.439725
Finished training it 58368/76743 of epoch 4, 70.97 ms/it, loss 0.442881
Finished training it 58368/76743 of epoch 4, 70.82 ms/it, loss 0.438416
Finished training it 59392/76743 of epoch 4, 71.14 ms/it, loss 0.441350
Finished training it 59392/76743 of epoch 4, 71.18 ms/it, loss 0.439634
Finished training it 59392/76743 of epoch 4, 71.31 ms/it, loss 0.438949
Finished training it 59392/76743 of epoch 4, 71.22 ms/it, loss 0.439442
Finished training it 60416/76743 of epoch 4, 70.86 ms/it, loss 0.439028
Finished training it 60416/76743 of epoch 4, 70.73 ms/it, loss 0.440099
Finished training it 60416/76743 of epoch 4, 70.88 ms/it, loss 0.441556
Finished training it 60416/76743 of epoch 4, 70.92 ms/it, loss 0.439459
Finished training it 61440/76743 of epoch 4, 71.75 ms/it, loss 0.441534
Finished training it 61440/76743 of epoch 4, 71.87 ms/it, loss 0.439125
Finished training it 61440/76743 of epoch 4, 71.89 ms/it, loss 0.440688
Finished training it 61440/76743 of epoch 4, 71.90 ms/it, loss 0.440565
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582519.0
get out
0 has test check 2582519.0 and sample count 3274240
 accuracy 78.874 %, best 78.922 %, roc auc score 0.8040, best 0.8040
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582519.0
get out
2 has test check 2582519.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 70.90 ms/it, loss 0.437476
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582519.0
get out
1 has test check 2582519.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 71.08 ms/it, loss 0.439844
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 70.97 ms/it, loss 0.440572
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582519.0
get out
3 has test check 2582519.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 71.00 ms/it, loss 0.440059
Finished training it 63488/76743 of epoch 4, 71.39 ms/it, loss 0.441414
Finished training it 63488/76743 of epoch 4, 71.49 ms/it, loss 0.440321
Finished training it 63488/76743 of epoch 4, 71.42 ms/it, loss 0.439553
Finished training it 63488/76743 of epoch 4, 71.53 ms/it, loss 0.441036
Finished training it 64512/76743 of epoch 4, 71.21 ms/it, loss 0.440337
Finished training it 64512/76743 of epoch 4, 71.32 ms/it, loss 0.438767
Finished training it 64512/76743 of epoch 4, 71.06 ms/it, loss 0.442230
Finished training it 64512/76743 of epoch 4, 70.93 ms/it, loss 0.441879
Finished training it 65536/76743 of epoch 4, 71.46 ms/it, loss 0.440664
Finished training it 65536/76743 of epoch 4, 71.60 ms/it, loss 0.439731
Finished training it 65536/76743 of epoch 4, 71.62 ms/it, loss 0.440358
Finished training it 65536/76743 of epoch 4, 71.66 ms/it, loss 0.439893
Finished training it 66560/76743 of epoch 4, 70.86 ms/it, loss 0.437219
Finished training it 66560/76743 of epoch 4, 70.94 ms/it, loss 0.440463
Finished training it 66560/76743 of epoch 4, 71.04 ms/it, loss 0.437284
Finished training it 66560/76743 of epoch 4, 71.03 ms/it, loss 0.443457
Finished training it 67584/76743 of epoch 4, 76.86 ms/it, loss 0.444013
Finished training it 67584/76743 of epoch 4, 71.83 ms/it, loss 0.438105
Finished training it 67584/76743 of epoch 4, 72.70 ms/it, loss 0.442509
Finished training it 67584/76743 of epoch 4, 72.48 ms/it, loss 0.440741
Finished training it 68608/76743 of epoch 4, 71.42 ms/it, loss 0.440539
Finished training it 68608/76743 of epoch 4, 71.29 ms/it, loss 0.438050
Finished training it 68608/76743 of epoch 4, 71.24 ms/it, loss 0.438992
Finished training it 68608/76743 of epoch 4, 71.23 ms/it, loss 0.438487
Finished training it 69632/76743 of epoch 4, 71.43 ms/it, loss 0.438263
Finished training it 69632/76743 of epoch 4, 71.23 ms/it, loss 0.438060
Finished training it 69632/76743 of epoch 4, 71.33 ms/it, loss 0.440278
Finished training it 69632/76743 of epoch 4, 71.39 ms/it, loss 0.439982
Finished training it 70656/76743 of epoch 4, 71.44 ms/it, loss 0.441173
Finished training it 70656/76743 of epoch 4, 71.26 ms/it, loss 0.437803
Finished training it 70656/76743 of epoch 4, 71.29 ms/it, loss 0.439799
Finished training it 70656/76743 of epoch 4, 71.48 ms/it, loss 0.441073
Finished training it 71680/76743 of epoch 4, 71.11 ms/it, loss 0.441959
Finished training it 71680/76743 of epoch 4, 71.11 ms/it, loss 0.439411
Finished training it 71680/76743 of epoch 4, 70.95 ms/it, loss 0.441273
Finished training it 71680/76743 of epoch 4, 70.98 ms/it, loss 0.440368
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584543.0
get out
0 has test check 2584543.0 and sample count 3274240
 accuracy 78.936 %, best 78.936 %, roc auc score 0.8040, best 0.8040
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584543.0
get out
1 has test check 2584543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 71.50 ms/it, loss 0.440317
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584543.0
get out
3 has test check 2584543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 71.46 ms/it, loss 0.438595
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584543.0
get out
2 has test check 2584543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 71.11 ms/it, loss 0.438797
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 71.35 ms/it, loss 0.440449
Finished training it 73728/76743 of epoch 4, 71.19 ms/it, loss 0.439012
Finished training it 73728/76743 of epoch 4, 70.99 ms/it, loss 0.438849
Finished training it 73728/76743 of epoch 4, 70.92 ms/it, loss 0.442176
Finished training it 73728/76743 of epoch 4, 70.98 ms/it, loss 0.442478
Finished training it 74752/76743 of epoch 4, 71.91 ms/it, loss 0.440010
Finished training it 74752/76743 of epoch 4, 72.12 ms/it, loss 0.441738
Finished training it 74752/76743 of epoch 4, 71.85 ms/it, loss 0.440383
Finished training it 74752/76743 of epoch 4, 71.81 ms/it, loss 0.439892
Finished training it 75776/76743 of epoch 4, 76.40 ms/it, loss 0.440214
Finished training it 75776/76743 of epoch 4, 76.60 ms/it, loss 0.441455
Finished training it 75776/76743 of epoch 4, 71.72 ms/it, loss 0.440279
Finished training it 75776/76743 of epoch 4, 76.52 ms/it, loss 0.440582
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584471.0
get out
0 has test check 2584471.0 and sample count 3274240
 accuracy 78.933 %, best 78.936 %, roc auc score 0.8040, best 0.8040
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584471.0
get out
2 has test check 2584471.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584471.0
get out
3 has test check 2584471.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584471.0
get out
1 has test check 2584471.0 and sample count 3274240
