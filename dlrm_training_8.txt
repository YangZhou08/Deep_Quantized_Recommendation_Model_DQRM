Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 8
---------- Embedding Table 1, quantization used, quantization bit set to 8
---------- Embedding Table 2, quantization used, quantization bit set to 8
---------- Embedding Table 3, quantization used, quantization bit set to 8
---------- Embedding Table 4, quantization used, quantization bit set to 8
---------- Embedding Table 5, quantization used, quantization bit set to 8
---------- Embedding Table 6, quantization used, quantization bit set to 8
---------- Embedding Table 7, quantization used, quantization bit set to 8
---------- Embedding Table 8, quantization used, quantization bit set to 8
---------- Embedding Table 9, quantization used, quantization bit set to 8
---------- Embedding Table 10, quantization used, quantization bit set to 8
---------- Embedding Table 11, quantization used, quantization bit set to 8
---------- Embedding Table 12, quantization used, quantization bit set to 8
---------- Embedding Table 13, quantization used, quantization bit set to 8
---------- Embedding Table 14, quantization used, quantization bit set to 8
---------- Embedding Table 15, quantization used, quantization bit set to 8
---------- Embedding Table 16, quantization used, quantization bit set to 8
---------- Embedding Table 17, quantization used, quantization bit set to 8
---------- Embedding Table 18, quantization used, quantization bit set to 8
---------- Embedding Table 19, quantization used, quantization bit set to 8
---------- Embedding Table 20, quantization used, quantization bit set to 8
---------- Embedding Table 21, quantization used, quantization bit set to 8
---------- Embedding Table 22, quantization used, quantization bit set to 8
---------- Embedding Table 23, quantization used, quantization bit set to 8
---------- Embedding Table 24, quantization used, quantization bit set to 8
---------- Embedding Table 25, quantization used, quantization bit set to 8
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 97.00 ms/it, loss 0.519133
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 8
---------- Embedding Table 1, quantization used, quantization bit set to 8
---------- Embedding Table 2, quantization used, quantization bit set to 8
---------- Embedding Table 3, quantization used, quantization bit set to 8
---------- Embedding Table 4, quantization used, quantization bit set to 8
---------- Embedding Table 5, quantization used, quantization bit set to 8
---------- Embedding Table 6, quantization used, quantization bit set to 8
---------- Embedding Table 7, quantization used, quantization bit set to 8
---------- Embedding Table 8, quantization used, quantization bit set to 8
---------- Embedding Table 9, quantization used, quantization bit set to 8
---------- Embedding Table 10, quantization used, quantization bit set to 8
---------- Embedding Table 11, quantization used, quantization bit set to 8
---------- Embedding Table 12, quantization used, quantization bit set to 8
---------- Embedding Table 13, quantization used, quantization bit set to 8
---------- Embedding Table 14, quantization used, quantization bit set to 8
---------- Embedding Table 15, quantization used, quantization bit set to 8
---------- Embedding Table 16, quantization used, quantization bit set to 8
---------- Embedding Table 17, quantization used, quantization bit set to 8
---------- Embedding Table 18, quantization used, quantization bit set to 8
---------- Embedding Table 19, quantization used, quantization bit set to 8
---------- Embedding Table 20, quantization used, quantization bit set to 8
---------- Embedding Table 21, quantization used, quantization bit set to 8
---------- Embedding Table 22, quantization used, quantization bit set to 8
---------- Embedding Table 23, quantization used, quantization bit set to 8
---------- Embedding Table 24, quantization used, quantization bit set to 8
---------- Embedding Table 25, quantization used, quantization bit set to 8
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 95.95 ms/it, loss 0.518943
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 8
---------- Embedding Table 1, quantization used, quantization bit set to 8
---------- Embedding Table 2, quantization used, quantization bit set to 8
---------- Embedding Table 3, quantization used, quantization bit set to 8
---------- Embedding Table 4, quantization used, quantization bit set to 8
---------- Embedding Table 5, quantization used, quantization bit set to 8
---------- Embedding Table 6, quantization used, quantization bit set to 8
---------- Embedding Table 7, quantization used, quantization bit set to 8
---------- Embedding Table 8, quantization used, quantization bit set to 8
---------- Embedding Table 9, quantization used, quantization bit set to 8
---------- Embedding Table 10, quantization used, quantization bit set to 8
---------- Embedding Table 11, quantization used, quantization bit set to 8
---------- Embedding Table 12, quantization used, quantization bit set to 8
---------- Embedding Table 13, quantization used, quantization bit set to 8
---------- Embedding Table 14, quantization used, quantization bit set to 8
---------- Embedding Table 15, quantization used, quantization bit set to 8
---------- Embedding Table 16, quantization used, quantization bit set to 8
---------- Embedding Table 17, quantization used, quantization bit set to 8
---------- Embedding Table 18, quantization used, quantization bit set to 8
---------- Embedding Table 19, quantization used, quantization bit set to 8
---------- Embedding Table 20, quantization used, quantization bit set to 8
---------- Embedding Table 21, quantization used, quantization bit set to 8
---------- Embedding Table 22, quantization used, quantization bit set to 8
---------- Embedding Table 23, quantization used, quantization bit set to 8
---------- Embedding Table 24, quantization used, quantization bit set to 8
---------- Embedding Table 25, quantization used, quantization bit set to 8
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 95.97 ms/it, loss 0.519757
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 8
---------- Embedding Table 1, quantization used, quantization bit set to 8
---------- Embedding Table 2, quantization used, quantization bit set to 8
---------- Embedding Table 3, quantization used, quantization bit set to 8
---------- Embedding Table 4, quantization used, quantization bit set to 8
---------- Embedding Table 5, quantization used, quantization bit set to 8
---------- Embedding Table 6, quantization used, quantization bit set to 8
---------- Embedding Table 7, quantization used, quantization bit set to 8
---------- Embedding Table 8, quantization used, quantization bit set to 8
---------- Embedding Table 9, quantization used, quantization bit set to 8
---------- Embedding Table 10, quantization used, quantization bit set to 8
---------- Embedding Table 11, quantization used, quantization bit set to 8
---------- Embedding Table 12, quantization used, quantization bit set to 8
---------- Embedding Table 13, quantization used, quantization bit set to 8
---------- Embedding Table 14, quantization used, quantization bit set to 8
---------- Embedding Table 15, quantization used, quantization bit set to 8
---------- Embedding Table 16, quantization used, quantization bit set to 8
---------- Embedding Table 17, quantization used, quantization bit set to 8
---------- Embedding Table 18, quantization used, quantization bit set to 8
---------- Embedding Table 19, quantization used, quantization bit set to 8
---------- Embedding Table 20, quantization used, quantization bit set to 8
---------- Embedding Table 21, quantization used, quantization bit set to 8
---------- Embedding Table 22, quantization used, quantization bit set to 8
---------- Embedding Table 23, quantization used, quantization bit set to 8
---------- Embedding Table 24, quantization used, quantization bit set to 8
---------- Embedding Table 25, quantization used, quantization bit set to 8
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 96.95 ms/it, loss 0.520097
Finished training it 2048/76743 of epoch 0, 86.35 ms/it, loss 0.509291
Finished training it 2048/76743 of epoch 0, 87.13 ms/it, loss 0.513897
Finished training it 2048/76743 of epoch 0, 86.99 ms/it, loss 0.510916
Finished training it 2048/76743 of epoch 0, 86.59 ms/it, loss 0.512891
Finished training it 3072/76743 of epoch 0, 84.42 ms/it, loss 0.511824
Finished training it 3072/76743 of epoch 0, 84.89 ms/it, loss 0.509714
Finished training it 3072/76743 of epoch 0, 84.32 ms/it, loss 0.512715
Finished training it 3072/76743 of epoch 0, 85.03 ms/it, loss 0.508599
Finished training it 4096/76743 of epoch 0, 85.53 ms/it, loss 0.512503
Finished training it 4096/76743 of epoch 0, 84.90 ms/it, loss 0.512697
Finished training it 4096/76743 of epoch 0, 84.87 ms/it, loss 0.510358
Finished training it 4096/76743 of epoch 0, 85.43 ms/it, loss 0.511633
Finished training it 5120/76743 of epoch 0, 85.11 ms/it, loss 0.510447
Finished training it 5120/76743 of epoch 0, 85.63 ms/it, loss 0.509672
Finished training it 5120/76743 of epoch 0, 85.24 ms/it, loss 0.510431
Finished training it 5120/76743 of epoch 0, 85.73 ms/it, loss 0.508452
Finished training it 6144/76743 of epoch 0, 84.54 ms/it, loss 0.508902
Finished training it 6144/76743 of epoch 0, 84.89 ms/it, loss 0.510002
Finished training it 6144/76743 of epoch 0, 84.51 ms/it, loss 0.509629
Finished training it 6144/76743 of epoch 0, 84.88 ms/it, loss 0.511367
Finished training it 7168/76743 of epoch 0, 85.08 ms/it, loss 0.511253
Finished training it 7168/76743 of epoch 0, 85.53 ms/it, loss 0.508585
Finished training it 7168/76743 of epoch 0, 85.55 ms/it, loss 0.509359
Finished training it 7168/76743 of epoch 0, 85.15 ms/it, loss 0.510531
Finished training it 8192/76743 of epoch 0, 85.88 ms/it, loss 0.509234
Finished training it 8192/76743 of epoch 0, 86.03 ms/it, loss 0.508062
Finished training it 8192/76743 of epoch 0, 85.64 ms/it, loss 0.506472
Finished training it 8192/76743 of epoch 0, 85.65 ms/it, loss 0.510123
Finished training it 9216/76743 of epoch 0, 62.96 ms/it, loss 0.506739
Finished training it 9216/76743 of epoch 0, 63.30 ms/it, loss 0.504648
Finished training it 9216/76743 of epoch 0, 63.01 ms/it, loss 0.503462
Finished training it 9216/76743 of epoch 0, 63.06 ms/it, loss 0.501811
Finished training it 10240/76743 of epoch 0, 64.60 ms/it, loss 0.504646
Finished training it 10240/76743 of epoch 0, 64.63 ms/it, loss 0.506585
Finished training it 10240/76743 of epoch 0, 64.57 ms/it, loss 0.502181
Finished training it 10240/76743 of epoch 0, 64.78 ms/it, loss 0.501206
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2488413.0
get out
0 has test check 2488413.0 and sample count 3274240
 accuracy 76.000 %, best 76.000 %, roc auc score 0.7294, best 0.7294
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 63.77 ms/it, loss 0.504106
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2488413.0
get out
3 has test check 2488413.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 63.74 ms/it, loss 0.502513
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2488413.0
get out
1 has test check 2488413.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 63.83 ms/it, loss 0.502233
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2488413.0
get out
2 has test check 2488413.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 63.95 ms/it, loss 0.507350
Finished training it 12288/76743 of epoch 0, 64.05 ms/it, loss 0.502299
Finished training it 12288/76743 of epoch 0, 64.12 ms/it, loss 0.501439
Finished training it 12288/76743 of epoch 0, 64.31 ms/it, loss 0.503027
Finished training it 12288/76743 of epoch 0, 64.08 ms/it, loss 0.499461
Finished training it 13312/76743 of epoch 0, 63.92 ms/it, loss 0.501714
Finished training it 13312/76743 of epoch 0, 63.72 ms/it, loss 0.500873
Finished training it 13312/76743 of epoch 0, 63.78 ms/it, loss 0.503885
Finished training it 13312/76743 of epoch 0, 63.64 ms/it, loss 0.502297
Finished training it 14336/76743 of epoch 0, 64.44 ms/it, loss 0.500812
Finished training it 14336/76743 of epoch 0, 64.25 ms/it, loss 0.500927
Finished training it 14336/76743 of epoch 0, 64.24 ms/it, loss 0.500713
Finished training it 14336/76743 of epoch 0, 64.11 ms/it, loss 0.503518
Finished training it 15360/76743 of epoch 0, 75.13 ms/it, loss 0.502223
Finished training it 15360/76743 of epoch 0, 74.79 ms/it, loss 0.499679
Finished training it 15360/76743 of epoch 0, 74.76 ms/it, loss 0.498185
Finished training it 15360/76743 of epoch 0, 74.79 ms/it, loss 0.499762
Finished training it 16384/76743 of epoch 0, 66.85 ms/it, loss 0.501987
Finished training it 16384/76743 of epoch 0, 66.89 ms/it, loss 0.499433
Finished training it 16384/76743 of epoch 0, 67.03 ms/it, loss 0.500435
Finished training it 16384/76743 of epoch 0, 66.98 ms/it, loss 0.501534
Finished training it 17408/76743 of epoch 0, 65.19 ms/it, loss 0.499567
Finished training it 17408/76743 of epoch 0, 65.16 ms/it, loss 0.500471
Finished training it 17408/76743 of epoch 0, 65.17 ms/it, loss 0.498629
Finished training it 17408/76743 of epoch 0, 65.12 ms/it, loss 0.500598
Finished training it 18432/76743 of epoch 0, 64.86 ms/it, loss 0.497244
Finished training it 18432/76743 of epoch 0, 65.06 ms/it, loss 0.499197
Finished training it 18432/76743 of epoch 0, 64.86 ms/it, loss 0.501555
Finished training it 18432/76743 of epoch 0, 64.86 ms/it, loss 0.500301
Finished training it 19456/76743 of epoch 0, 64.04 ms/it, loss 0.500417
Finished training it 19456/76743 of epoch 0, 64.29 ms/it, loss 0.499424
Finished training it 19456/76743 of epoch 0, 64.05 ms/it, loss 0.498532
Finished training it 19456/76743 of epoch 0, 63.96 ms/it, loss 0.502027
Finished training it 20480/76743 of epoch 0, 64.70 ms/it, loss 0.499468
Finished training it 20480/76743 of epoch 0, 64.47 ms/it, loss 0.498142
Finished training it 20480/76743 of epoch 0, 64.46 ms/it, loss 0.498302
Finished training it 20480/76743 of epoch 0, 64.55 ms/it, loss 0.497612
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2493957.0
get out
0 has test check 2493957.0 and sample count 3274240
 accuracy 76.169 %, best 76.169 %, roc auc score 0.7382, best 0.7382
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2493957.0
get out
1 has test check 2493957.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 63.87 ms/it, loss 0.497612
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2493957.0
get out
2 has test check 2493957.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 64.19 ms/it, loss 0.498896
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 63.95 ms/it, loss 0.496842
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2493957.0
get out
3 has test check 2493957.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 63.99 ms/it, loss 0.497292
Finished training it 22528/76743 of epoch 0, 63.47 ms/it, loss 0.494259
Finished training it 22528/76743 of epoch 0, 63.85 ms/it, loss 0.497957
Finished training it 22528/76743 of epoch 0, 63.56 ms/it, loss 0.496650
Finished training it 22528/76743 of epoch 0, 63.41 ms/it, loss 0.498790
Finished training it 23552/76743 of epoch 0, 63.76 ms/it, loss 0.497522
Finished training it 23552/76743 of epoch 0, 63.67 ms/it, loss 0.494977
Finished training it 23552/76743 of epoch 0, 63.66 ms/it, loss 0.496586
Finished training it 23552/76743 of epoch 0, 63.93 ms/it, loss 0.500013
Finished training it 24576/76743 of epoch 0, 63.61 ms/it, loss 0.497118
Finished training it 24576/76743 of epoch 0, 63.30 ms/it, loss 0.496327
Finished training it 24576/76743 of epoch 0, 63.46 ms/it, loss 0.496756
Finished training it 24576/76743 of epoch 0, 63.41 ms/it, loss 0.496835
Finished training it 25600/76743 of epoch 0, 63.93 ms/it, loss 0.496407
Finished training it 25600/76743 of epoch 0, 63.97 ms/it, loss 0.496405
Finished training it 25600/76743 of epoch 0, 64.30 ms/it, loss 0.497975
Finished training it 25600/76743 of epoch 0, 63.90 ms/it, loss 0.496647
Finished training it 26624/76743 of epoch 0, 64.11 ms/it, loss 0.496342
Finished training it 26624/76743 of epoch 0, 63.59 ms/it, loss 0.496521
Finished training it 26624/76743 of epoch 0, 63.68 ms/it, loss 0.495754
Finished training it 26624/76743 of epoch 0, 63.61 ms/it, loss 0.495235
Finished training it 27648/76743 of epoch 0, 63.68 ms/it, loss 0.498074
Finished training it 27648/76743 of epoch 0, 63.79 ms/it, loss 0.495191
Finished training it 27648/76743 of epoch 0, 64.02 ms/it, loss 0.500245
Finished training it 27648/76743 of epoch 0, 63.67 ms/it, loss 0.496889
Finished training it 28672/76743 of epoch 0, 63.78 ms/it, loss 0.495509
Finished training it 28672/76743 of epoch 0, 63.89 ms/it, loss 0.498563
Finished training it 28672/76743 of epoch 0, 64.06 ms/it, loss 0.494679
Finished training it 28672/76743 of epoch 0, 63.65 ms/it, loss 0.497342
Finished training it 29696/76743 of epoch 0, 63.84 ms/it, loss 0.492238
Finished training it 29696/76743 of epoch 0, 63.58 ms/it, loss 0.494431
Finished training it 29696/76743 of epoch 0, 63.68 ms/it, loss 0.497328
Finished training it 29696/76743 of epoch 0, 63.73 ms/it, loss 0.494356
Finished training it 30720/76743 of epoch 0, 63.88 ms/it, loss 0.495983
Finished training it 30720/76743 of epoch 0, 64.14 ms/it, loss 0.494580
Finished training it 30720/76743 of epoch 0, 63.73 ms/it, loss 0.494978
Finished training it 30720/76743 of epoch 0, 63.80 ms/it, loss 0.495096
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2498143.0
get out
0 has test check 2498143.0 and sample count 3274240
 accuracy 76.297 %, best 76.297 %, roc auc score 0.7413, best 0.7413
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2498143.0
get out
2 has test check 2498143.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 63.89 ms/it, loss 0.494679
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2498143.0
get out
3 has test check 2498143.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 63.53 ms/it, loss 0.495303
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 63.63 ms/it, loss 0.494345
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2498143.0
get out
1 has test check 2498143.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 63.49 ms/it, loss 0.496068
Finished training it 32768/76743 of epoch 0, 63.40 ms/it, loss 0.492200
Finished training it 32768/76743 of epoch 0, 63.53 ms/it, loss 0.492603
Finished training it 32768/76743 of epoch 0, 63.86 ms/it, loss 0.493629
Finished training it 32768/76743 of epoch 0, 63.44 ms/it, loss 0.495356
Finished training it 33792/76743 of epoch 0, 63.63 ms/it, loss 0.494120
Finished training it 33792/76743 of epoch 0, 63.60 ms/it, loss 0.494011
Finished training it 33792/76743 of epoch 0, 63.64 ms/it, loss 0.490956
Finished training it 33792/76743 of epoch 0, 64.03 ms/it, loss 0.491054
Finished training it 34816/76743 of epoch 0, 69.13 ms/it, loss 0.495329
Finished training it 34816/76743 of epoch 0, 68.88 ms/it, loss 0.495817
Finished training it 34816/76743 of epoch 0, 68.87 ms/it, loss 0.490838
Finished training it 34816/76743 of epoch 0, 68.86 ms/it, loss 0.494134
Finished training it 35840/76743 of epoch 0, 69.32 ms/it, loss 0.496384
Finished training it 35840/76743 of epoch 0, 69.78 ms/it, loss 0.493377
Finished training it 35840/76743 of epoch 0, 68.79 ms/it, loss 0.493091
Finished training it 35840/76743 of epoch 0, 69.30 ms/it, loss 0.492631
Finished training it 36864/76743 of epoch 0, 64.10 ms/it, loss 0.493004
Finished training it 36864/76743 of epoch 0, 63.76 ms/it, loss 0.491573
Finished training it 36864/76743 of epoch 0, 63.81 ms/it, loss 0.492255
Finished training it 36864/76743 of epoch 0, 63.79 ms/it, loss 0.494242
Finished training it 37888/76743 of epoch 0, 63.97 ms/it, loss 0.494748
Finished training it 37888/76743 of epoch 0, 63.48 ms/it, loss 0.492120
Finished training it 37888/76743 of epoch 0, 63.65 ms/it, loss 0.494016
Finished training it 37888/76743 of epoch 0, 63.62 ms/it, loss 0.492264
Finished training it 38912/76743 of epoch 0, 63.55 ms/it, loss 0.490690
Finished training it 38912/76743 of epoch 0, 63.24 ms/it, loss 0.491671
Finished training it 38912/76743 of epoch 0, 63.21 ms/it, loss 0.496285
Finished training it 38912/76743 of epoch 0, 63.32 ms/it, loss 0.492574
Finished training it 39936/76743 of epoch 0, 63.40 ms/it, loss 0.493428
Finished training it 39936/76743 of epoch 0, 63.35 ms/it, loss 0.492199
Finished training it 39936/76743 of epoch 0, 63.66 ms/it, loss 0.492636
Finished training it 39936/76743 of epoch 0, 63.37 ms/it, loss 0.493861
Finished training it 40960/76743 of epoch 0, 63.69 ms/it, loss 0.492765
Finished training it 40960/76743 of epoch 0, 63.89 ms/it, loss 0.490646
Finished training it 40960/76743 of epoch 0, 63.66 ms/it, loss 0.491372
Finished training it 40960/76743 of epoch 0, 63.44 ms/it, loss 0.491177
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2507736.0
get out
0 has test check 2507736.0 and sample count 3274240
 accuracy 76.590 %, best 76.590 %, roc auc score 0.7468, best 0.7468
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2507736.0
get out
3 has test check 2507736.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 63.40 ms/it, loss 0.491632
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2507736.0
get out
2 has test check 2507736.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 63.74 ms/it, loss 0.492247
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 63.41 ms/it, loss 0.490899
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2507736.0
get out
1 has test check 2507736.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 63.37 ms/it, loss 0.489858
Finished training it 43008/76743 of epoch 0, 63.51 ms/it, loss 0.489533
Finished training it 43008/76743 of epoch 0, 63.45 ms/it, loss 0.494122
Finished training it 43008/76743 of epoch 0, 63.74 ms/it, loss 0.487612
Finished training it 43008/76743 of epoch 0, 63.58 ms/it, loss 0.490561
Finished training it 44032/76743 of epoch 0, 63.58 ms/it, loss 0.494331
Finished training it 44032/76743 of epoch 0, 63.13 ms/it, loss 0.488363
Finished training it 44032/76743 of epoch 0, 63.21 ms/it, loss 0.490155
Finished training it 44032/76743 of epoch 0, 63.24 ms/it, loss 0.490946
Finished training it 45056/76743 of epoch 0, 63.90 ms/it, loss 0.489692
Finished training it 45056/76743 of epoch 0, 63.54 ms/it, loss 0.490508
Finished training it 45056/76743 of epoch 0, 63.64 ms/it, loss 0.492733
Finished training it 45056/76743 of epoch 0, 63.47 ms/it, loss 0.490964
Finished training it 46080/76743 of epoch 0, 63.52 ms/it, loss 0.490509
Finished training it 46080/76743 of epoch 0, 63.44 ms/it, loss 0.490170
Finished training it 46080/76743 of epoch 0, 63.49 ms/it, loss 0.492628
Finished training it 46080/76743 of epoch 0, 63.81 ms/it, loss 0.491856
Finished training it 47104/76743 of epoch 0, 63.69 ms/it, loss 0.488509
Finished training it 47104/76743 of epoch 0, 63.35 ms/it, loss 0.491883
Finished training it 47104/76743 of epoch 0, 63.34 ms/it, loss 0.490395
Finished training it 47104/76743 of epoch 0, 63.32 ms/it, loss 0.492450
Finished training it 48128/76743 of epoch 0, 63.23 ms/it, loss 0.490136
Finished training it 48128/76743 of epoch 0, 63.27 ms/it, loss 0.490241
Finished training it 48128/76743 of epoch 0, 63.62 ms/it, loss 0.492843
Finished training it 48128/76743 of epoch 0, 63.22 ms/it, loss 0.489259
Finished training it 49152/76743 of epoch 0, 63.51 ms/it, loss 0.490001
Finished training it 49152/76743 of epoch 0, 63.11 ms/it, loss 0.490565
Finished training it 49152/76743 of epoch 0, 63.28 ms/it, loss 0.489026
Finished training it 49152/76743 of epoch 0, 63.25 ms/it, loss 0.487825
Finished training it 50176/76743 of epoch 0, 63.89 ms/it, loss 0.488691
Finished training it 50176/76743 of epoch 0, 63.63 ms/it, loss 0.488242
Finished training it 50176/76743 of epoch 0, 63.65 ms/it, loss 0.486754
Finished training it 50176/76743 of epoch 0, 63.80 ms/it, loss 0.488630
Finished training it 51200/76743 of epoch 0, 63.99 ms/it, loss 0.488220
Finished training it 51200/76743 of epoch 0, 63.65 ms/it, loss 0.488737
Finished training it 51200/76743 of epoch 0, 63.63 ms/it, loss 0.490419
Finished training it 51200/76743 of epoch 0, 63.78 ms/it, loss 0.490123
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2507868.0
get out
0 has test check 2507868.0 and sample count 3274240
 accuracy 76.594 %, best 76.594 %, roc auc score 0.7485, best 0.7485
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 63.19 ms/it, loss 0.490274
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2507868.0
get out
1 has test check 2507868.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 63.25 ms/it, loss 0.487576
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2507868.0
get out
2 has test check 2507868.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 63.42 ms/it, loss 0.487619
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2507868.0
get out
3 has test check 2507868.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 63.14 ms/it, loss 0.489411
Finished training it 53248/76743 of epoch 0, 63.42 ms/it, loss 0.488761
Finished training it 53248/76743 of epoch 0, 63.76 ms/it, loss 0.484818
Finished training it 53248/76743 of epoch 0, 63.48 ms/it, loss 0.488031
Finished training it 53248/76743 of epoch 0, 63.55 ms/it, loss 0.487177
Finished training it 54272/76743 of epoch 0, 63.54 ms/it, loss 0.487889
Finished training it 54272/76743 of epoch 0, 63.54 ms/it, loss 0.491064
Finished training it 54272/76743 of epoch 0, 63.54 ms/it, loss 0.488751
Finished training it 54272/76743 of epoch 0, 63.80 ms/it, loss 0.488643
Finished training it 55296/76743 of epoch 0, 73.18 ms/it, loss 0.491399
Finished training it 55296/76743 of epoch 0, 73.24 ms/it, loss 0.486904
Finished training it 55296/76743 of epoch 0, 73.33 ms/it, loss 0.488911
Finished training it 55296/76743 of epoch 0, 72.94 ms/it, loss 0.485573
Finished training it 56320/76743 of epoch 0, 64.22 ms/it, loss 0.485788
Finished training it 56320/76743 of epoch 0, 63.91 ms/it, loss 0.486619
Finished training it 56320/76743 of epoch 0, 63.97 ms/it, loss 0.489212
Finished training it 56320/76743 of epoch 0, 64.09 ms/it, loss 0.492882
Finished training it 57344/76743 of epoch 0, 63.52 ms/it, loss 0.486135
Finished training it 57344/76743 of epoch 0, 63.51 ms/it, loss 0.485751
Finished training it 57344/76743 of epoch 0, 63.87 ms/it, loss 0.487846
Finished training it 57344/76743 of epoch 0, 63.44 ms/it, loss 0.486722
Finished training it 58368/76743 of epoch 0, 63.61 ms/it, loss 0.488607
Finished training it 58368/76743 of epoch 0, 63.86 ms/it, loss 0.488850
Finished training it 58368/76743 of epoch 0, 63.56 ms/it, loss 0.487352
Finished training it 58368/76743 of epoch 0, 63.53 ms/it, loss 0.487354
Finished training it 59392/76743 of epoch 0, 63.71 ms/it, loss 0.486306
Finished training it 59392/76743 of epoch 0, 63.79 ms/it, loss 0.485489
Finished training it 59392/76743 of epoch 0, 63.84 ms/it, loss 0.486405
Finished training it 59392/76743 of epoch 0, 64.08 ms/it, loss 0.487982
Finished training it 60416/76743 of epoch 0, 63.47 ms/it, loss 0.486974
Finished training it 60416/76743 of epoch 0, 63.04 ms/it, loss 0.488648
Finished training it 60416/76743 of epoch 0, 63.17 ms/it, loss 0.486449
Finished training it 60416/76743 of epoch 0, 63.29 ms/it, loss 0.486642
Finished training it 61440/76743 of epoch 0, 63.40 ms/it, loss 0.487075
Finished training it 61440/76743 of epoch 0, 63.30 ms/it, loss 0.485900
Finished training it 61440/76743 of epoch 0, 63.40 ms/it, loss 0.487982
Finished training it 61440/76743 of epoch 0, 63.61 ms/it, loss 0.488797
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2515019.0
get out
0 has test check 2515019.0 and sample count 3274240
 accuracy 76.812 %, best 76.812 %, roc auc score 0.7517, best 0.7517
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2515019.0
get out
2 has test check 2515019.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 63.80 ms/it, loss 0.488488
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2515019.0
get out
1 has test check 2515019.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 63.53 ms/it, loss 0.487377
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 63.54 ms/it, loss 0.485891
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2515019.0
get out
3 has test check 2515019.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 63.50 ms/it, loss 0.486606
Finished training it 63488/76743 of epoch 0, 63.41 ms/it, loss 0.490011
Finished training it 63488/76743 of epoch 0, 63.49 ms/it, loss 0.485573
Finished training it 63488/76743 of epoch 0, 63.46 ms/it, loss 0.485188
Finished training it 63488/76743 of epoch 0, 63.74 ms/it, loss 0.487085
Finished training it 64512/76743 of epoch 0, 63.27 ms/it, loss 0.487900
Finished training it 64512/76743 of epoch 0, 63.24 ms/it, loss 0.487647
Finished training it 64512/76743 of epoch 0, 63.34 ms/it, loss 0.488245
Finished training it 64512/76743 of epoch 0, 63.54 ms/it, loss 0.484536
Finished training it 65536/76743 of epoch 0, 64.01 ms/it, loss 0.487426
Finished training it 65536/76743 of epoch 0, 63.96 ms/it, loss 0.486035
Finished training it 65536/76743 of epoch 0, 64.25 ms/it, loss 0.486834
Finished training it 65536/76743 of epoch 0, 64.19 ms/it, loss 0.484323
Finished training it 66560/76743 of epoch 0, 63.33 ms/it, loss 0.484979
Finished training it 66560/76743 of epoch 0, 63.18 ms/it, loss 0.484824
Finished training it 66560/76743 of epoch 0, 63.60 ms/it, loss 0.486110
Finished training it 66560/76743 of epoch 0, 63.33 ms/it, loss 0.487043
Finished training it 67584/76743 of epoch 0, 64.20 ms/it, loss 0.484893
Finished training it 67584/76743 of epoch 0, 63.84 ms/it, loss 0.485304
Finished training it 67584/76743 of epoch 0, 63.97 ms/it, loss 0.488830
Finished training it 67584/76743 of epoch 0, 63.93 ms/it, loss 0.486484
Finished training it 68608/76743 of epoch 0, 63.26 ms/it, loss 0.484114
Finished training it 68608/76743 of epoch 0, 63.54 ms/it, loss 0.482059
Finished training it 68608/76743 of epoch 0, 63.18 ms/it, loss 0.485828
Finished training it 68608/76743 of epoch 0, 63.21 ms/it, loss 0.485435
Finished training it 69632/76743 of epoch 0, 63.65 ms/it, loss 0.485604
Finished training it 69632/76743 of epoch 0, 63.40 ms/it, loss 0.484793
Finished training it 69632/76743 of epoch 0, 63.38 ms/it, loss 0.485610
Finished training it 69632/76743 of epoch 0, 63.40 ms/it, loss 0.484638
Finished training it 70656/76743 of epoch 0, 63.80 ms/it, loss 0.485372
Finished training it 70656/76743 of epoch 0, 63.57 ms/it, loss 0.482211
Finished training it 70656/76743 of epoch 0, 63.47 ms/it, loss 0.487689
Finished training it 70656/76743 of epoch 0, 63.53 ms/it, loss 0.485656
Finished training it 71680/76743 of epoch 0, 63.48 ms/it, loss 0.483590
Finished training it 71680/76743 of epoch 0, 63.20 ms/it, loss 0.484361
Finished training it 71680/76743 of epoch 0, 63.18 ms/it, loss 0.487607
Finished training it 71680/76743 of epoch 0, 63.02 ms/it, loss 0.483797
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2517588.0
get out
0 has test check 2517588.0 and sample count 3274240
 accuracy 76.891 %, best 76.891 %, roc auc score 0.7545, best 0.7545
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 63.50 ms/it, loss 0.485187
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2517588.0
get out
2 has test check 2517588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 63.75 ms/it, loss 0.485321
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2517588.0
get out
1 has test check 2517588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 63.56 ms/it, loss 0.483085
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2517588.0
get out
3 has test check 2517588.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 63.58 ms/it, loss 0.487838
Finished training it 73728/76743 of epoch 0, 63.23 ms/it, loss 0.484740
Finished training it 73728/76743 of epoch 0, 63.50 ms/it, loss 0.484302
Finished training it 73728/76743 of epoch 0, 63.27 ms/it, loss 0.484849
Finished training it 73728/76743 of epoch 0, 63.18 ms/it, loss 0.485901
Finished training it 74752/76743 of epoch 0, 63.36 ms/it, loss 0.486410
Finished training it 74752/76743 of epoch 0, 63.39 ms/it, loss 0.484685
Finished training it 74752/76743 of epoch 0, 63.62 ms/it, loss 0.484688
Finished training it 74752/76743 of epoch 0, 63.34 ms/it, loss 0.484031
Finished training it 75776/76743 of epoch 0, 69.47 ms/it, loss 0.483790
Finished training it 75776/76743 of epoch 0, 69.29 ms/it, loss 0.482261
Finished training it 75776/76743 of epoch 0, 69.01 ms/it, loss 0.485592
Finished training it 75776/76743 of epoch 0, 69.09 ms/it, loss 0.485644
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 66.90 ms/it, loss 0.484959
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 67.09 ms/it, loss 0.484643
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 67.51 ms/it, loss 0.483816
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 67.27 ms/it, loss 0.484568
Finished training it 2048/76743 of epoch 1, 64.09 ms/it, loss 0.483888
Finished training it 2048/76743 of epoch 1, 63.75 ms/it, loss 0.484249
Finished training it 2048/76743 of epoch 1, 63.86 ms/it, loss 0.483254
Finished training it 2048/76743 of epoch 1, 63.86 ms/it, loss 0.484808
Finished training it 3072/76743 of epoch 1, 63.60 ms/it, loss 0.485303
Finished training it 3072/76743 of epoch 1, 63.14 ms/it, loss 0.485691
Finished training it 3072/76743 of epoch 1, 63.37 ms/it, loss 0.484504
Finished training it 3072/76743 of epoch 1, 63.40 ms/it, loss 0.483717
Finished training it 4096/76743 of epoch 1, 63.56 ms/it, loss 0.483399
Finished training it 4096/76743 of epoch 1, 63.65 ms/it, loss 0.485684
Finished training it 4096/76743 of epoch 1, 63.53 ms/it, loss 0.484204
Finished training it 4096/76743 of epoch 1, 63.87 ms/it, loss 0.485246
Finished training it 5120/76743 of epoch 1, 63.73 ms/it, loss 0.486407
Finished training it 5120/76743 of epoch 1, 63.40 ms/it, loss 0.486801
Finished training it 5120/76743 of epoch 1, 63.47 ms/it, loss 0.484733
Finished training it 5120/76743 of epoch 1, 63.40 ms/it, loss 0.486296
Finished training it 6144/76743 of epoch 1, 63.44 ms/it, loss 0.487330
Finished training it 6144/76743 of epoch 1, 63.31 ms/it, loss 0.483338
Finished training it 6144/76743 of epoch 1, 63.63 ms/it, loss 0.484055
Finished training it 6144/76743 of epoch 1, 63.27 ms/it, loss 0.484430
Finished training it 7168/76743 of epoch 1, 63.73 ms/it, loss 0.482386
Finished training it 7168/76743 of epoch 1, 63.58 ms/it, loss 0.483447
Finished training it 7168/76743 of epoch 1, 63.85 ms/it, loss 0.484773
Finished training it 7168/76743 of epoch 1, 63.66 ms/it, loss 0.484899
Finished training it 8192/76743 of epoch 1, 63.25 ms/it, loss 0.483538
Finished training it 8192/76743 of epoch 1, 62.96 ms/it, loss 0.485024
Finished training it 8192/76743 of epoch 1, 62.97 ms/it, loss 0.484385
Finished training it 8192/76743 of epoch 1, 63.08 ms/it, loss 0.484649
Finished training it 9216/76743 of epoch 1, 63.38 ms/it, loss 0.484291
Finished training it 9216/76743 of epoch 1, 63.27 ms/it, loss 0.481563
Finished training it 9216/76743 of epoch 1, 63.20 ms/it, loss 0.485757
Finished training it 9216/76743 of epoch 1, 63.48 ms/it, loss 0.484071
Finished training it 10240/76743 of epoch 1, 63.52 ms/it, loss 0.481533
Finished training it 10240/76743 of epoch 1, 63.24 ms/it, loss 0.483175
Finished training it 10240/76743 of epoch 1, 63.19 ms/it, loss 0.486378
Finished training it 10240/76743 of epoch 1, 63.11 ms/it, loss 0.481414
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2519453.0
get out
0 has test check 2519453.0 and sample count 3274240
 accuracy 76.948 %, best 76.948 %, roc auc score 0.7551, best 0.7551
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2519453.0
get out
2 has test check 2519453.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 63.77 ms/it, loss 0.486601
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 63.58 ms/it, loss 0.483563
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2519453.0
get out
1 has test check 2519453.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 63.49 ms/it, loss 0.481582
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2519453.0
get out
3 has test check 2519453.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 63.61 ms/it, loss 0.481669
Finished training it 12288/76743 of epoch 1, 63.82 ms/it, loss 0.484816
Finished training it 12288/76743 of epoch 1, 63.57 ms/it, loss 0.484055
Finished training it 12288/76743 of epoch 1, 63.60 ms/it, loss 0.482732
Finished training it 12288/76743 of epoch 1, 63.49 ms/it, loss 0.481793
Finished training it 13312/76743 of epoch 1, 63.45 ms/it, loss 0.483578
Finished training it 13312/76743 of epoch 1, 63.36 ms/it, loss 0.485947
Finished training it 13312/76743 of epoch 1, 63.23 ms/it, loss 0.482305
Finished training it 13312/76743 of epoch 1, 63.18 ms/it, loss 0.483789
Finished training it 14336/76743 of epoch 1, 63.84 ms/it, loss 0.482746
Finished training it 14336/76743 of epoch 1, 63.50 ms/it, loss 0.483196
Finished training it 14336/76743 of epoch 1, 63.72 ms/it, loss 0.483070
Finished training it 14336/76743 of epoch 1, 63.54 ms/it, loss 0.486617
Finished training it 15360/76743 of epoch 1, 63.08 ms/it, loss 0.481788
Finished training it 15360/76743 of epoch 1, 63.32 ms/it, loss 0.484610
Finished training it 15360/76743 of epoch 1, 63.04 ms/it, loss 0.479682
Finished training it 15360/76743 of epoch 1, 63.11 ms/it, loss 0.481345
Finished training it 16384/76743 of epoch 1, 63.67 ms/it, loss 0.482311
Finished training it 16384/76743 of epoch 1, 63.73 ms/it, loss 0.484095
Finished training it 16384/76743 of epoch 1, 63.56 ms/it, loss 0.483559
Finished training it 16384/76743 of epoch 1, 64.01 ms/it, loss 0.483219
Finished training it 17408/76743 of epoch 1, 63.91 ms/it, loss 0.482421
Finished training it 17408/76743 of epoch 1, 63.70 ms/it, loss 0.483115
Finished training it 17408/76743 of epoch 1, 63.68 ms/it, loss 0.481557
Finished training it 17408/76743 of epoch 1, 63.51 ms/it, loss 0.483290
Finished training it 18432/76743 of epoch 1, 63.57 ms/it, loss 0.483246
Finished training it 18432/76743 of epoch 1, 63.67 ms/it, loss 0.481436
Finished training it 18432/76743 of epoch 1, 63.73 ms/it, loss 0.484624
Finished training it 18432/76743 of epoch 1, 63.89 ms/it, loss 0.482704
Finished training it 19456/76743 of epoch 1, 63.94 ms/it, loss 0.484739
Finished training it 19456/76743 of epoch 1, 64.12 ms/it, loss 0.483029
Finished training it 19456/76743 of epoch 1, 64.01 ms/it, loss 0.481664
Finished training it 19456/76743 of epoch 1, 63.89 ms/it, loss 0.483513
Finished training it 20480/76743 of epoch 1, 63.18 ms/it, loss 0.481165
Finished training it 20480/76743 of epoch 1, 63.48 ms/it, loss 0.483651
Finished training it 20480/76743 of epoch 1, 63.23 ms/it, loss 0.481822
Finished training it 20480/76743 of epoch 1, 63.34 ms/it, loss 0.480720
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2520847.0
get out
0 has test check 2520847.0 and sample count 3274240
 accuracy 76.990 %, best 76.990 %, roc auc score 0.7574, best 0.7574
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2520847.0
get out
1 has test check 2520847.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 63.60 ms/it, loss 0.481532
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 63.42 ms/it, loss 0.480207
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2520847.0
get out
3 has test check 2520847.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 63.52 ms/it, loss 0.481317
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2520847.0
get out
2 has test check 2520847.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 63.83 ms/it, loss 0.483744
Finished training it 22528/76743 of epoch 1, 63.59 ms/it, loss 0.481752
Finished training it 22528/76743 of epoch 1, 63.41 ms/it, loss 0.482925
Finished training it 22528/76743 of epoch 1, 63.48 ms/it, loss 0.481631
Finished training it 22528/76743 of epoch 1, 63.40 ms/it, loss 0.479365
Finished training it 23552/76743 of epoch 1, 63.83 ms/it, loss 0.484314
Finished training it 23552/76743 of epoch 1, 63.61 ms/it, loss 0.479622
Finished training it 23552/76743 of epoch 1, 63.67 ms/it, loss 0.482314
Finished training it 23552/76743 of epoch 1, 63.32 ms/it, loss 0.480834
Finished training it 24576/76743 of epoch 1, 74.57 ms/it, loss 0.481113
Finished training it 24576/76743 of epoch 1, 74.39 ms/it, loss 0.481862
Finished training it 24576/76743 of epoch 1, 74.38 ms/it, loss 0.481767
Finished training it 24576/76743 of epoch 1, 74.69 ms/it, loss 0.482968
Finished training it 25600/76743 of epoch 1, 64.21 ms/it, loss 0.483730
Finished training it 25600/76743 of epoch 1, 63.97 ms/it, loss 0.482057
Finished training it 25600/76743 of epoch 1, 64.17 ms/it, loss 0.480775
Finished training it 25600/76743 of epoch 1, 63.83 ms/it, loss 0.481591
Finished training it 26624/76743 of epoch 1, 63.60 ms/it, loss 0.482213
Finished training it 26624/76743 of epoch 1, 63.94 ms/it, loss 0.481813
Finished training it 26624/76743 of epoch 1, 63.67 ms/it, loss 0.481070
Finished training it 26624/76743 of epoch 1, 63.57 ms/it, loss 0.481577
Finished training it 27648/76743 of epoch 1, 63.61 ms/it, loss 0.485644
Finished training it 27648/76743 of epoch 1, 63.50 ms/it, loss 0.483366
Finished training it 27648/76743 of epoch 1, 63.27 ms/it, loss 0.481195
Finished training it 27648/76743 of epoch 1, 63.26 ms/it, loss 0.482653
Finished training it 28672/76743 of epoch 1, 63.45 ms/it, loss 0.483021
Finished training it 28672/76743 of epoch 1, 63.78 ms/it, loss 0.480779
Finished training it 28672/76743 of epoch 1, 63.47 ms/it, loss 0.484197
Finished training it 28672/76743 of epoch 1, 63.50 ms/it, loss 0.480886
Finished training it 29696/76743 of epoch 1, 63.42 ms/it, loss 0.479900
Finished training it 29696/76743 of epoch 1, 63.24 ms/it, loss 0.482821
Finished training it 29696/76743 of epoch 1, 63.20 ms/it, loss 0.480201
Finished training it 29696/76743 of epoch 1, 63.58 ms/it, loss 0.479382
Finished training it 30720/76743 of epoch 1, 63.54 ms/it, loss 0.481145
Finished training it 30720/76743 of epoch 1, 63.39 ms/it, loss 0.481614
Finished training it 30720/76743 of epoch 1, 63.72 ms/it, loss 0.479909
Finished training it 30720/76743 of epoch 1, 63.54 ms/it, loss 0.480935
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2522658.0
get out
0 has test check 2522658.0 and sample count 3274240
 accuracy 77.046 %, best 77.046 %, roc auc score 0.7586, best 0.7586
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2522658.0
get out
2 has test check 2522658.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 63.81 ms/it, loss 0.481116
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2522658.0
get out
1 has test check 2522658.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 63.65 ms/it, loss 0.482921
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 63.51 ms/it, loss 0.481237
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2522658.0
get out
3 has test check 2522658.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 63.45 ms/it, loss 0.481752
Finished training it 32768/76743 of epoch 1, 63.41 ms/it, loss 0.482495
Finished training it 32768/76743 of epoch 1, 63.27 ms/it, loss 0.478545
Finished training it 32768/76743 of epoch 1, 63.19 ms/it, loss 0.479831
Finished training it 32768/76743 of epoch 1, 63.55 ms/it, loss 0.481271
Finished training it 33792/76743 of epoch 1, 63.84 ms/it, loss 0.478015
Finished training it 33792/76743 of epoch 1, 63.48 ms/it, loss 0.480743
Finished training it 33792/76743 of epoch 1, 63.66 ms/it, loss 0.478218
Finished training it 33792/76743 of epoch 1, 63.73 ms/it, loss 0.481770
Finished training it 34816/76743 of epoch 1, 63.27 ms/it, loss 0.482902
Finished training it 34816/76743 of epoch 1, 63.30 ms/it, loss 0.478342
Finished training it 34816/76743 of epoch 1, 63.55 ms/it, loss 0.482676
Finished training it 34816/76743 of epoch 1, 63.36 ms/it, loss 0.481429
Finished training it 35840/76743 of epoch 1, 63.64 ms/it, loss 0.480730
Finished training it 35840/76743 of epoch 1, 63.34 ms/it, loss 0.479837
Finished training it 35840/76743 of epoch 1, 63.38 ms/it, loss 0.482804
Finished training it 35840/76743 of epoch 1, 63.39 ms/it, loss 0.480215
Finished training it 36864/76743 of epoch 1, 64.09 ms/it, loss 0.480816
Finished training it 36864/76743 of epoch 1, 63.76 ms/it, loss 0.479606
Finished training it 36864/76743 of epoch 1, 63.82 ms/it, loss 0.479749
Finished training it 36864/76743 of epoch 1, 63.85 ms/it, loss 0.482085
Finished training it 37888/76743 of epoch 1, 63.75 ms/it, loss 0.483093
Finished training it 37888/76743 of epoch 1, 63.49 ms/it, loss 0.482399
Finished training it 37888/76743 of epoch 1, 63.45 ms/it, loss 0.480288
Finished training it 37888/76743 of epoch 1, 63.38 ms/it, loss 0.479657
Finished training it 38912/76743 of epoch 1, 63.14 ms/it, loss 0.484312
Finished training it 38912/76743 of epoch 1, 62.98 ms/it, loss 0.480119
Finished training it 38912/76743 of epoch 1, 63.27 ms/it, loss 0.479881
Finished training it 38912/76743 of epoch 1, 63.07 ms/it, loss 0.481521
Finished training it 39936/76743 of epoch 1, 63.41 ms/it, loss 0.482192
Finished training it 39936/76743 of epoch 1, 63.75 ms/it, loss 0.481137
Finished training it 39936/76743 of epoch 1, 63.50 ms/it, loss 0.481293
Finished training it 39936/76743 of epoch 1, 63.51 ms/it, loss 0.482181
Finished training it 40960/76743 of epoch 1, 63.30 ms/it, loss 0.480751
Finished training it 40960/76743 of epoch 1, 63.51 ms/it, loss 0.478993
Finished training it 40960/76743 of epoch 1, 63.40 ms/it, loss 0.479688
Finished training it 40960/76743 of epoch 1, 63.35 ms/it, loss 0.481902
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2525670.0
get out
0 has test check 2525670.0 and sample count 3274240
 accuracy 77.138 %, best 77.138 %, roc auc score 0.7599, best 0.7599
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2525670.0
get out
3 has test check 2525670.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 63.45 ms/it, loss 0.480161
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2525670.0
get out
2 has test check 2525670.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 63.71 ms/it, loss 0.480279
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 63.43 ms/it, loss 0.480255
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2525670.0
get out
1 has test check 2525670.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 63.52 ms/it, loss 0.479046
Finished training it 43008/76743 of epoch 1, 63.64 ms/it, loss 0.476626
Finished training it 43008/76743 of epoch 1, 63.33 ms/it, loss 0.480052
Finished training it 43008/76743 of epoch 1, 63.45 ms/it, loss 0.483785
Finished training it 43008/76743 of epoch 1, 63.23 ms/it, loss 0.478459
Finished training it 44032/76743 of epoch 1, 63.66 ms/it, loss 0.477304
Finished training it 44032/76743 of epoch 1, 63.77 ms/it, loss 0.479157
Finished training it 44032/76743 of epoch 1, 63.75 ms/it, loss 0.480018
Finished training it 44032/76743 of epoch 1, 64.08 ms/it, loss 0.484086
Finished training it 45056/76743 of epoch 1, 73.53 ms/it, loss 0.479635
Finished training it 45056/76743 of epoch 1, 73.51 ms/it, loss 0.479167
Finished training it 45056/76743 of epoch 1, 73.76 ms/it, loss 0.479183
Finished training it 45056/76743 of epoch 1, 73.63 ms/it, loss 0.482393
Finished training it 46080/76743 of epoch 1, 63.89 ms/it, loss 0.481159
Finished training it 46080/76743 of epoch 1, 63.58 ms/it, loss 0.479281
Finished training it 46080/76743 of epoch 1, 63.56 ms/it, loss 0.481015
Finished training it 46080/76743 of epoch 1, 63.62 ms/it, loss 0.479868
Finished training it 47104/76743 of epoch 1, 63.80 ms/it, loss 0.479681
Finished training it 47104/76743 of epoch 1, 64.02 ms/it, loss 0.478449
Finished training it 47104/76743 of epoch 1, 63.77 ms/it, loss 0.482634
Finished training it 47104/76743 of epoch 1, 63.89 ms/it, loss 0.481763
Finished training it 48128/76743 of epoch 1, 63.69 ms/it, loss 0.478205
Finished training it 48128/76743 of epoch 1, 63.57 ms/it, loss 0.479380
Finished training it 48128/76743 of epoch 1, 63.88 ms/it, loss 0.483145
Finished training it 48128/76743 of epoch 1, 63.64 ms/it, loss 0.479688
Finished training it 49152/76743 of epoch 1, 63.72 ms/it, loss 0.479226
Finished training it 49152/76743 of epoch 1, 63.89 ms/it, loss 0.480257
Finished training it 49152/76743 of epoch 1, 63.65 ms/it, loss 0.479560
Finished training it 49152/76743 of epoch 1, 63.50 ms/it, loss 0.480470
Finished training it 50176/76743 of epoch 1, 63.93 ms/it, loss 0.477486
Finished training it 50176/76743 of epoch 1, 64.22 ms/it, loss 0.479487
Finished training it 50176/76743 of epoch 1, 64.00 ms/it, loss 0.478878
Finished training it 50176/76743 of epoch 1, 64.02 ms/it, loss 0.478867
Finished training it 51200/76743 of epoch 1, 63.62 ms/it, loss 0.480477
Finished training it 51200/76743 of epoch 1, 63.57 ms/it, loss 0.481218
Finished training it 51200/76743 of epoch 1, 63.84 ms/it, loss 0.478831
Finished training it 51200/76743 of epoch 1, 63.66 ms/it, loss 0.479414
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2524799.0
get out
0 has test check 2524799.0 and sample count 3274240
 accuracy 77.111 %, best 77.138 %, roc auc score 0.7604, best 0.7604
Finished training it 52224/76743 of epoch 1, 63.47 ms/it, loss 0.480854
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2524799.0
get out
3 has test check 2524799.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 63.36 ms/it, loss 0.481042
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2524799.0
get out
2 has test check 2524799.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 63.66 ms/it, loss 0.478469
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2524799.0
get out
1 has test check 2524799.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 63.36 ms/it, loss 0.478279
Finished training it 53248/76743 of epoch 1, 63.34 ms/it, loss 0.479661
Finished training it 53248/76743 of epoch 1, 63.54 ms/it, loss 0.477474
Finished training it 53248/76743 of epoch 1, 63.61 ms/it, loss 0.479420
Finished training it 53248/76743 of epoch 1, 63.86 ms/it, loss 0.475972
Finished training it 54272/76743 of epoch 1, 63.70 ms/it, loss 0.479744
Finished training it 54272/76743 of epoch 1, 63.33 ms/it, loss 0.478738
Finished training it 54272/76743 of epoch 1, 63.46 ms/it, loss 0.479674
Finished training it 54272/76743 of epoch 1, 63.27 ms/it, loss 0.481632
Finished training it 55296/76743 of epoch 1, 63.29 ms/it, loss 0.477803
Finished training it 55296/76743 of epoch 1, 63.49 ms/it, loss 0.477179
Finished training it 55296/76743 of epoch 1, 63.40 ms/it, loss 0.483316
Finished training it 55296/76743 of epoch 1, 63.66 ms/it, loss 0.480674
Finished training it 56320/76743 of epoch 1, 63.82 ms/it, loss 0.477224
Finished training it 56320/76743 of epoch 1, 63.54 ms/it, loss 0.483776
Finished training it 56320/76743 of epoch 1, 63.45 ms/it, loss 0.481256
Finished training it 56320/76743 of epoch 1, 63.62 ms/it, loss 0.478097
Finished training it 57344/76743 of epoch 1, 63.94 ms/it, loss 0.477831
Finished training it 57344/76743 of epoch 1, 63.89 ms/it, loss 0.477493
Finished training it 57344/76743 of epoch 1, 63.79 ms/it, loss 0.477899
Finished training it 57344/76743 of epoch 1, 64.16 ms/it, loss 0.479498
Finished training it 58368/76743 of epoch 1, 64.04 ms/it, loss 0.479458
Finished training it 58368/76743 of epoch 1, 63.98 ms/it, loss 0.478292
Finished training it 58368/76743 of epoch 1, 64.18 ms/it, loss 0.480547
Finished training it 58368/76743 of epoch 1, 64.07 ms/it, loss 0.480665
Finished training it 59392/76743 of epoch 1, 63.97 ms/it, loss 0.477692
Finished training it 59392/76743 of epoch 1, 63.98 ms/it, loss 0.477521
Finished training it 59392/76743 of epoch 1, 63.88 ms/it, loss 0.477261
Finished training it 59392/76743 of epoch 1, 64.22 ms/it, loss 0.480189
Finished training it 60416/76743 of epoch 1, 63.23 ms/it, loss 0.477365
Finished training it 60416/76743 of epoch 1, 63.51 ms/it, loss 0.479160
Finished training it 60416/76743 of epoch 1, 63.26 ms/it, loss 0.478839
Finished training it 60416/76743 of epoch 1, 63.30 ms/it, loss 0.480142
Finished training it 61440/76743 of epoch 1, 63.82 ms/it, loss 0.480521
Finished training it 61440/76743 of epoch 1, 63.57 ms/it, loss 0.479577
Finished training it 61440/76743 of epoch 1, 63.53 ms/it, loss 0.477900
Finished training it 61440/76743 of epoch 1, 63.49 ms/it, loss 0.479132
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2527166.0
get out
0 has test check 2527166.0 and sample count 3274240
 accuracy 77.183 %, best 77.183 %, roc auc score 0.7611, best 0.7611
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2527166.0
get out
3 has test check 2527166.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 63.58 ms/it, loss 0.479235
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2527166.0
get out
2 has test check 2527166.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 63.86 ms/it, loss 0.480326
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2527166.0
get out
1 has test check 2527166.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 63.53 ms/it, loss 0.480055
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 63.44 ms/it, loss 0.478499
Finished training it 63488/76743 of epoch 1, 63.60 ms/it, loss 0.477779
Finished training it 63488/76743 of epoch 1, 63.49 ms/it, loss 0.477422
Finished training it 63488/76743 of epoch 1, 63.71 ms/it, loss 0.478786
Finished training it 63488/76743 of epoch 1, 63.44 ms/it, loss 0.481959
Finished training it 64512/76743 of epoch 1, 63.31 ms/it, loss 0.479423
Finished training it 64512/76743 of epoch 1, 63.41 ms/it, loss 0.475788
Finished training it 64512/76743 of epoch 1, 63.17 ms/it, loss 0.480714
Finished training it 64512/76743 of epoch 1, 63.33 ms/it, loss 0.479363
Finished training it 65536/76743 of epoch 1, 68.61 ms/it, loss 0.479032
Finished training it 65536/76743 of epoch 1, 68.77 ms/it, loss 0.477425
Finished training it 65536/76743 of epoch 1, 68.29 ms/it, loss 0.478695
Finished training it 65536/76743 of epoch 1, 68.37 ms/it, loss 0.476148
Finished training it 66560/76743 of epoch 1, 69.20 ms/it, loss 0.478490
Finished training it 66560/76743 of epoch 1, 68.59 ms/it, loss 0.477444
Finished training it 66560/76743 of epoch 1, 68.61 ms/it, loss 0.477094
Finished training it 66560/76743 of epoch 1, 68.43 ms/it, loss 0.479843
Finished training it 67584/76743 of epoch 1, 63.13 ms/it, loss 0.478588
Finished training it 67584/76743 of epoch 1, 63.21 ms/it, loss 0.477585
Finished training it 67584/76743 of epoch 1, 63.45 ms/it, loss 0.477333
Finished training it 67584/76743 of epoch 1, 63.26 ms/it, loss 0.481064
Finished training it 68608/76743 of epoch 1, 63.62 ms/it, loss 0.476455
Finished training it 68608/76743 of epoch 1, 63.42 ms/it, loss 0.478200
Finished training it 68608/76743 of epoch 1, 63.80 ms/it, loss 0.475279
Finished training it 68608/76743 of epoch 1, 63.56 ms/it, loss 0.478099
Finished training it 69632/76743 of epoch 1, 63.88 ms/it, loss 0.477494
Finished training it 69632/76743 of epoch 1, 63.87 ms/it, loss 0.478178
Finished training it 69632/76743 of epoch 1, 64.09 ms/it, loss 0.478253
Finished training it 69632/76743 of epoch 1, 63.85 ms/it, loss 0.477681
Finished training it 70656/76743 of epoch 1, 63.55 ms/it, loss 0.474457
Finished training it 70656/76743 of epoch 1, 63.74 ms/it, loss 0.480212
Finished training it 70656/76743 of epoch 1, 63.57 ms/it, loss 0.478497
Finished training it 70656/76743 of epoch 1, 63.94 ms/it, loss 0.478340
Finished training it 71680/76743 of epoch 1, 63.94 ms/it, loss 0.476966
Finished training it 71680/76743 of epoch 1, 63.66 ms/it, loss 0.480727
Finished training it 71680/76743 of epoch 1, 63.64 ms/it, loss 0.477446
Finished training it 71680/76743 of epoch 1, 63.61 ms/it, loss 0.477034
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2527591.0
get out
0 has test check 2527591.0 and sample count 3274240
 accuracy 77.196 %, best 77.196 %, roc auc score 0.7626, best 0.7626
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2527591.0
get out
3 has test check 2527591.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 63.44 ms/it, loss 0.480991
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2527591.0
get out
1 has test check 2527591.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 63.43 ms/it, loss 0.476236
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 63.31 ms/it, loss 0.477723
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2527591.0
get out
2 has test check 2527591.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 63.70 ms/it, loss 0.477960
Finished training it 73728/76743 of epoch 1, 64.04 ms/it, loss 0.477753
Finished training it 73728/76743 of epoch 1, 63.96 ms/it, loss 0.478392
Finished training it 73728/76743 of epoch 1, 64.13 ms/it, loss 0.477099
Finished training it 73728/76743 of epoch 1, 63.82 ms/it, loss 0.477938
Finished training it 74752/76743 of epoch 1, 63.22 ms/it, loss 0.477258
Finished training it 74752/76743 of epoch 1, 63.33 ms/it, loss 0.479274
Finished training it 74752/76743 of epoch 1, 63.14 ms/it, loss 0.477183
Finished training it 74752/76743 of epoch 1, 63.51 ms/it, loss 0.477386
Finished training it 75776/76743 of epoch 1, 63.45 ms/it, loss 0.475391
Finished training it 75776/76743 of epoch 1, 63.56 ms/it, loss 0.477853
Finished training it 75776/76743 of epoch 1, 63.62 ms/it, loss 0.478436
Finished training it 75776/76743 of epoch 1, 63.78 ms/it, loss 0.476577
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 64.42 ms/it, loss 0.477743
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 64.92 ms/it, loss 0.476978
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 64.43 ms/it, loss 0.477858
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 64.37 ms/it, loss 0.477725
Finished training it 2048/76743 of epoch 2, 63.86 ms/it, loss 0.477147
Finished training it 2048/76743 of epoch 2, 63.48 ms/it, loss 0.477597
Finished training it 2048/76743 of epoch 2, 63.30 ms/it, loss 0.476480
Finished training it 2048/76743 of epoch 2, 63.47 ms/it, loss 0.477628
Finished training it 3072/76743 of epoch 2, 63.36 ms/it, loss 0.478999
Finished training it 3072/76743 of epoch 2, 63.71 ms/it, loss 0.478877
Finished training it 3072/76743 of epoch 2, 63.40 ms/it, loss 0.477531
Finished training it 3072/76743 of epoch 2, 63.47 ms/it, loss 0.476784
Finished training it 4096/76743 of epoch 2, 64.23 ms/it, loss 0.477956
Finished training it 4096/76743 of epoch 2, 64.30 ms/it, loss 0.476226
Finished training it 4096/76743 of epoch 2, 64.22 ms/it, loss 0.477244
Finished training it 4096/76743 of epoch 2, 64.43 ms/it, loss 0.478633
Finished training it 5120/76743 of epoch 2, 63.46 ms/it, loss 0.477680
Finished training it 5120/76743 of epoch 2, 63.39 ms/it, loss 0.478864
Finished training it 5120/76743 of epoch 2, 63.79 ms/it, loss 0.479125
Finished training it 5120/76743 of epoch 2, 63.58 ms/it, loss 0.480251
Finished training it 6144/76743 of epoch 2, 63.51 ms/it, loss 0.476361
Finished training it 6144/76743 of epoch 2, 63.57 ms/it, loss 0.477563
Finished training it 6144/76743 of epoch 2, 63.85 ms/it, loss 0.477410
Finished training it 6144/76743 of epoch 2, 63.44 ms/it, loss 0.479951
Finished training it 7168/76743 of epoch 2, 63.31 ms/it, loss 0.475832
Finished training it 7168/76743 of epoch 2, 63.25 ms/it, loss 0.477499
Finished training it 7168/76743 of epoch 2, 63.55 ms/it, loss 0.478627
Finished training it 7168/76743 of epoch 2, 63.13 ms/it, loss 0.478801
Finished training it 8192/76743 of epoch 2, 63.41 ms/it, loss 0.477598
Finished training it 8192/76743 of epoch 2, 63.63 ms/it, loss 0.476949
Finished training it 8192/76743 of epoch 2, 63.34 ms/it, loss 0.478441
Finished training it 8192/76743 of epoch 2, 63.41 ms/it, loss 0.477450
Finished training it 9216/76743 of epoch 2, 64.28 ms/it, loss 0.477359
Finished training it 9216/76743 of epoch 2, 64.08 ms/it, loss 0.474900
Finished training it 9216/76743 of epoch 2, 64.16 ms/it, loss 0.477533
Finished training it 9216/76743 of epoch 2, 64.00 ms/it, loss 0.479278
Finished training it 10240/76743 of epoch 2, 63.46 ms/it, loss 0.480178
Finished training it 10240/76743 of epoch 2, 63.75 ms/it, loss 0.475340
Finished training it 10240/76743 of epoch 2, 63.39 ms/it, loss 0.475265
Finished training it 10240/76743 of epoch 2, 63.45 ms/it, loss 0.476746
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2531201.0
get out
0 has test check 2531201.0 and sample count 3274240
 accuracy 77.307 %, best 77.307 %, roc auc score 0.7641, best 0.7641
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2531201.0
get out
1 has test check 2531201.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 63.64 ms/it, loss 0.474733
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2531201.0
get out
2 has test check 2531201.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 64.05 ms/it, loss 0.479484
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2531201.0
get out
3 has test check 2531201.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 63.63 ms/it, loss 0.475378
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 63.82 ms/it, loss 0.477192
Finished training it 12288/76743 of epoch 2, 63.29 ms/it, loss 0.477853
Finished training it 12288/76743 of epoch 2, 63.09 ms/it, loss 0.476622
Finished training it 12288/76743 of epoch 2, 62.90 ms/it, loss 0.476072
Finished training it 12288/76743 of epoch 2, 62.99 ms/it, loss 0.474821
Finished training it 13312/76743 of epoch 2, 63.29 ms/it, loss 0.475550
Finished training it 13312/76743 of epoch 2, 63.58 ms/it, loss 0.476801
Finished training it 13312/76743 of epoch 2, 63.39 ms/it, loss 0.479530
Finished training it 13312/76743 of epoch 2, 63.30 ms/it, loss 0.476818
Finished training it 14336/76743 of epoch 2, 73.71 ms/it, loss 0.479244
Finished training it 14336/76743 of epoch 2, 73.85 ms/it, loss 0.476537
Finished training it 14336/76743 of epoch 2, 73.90 ms/it, loss 0.476369
Finished training it 14336/76743 of epoch 2, 74.06 ms/it, loss 0.476129
Finished training it 15360/76743 of epoch 2, 63.85 ms/it, loss 0.475332
Finished training it 15360/76743 of epoch 2, 64.08 ms/it, loss 0.477796
Finished training it 15360/76743 of epoch 2, 63.97 ms/it, loss 0.474743
Finished training it 15360/76743 of epoch 2, 63.66 ms/it, loss 0.473486
Finished training it 16384/76743 of epoch 2, 63.22 ms/it, loss 0.477589
Finished training it 16384/76743 of epoch 2, 63.21 ms/it, loss 0.475667
Finished training it 16384/76743 of epoch 2, 63.53 ms/it, loss 0.476943
Finished training it 16384/76743 of epoch 2, 63.37 ms/it, loss 0.477268
Finished training it 17408/76743 of epoch 2, 63.26 ms/it, loss 0.475247
Finished training it 17408/76743 of epoch 2, 63.62 ms/it, loss 0.475786
Finished training it 17408/76743 of epoch 2, 63.38 ms/it, loss 0.477288
Finished training it 17408/76743 of epoch 2, 63.20 ms/it, loss 0.477047
Finished training it 18432/76743 of epoch 2, 63.79 ms/it, loss 0.475703
Finished training it 18432/76743 of epoch 2, 63.53 ms/it, loss 0.478459
Finished training it 18432/76743 of epoch 2, 63.67 ms/it, loss 0.476756
Finished training it 18432/76743 of epoch 2, 63.50 ms/it, loss 0.475094
Finished training it 19456/76743 of epoch 2, 63.68 ms/it, loss 0.476774
Finished training it 19456/76743 of epoch 2, 63.26 ms/it, loss 0.474990
Finished training it 19456/76743 of epoch 2, 63.36 ms/it, loss 0.477029
Finished training it 19456/76743 of epoch 2, 63.34 ms/it, loss 0.478556
Finished training it 20480/76743 of epoch 2, 63.46 ms/it, loss 0.474306
Finished training it 20480/76743 of epoch 2, 63.64 ms/it, loss 0.476439
Finished training it 20480/76743 of epoch 2, 63.34 ms/it, loss 0.474886
Finished training it 20480/76743 of epoch 2, 63.31 ms/it, loss 0.475629
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2530535.0
get out
0 has test check 2530535.0 and sample count 3274240
 accuracy 77.286 %, best 77.307 %, roc auc score 0.7650, best 0.7650
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2530535.0
get out
3 has test check 2530535.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 63.47 ms/it, loss 0.475559
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2530535.0
get out
2 has test check 2530535.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 63.72 ms/it, loss 0.477378
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2530535.0
get out
1 has test check 2530535.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 63.50 ms/it, loss 0.475521
Finished training it 21504/76743 of epoch 2, 63.53 ms/it, loss 0.473925
Finished training it 22528/76743 of epoch 2, 63.20 ms/it, loss 0.477178
Finished training it 22528/76743 of epoch 2, 63.16 ms/it, loss 0.474972
Finished training it 22528/76743 of epoch 2, 63.12 ms/it, loss 0.473327
Finished training it 22528/76743 of epoch 2, 63.39 ms/it, loss 0.475914
Finished training it 23552/76743 of epoch 2, 63.29 ms/it, loss 0.478342
Finished training it 23552/76743 of epoch 2, 63.13 ms/it, loss 0.475590
Finished training it 23552/76743 of epoch 2, 63.14 ms/it, loss 0.473258
Finished training it 23552/76743 of epoch 2, 63.05 ms/it, loss 0.476241
Finished training it 24576/76743 of epoch 2, 63.32 ms/it, loss 0.476279
Finished training it 24576/76743 of epoch 2, 63.24 ms/it, loss 0.474782
Finished training it 24576/76743 of epoch 2, 63.14 ms/it, loss 0.476336
Finished training it 24576/76743 of epoch 2, 63.43 ms/it, loss 0.477223
Finished training it 25600/76743 of epoch 2, 63.79 ms/it, loss 0.476536
Finished training it 25600/76743 of epoch 2, 64.06 ms/it, loss 0.477846
Finished training it 25600/76743 of epoch 2, 63.77 ms/it, loss 0.475155
Finished training it 25600/76743 of epoch 2, 63.94 ms/it, loss 0.475947
Finished training it 26624/76743 of epoch 2, 63.82 ms/it, loss 0.475500
Finished training it 26624/76743 of epoch 2, 63.47 ms/it, loss 0.475451
Finished training it 26624/76743 of epoch 2, 63.50 ms/it, loss 0.475309
Finished training it 26624/76743 of epoch 2, 63.51 ms/it, loss 0.476250
Finished training it 27648/76743 of epoch 2, 63.90 ms/it, loss 0.478892
Finished training it 27648/76743 of epoch 2, 63.76 ms/it, loss 0.476606
Finished training it 27648/76743 of epoch 2, 63.55 ms/it, loss 0.476848
Finished training it 27648/76743 of epoch 2, 63.55 ms/it, loss 0.474844
Finished training it 28672/76743 of epoch 2, 63.21 ms/it, loss 0.477774
Finished training it 28672/76743 of epoch 2, 62.93 ms/it, loss 0.474776
Finished training it 28672/76743 of epoch 2, 63.50 ms/it, loss 0.474950
Finished training it 28672/76743 of epoch 2, 63.34 ms/it, loss 0.476130
Finished training it 29696/76743 of epoch 2, 63.59 ms/it, loss 0.473230
Finished training it 29696/76743 of epoch 2, 63.81 ms/it, loss 0.473208
Finished training it 29696/76743 of epoch 2, 63.50 ms/it, loss 0.476528
Finished training it 29696/76743 of epoch 2, 63.40 ms/it, loss 0.473263
Finished training it 30720/76743 of epoch 2, 63.55 ms/it, loss 0.475442
Finished training it 30720/76743 of epoch 2, 63.40 ms/it, loss 0.475170
Finished training it 30720/76743 of epoch 2, 63.47 ms/it, loss 0.475197
Finished training it 30720/76743 of epoch 2, 63.79 ms/it, loss 0.473375
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533350.0
get out
0 has test check 2533350.0 and sample count 3274240
 accuracy 77.372 %, best 77.372 %, roc auc score 0.7663, best 0.7663
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 63.50 ms/it, loss 0.475786
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533350.0
get out
3 has test check 2533350.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 63.61 ms/it, loss 0.475430
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533350.0
get out
1 has test check 2533350.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 63.58 ms/it, loss 0.476580
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533350.0
get out
2 has test check 2533350.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 63.85 ms/it, loss 0.474918
Finished training it 32768/76743 of epoch 2, 63.99 ms/it, loss 0.475495
Finished training it 32768/76743 of epoch 2, 63.79 ms/it, loss 0.472970
Finished training it 32768/76743 of epoch 2, 63.67 ms/it, loss 0.473942
Finished training it 32768/76743 of epoch 2, 63.52 ms/it, loss 0.476585
Finished training it 33792/76743 of epoch 2, 64.07 ms/it, loss 0.472439
Finished training it 33792/76743 of epoch 2, 63.85 ms/it, loss 0.475733
Finished training it 33792/76743 of epoch 2, 63.76 ms/it, loss 0.475215
Finished training it 33792/76743 of epoch 2, 63.88 ms/it, loss 0.472071
Finished training it 34816/76743 of epoch 2, 68.19 ms/it, loss 0.477037
Finished training it 34816/76743 of epoch 2, 68.71 ms/it, loss 0.475911
Finished training it 34816/76743 of epoch 2, 68.63 ms/it, loss 0.472637
Finished training it 34816/76743 of epoch 2, 68.83 ms/it, loss 0.474862
Finished training it 35840/76743 of epoch 2, 68.28 ms/it, loss 0.476586
Finished training it 35840/76743 of epoch 2, 68.37 ms/it, loss 0.474581
Finished training it 35840/76743 of epoch 2, 68.52 ms/it, loss 0.474295
Finished training it 35840/76743 of epoch 2, 68.68 ms/it, loss 0.474372
Finished training it 36864/76743 of epoch 2, 63.48 ms/it, loss 0.475793
Finished training it 36864/76743 of epoch 2, 63.88 ms/it, loss 0.475008
Finished training it 36864/76743 of epoch 2, 63.63 ms/it, loss 0.473700
Finished training it 36864/76743 of epoch 2, 63.62 ms/it, loss 0.473842
Finished training it 37888/76743 of epoch 2, 63.39 ms/it, loss 0.475865
Finished training it 37888/76743 of epoch 2, 63.34 ms/it, loss 0.474165
Finished training it 37888/76743 of epoch 2, 63.47 ms/it, loss 0.473375
Finished training it 37888/76743 of epoch 2, 63.65 ms/it, loss 0.476747
Finished training it 38912/76743 of epoch 2, 63.41 ms/it, loss 0.474277
Finished training it 38912/76743 of epoch 2, 63.32 ms/it, loss 0.475757
Finished training it 38912/76743 of epoch 2, 63.62 ms/it, loss 0.473619
Finished training it 38912/76743 of epoch 2, 63.30 ms/it, loss 0.477899
Finished training it 39936/76743 of epoch 2, 63.27 ms/it, loss 0.475199
Finished training it 39936/76743 of epoch 2, 63.53 ms/it, loss 0.474886
Finished training it 39936/76743 of epoch 2, 63.40 ms/it, loss 0.476294
Finished training it 39936/76743 of epoch 2, 63.14 ms/it, loss 0.476598
Finished training it 40960/76743 of epoch 2, 63.43 ms/it, loss 0.474363
Finished training it 40960/76743 of epoch 2, 63.64 ms/it, loss 0.473430
Finished training it 40960/76743 of epoch 2, 63.43 ms/it, loss 0.476008
Finished training it 40960/76743 of epoch 2, 63.47 ms/it, loss 0.473996
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535729.0
get out
0 has test check 2535729.0 and sample count 3274240
 accuracy 77.445 %, best 77.445 %, roc auc score 0.7676, best 0.7676
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 63.40 ms/it, loss 0.474142
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535729.0
get out
2 has test check 2535729.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 63.68 ms/it, loss 0.474381
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535729.0
get out
3 has test check 2535729.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 63.37 ms/it, loss 0.473966
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535729.0
get out
1 has test check 2535729.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 63.50 ms/it, loss 0.472996
Finished training it 43008/76743 of epoch 2, 63.60 ms/it, loss 0.469998
Finished training it 43008/76743 of epoch 2, 63.37 ms/it, loss 0.472749
Finished training it 43008/76743 of epoch 2, 63.36 ms/it, loss 0.477545
Finished training it 43008/76743 of epoch 2, 63.28 ms/it, loss 0.474105
Finished training it 44032/76743 of epoch 2, 63.69 ms/it, loss 0.477860
Finished training it 44032/76743 of epoch 2, 63.52 ms/it, loss 0.471672
Finished training it 44032/76743 of epoch 2, 63.27 ms/it, loss 0.472941
Finished training it 44032/76743 of epoch 2, 63.31 ms/it, loss 0.474248
Finished training it 45056/76743 of epoch 2, 63.67 ms/it, loss 0.476439
Finished training it 45056/76743 of epoch 2, 63.48 ms/it, loss 0.473493
Finished training it 45056/76743 of epoch 2, 63.60 ms/it, loss 0.473842
Finished training it 45056/76743 of epoch 2, 63.83 ms/it, loss 0.472976
Finished training it 46080/76743 of epoch 2, 63.22 ms/it, loss 0.473415
Finished training it 46080/76743 of epoch 2, 63.56 ms/it, loss 0.474977
Finished training it 46080/76743 of epoch 2, 63.26 ms/it, loss 0.475173
Finished training it 46080/76743 of epoch 2, 63.18 ms/it, loss 0.474134
Finished training it 47104/76743 of epoch 2, 63.52 ms/it, loss 0.477065
Finished training it 47104/76743 of epoch 2, 63.61 ms/it, loss 0.476613
Finished training it 47104/76743 of epoch 2, 63.77 ms/it, loss 0.472416
Finished training it 47104/76743 of epoch 2, 63.41 ms/it, loss 0.474352
Finished training it 48128/76743 of epoch 2, 63.49 ms/it, loss 0.473846
Finished training it 48128/76743 of epoch 2, 63.42 ms/it, loss 0.472595
Finished training it 48128/76743 of epoch 2, 63.48 ms/it, loss 0.473769
Finished training it 48128/76743 of epoch 2, 63.69 ms/it, loss 0.477517
Finished training it 49152/76743 of epoch 2, 63.50 ms/it, loss 0.474884
Finished training it 49152/76743 of epoch 2, 63.48 ms/it, loss 0.474265
Finished training it 49152/76743 of epoch 2, 63.80 ms/it, loss 0.474729
Finished training it 49152/76743 of epoch 2, 63.61 ms/it, loss 0.473563
Finished training it 50176/76743 of epoch 2, 63.57 ms/it, loss 0.472805
Finished training it 50176/76743 of epoch 2, 63.70 ms/it, loss 0.474212
Finished training it 50176/76743 of epoch 2, 63.57 ms/it, loss 0.473315
Finished training it 50176/76743 of epoch 2, 63.40 ms/it, loss 0.472415
Finished training it 51200/76743 of epoch 2, 63.20 ms/it, loss 0.473873
Finished training it 51200/76743 of epoch 2, 63.68 ms/it, loss 0.472750
Finished training it 51200/76743 of epoch 2, 63.34 ms/it, loss 0.475866
Finished training it 51200/76743 of epoch 2, 63.38 ms/it, loss 0.475287
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533306.0
get out
0 has test check 2533306.0 and sample count 3274240
 accuracy 77.371 %, best 77.445 %, roc auc score 0.7671, best 0.7676
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533306.0
get out
2 has test check 2533306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 63.70 ms/it, loss 0.472595
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533306.0
get out
1 has test check 2533306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 63.51 ms/it, loss 0.472515
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533306.0
get out
3 has test check 2533306.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 63.37 ms/it, loss 0.474851
Finished training it 52224/76743 of epoch 2, 63.49 ms/it, loss 0.474537
Finished training it 53248/76743 of epoch 2, 63.84 ms/it, loss 0.470838
Finished training it 53248/76743 of epoch 2, 63.66 ms/it, loss 0.473815
Finished training it 53248/76743 of epoch 2, 63.65 ms/it, loss 0.472217
Finished training it 53248/76743 of epoch 2, 63.48 ms/it, loss 0.473625
Finished training it 54272/76743 of epoch 2, 63.50 ms/it, loss 0.473017
Finished training it 54272/76743 of epoch 2, 63.43 ms/it, loss 0.474033
Finished training it 54272/76743 of epoch 2, 63.26 ms/it, loss 0.475990
Finished training it 54272/76743 of epoch 2, 63.72 ms/it, loss 0.473283
Finished training it 55296/76743 of epoch 2, 68.56 ms/it, loss 0.471665
Finished training it 55296/76743 of epoch 2, 68.85 ms/it, loss 0.474474
Finished training it 55296/76743 of epoch 2, 68.42 ms/it, loss 0.472413
Finished training it 55296/76743 of epoch 2, 68.54 ms/it, loss 0.478053
Finished training it 56320/76743 of epoch 2, 69.33 ms/it, loss 0.478294
Finished training it 56320/76743 of epoch 2, 69.13 ms/it, loss 0.475671
Finished training it 56320/76743 of epoch 2, 69.38 ms/it, loss 0.471773
Finished training it 56320/76743 of epoch 2, 69.09 ms/it, loss 0.472537
Finished training it 57344/76743 of epoch 2, 64.08 ms/it, loss 0.474110
Finished training it 57344/76743 of epoch 2, 63.78 ms/it, loss 0.472256
Finished training it 57344/76743 of epoch 2, 63.78 ms/it, loss 0.472181
Finished training it 57344/76743 of epoch 2, 63.75 ms/it, loss 0.472498
Finished training it 58368/76743 of epoch 2, 63.39 ms/it, loss 0.472364
Finished training it 58368/76743 of epoch 2, 63.55 ms/it, loss 0.474253
Finished training it 58368/76743 of epoch 2, 63.30 ms/it, loss 0.473612
Finished training it 58368/76743 of epoch 2, 63.38 ms/it, loss 0.475436
Finished training it 59392/76743 of epoch 2, 63.47 ms/it, loss 0.471953
Finished training it 59392/76743 of epoch 2, 63.74 ms/it, loss 0.474473
Finished training it 59392/76743 of epoch 2, 63.52 ms/it, loss 0.472216
Finished training it 59392/76743 of epoch 2, 63.49 ms/it, loss 0.471980
Finished training it 60416/76743 of epoch 2, 63.58 ms/it, loss 0.472782
Finished training it 60416/76743 of epoch 2, 63.48 ms/it, loss 0.471515
Finished training it 60416/76743 of epoch 2, 63.85 ms/it, loss 0.474036
Finished training it 60416/76743 of epoch 2, 63.43 ms/it, loss 0.474203
Finished training it 61440/76743 of epoch 2, 63.75 ms/it, loss 0.473703
Finished training it 61440/76743 of epoch 2, 63.64 ms/it, loss 0.472065
Finished training it 61440/76743 of epoch 2, 63.92 ms/it, loss 0.475282
Finished training it 61440/76743 of epoch 2, 63.62 ms/it, loss 0.474439
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537369.0
get out
0 has test check 2537369.0 and sample count 3274240
 accuracy 77.495 %, best 77.495 %, roc auc score 0.7687, best 0.7687
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537369.0
get out
1 has test check 2537369.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 63.26 ms/it, loss 0.474658
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537369.0
get out
2 has test check 2537369.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 63.69 ms/it, loss 0.474795
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 63.33 ms/it, loss 0.473072
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537369.0
get out
3 has test check 2537369.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 63.37 ms/it, loss 0.474205
Finished training it 63488/76743 of epoch 2, 64.24 ms/it, loss 0.472433
Finished training it 63488/76743 of epoch 2, 63.87 ms/it, loss 0.476585
Finished training it 63488/76743 of epoch 2, 63.96 ms/it, loss 0.471767
Finished training it 63488/76743 of epoch 2, 64.05 ms/it, loss 0.472147
Finished training it 64512/76743 of epoch 2, 63.59 ms/it, loss 0.470312
Finished training it 64512/76743 of epoch 2, 63.27 ms/it, loss 0.474106
Finished training it 64512/76743 of epoch 2, 63.28 ms/it, loss 0.473610
Finished training it 64512/76743 of epoch 2, 63.37 ms/it, loss 0.475474
Finished training it 65536/76743 of epoch 2, 63.42 ms/it, loss 0.473288
Finished training it 65536/76743 of epoch 2, 63.59 ms/it, loss 0.473555
Finished training it 65536/76743 of epoch 2, 63.33 ms/it, loss 0.472375
Finished training it 65536/76743 of epoch 2, 63.48 ms/it, loss 0.470963
Finished training it 66560/76743 of epoch 2, 63.67 ms/it, loss 0.472638
Finished training it 66560/76743 of epoch 2, 63.38 ms/it, loss 0.471661
Finished training it 66560/76743 of epoch 2, 63.56 ms/it, loss 0.473972
Finished training it 66560/76743 of epoch 2, 63.38 ms/it, loss 0.472134
Finished training it 67584/76743 of epoch 2, 63.66 ms/it, loss 0.476142
Finished training it 67584/76743 of epoch 2, 63.59 ms/it, loss 0.472579
Finished training it 67584/76743 of epoch 2, 63.73 ms/it, loss 0.472557
Finished training it 67584/76743 of epoch 2, 63.60 ms/it, loss 0.473828
Finished training it 68608/76743 of epoch 2, 62.97 ms/it, loss 0.472662
Finished training it 68608/76743 of epoch 2, 62.88 ms/it, loss 0.473238
Finished training it 68608/76743 of epoch 2, 63.15 ms/it, loss 0.469716
Finished training it 68608/76743 of epoch 2, 62.94 ms/it, loss 0.471329
Finished training it 69632/76743 of epoch 2, 64.69 ms/it, loss 0.472294
Finished training it 69632/76743 of epoch 2, 64.80 ms/it, loss 0.472419
Finished training it 69632/76743 of epoch 2, 64.76 ms/it, loss 0.472759
Finished training it 69632/76743 of epoch 2, 64.89 ms/it, loss 0.472788
Finished training it 70656/76743 of epoch 2, 64.31 ms/it, loss 0.469320
Finished training it 70656/76743 of epoch 2, 64.36 ms/it, loss 0.474326
Finished training it 70656/76743 of epoch 2, 64.26 ms/it, loss 0.473084
Finished training it 70656/76743 of epoch 2, 64.31 ms/it, loss 0.472944
Finished training it 71680/76743 of epoch 2, 63.34 ms/it, loss 0.471788
Finished training it 71680/76743 of epoch 2, 63.67 ms/it, loss 0.471839
Finished training it 71680/76743 of epoch 2, 63.48 ms/it, loss 0.471630
Finished training it 71680/76743 of epoch 2, 63.45 ms/it, loss 0.475189
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535441.0
get out
0 has test check 2535441.0 and sample count 3274240
 accuracy 77.436 %, best 77.495 %, roc auc score 0.7683, best 0.7687
Finished training it 72704/76743 of epoch 2, 63.76 ms/it, loss 0.472918
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535441.0
get out
2 has test check 2535441.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 63.98 ms/it, loss 0.472537
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535441.0
get out
1 has test check 2535441.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 63.64 ms/it, loss 0.470328
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535441.0
get out
3 has test check 2535441.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 63.68 ms/it, loss 0.476127
Finished training it 73728/76743 of epoch 2, 64.03 ms/it, loss 0.471550
Finished training it 73728/76743 of epoch 2, 63.77 ms/it, loss 0.472915
Finished training it 73728/76743 of epoch 2, 63.89 ms/it, loss 0.472656
Finished training it 73728/76743 of epoch 2, 63.83 ms/it, loss 0.472543
Finished training it 74752/76743 of epoch 2, 64.46 ms/it, loss 0.472003
Finished training it 74752/76743 of epoch 2, 64.26 ms/it, loss 0.474125
Finished training it 74752/76743 of epoch 2, 64.47 ms/it, loss 0.472770
Finished training it 74752/76743 of epoch 2, 64.55 ms/it, loss 0.472173
Finished training it 75776/76743 of epoch 2, 75.27 ms/it, loss 0.473602
Finished training it 75776/76743 of epoch 2, 75.10 ms/it, loss 0.473805
Finished training it 75776/76743 of epoch 2, 75.24 ms/it, loss 0.470033
Finished training it 75776/76743 of epoch 2, 75.22 ms/it, loss 0.471689
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 70.91 ms/it, loss 0.471238
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.39 ms/it, loss 0.472627
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.28 ms/it, loss 0.471892
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 72.53 ms/it, loss 0.472396
Finished training it 2048/76743 of epoch 3, 78.76 ms/it, loss 0.471252
Finished training it 2048/76743 of epoch 3, 78.75 ms/it, loss 0.471969
Finished training it 2048/76743 of epoch 3, 78.07 ms/it, loss 0.471786
Finished training it 2048/76743 of epoch 3, 78.77 ms/it, loss 0.472723
Finished training it 3072/76743 of epoch 3, 80.45 ms/it, loss 0.472288
Finished training it 3072/76743 of epoch 3, 80.46 ms/it, loss 0.471272
Finished training it 3072/76743 of epoch 3, 80.44 ms/it, loss 0.473790
Finished training it 3072/76743 of epoch 3, 80.45 ms/it, loss 0.473282
Finished training it 4096/76743 of epoch 3, 66.50 ms/it, loss 0.473913
Finished training it 4096/76743 of epoch 3, 66.52 ms/it, loss 0.473596
Finished training it 4096/76743 of epoch 3, 66.44 ms/it, loss 0.472108
Finished training it 4096/76743 of epoch 3, 66.60 ms/it, loss 0.471455
Finished training it 5120/76743 of epoch 3, 63.67 ms/it, loss 0.475064
Finished training it 5120/76743 of epoch 3, 64.01 ms/it, loss 0.473777
Finished training it 5120/76743 of epoch 3, 63.88 ms/it, loss 0.473754
Finished training it 5120/76743 of epoch 3, 63.88 ms/it, loss 0.472923
Finished training it 6144/76743 of epoch 3, 64.05 ms/it, loss 0.475045
Finished training it 6144/76743 of epoch 3, 64.12 ms/it, loss 0.472217
Finished training it 6144/76743 of epoch 3, 64.22 ms/it, loss 0.472284
Finished training it 6144/76743 of epoch 3, 64.15 ms/it, loss 0.471127
Finished training it 7168/76743 of epoch 3, 63.88 ms/it, loss 0.472879
Finished training it 7168/76743 of epoch 3, 63.96 ms/it, loss 0.473719
Finished training it 7168/76743 of epoch 3, 63.93 ms/it, loss 0.470802
Finished training it 7168/76743 of epoch 3, 63.97 ms/it, loss 0.472976
Finished training it 8192/76743 of epoch 3, 63.76 ms/it, loss 0.473754
Finished training it 8192/76743 of epoch 3, 63.67 ms/it, loss 0.471987
Finished training it 8192/76743 of epoch 3, 63.78 ms/it, loss 0.472471
Finished training it 8192/76743 of epoch 3, 63.87 ms/it, loss 0.472045
Finished training it 9216/76743 of epoch 3, 65.26 ms/it, loss 0.474467
Finished training it 9216/76743 of epoch 3, 65.39 ms/it, loss 0.473234
Finished training it 9216/76743 of epoch 3, 65.15 ms/it, loss 0.472336
Finished training it 9216/76743 of epoch 3, 65.20 ms/it, loss 0.470609
Finished training it 10240/76743 of epoch 3, 65.19 ms/it, loss 0.469675
Finished training it 10240/76743 of epoch 3, 65.22 ms/it, loss 0.475065
Finished training it 10240/76743 of epoch 3, 65.25 ms/it, loss 0.470353
Finished training it 10240/76743 of epoch 3, 65.16 ms/it, loss 0.472564
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2538168.0
get out
0 has test check 2538168.0 and sample count 3274240
 accuracy 77.519 %, best 77.519 %, roc auc score 0.7704, best 0.7704
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2538168.0
get out
3 has test check 2538168.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.52 ms/it, loss 0.470052
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2538168.0
get out
2 has test check 2538168.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.63 ms/it, loss 0.474064
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2538168.0
get out
1 has test check 2538168.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.54 ms/it, loss 0.469070
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 81.50 ms/it, loss 0.471995
Finished training it 12288/76743 of epoch 3, 74.51 ms/it, loss 0.473428
Finished training it 12288/76743 of epoch 3, 74.30 ms/it, loss 0.472198
Finished training it 12288/76743 of epoch 3, 74.35 ms/it, loss 0.470290
Finished training it 12288/76743 of epoch 3, 74.26 ms/it, loss 0.470971
Finished training it 13312/76743 of epoch 3, 65.36 ms/it, loss 0.474534
Finished training it 13312/76743 of epoch 3, 65.35 ms/it, loss 0.472667
Finished training it 13312/76743 of epoch 3, 65.30 ms/it, loss 0.470710
Finished training it 13312/76743 of epoch 3, 65.56 ms/it, loss 0.472014
Finished training it 14336/76743 of epoch 3, 65.17 ms/it, loss 0.471297
Finished training it 14336/76743 of epoch 3, 65.11 ms/it, loss 0.471097
Finished training it 14336/76743 of epoch 3, 65.05 ms/it, loss 0.473851
Finished training it 14336/76743 of epoch 3, 65.08 ms/it, loss 0.471379
Finished training it 15360/76743 of epoch 3, 77.51 ms/it, loss 0.470573
Finished training it 15360/76743 of epoch 3, 77.43 ms/it, loss 0.468542
Finished training it 15360/76743 of epoch 3, 77.51 ms/it, loss 0.469439
Finished training it 15360/76743 of epoch 3, 77.51 ms/it, loss 0.473512
Finished training it 16384/76743 of epoch 3, 83.34 ms/it, loss 0.472589
Finished training it 16384/76743 of epoch 3, 83.32 ms/it, loss 0.471457
Finished training it 16384/76743 of epoch 3, 83.34 ms/it, loss 0.472256
Finished training it 16384/76743 of epoch 3, 83.42 ms/it, loss 0.472111
Finished training it 17408/76743 of epoch 3, 63.67 ms/it, loss 0.471676
Finished training it 17408/76743 of epoch 3, 63.98 ms/it, loss 0.471011
Finished training it 17408/76743 of epoch 3, 63.83 ms/it, loss 0.471676
Finished training it 17408/76743 of epoch 3, 63.83 ms/it, loss 0.470183
Finished training it 18432/76743 of epoch 3, 63.66 ms/it, loss 0.471576
Finished training it 18432/76743 of epoch 3, 63.87 ms/it, loss 0.473008
Finished training it 18432/76743 of epoch 3, 64.09 ms/it, loss 0.470603
Finished training it 18432/76743 of epoch 3, 63.85 ms/it, loss 0.470700
Finished training it 19456/76743 of epoch 3, 63.30 ms/it, loss 0.472386
Finished training it 19456/76743 of epoch 3, 63.12 ms/it, loss 0.474010
Finished training it 19456/76743 of epoch 3, 63.51 ms/it, loss 0.472316
Finished training it 19456/76743 of epoch 3, 63.25 ms/it, loss 0.470324
Finished training it 20480/76743 of epoch 3, 63.85 ms/it, loss 0.470369
Finished training it 20480/76743 of epoch 3, 63.73 ms/it, loss 0.469868
Finished training it 20480/76743 of epoch 3, 64.06 ms/it, loss 0.471752
Finished training it 20480/76743 of epoch 3, 63.80 ms/it, loss 0.469590
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537956.0
get out
0 has test check 2537956.0 and sample count 3274240
 accuracy 77.513 %, best 77.519 %, roc auc score 0.7700, best 0.7704
Finished training it 21504/76743 of epoch 3, 63.24 ms/it, loss 0.469055
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537956.0
get out
1 has test check 2537956.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 63.28 ms/it, loss 0.470504
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537956.0
get out
3 has test check 2537956.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 63.24 ms/it, loss 0.471279
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537956.0
get out
2 has test check 2537956.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 63.54 ms/it, loss 0.472706
Finished training it 22528/76743 of epoch 3, 63.64 ms/it, loss 0.469021
Finished training it 22528/76743 of epoch 3, 63.93 ms/it, loss 0.470892
Finished training it 22528/76743 of epoch 3, 63.54 ms/it, loss 0.472066
Finished training it 22528/76743 of epoch 3, 63.57 ms/it, loss 0.470199
Finished training it 23552/76743 of epoch 3, 63.81 ms/it, loss 0.471684
Finished training it 23552/76743 of epoch 3, 63.78 ms/it, loss 0.468574
Finished training it 23552/76743 of epoch 3, 64.12 ms/it, loss 0.473575
Finished training it 23552/76743 of epoch 3, 63.74 ms/it, loss 0.470492
Finished training it 24576/76743 of epoch 3, 74.42 ms/it, loss 0.471947
Finished training it 24576/76743 of epoch 3, 74.12 ms/it, loss 0.470510
Finished training it 24576/76743 of epoch 3, 74.09 ms/it, loss 0.471565
Finished training it 24576/76743 of epoch 3, 74.08 ms/it, loss 0.470386
Finished training it 25600/76743 of epoch 3, 64.29 ms/it, loss 0.473049
Finished training it 25600/76743 of epoch 3, 64.04 ms/it, loss 0.470821
Finished training it 25600/76743 of epoch 3, 63.96 ms/it, loss 0.471847
Finished training it 25600/76743 of epoch 3, 63.95 ms/it, loss 0.471039
Finished training it 26624/76743 of epoch 3, 64.24 ms/it, loss 0.471874
Finished training it 26624/76743 of epoch 3, 64.22 ms/it, loss 0.470552
Finished training it 26624/76743 of epoch 3, 64.19 ms/it, loss 0.470671
Finished training it 26624/76743 of epoch 3, 64.35 ms/it, loss 0.470494
Finished training it 27648/76743 of epoch 3, 63.67 ms/it, loss 0.472910
Finished training it 27648/76743 of epoch 3, 63.86 ms/it, loss 0.470607
Finished training it 27648/76743 of epoch 3, 64.06 ms/it, loss 0.475021
Finished training it 27648/76743 of epoch 3, 63.81 ms/it, loss 0.471681
Finished training it 28672/76743 of epoch 3, 64.07 ms/it, loss 0.473113
Finished training it 28672/76743 of epoch 3, 64.00 ms/it, loss 0.472183
Finished training it 28672/76743 of epoch 3, 64.21 ms/it, loss 0.470525
Finished training it 28672/76743 of epoch 3, 63.94 ms/it, loss 0.470252
Finished training it 29696/76743 of epoch 3, 63.91 ms/it, loss 0.468376
Finished training it 29696/76743 of epoch 3, 63.76 ms/it, loss 0.468620
Finished training it 29696/76743 of epoch 3, 63.70 ms/it, loss 0.471919
Finished training it 29696/76743 of epoch 3, 63.79 ms/it, loss 0.468903
Finished training it 30720/76743 of epoch 3, 63.93 ms/it, loss 0.470783
Finished training it 30720/76743 of epoch 3, 64.08 ms/it, loss 0.468842
Finished training it 30720/76743 of epoch 3, 63.78 ms/it, loss 0.471159
Finished training it 30720/76743 of epoch 3, 63.96 ms/it, loss 0.471156
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541043.0
get out
0 has test check 2541043.0 and sample count 3274240
 accuracy 77.607 %, best 77.607 %, roc auc score 0.7721, best 0.7721
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 63.39 ms/it, loss 0.471267
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541043.0
get out
1 has test check 2541043.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 63.42 ms/it, loss 0.472221
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541043.0
get out
2 has test check 2541043.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 63.57 ms/it, loss 0.470465
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541043.0
get out
3 has test check 2541043.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 63.43 ms/it, loss 0.470662
Finished training it 32768/76743 of epoch 3, 63.54 ms/it, loss 0.468990
Finished training it 32768/76743 of epoch 3, 63.79 ms/it, loss 0.470660
Finished training it 32768/76743 of epoch 3, 63.56 ms/it, loss 0.471836
Finished training it 32768/76743 of epoch 3, 63.57 ms/it, loss 0.468660
Finished training it 33792/76743 of epoch 3, 63.74 ms/it, loss 0.467702
Finished training it 33792/76743 of epoch 3, 64.02 ms/it, loss 0.467405
Finished training it 33792/76743 of epoch 3, 63.77 ms/it, loss 0.470774
Finished training it 33792/76743 of epoch 3, 63.72 ms/it, loss 0.470901
Finished training it 34816/76743 of epoch 3, 63.86 ms/it, loss 0.471327
Finished training it 34816/76743 of epoch 3, 63.62 ms/it, loss 0.467776
Finished training it 34816/76743 of epoch 3, 63.65 ms/it, loss 0.470680
Finished training it 34816/76743 of epoch 3, 63.57 ms/it, loss 0.473105
Finished training it 35840/76743 of epoch 3, 63.59 ms/it, loss 0.472569
Finished training it 35840/76743 of epoch 3, 63.42 ms/it, loss 0.470203
Finished training it 35840/76743 of epoch 3, 63.59 ms/it, loss 0.470319
Finished training it 35840/76743 of epoch 3, 63.81 ms/it, loss 0.469816
Finished training it 36864/76743 of epoch 3, 63.46 ms/it, loss 0.469430
Finished training it 36864/76743 of epoch 3, 63.32 ms/it, loss 0.471610
Finished training it 36864/76743 of epoch 3, 63.78 ms/it, loss 0.471026
Finished training it 36864/76743 of epoch 3, 63.27 ms/it, loss 0.469301
Finished training it 37888/76743 of epoch 3, 63.78 ms/it, loss 0.472381
Finished training it 37888/76743 of epoch 3, 63.56 ms/it, loss 0.469991
Finished training it 37888/76743 of epoch 3, 63.39 ms/it, loss 0.469135
Finished training it 37888/76743 of epoch 3, 63.45 ms/it, loss 0.471695
Finished training it 38912/76743 of epoch 3, 63.39 ms/it, loss 0.469575
Finished training it 38912/76743 of epoch 3, 63.12 ms/it, loss 0.473276
Finished training it 38912/76743 of epoch 3, 63.26 ms/it, loss 0.470263
Finished training it 38912/76743 of epoch 3, 63.27 ms/it, loss 0.471279
Finished training it 39936/76743 of epoch 3, 63.35 ms/it, loss 0.470355
Finished training it 39936/76743 of epoch 3, 63.56 ms/it, loss 0.470944
Finished training it 39936/76743 of epoch 3, 63.32 ms/it, loss 0.471648
Finished training it 39936/76743 of epoch 3, 63.29 ms/it, loss 0.472538
Finished training it 40960/76743 of epoch 3, 63.58 ms/it, loss 0.470463
Finished training it 40960/76743 of epoch 3, 63.57 ms/it, loss 0.471762
Finished training it 40960/76743 of epoch 3, 63.73 ms/it, loss 0.468824
Finished training it 40960/76743 of epoch 3, 63.54 ms/it, loss 0.469706
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541245.0
get out
0 has test check 2541245.0 and sample count 3274240
 accuracy 77.613 %, best 77.613 %, roc auc score 0.7723, best 0.7723
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541245.0
get out
2 has test check 2541245.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 63.83 ms/it, loss 0.470578
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541245.0
get out
3 has test check 2541245.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 63.61 ms/it, loss 0.469599
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541245.0
get out
1 has test check 2541245.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 63.73 ms/it, loss 0.469034
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 63.70 ms/it, loss 0.470264
Finished training it 43008/76743 of epoch 3, 63.51 ms/it, loss 0.468635
Finished training it 43008/76743 of epoch 3, 63.83 ms/it, loss 0.466005
Finished training it 43008/76743 of epoch 3, 63.63 ms/it, loss 0.473847
Finished training it 43008/76743 of epoch 3, 63.40 ms/it, loss 0.470651
Finished training it 44032/76743 of epoch 3, 64.10 ms/it, loss 0.470270
Finished training it 44032/76743 of epoch 3, 64.21 ms/it, loss 0.473890
Finished training it 44032/76743 of epoch 3, 64.03 ms/it, loss 0.469296
Finished training it 44032/76743 of epoch 3, 63.68 ms/it, loss 0.467942
Finished training it 45056/76743 of epoch 3, 69.02 ms/it, loss 0.469903
Finished training it 45056/76743 of epoch 3, 69.13 ms/it, loss 0.472558
Finished training it 45056/76743 of epoch 3, 69.53 ms/it, loss 0.470225
Finished training it 45056/76743 of epoch 3, 69.29 ms/it, loss 0.469320
Finished training it 46080/76743 of epoch 3, 68.67 ms/it, loss 0.469683
Finished training it 46080/76743 of epoch 3, 69.08 ms/it, loss 0.470934
Finished training it 46080/76743 of epoch 3, 68.48 ms/it, loss 0.470153
Finished training it 46080/76743 of epoch 3, 68.78 ms/it, loss 0.471557
Finished training it 47104/76743 of epoch 3, 63.77 ms/it, loss 0.468848
Finished training it 47104/76743 of epoch 3, 63.60 ms/it, loss 0.471991
Finished training it 47104/76743 of epoch 3, 63.52 ms/it, loss 0.469724
Finished training it 47104/76743 of epoch 3, 63.53 ms/it, loss 0.472453
Finished training it 48128/76743 of epoch 3, 63.50 ms/it, loss 0.469686
Finished training it 48128/76743 of epoch 3, 63.53 ms/it, loss 0.468071
Finished training it 48128/76743 of epoch 3, 63.49 ms/it, loss 0.469127
Finished training it 48128/76743 of epoch 3, 63.72 ms/it, loss 0.473138
Finished training it 49152/76743 of epoch 3, 63.47 ms/it, loss 0.469111
Finished training it 49152/76743 of epoch 3, 63.55 ms/it, loss 0.470547
Finished training it 49152/76743 of epoch 3, 63.84 ms/it, loss 0.470539
Finished training it 49152/76743 of epoch 3, 63.59 ms/it, loss 0.470815
Finished training it 50176/76743 of epoch 3, 63.44 ms/it, loss 0.467935
Finished training it 50176/76743 of epoch 3, 63.58 ms/it, loss 0.469240
Finished training it 50176/76743 of epoch 3, 63.40 ms/it, loss 0.468362
Finished training it 50176/76743 of epoch 3, 63.81 ms/it, loss 0.469414
Finished training it 51200/76743 of epoch 3, 63.50 ms/it, loss 0.471158
Finished training it 51200/76743 of epoch 3, 63.88 ms/it, loss 0.468722
Finished training it 51200/76743 of epoch 3, 63.58 ms/it, loss 0.469333
Finished training it 51200/76743 of epoch 3, 63.53 ms/it, loss 0.470928
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2539621.0
get out
0 has test check 2539621.0 and sample count 3274240
 accuracy 77.564 %, best 77.613 %, roc auc score 0.7728, best 0.7728
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2539621.0
get out
3 has test check 2539621.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 63.59 ms/it, loss 0.470925
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2539621.0
get out
2 has test check 2539621.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 63.81 ms/it, loss 0.468329
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2539621.0
get out
1 has test check 2539621.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 63.56 ms/it, loss 0.468244
Finished training it 52224/76743 of epoch 3, 63.59 ms/it, loss 0.470730
Finished training it 53248/76743 of epoch 3, 63.56 ms/it, loss 0.469988
Finished training it 53248/76743 of epoch 3, 63.52 ms/it, loss 0.470590
Finished training it 53248/76743 of epoch 3, 63.82 ms/it, loss 0.466793
Finished training it 53248/76743 of epoch 3, 63.68 ms/it, loss 0.468184
Finished training it 54272/76743 of epoch 3, 63.56 ms/it, loss 0.470246
Finished training it 54272/76743 of epoch 3, 63.63 ms/it, loss 0.469266
Finished training it 54272/76743 of epoch 3, 63.50 ms/it, loss 0.472417
Finished training it 54272/76743 of epoch 3, 63.86 ms/it, loss 0.469628
Finished training it 55296/76743 of epoch 3, 63.74 ms/it, loss 0.474294
Finished training it 55296/76743 of epoch 3, 63.85 ms/it, loss 0.468714
Finished training it 55296/76743 of epoch 3, 64.08 ms/it, loss 0.471246
Finished training it 55296/76743 of epoch 3, 63.82 ms/it, loss 0.467590
Finished training it 56320/76743 of epoch 3, 63.98 ms/it, loss 0.467936
Finished training it 56320/76743 of epoch 3, 63.75 ms/it, loss 0.468660
Finished training it 56320/76743 of epoch 3, 63.85 ms/it, loss 0.471631
Finished training it 56320/76743 of epoch 3, 63.65 ms/it, loss 0.473782
Finished training it 57344/76743 of epoch 3, 63.23 ms/it, loss 0.468301
Finished training it 57344/76743 of epoch 3, 63.29 ms/it, loss 0.468523
Finished training it 57344/76743 of epoch 3, 63.48 ms/it, loss 0.469859
Finished training it 57344/76743 of epoch 3, 63.27 ms/it, loss 0.468202
Finished training it 58368/76743 of epoch 3, 63.92 ms/it, loss 0.471809
Finished training it 58368/76743 of epoch 3, 64.03 ms/it, loss 0.470836
Finished training it 58368/76743 of epoch 3, 63.69 ms/it, loss 0.468276
Finished training it 58368/76743 of epoch 3, 63.78 ms/it, loss 0.468950
Finished training it 59392/76743 of epoch 3, 63.92 ms/it, loss 0.470381
Finished training it 59392/76743 of epoch 3, 63.62 ms/it, loss 0.468403
Finished training it 59392/76743 of epoch 3, 63.67 ms/it, loss 0.467791
Finished training it 59392/76743 of epoch 3, 63.64 ms/it, loss 0.467705
Finished training it 60416/76743 of epoch 3, 64.13 ms/it, loss 0.470191
Finished training it 60416/76743 of epoch 3, 63.82 ms/it, loss 0.469133
Finished training it 60416/76743 of epoch 3, 63.90 ms/it, loss 0.467684
Finished training it 60416/76743 of epoch 3, 63.93 ms/it, loss 0.470866
Finished training it 61440/76743 of epoch 3, 64.00 ms/it, loss 0.471070
Finished training it 61440/76743 of epoch 3, 63.97 ms/it, loss 0.468219
Finished training it 61440/76743 of epoch 3, 64.09 ms/it, loss 0.471129
Finished training it 61440/76743 of epoch 3, 63.94 ms/it, loss 0.469444
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543244.0
get out
0 has test check 2543244.0 and sample count 3274240
 accuracy 77.674 %, best 77.674 %, roc auc score 0.7730, best 0.7730
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 63.28 ms/it, loss 0.469255
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543244.0
get out
3 has test check 2543244.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 63.24 ms/it, loss 0.470640
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543244.0
get out
2 has test check 2543244.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 63.56 ms/it, loss 0.470759
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543244.0
get out
1 has test check 2543244.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 63.33 ms/it, loss 0.470418
Finished training it 63488/76743 of epoch 3, 64.06 ms/it, loss 0.468066
Finished training it 63488/76743 of epoch 3, 64.03 ms/it, loss 0.468660
Finished training it 63488/76743 of epoch 3, 63.88 ms/it, loss 0.473041
Finished training it 63488/76743 of epoch 3, 64.23 ms/it, loss 0.469361
Finished training it 64512/76743 of epoch 3, 64.03 ms/it, loss 0.472496
Finished training it 64512/76743 of epoch 3, 64.25 ms/it, loss 0.466947
Finished training it 64512/76743 of epoch 3, 63.90 ms/it, loss 0.470179
Finished training it 64512/76743 of epoch 3, 63.93 ms/it, loss 0.470779
Finished training it 65536/76743 of epoch 3, 63.90 ms/it, loss 0.470504
Finished training it 65536/76743 of epoch 3, 63.65 ms/it, loss 0.468750
Finished training it 65536/76743 of epoch 3, 63.76 ms/it, loss 0.467173
Finished training it 65536/76743 of epoch 3, 63.63 ms/it, loss 0.469606
Finished training it 66560/76743 of epoch 3, 73.78 ms/it, loss 0.467878
Finished training it 66560/76743 of epoch 3, 73.85 ms/it, loss 0.468513
Finished training it 66560/76743 of epoch 3, 74.15 ms/it, loss 0.468930
Finished training it 66560/76743 of epoch 3, 73.83 ms/it, loss 0.469935
Finished training it 67584/76743 of epoch 3, 63.73 ms/it, loss 0.472126
Finished training it 67584/76743 of epoch 3, 63.78 ms/it, loss 0.470508
Finished training it 67584/76743 of epoch 3, 63.97 ms/it, loss 0.468964
Finished training it 67584/76743 of epoch 3, 63.52 ms/it, loss 0.469350
Finished training it 68608/76743 of epoch 3, 63.70 ms/it, loss 0.469333
Finished training it 68608/76743 of epoch 3, 64.00 ms/it, loss 0.465511
Finished training it 68608/76743 of epoch 3, 63.62 ms/it, loss 0.469054
Finished training it 68608/76743 of epoch 3, 63.74 ms/it, loss 0.468030
Finished training it 69632/76743 of epoch 3, 63.36 ms/it, loss 0.468432
Finished training it 69632/76743 of epoch 3, 63.44 ms/it, loss 0.469852
Finished training it 69632/76743 of epoch 3, 63.25 ms/it, loss 0.469179
Finished training it 69632/76743 of epoch 3, 63.15 ms/it, loss 0.468785
Finished training it 70656/76743 of epoch 3, 63.66 ms/it, loss 0.469876
Finished training it 70656/76743 of epoch 3, 63.38 ms/it, loss 0.466166
Finished training it 70656/76743 of epoch 3, 63.44 ms/it, loss 0.470965
Finished training it 70656/76743 of epoch 3, 63.43 ms/it, loss 0.469401
Finished training it 71680/76743 of epoch 3, 64.06 ms/it, loss 0.468224
Finished training it 71680/76743 of epoch 3, 64.15 ms/it, loss 0.468254
Finished training it 71680/76743 of epoch 3, 63.93 ms/it, loss 0.468000
Finished training it 71680/76743 of epoch 3, 63.83 ms/it, loss 0.471184
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541988.0
get out
0 has test check 2541988.0 and sample count 3274240
 accuracy 77.636 %, best 77.674 %, roc auc score 0.7733, best 0.7733
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541988.0
get out
2 has test check 2541988.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 63.54 ms/it, loss 0.468837
Finished training it 72704/76743 of epoch 3, 63.35 ms/it, loss 0.468863
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541988.0
get out
3 has test check 2541988.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 63.32 ms/it, loss 0.472489
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541988.0
get out
1 has test check 2541988.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 63.29 ms/it, loss 0.467214
Finished training it 73728/76743 of epoch 3, 64.61 ms/it, loss 0.469155
Finished training it 73728/76743 of epoch 3, 64.80 ms/it, loss 0.468392
Finished training it 73728/76743 of epoch 3, 64.60 ms/it, loss 0.468772
Finished training it 73728/76743 of epoch 3, 64.66 ms/it, loss 0.469666
Finished training it 74752/76743 of epoch 3, 63.46 ms/it, loss 0.468249
Finished training it 74752/76743 of epoch 3, 63.59 ms/it, loss 0.470479
Finished training it 74752/76743 of epoch 3, 63.57 ms/it, loss 0.469034
Finished training it 74752/76743 of epoch 3, 63.79 ms/it, loss 0.468111
Finished training it 75776/76743 of epoch 3, 63.30 ms/it, loss 0.469833
Finished training it 75776/76743 of epoch 3, 63.59 ms/it, loss 0.467924
Finished training it 75776/76743 of epoch 3, 63.34 ms/it, loss 0.466979
Finished training it 75776/76743 of epoch 3, 63.33 ms/it, loss 0.470146
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 64.95 ms/it, loss 0.469134
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 64.93 ms/it, loss 0.468405
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 64.90 ms/it, loss 0.469327
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 65.31 ms/it, loss 0.467818
Finished training it 2048/76743 of epoch 4, 63.68 ms/it, loss 0.468424
Finished training it 2048/76743 of epoch 4, 63.44 ms/it, loss 0.467295
Finished training it 2048/76743 of epoch 4, 63.45 ms/it, loss 0.469208
Finished training it 2048/76743 of epoch 4, 63.53 ms/it, loss 0.468756
Finished training it 3072/76743 of epoch 4, 64.27 ms/it, loss 0.468849
Finished training it 3072/76743 of epoch 4, 64.45 ms/it, loss 0.470584
Finished training it 3072/76743 of epoch 4, 64.02 ms/it, loss 0.470076
Finished training it 3072/76743 of epoch 4, 64.17 ms/it, loss 0.468456
Finished training it 4096/76743 of epoch 4, 63.42 ms/it, loss 0.468133
Finished training it 4096/76743 of epoch 4, 63.29 ms/it, loss 0.468791
Finished training it 4096/76743 of epoch 4, 63.62 ms/it, loss 0.470378
Finished training it 4096/76743 of epoch 4, 63.32 ms/it, loss 0.470048
Finished training it 5120/76743 of epoch 4, 63.55 ms/it, loss 0.469963
Finished training it 5120/76743 of epoch 4, 63.56 ms/it, loss 0.471744
Finished training it 5120/76743 of epoch 4, 63.56 ms/it, loss 0.468999
Finished training it 5120/76743 of epoch 4, 63.78 ms/it, loss 0.470221
Finished training it 6144/76743 of epoch 4, 63.43 ms/it, loss 0.467610
Finished training it 6144/76743 of epoch 4, 63.77 ms/it, loss 0.469371
Finished training it 6144/76743 of epoch 4, 63.56 ms/it, loss 0.468584
Finished training it 6144/76743 of epoch 4, 63.54 ms/it, loss 0.471596
Finished training it 7168/76743 of epoch 4, 63.32 ms/it, loss 0.469297
Finished training it 7168/76743 of epoch 4, 63.41 ms/it, loss 0.467350
Finished training it 7168/76743 of epoch 4, 63.49 ms/it, loss 0.470328
Finished training it 7168/76743 of epoch 4, 63.61 ms/it, loss 0.470025
Finished training it 8192/76743 of epoch 4, 63.75 ms/it, loss 0.468482
Finished training it 8192/76743 of epoch 4, 63.96 ms/it, loss 0.468376
Finished training it 8192/76743 of epoch 4, 63.68 ms/it, loss 0.468328
Finished training it 8192/76743 of epoch 4, 63.72 ms/it, loss 0.469810
Finished training it 9216/76743 of epoch 4, 63.76 ms/it, loss 0.469572
Finished training it 9216/76743 of epoch 4, 63.45 ms/it, loss 0.467286
Finished training it 9216/76743 of epoch 4, 63.53 ms/it, loss 0.471441
Finished training it 9216/76743 of epoch 4, 63.49 ms/it, loss 0.468899
Finished training it 10240/76743 of epoch 4, 69.69 ms/it, loss 0.466669
Finished training it 10240/76743 of epoch 4, 69.45 ms/it, loss 0.465539
Finished training it 10240/76743 of epoch 4, 69.34 ms/it, loss 0.471533
Finished training it 10240/76743 of epoch 4, 69.12 ms/it, loss 0.468847
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543496.0
get out
0 has test check 2543496.0 and sample count 3274240
 accuracy 77.682 %, best 77.682 %, roc auc score 0.7744, best 0.7744
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543496.0
get out
3 has test check 2543496.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 63.47 ms/it, loss 0.466279
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543496.0
get out
2 has test check 2543496.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 63.71 ms/it, loss 0.471311
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 63.46 ms/it, loss 0.468663
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543496.0
get out
1 has test check 2543496.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 63.59 ms/it, loss 0.465828
Finished training it 12288/76743 of epoch 4, 63.88 ms/it, loss 0.467244
Finished training it 12288/76743 of epoch 4, 63.71 ms/it, loss 0.469094
Finished training it 12288/76743 of epoch 4, 64.15 ms/it, loss 0.470025
Finished training it 12288/76743 of epoch 4, 63.88 ms/it, loss 0.468000
Finished training it 13312/76743 of epoch 4, 70.30 ms/it, loss 0.469049
Finished training it 13312/76743 of epoch 4, 69.77 ms/it, loss 0.467573
Finished training it 13312/76743 of epoch 4, 70.52 ms/it, loss 0.468925
Finished training it 13312/76743 of epoch 4, 70.14 ms/it, loss 0.471322
Finished training it 14336/76743 of epoch 4, 68.96 ms/it, loss 0.468239
Finished training it 14336/76743 of epoch 4, 69.00 ms/it, loss 0.471010
Finished training it 14336/76743 of epoch 4, 68.69 ms/it, loss 0.468109
Finished training it 14336/76743 of epoch 4, 68.79 ms/it, loss 0.468089
Finished training it 15360/76743 of epoch 4, 63.69 ms/it, loss 0.466178
Finished training it 15360/76743 of epoch 4, 63.91 ms/it, loss 0.470958
Finished training it 15360/76743 of epoch 4, 63.63 ms/it, loss 0.467525
Finished training it 15360/76743 of epoch 4, 63.67 ms/it, loss 0.465477
Finished training it 16384/76743 of epoch 4, 63.96 ms/it, loss 0.469274
Finished training it 16384/76743 of epoch 4, 64.21 ms/it, loss 0.468851
Finished training it 16384/76743 of epoch 4, 63.98 ms/it, loss 0.467717
Finished training it 16384/76743 of epoch 4, 63.92 ms/it, loss 0.469260
Finished training it 17408/76743 of epoch 4, 63.57 ms/it, loss 0.468285
Finished training it 17408/76743 of epoch 4, 63.63 ms/it, loss 0.466652
Finished training it 17408/76743 of epoch 4, 63.82 ms/it, loss 0.467993
Finished training it 17408/76743 of epoch 4, 63.61 ms/it, loss 0.468777
Finished training it 18432/76743 of epoch 4, 63.91 ms/it, loss 0.467788
Finished training it 18432/76743 of epoch 4, 63.88 ms/it, loss 0.466724
Finished training it 18432/76743 of epoch 4, 63.60 ms/it, loss 0.467935
Finished training it 18432/76743 of epoch 4, 63.74 ms/it, loss 0.469629
Finished training it 19456/76743 of epoch 4, 63.70 ms/it, loss 0.468361
Finished training it 19456/76743 of epoch 4, 63.68 ms/it, loss 0.466710
Finished training it 19456/76743 of epoch 4, 63.76 ms/it, loss 0.470666
Finished training it 19456/76743 of epoch 4, 63.95 ms/it, loss 0.469206
Finished training it 20480/76743 of epoch 4, 63.86 ms/it, loss 0.465748
Finished training it 20480/76743 of epoch 4, 63.56 ms/it, loss 0.466321
Finished training it 20480/76743 of epoch 4, 63.97 ms/it, loss 0.468341
Finished training it 20480/76743 of epoch 4, 63.81 ms/it, loss 0.467048
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544180.0
get out
0 has test check 2544180.0 and sample count 3274240
 accuracy 77.703 %, best 77.703 %, roc auc score 0.7749, best 0.7749
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544180.0
get out
3 has test check 2544180.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 63.59 ms/it, loss 0.467736
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544180.0
get out
2 has test check 2544180.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 63.83 ms/it, loss 0.469659
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 63.70 ms/it, loss 0.465878
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544180.0
get out
1 has test check 2544180.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 63.73 ms/it, loss 0.467358
Finished training it 22528/76743 of epoch 4, 63.34 ms/it, loss 0.467878
Finished training it 22528/76743 of epoch 4, 63.17 ms/it, loss 0.469292
Finished training it 22528/76743 of epoch 4, 63.18 ms/it, loss 0.467201
Finished training it 22528/76743 of epoch 4, 63.07 ms/it, loss 0.465756
Finished training it 23552/76743 of epoch 4, 63.83 ms/it, loss 0.470828
Finished training it 23552/76743 of epoch 4, 63.61 ms/it, loss 0.465405
Finished training it 23552/76743 of epoch 4, 63.68 ms/it, loss 0.467832
Finished training it 23552/76743 of epoch 4, 63.50 ms/it, loss 0.468395
Finished training it 24576/76743 of epoch 4, 63.28 ms/it, loss 0.467897
Finished training it 24576/76743 of epoch 4, 63.39 ms/it, loss 0.468518
Finished training it 24576/76743 of epoch 4, 63.52 ms/it, loss 0.468492
Finished training it 24576/76743 of epoch 4, 63.38 ms/it, loss 0.468269
Finished training it 25600/76743 of epoch 4, 63.41 ms/it, loss 0.469815
Finished training it 25600/76743 of epoch 4, 63.30 ms/it, loss 0.467938
Finished training it 25600/76743 of epoch 4, 63.11 ms/it, loss 0.467497
Finished training it 25600/76743 of epoch 4, 63.30 ms/it, loss 0.469043
Finished training it 26624/76743 of epoch 4, 63.14 ms/it, loss 0.467876
Finished training it 26624/76743 of epoch 4, 63.23 ms/it, loss 0.469627
Finished training it 26624/76743 of epoch 4, 63.21 ms/it, loss 0.467936
Finished training it 26624/76743 of epoch 4, 63.42 ms/it, loss 0.467541
Finished training it 27648/76743 of epoch 4, 63.57 ms/it, loss 0.467780
Finished training it 27648/76743 of epoch 4, 63.59 ms/it, loss 0.469650
Finished training it 27648/76743 of epoch 4, 63.73 ms/it, loss 0.472267
Finished training it 27648/76743 of epoch 4, 63.55 ms/it, loss 0.468657
Finished training it 28672/76743 of epoch 4, 63.82 ms/it, loss 0.467409
Finished training it 28672/76743 of epoch 4, 63.62 ms/it, loss 0.469103
Finished training it 28672/76743 of epoch 4, 63.48 ms/it, loss 0.470211
Finished training it 28672/76743 of epoch 4, 63.62 ms/it, loss 0.467169
Finished training it 29696/76743 of epoch 4, 63.29 ms/it, loss 0.468925
Finished training it 29696/76743 of epoch 4, 63.22 ms/it, loss 0.465904
Finished training it 29696/76743 of epoch 4, 63.46 ms/it, loss 0.466026
Finished training it 29696/76743 of epoch 4, 63.25 ms/it, loss 0.465231
Finished training it 30720/76743 of epoch 4, 63.90 ms/it, loss 0.468368
Finished training it 30720/76743 of epoch 4, 64.05 ms/it, loss 0.465867
Finished training it 30720/76743 of epoch 4, 63.77 ms/it, loss 0.468253
Finished training it 30720/76743 of epoch 4, 63.82 ms/it, loss 0.467989
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545320.0
get out
0 has test check 2545320.0 and sample count 3274240
 accuracy 77.738 %, best 77.738 %, roc auc score 0.7751, best 0.7751
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545320.0
get out
2 has test check 2545320.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 63.75 ms/it, loss 0.468148
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545320.0
get out
3 has test check 2545320.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 63.32 ms/it, loss 0.467713
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 63.53 ms/it, loss 0.468653
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545320.0
get out
1 has test check 2545320.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 63.47 ms/it, loss 0.469107
Finished training it 32768/76743 of epoch 4, 63.80 ms/it, loss 0.469096
Finished training it 32768/76743 of epoch 4, 63.77 ms/it, loss 0.466065
Finished training it 32768/76743 of epoch 4, 63.80 ms/it, loss 0.466236
Finished training it 32768/76743 of epoch 4, 64.05 ms/it, loss 0.467521
Finished training it 33792/76743 of epoch 4, 68.25 ms/it, loss 0.465074
Finished training it 33792/76743 of epoch 4, 68.06 ms/it, loss 0.465180
Finished training it 33792/76743 of epoch 4, 67.98 ms/it, loss 0.468125
Finished training it 33792/76743 of epoch 4, 67.98 ms/it, loss 0.468383
Finished training it 34816/76743 of epoch 4, 68.88 ms/it, loss 0.467808
Finished training it 34816/76743 of epoch 4, 69.03 ms/it, loss 0.470510
Finished training it 34816/76743 of epoch 4, 69.04 ms/it, loss 0.465385
Finished training it 34816/76743 of epoch 4, 69.19 ms/it, loss 0.468590
Finished training it 35840/76743 of epoch 4, 63.43 ms/it, loss 0.467455
Finished training it 35840/76743 of epoch 4, 63.38 ms/it, loss 0.469536
Finished training it 35840/76743 of epoch 4, 63.47 ms/it, loss 0.467251
Finished training it 35840/76743 of epoch 4, 63.84 ms/it, loss 0.467021
Finished training it 36864/76743 of epoch 4, 63.59 ms/it, loss 0.466880
Finished training it 36864/76743 of epoch 4, 63.77 ms/it, loss 0.467963
Finished training it 36864/76743 of epoch 4, 63.61 ms/it, loss 0.466840
Finished training it 36864/76743 of epoch 4, 63.69 ms/it, loss 0.468820
Finished training it 37888/76743 of epoch 4, 63.77 ms/it, loss 0.469653
Finished training it 37888/76743 of epoch 4, 63.52 ms/it, loss 0.467648
Finished training it 37888/76743 of epoch 4, 63.40 ms/it, loss 0.466644
Finished training it 37888/76743 of epoch 4, 63.59 ms/it, loss 0.469418
Finished training it 38912/76743 of epoch 4, 63.66 ms/it, loss 0.467290
Finished training it 38912/76743 of epoch 4, 63.86 ms/it, loss 0.466861
Finished training it 38912/76743 of epoch 4, 63.54 ms/it, loss 0.471342
Finished training it 38912/76743 of epoch 4, 63.63 ms/it, loss 0.468550
Finished training it 39936/76743 of epoch 4, 64.10 ms/it, loss 0.468783
Finished training it 39936/76743 of epoch 4, 64.27 ms/it, loss 0.468229
Finished training it 39936/76743 of epoch 4, 64.00 ms/it, loss 0.469764
Finished training it 39936/76743 of epoch 4, 64.04 ms/it, loss 0.467788
Finished training it 40960/76743 of epoch 4, 63.61 ms/it, loss 0.469219
Finished training it 40960/76743 of epoch 4, 63.84 ms/it, loss 0.466882
Finished training it 40960/76743 of epoch 4, 63.64 ms/it, loss 0.468109
Finished training it 40960/76743 of epoch 4, 63.56 ms/it, loss 0.467252
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544734.0
get out
0 has test check 2544734.0 and sample count 3274240
 accuracy 77.720 %, best 77.738 %, roc auc score 0.7750, best 0.7751
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544734.0
get out
1 has test check 2544734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 63.79 ms/it, loss 0.466599
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544734.0
get out
3 has test check 2544734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 63.67 ms/it, loss 0.466619
Finished training it 41984/76743 of epoch 4, 63.91 ms/it, loss 0.467773
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544734.0
get out
2 has test check 2544734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 64.11 ms/it, loss 0.467875
Finished training it 43008/76743 of epoch 4, 63.35 ms/it, loss 0.470999
Finished training it 43008/76743 of epoch 4, 63.69 ms/it, loss 0.463778
Finished training it 43008/76743 of epoch 4, 63.50 ms/it, loss 0.467720
Finished training it 43008/76743 of epoch 4, 63.37 ms/it, loss 0.466048
Finished training it 44032/76743 of epoch 4, 63.47 ms/it, loss 0.465137
Finished training it 44032/76743 of epoch 4, 63.68 ms/it, loss 0.471074
Finished training it 44032/76743 of epoch 4, 63.48 ms/it, loss 0.467344
Finished training it 44032/76743 of epoch 4, 63.40 ms/it, loss 0.466466
Finished training it 45056/76743 of epoch 4, 63.51 ms/it, loss 0.469690
Finished training it 45056/76743 of epoch 4, 63.60 ms/it, loss 0.466199
Finished training it 45056/76743 of epoch 4, 63.36 ms/it, loss 0.467255
Finished training it 45056/76743 of epoch 4, 63.40 ms/it, loss 0.467575
Finished training it 46080/76743 of epoch 4, 63.61 ms/it, loss 0.468694
Finished training it 46080/76743 of epoch 4, 63.55 ms/it, loss 0.467066
Finished training it 46080/76743 of epoch 4, 63.59 ms/it, loss 0.467106
Finished training it 46080/76743 of epoch 4, 63.86 ms/it, loss 0.467910
Finished training it 47104/76743 of epoch 4, 63.61 ms/it, loss 0.465822
Finished training it 47104/76743 of epoch 4, 63.46 ms/it, loss 0.469960
Finished training it 47104/76743 of epoch 4, 63.42 ms/it, loss 0.467218
Finished training it 47104/76743 of epoch 4, 63.36 ms/it, loss 0.469833
Finished training it 48128/76743 of epoch 4, 63.44 ms/it, loss 0.466075
Finished training it 48128/76743 of epoch 4, 63.35 ms/it, loss 0.466785
Finished training it 48128/76743 of epoch 4, 63.25 ms/it, loss 0.465287
Finished training it 48128/76743 of epoch 4, 63.67 ms/it, loss 0.470308
Finished training it 49152/76743 of epoch 4, 63.63 ms/it, loss 0.466632
Finished training it 49152/76743 of epoch 4, 63.54 ms/it, loss 0.467955
Finished training it 49152/76743 of epoch 4, 63.76 ms/it, loss 0.467700
Finished training it 49152/76743 of epoch 4, 63.59 ms/it, loss 0.467935
Finished training it 50176/76743 of epoch 4, 63.69 ms/it, loss 0.465417
Finished training it 50176/76743 of epoch 4, 63.53 ms/it, loss 0.466614
Finished training it 50176/76743 of epoch 4, 63.63 ms/it, loss 0.465166
Finished training it 50176/76743 of epoch 4, 63.90 ms/it, loss 0.466631
Finished training it 51200/76743 of epoch 4, 63.86 ms/it, loss 0.468075
Finished training it 51200/76743 of epoch 4, 64.01 ms/it, loss 0.466009
Finished training it 51200/76743 of epoch 4, 63.76 ms/it, loss 0.468920
Finished training it 51200/76743 of epoch 4, 63.79 ms/it, loss 0.466522
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543565.0
get out
0 has test check 2543565.0 and sample count 3274240
 accuracy 77.684 %, best 77.738 %, roc auc score 0.7758, best 0.7758
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543565.0
get out
3 has test check 2543565.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 63.86 ms/it, loss 0.467991
Finished training it 52224/76743 of epoch 4, 63.83 ms/it, loss 0.467943
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543565.0
get out
2 has test check 2543565.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 64.07 ms/it, loss 0.465893
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543565.0
get out
1 has test check 2543565.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 63.86 ms/it, loss 0.465725
Finished training it 53248/76743 of epoch 4, 63.61 ms/it, loss 0.464440
Finished training it 53248/76743 of epoch 4, 63.38 ms/it, loss 0.467655
Finished training it 53248/76743 of epoch 4, 63.29 ms/it, loss 0.467353
Finished training it 53248/76743 of epoch 4, 63.36 ms/it, loss 0.465685
Finished training it 54272/76743 of epoch 4, 63.77 ms/it, loss 0.466609
Finished training it 54272/76743 of epoch 4, 64.03 ms/it, loss 0.466447
Finished training it 54272/76743 of epoch 4, 63.69 ms/it, loss 0.467938
Finished training it 54272/76743 of epoch 4, 63.74 ms/it, loss 0.469841
Finished training it 55296/76743 of epoch 4, 73.66 ms/it, loss 0.465997
Finished training it 55296/76743 of epoch 4, 73.61 ms/it, loss 0.471816
Finished training it 55296/76743 of epoch 4, 74.03 ms/it, loss 0.468985
Finished training it 55296/76743 of epoch 4, 73.48 ms/it, loss 0.464594
Finished training it 56320/76743 of epoch 4, 63.53 ms/it, loss 0.469402
Finished training it 56320/76743 of epoch 4, 63.53 ms/it, loss 0.471659
Finished training it 56320/76743 of epoch 4, 63.60 ms/it, loss 0.466309
Finished training it 56320/76743 of epoch 4, 63.84 ms/it, loss 0.465945
Finished training it 57344/76743 of epoch 4, 63.85 ms/it, loss 0.467354
Finished training it 57344/76743 of epoch 4, 63.69 ms/it, loss 0.466068
Finished training it 57344/76743 of epoch 4, 63.68 ms/it, loss 0.466090
Finished training it 57344/76743 of epoch 4, 63.69 ms/it, loss 0.466309
Finished training it 58368/76743 of epoch 4, 63.88 ms/it, loss 0.468541
Finished training it 58368/76743 of epoch 4, 63.55 ms/it, loss 0.466364
Finished training it 58368/76743 of epoch 4, 63.72 ms/it, loss 0.469459
Finished training it 58368/76743 of epoch 4, 63.43 ms/it, loss 0.465800
Finished training it 59392/76743 of epoch 4, 64.04 ms/it, loss 0.467618
Finished training it 59392/76743 of epoch 4, 63.85 ms/it, loss 0.465182
Finished training it 59392/76743 of epoch 4, 63.77 ms/it, loss 0.465022
Finished training it 59392/76743 of epoch 4, 63.78 ms/it, loss 0.465845
Finished training it 60416/76743 of epoch 4, 63.94 ms/it, loss 0.467169
Finished training it 60416/76743 of epoch 4, 63.69 ms/it, loss 0.466417
Finished training it 60416/76743 of epoch 4, 63.83 ms/it, loss 0.468354
Finished training it 60416/76743 of epoch 4, 63.70 ms/it, loss 0.465078
Finished training it 61440/76743 of epoch 4, 63.50 ms/it, loss 0.468471
Finished training it 61440/76743 of epoch 4, 63.56 ms/it, loss 0.467181
Finished training it 61440/76743 of epoch 4, 63.84 ms/it, loss 0.468611
Finished training it 61440/76743 of epoch 4, 63.59 ms/it, loss 0.466513
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544960.0
get out
0 has test check 2544960.0 and sample count 3274240
 accuracy 77.727 %, best 77.738 %, roc auc score 0.7750, best 0.7758
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544960.0
get out
1 has test check 2544960.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 63.69 ms/it, loss 0.468424
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544960.0
get out
3 has test check 2544960.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 63.65 ms/it, loss 0.468529
Finished training it 62464/76743 of epoch 4, 63.42 ms/it, loss 0.467037
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544960.0
get out
2 has test check 2544960.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 63.91 ms/it, loss 0.468599
Finished training it 63488/76743 of epoch 4, 63.72 ms/it, loss 0.467528
Finished training it 63488/76743 of epoch 4, 63.42 ms/it, loss 0.465422
Finished training it 63488/76743 of epoch 4, 63.43 ms/it, loss 0.466175
Finished training it 63488/76743 of epoch 4, 63.35 ms/it, loss 0.470379
Finished training it 64512/76743 of epoch 4, 63.49 ms/it, loss 0.467244
Finished training it 64512/76743 of epoch 4, 63.80 ms/it, loss 0.464597
Finished training it 64512/76743 of epoch 4, 63.50 ms/it, loss 0.468704
Finished training it 64512/76743 of epoch 4, 63.56 ms/it, loss 0.470008
Finished training it 65536/76743 of epoch 4, 63.89 ms/it, loss 0.464865
Finished training it 65536/76743 of epoch 4, 64.12 ms/it, loss 0.467790
Finished training it 65536/76743 of epoch 4, 63.84 ms/it, loss 0.466190
Finished training it 65536/76743 of epoch 4, 63.85 ms/it, loss 0.467250
Finished training it 66560/76743 of epoch 4, 63.65 ms/it, loss 0.465828
Finished training it 66560/76743 of epoch 4, 63.68 ms/it, loss 0.467558
Finished training it 66560/76743 of epoch 4, 63.90 ms/it, loss 0.466637
Finished training it 66560/76743 of epoch 4, 63.69 ms/it, loss 0.465635
Finished training it 67584/76743 of epoch 4, 63.46 ms/it, loss 0.466596
Finished training it 67584/76743 of epoch 4, 63.21 ms/it, loss 0.466493
Finished training it 67584/76743 of epoch 4, 63.26 ms/it, loss 0.468203
Finished training it 67584/76743 of epoch 4, 63.23 ms/it, loss 0.469821
Finished training it 68608/76743 of epoch 4, 63.70 ms/it, loss 0.465477
Finished training it 68608/76743 of epoch 4, 63.94 ms/it, loss 0.463294
Finished training it 68608/76743 of epoch 4, 63.76 ms/it, loss 0.466841
Finished training it 68608/76743 of epoch 4, 63.66 ms/it, loss 0.467309
Finished training it 69632/76743 of epoch 4, 63.62 ms/it, loss 0.466384
Finished training it 69632/76743 of epoch 4, 63.50 ms/it, loss 0.466507
Finished training it 69632/76743 of epoch 4, 63.37 ms/it, loss 0.465948
Finished training it 69632/76743 of epoch 4, 63.81 ms/it, loss 0.467353
Finished training it 70656/76743 of epoch 4, 63.86 ms/it, loss 0.467542
Finished training it 70656/76743 of epoch 4, 63.55 ms/it, loss 0.464036
Finished training it 70656/76743 of epoch 4, 63.57 ms/it, loss 0.467287
Finished training it 70656/76743 of epoch 4, 63.60 ms/it, loss 0.468694
Finished training it 71680/76743 of epoch 4, 63.49 ms/it, loss 0.469208
Finished training it 71680/76743 of epoch 4, 63.61 ms/it, loss 0.465843
Finished training it 71680/76743 of epoch 4, 63.80 ms/it, loss 0.465942
Finished training it 71680/76743 of epoch 4, 63.64 ms/it, loss 0.465784
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546723.0
get out
0 has test check 2546723.0 and sample count 3274240
 accuracy 77.781 %, best 77.781 %, roc auc score 0.7767, best 0.7767
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546723.0
get out
1 has test check 2546723.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 63.71 ms/it, loss 0.465123
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 63.60 ms/it, loss 0.466988
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546723.0
get out
2 has test check 2546723.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 63.95 ms/it, loss 0.466235
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546723.0
get out
3 has test check 2546723.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 63.83 ms/it, loss 0.470176
Finished training it 73728/76743 of epoch 4, 63.36 ms/it, loss 0.467438
Finished training it 73728/76743 of epoch 4, 63.37 ms/it, loss 0.466586
Finished training it 73728/76743 of epoch 4, 63.25 ms/it, loss 0.467152
Finished training it 73728/76743 of epoch 4, 63.63 ms/it, loss 0.466651
Finished training it 74752/76743 of epoch 4, 63.89 ms/it, loss 0.466165
Finished training it 74752/76743 of epoch 4, 63.61 ms/it, loss 0.466800
Finished training it 74752/76743 of epoch 4, 63.66 ms/it, loss 0.465896
Finished training it 74752/76743 of epoch 4, 63.53 ms/it, loss 0.468353
Finished training it 75776/76743 of epoch 4, 63.83 ms/it, loss 0.466119
Finished training it 75776/76743 of epoch 4, 63.51 ms/it, loss 0.467651
Finished training it 75776/76743 of epoch 4, 63.65 ms/it, loss 0.467878
Finished training it 75776/76743 of epoch 4, 63.58 ms/it, loss 0.464452
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 69.99 ms/it, loss 0.467056
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 70.20 ms/it, loss 0.467134
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 70.07 ms/it, loss 0.466585
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 70.49 ms/it, loss 0.466148
Finished training it 2048/76743 of epoch 5, 63.67 ms/it, loss 0.466844
Finished training it 2048/76743 of epoch 5, 63.99 ms/it, loss 0.467425
Finished training it 2048/76743 of epoch 5, 63.68 ms/it, loss 0.467964
Finished training it 2048/76743 of epoch 5, 63.71 ms/it, loss 0.465964
Finished training it 3072/76743 of epoch 5, 63.60 ms/it, loss 0.467183
Finished training it 3072/76743 of epoch 5, 63.62 ms/it, loss 0.467601
Finished training it 3072/76743 of epoch 5, 63.97 ms/it, loss 0.468961
Finished training it 3072/76743 of epoch 5, 63.61 ms/it, loss 0.468670
Finished training it 4096/76743 of epoch 5, 63.14 ms/it, loss 0.466837
Finished training it 4096/76743 of epoch 5, 63.53 ms/it, loss 0.468059
Finished training it 4096/76743 of epoch 5, 63.29 ms/it, loss 0.466236
Finished training it 4096/76743 of epoch 5, 63.30 ms/it, loss 0.468165
Finished training it 5120/76743 of epoch 5, 63.93 ms/it, loss 0.468452
Finished training it 5120/76743 of epoch 5, 63.52 ms/it, loss 0.469541
Finished training it 5120/76743 of epoch 5, 63.62 ms/it, loss 0.467019
Finished training it 5120/76743 of epoch 5, 63.62 ms/it, loss 0.468026
Finished training it 6144/76743 of epoch 5, 63.40 ms/it, loss 0.469235
Finished training it 6144/76743 of epoch 5, 63.71 ms/it, loss 0.467070
Finished training it 6144/76743 of epoch 5, 63.35 ms/it, loss 0.466759
Finished training it 6144/76743 of epoch 5, 63.33 ms/it, loss 0.465488
Finished training it 7168/76743 of epoch 5, 63.41 ms/it, loss 0.467679
Finished training it 7168/76743 of epoch 5, 63.45 ms/it, loss 0.465271
Finished training it 7168/76743 of epoch 5, 63.45 ms/it, loss 0.468031
Finished training it 7168/76743 of epoch 5, 63.75 ms/it, loss 0.468228
Finished training it 8192/76743 of epoch 5, 63.59 ms/it, loss 0.466159
Finished training it 8192/76743 of epoch 5, 63.48 ms/it, loss 0.466608
Finished training it 8192/76743 of epoch 5, 63.52 ms/it, loss 0.467862
Finished training it 8192/76743 of epoch 5, 63.70 ms/it, loss 0.466287
Finished training it 9216/76743 of epoch 5, 63.39 ms/it, loss 0.467077
Finished training it 9216/76743 of epoch 5, 63.63 ms/it, loss 0.467181
Finished training it 9216/76743 of epoch 5, 63.34 ms/it, loss 0.465486
Finished training it 9216/76743 of epoch 5, 63.32 ms/it, loss 0.469583
Finished training it 10240/76743 of epoch 5, 63.27 ms/it, loss 0.469537
Finished training it 10240/76743 of epoch 5, 63.48 ms/it, loss 0.464918
Finished training it 10240/76743 of epoch 5, 63.19 ms/it, loss 0.467255
Finished training it 10240/76743 of epoch 5, 63.16 ms/it, loss 0.463588
Testing at - 10240/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546467.0
get out
0 has test check 2546467.0 and sample count 3274240
 accuracy 77.773 %, best 77.781 %, roc auc score 0.7768, best 0.7768
Finished training it 11264/76743 of epoch 5, 63.06 ms/it, loss 0.466697
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546467.0
get out
2 has test check 2546467.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 63.36 ms/it, loss 0.469423
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546467.0
get out
1 has test check 2546467.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 63.24 ms/it, loss 0.463575
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546467.0
get out
3 has test check 2546467.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 63.18 ms/it, loss 0.464498
Finished training it 12288/76743 of epoch 5, 63.57 ms/it, loss 0.465286
Finished training it 12288/76743 of epoch 5, 63.48 ms/it, loss 0.467044
Finished training it 12288/76743 of epoch 5, 63.77 ms/it, loss 0.468308
Finished training it 12288/76743 of epoch 5, 63.58 ms/it, loss 0.466103
Finished training it 13312/76743 of epoch 5, 63.59 ms/it, loss 0.467330
Finished training it 13312/76743 of epoch 5, 63.85 ms/it, loss 0.466530
Finished training it 13312/76743 of epoch 5, 63.58 ms/it, loss 0.469285
Finished training it 13312/76743 of epoch 5, 63.55 ms/it, loss 0.465653
Finished training it 14336/76743 of epoch 5, 63.61 ms/it, loss 0.465937
Finished training it 14336/76743 of epoch 5, 63.64 ms/it, loss 0.468516
Finished training it 14336/76743 of epoch 5, 63.66 ms/it, loss 0.465901
Finished training it 14336/76743 of epoch 5, 63.82 ms/it, loss 0.466149
Finished training it 15360/76743 of epoch 5, 63.36 ms/it, loss 0.464199
Finished training it 15360/76743 of epoch 5, 63.24 ms/it, loss 0.463454
Finished training it 15360/76743 of epoch 5, 63.57 ms/it, loss 0.467938
Finished training it 15360/76743 of epoch 5, 63.27 ms/it, loss 0.464848
Finished training it 16384/76743 of epoch 5, 63.43 ms/it, loss 0.467942
Finished training it 16384/76743 of epoch 5, 63.67 ms/it, loss 0.467445
Finished training it 16384/76743 of epoch 5, 63.55 ms/it, loss 0.466236
Finished training it 16384/76743 of epoch 5, 63.88 ms/it, loss 0.467154
Finished training it 17408/76743 of epoch 5, 63.58 ms/it, loss 0.467189
Finished training it 17408/76743 of epoch 5, 63.54 ms/it, loss 0.466666
Finished training it 17408/76743 of epoch 5, 63.64 ms/it, loss 0.465260
Finished training it 17408/76743 of epoch 5, 63.95 ms/it, loss 0.465794
Finished training it 18432/76743 of epoch 5, 63.44 ms/it, loss 0.467896
Finished training it 18432/76743 of epoch 5, 63.52 ms/it, loss 0.465046
Finished training it 18432/76743 of epoch 5, 63.34 ms/it, loss 0.466116
Finished training it 18432/76743 of epoch 5, 63.78 ms/it, loss 0.466243
Finished training it 19456/76743 of epoch 5, 63.95 ms/it, loss 0.466946
Finished training it 19456/76743 of epoch 5, 63.45 ms/it, loss 0.468695
Finished training it 19456/76743 of epoch 5, 63.77 ms/it, loss 0.465267
Finished training it 19456/76743 of epoch 5, 63.79 ms/it, loss 0.466987
Finished training it 20480/76743 of epoch 5, 63.37 ms/it, loss 0.465638
Finished training it 20480/76743 of epoch 5, 63.60 ms/it, loss 0.466442
Finished training it 20480/76743 of epoch 5, 63.32 ms/it, loss 0.464405
Finished training it 20480/76743 of epoch 5, 63.54 ms/it, loss 0.464312
Testing at - 20480/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545356.0
get out
0 has test check 2545356.0 and sample count 3274240
 accuracy 77.739 %, best 77.781 %, roc auc score 0.7763, best 0.7768
Finished training it 21504/76743 of epoch 5, 63.44 ms/it, loss 0.463725
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545356.0
get out
3 has test check 2545356.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 63.45 ms/it, loss 0.466125
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545356.0
get out
2 has test check 2545356.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 63.72 ms/it, loss 0.467040
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545356.0
get out
1 has test check 2545356.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 63.50 ms/it, loss 0.464558
Finished training it 22528/76743 of epoch 5, 63.89 ms/it, loss 0.466217
Finished training it 22528/76743 of epoch 5, 63.65 ms/it, loss 0.467459
Finished training it 22528/76743 of epoch 5, 63.68 ms/it, loss 0.465126
Finished training it 22528/76743 of epoch 5, 63.54 ms/it, loss 0.463944
Finished training it 23552/76743 of epoch 5, 63.76 ms/it, loss 0.468867
Finished training it 23552/76743 of epoch 5, 63.48 ms/it, loss 0.466891
Finished training it 23552/76743 of epoch 5, 63.45 ms/it, loss 0.466148
Finished training it 23552/76743 of epoch 5, 63.37 ms/it, loss 0.463506
Finished training it 24576/76743 of epoch 5, 73.55 ms/it, loss 0.465624
Finished training it 24576/76743 of epoch 5, 73.47 ms/it, loss 0.466430
Finished training it 24576/76743 of epoch 5, 73.92 ms/it, loss 0.466809
Finished training it 24576/76743 of epoch 5, 73.23 ms/it, loss 0.466405
Finished training it 25600/76743 of epoch 5, 63.57 ms/it, loss 0.467206
Finished training it 25600/76743 of epoch 5, 63.90 ms/it, loss 0.468501
Finished training it 25600/76743 of epoch 5, 63.67 ms/it, loss 0.466028
Finished training it 25600/76743 of epoch 5, 63.63 ms/it, loss 0.466199
Finished training it 26624/76743 of epoch 5, 63.40 ms/it, loss 0.466005
Finished training it 26624/76743 of epoch 5, 63.47 ms/it, loss 0.467770
Finished training it 26624/76743 of epoch 5, 63.37 ms/it, loss 0.465546
Finished training it 26624/76743 of epoch 5, 63.70 ms/it, loss 0.465719
Finished training it 27648/76743 of epoch 5, 63.59 ms/it, loss 0.468061
Finished training it 27648/76743 of epoch 5, 63.79 ms/it, loss 0.470520
Finished training it 27648/76743 of epoch 5, 63.51 ms/it, loss 0.466281
Finished training it 27648/76743 of epoch 5, 63.52 ms/it, loss 0.465575
Finished training it 28672/76743 of epoch 5, 63.59 ms/it, loss 0.467377
Finished training it 28672/76743 of epoch 5, 63.67 ms/it, loss 0.465268
Finished training it 28672/76743 of epoch 5, 63.94 ms/it, loss 0.465844
Finished training it 28672/76743 of epoch 5, 63.70 ms/it, loss 0.468301
Finished training it 29696/76743 of epoch 5, 64.11 ms/it, loss 0.464365
Finished training it 29696/76743 of epoch 5, 64.00 ms/it, loss 0.467433
Finished training it 29696/76743 of epoch 5, 64.01 ms/it, loss 0.464138
Finished training it 29696/76743 of epoch 5, 63.82 ms/it, loss 0.463642
Finished training it 30720/76743 of epoch 5, 63.39 ms/it, loss 0.466473
Finished training it 30720/76743 of epoch 5, 63.62 ms/it, loss 0.464037
Finished training it 30720/76743 of epoch 5, 63.31 ms/it, loss 0.465662
Finished training it 30720/76743 of epoch 5, 63.22 ms/it, loss 0.466631
Testing at - 30720/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547278.0
get out
0 has test check 2547278.0 and sample count 3274240
 accuracy 77.798 %, best 77.798 %, roc auc score 0.7775, best 0.7775
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547278.0
get out
2 has test check 2547278.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 63.69 ms/it, loss 0.466584
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547278.0
get out
1 has test check 2547278.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 63.49 ms/it, loss 0.467181
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547278.0
get out
3 has test check 2547278.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 63.25 ms/it, loss 0.465970
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 5, 63.47 ms/it, loss 0.466927
Finished training it 32768/76743 of epoch 5, 63.79 ms/it, loss 0.466061
Finished training it 32768/76743 of epoch 5, 63.56 ms/it, loss 0.467550
Finished training it 32768/76743 of epoch 5, 63.49 ms/it, loss 0.463954
Finished training it 32768/76743 of epoch 5, 63.48 ms/it, loss 0.464675
Finished training it 33792/76743 of epoch 5, 63.17 ms/it, loss 0.462914
Finished training it 33792/76743 of epoch 5, 63.43 ms/it, loss 0.462931
Finished training it 33792/76743 of epoch 5, 63.16 ms/it, loss 0.466916
Finished training it 33792/76743 of epoch 5, 63.15 ms/it, loss 0.466298
Finished training it 34816/76743 of epoch 5, 63.54 ms/it, loss 0.463396
Finished training it 34816/76743 of epoch 5, 63.60 ms/it, loss 0.468384
Finished training it 34816/76743 of epoch 5, 63.68 ms/it, loss 0.465794
Finished training it 34816/76743 of epoch 5, 63.88 ms/it, loss 0.466755
Finished training it 35840/76743 of epoch 5, 63.58 ms/it, loss 0.465517
Finished training it 35840/76743 of epoch 5, 63.74 ms/it, loss 0.465874
Finished training it 35840/76743 of epoch 5, 63.87 ms/it, loss 0.465460
Finished training it 35840/76743 of epoch 5, 63.77 ms/it, loss 0.467782
Finished training it 36864/76743 of epoch 5, 63.72 ms/it, loss 0.466146
Finished training it 36864/76743 of epoch 5, 63.45 ms/it, loss 0.464767
Finished training it 36864/76743 of epoch 5, 63.46 ms/it, loss 0.464663
Finished training it 36864/76743 of epoch 5, 63.32 ms/it, loss 0.467190
Finished training it 37888/76743 of epoch 5, 63.77 ms/it, loss 0.467510
Finished training it 37888/76743 of epoch 5, 63.50 ms/it, loss 0.465875
Finished training it 37888/76743 of epoch 5, 63.62 ms/it, loss 0.467164
Finished training it 37888/76743 of epoch 5, 63.43 ms/it, loss 0.464553
Finished training it 38912/76743 of epoch 5, 63.67 ms/it, loss 0.466800
Finished training it 38912/76743 of epoch 5, 63.36 ms/it, loss 0.468484
Finished training it 38912/76743 of epoch 5, 63.58 ms/it, loss 0.465146
Finished training it 38912/76743 of epoch 5, 63.81 ms/it, loss 0.464039
Finished training it 39936/76743 of epoch 5, 64.16 ms/it, loss 0.466305
Finished training it 39936/76743 of epoch 5, 63.91 ms/it, loss 0.467715
Finished training it 39936/76743 of epoch 5, 63.96 ms/it, loss 0.466883
Finished training it 39936/76743 of epoch 5, 63.89 ms/it, loss 0.465672
Finished training it 40960/76743 of epoch 5, 63.40 ms/it, loss 0.467045
Finished training it 40960/76743 of epoch 5, 63.35 ms/it, loss 0.465366
Finished training it 40960/76743 of epoch 5, 63.63 ms/it, loss 0.465007
Finished training it 40960/76743 of epoch 5, 63.73 ms/it, loss 0.464323
Testing at - 40960/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548917.0
get out
0 has test check 2548917.0 and sample count 3274240
 accuracy 77.848 %, best 77.848 %, roc auc score 0.7782, best 0.7782
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548917.0
get out
2 has test check 2548917.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 63.59 ms/it, loss 0.466145
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 5, 63.32 ms/it, loss 0.465872
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548917.0
get out
3 has test check 2548917.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 63.32 ms/it, loss 0.464794
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548917.0
get out
1 has test check 2548917.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 63.35 ms/it, loss 0.464961
Finished training it 43008/76743 of epoch 5, 63.53 ms/it, loss 0.464203
Finished training it 43008/76743 of epoch 5, 63.72 ms/it, loss 0.465145
Finished training it 43008/76743 of epoch 5, 63.99 ms/it, loss 0.462043
Finished training it 43008/76743 of epoch 5, 63.78 ms/it, loss 0.468957
Finished training it 44032/76743 of epoch 5, 64.09 ms/it, loss 0.469207
Finished training it 44032/76743 of epoch 5, 63.71 ms/it, loss 0.463695
Finished training it 44032/76743 of epoch 5, 63.85 ms/it, loss 0.464448
Finished training it 44032/76743 of epoch 5, 63.79 ms/it, loss 0.465648
Finished training it 45056/76743 of epoch 5, 68.48 ms/it, loss 0.465484
Finished training it 45056/76743 of epoch 5, 68.54 ms/it, loss 0.465401
Finished training it 45056/76743 of epoch 5, 68.45 ms/it, loss 0.467513
Finished training it 45056/76743 of epoch 5, 68.77 ms/it, loss 0.464329
Finished training it 46080/76743 of epoch 5, 68.57 ms/it, loss 0.464913
Finished training it 46080/76743 of epoch 5, 68.25 ms/it, loss 0.466698
Finished training it 46080/76743 of epoch 5, 68.66 ms/it, loss 0.465337
Finished training it 46080/76743 of epoch 5, 68.94 ms/it, loss 0.466068
Finished training it 47104/76743 of epoch 5, 63.68 ms/it, loss 0.464447
Finished training it 47104/76743 of epoch 5, 63.36 ms/it, loss 0.467640
Finished training it 47104/76743 of epoch 5, 63.52 ms/it, loss 0.465456
Finished training it 47104/76743 of epoch 5, 63.38 ms/it, loss 0.468495
Finished training it 48128/76743 of epoch 5, 63.95 ms/it, loss 0.468209
Finished training it 48128/76743 of epoch 5, 63.74 ms/it, loss 0.463650
Finished training it 48128/76743 of epoch 5, 63.72 ms/it, loss 0.464384
Finished training it 48128/76743 of epoch 5, 63.63 ms/it, loss 0.465595
Finished training it 49152/76743 of epoch 5, 63.84 ms/it, loss 0.466189
Finished training it 49152/76743 of epoch 5, 63.51 ms/it, loss 0.466208
Finished training it 49152/76743 of epoch 5, 63.49 ms/it, loss 0.465905
Finished training it 49152/76743 of epoch 5, 63.47 ms/it, loss 0.464387
Finished training it 50176/76743 of epoch 5, 64.23 ms/it, loss 0.463208
Finished training it 50176/76743 of epoch 5, 64.33 ms/it, loss 0.464748
Finished training it 50176/76743 of epoch 5, 64.57 ms/it, loss 0.464817
Finished training it 50176/76743 of epoch 5, 64.20 ms/it, loss 0.463736
Finished training it 51200/76743 of epoch 5, 63.57 ms/it, loss 0.466481
Finished training it 51200/76743 of epoch 5, 63.88 ms/it, loss 0.464603
Finished training it 51200/76743 of epoch 5, 63.54 ms/it, loss 0.466902
Finished training it 51200/76743 of epoch 5, 63.74 ms/it, loss 0.464793
Testing at - 51200/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547389.0
get out
0 has test check 2547389.0 and sample count 3274240
 accuracy 77.801 %, best 77.848 %, roc auc score 0.7784, best 0.7784
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547389.0
get out
2 has test check 2547389.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 63.83 ms/it, loss 0.464430
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547389.0
get out
1 has test check 2547389.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 63.64 ms/it, loss 0.463649
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547389.0
get out
3 has test check 2547389.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 63.59 ms/it, loss 0.466100
Finished training it 52224/76743 of epoch 5, 63.65 ms/it, loss 0.465997
Finished training it 53248/76743 of epoch 5, 63.24 ms/it, loss 0.465693
Finished training it 53248/76743 of epoch 5, 63.34 ms/it, loss 0.466250
Finished training it 53248/76743 of epoch 5, 63.29 ms/it, loss 0.464251
Finished training it 53248/76743 of epoch 5, 63.50 ms/it, loss 0.462787
Finished training it 54272/76743 of epoch 5, 63.31 ms/it, loss 0.465048
Finished training it 54272/76743 of epoch 5, 63.20 ms/it, loss 0.465870
Finished training it 54272/76743 of epoch 5, 63.17 ms/it, loss 0.467777
Finished training it 54272/76743 of epoch 5, 63.49 ms/it, loss 0.465434
Finished training it 55296/76743 of epoch 5, 63.28 ms/it, loss 0.464394
Finished training it 55296/76743 of epoch 5, 63.57 ms/it, loss 0.467127
Finished training it 55296/76743 of epoch 5, 63.43 ms/it, loss 0.463482
Finished training it 55296/76743 of epoch 5, 63.23 ms/it, loss 0.470236
Finished training it 56320/76743 of epoch 5, 63.87 ms/it, loss 0.464238
Finished training it 56320/76743 of epoch 5, 63.61 ms/it, loss 0.470136
Finished training it 56320/76743 of epoch 5, 63.73 ms/it, loss 0.467777
Finished training it 56320/76743 of epoch 5, 63.61 ms/it, loss 0.464564
Finished training it 57344/76743 of epoch 5, 63.67 ms/it, loss 0.465969
Finished training it 57344/76743 of epoch 5, 63.46 ms/it, loss 0.464535
Finished training it 57344/76743 of epoch 5, 63.35 ms/it, loss 0.464341
Finished training it 57344/76743 of epoch 5, 63.33 ms/it, loss 0.464727
Finished training it 58368/76743 of epoch 5, 63.85 ms/it, loss 0.463657
Finished training it 58368/76743 of epoch 5, 64.02 ms/it, loss 0.467331
Finished training it 58368/76743 of epoch 5, 63.94 ms/it, loss 0.465167
Finished training it 58368/76743 of epoch 5, 64.23 ms/it, loss 0.466921
Finished training it 59392/76743 of epoch 5, 63.72 ms/it, loss 0.464120
Finished training it 59392/76743 of epoch 5, 63.74 ms/it, loss 0.463754
Finished training it 59392/76743 of epoch 5, 63.80 ms/it, loss 0.463923
Finished training it 59392/76743 of epoch 5, 63.99 ms/it, loss 0.466487
Finished training it 60416/76743 of epoch 5, 63.62 ms/it, loss 0.463509
Finished training it 60416/76743 of epoch 5, 63.86 ms/it, loss 0.466048
Finished training it 60416/76743 of epoch 5, 63.78 ms/it, loss 0.467384
Finished training it 60416/76743 of epoch 5, 63.45 ms/it, loss 0.464864
Finished training it 61440/76743 of epoch 5, 63.78 ms/it, loss 0.466891
Finished training it 61440/76743 of epoch 5, 63.54 ms/it, loss 0.465447
Finished training it 61440/76743 of epoch 5, 63.52 ms/it, loss 0.466439
Finished training it 61440/76743 of epoch 5, 63.56 ms/it, loss 0.464173
Testing at - 61440/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548906.0
get out
0 has test check 2548906.0 and sample count 3274240
 accuracy 77.847 %, best 77.848 %, roc auc score 0.7784, best 0.7784
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548906.0
get out
3 has test check 2548906.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 63.55 ms/it, loss 0.466033
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548906.0
get out
2 has test check 2548906.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 63.81 ms/it, loss 0.467224
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548906.0
get out
1 has test check 2548906.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 63.49 ms/it, loss 0.466146
Finished training it 62464/76743 of epoch 5, 63.57 ms/it, loss 0.465171
Finished training it 63488/76743 of epoch 5, 64.01 ms/it, loss 0.464967
Finished training it 63488/76743 of epoch 5, 63.65 ms/it, loss 0.468683
Finished training it 63488/76743 of epoch 5, 63.81 ms/it, loss 0.463628
Finished training it 63488/76743 of epoch 5, 63.63 ms/it, loss 0.464408
Finished training it 64512/76743 of epoch 5, 63.65 ms/it, loss 0.462655
Finished training it 64512/76743 of epoch 5, 63.26 ms/it, loss 0.467837
Finished training it 64512/76743 of epoch 5, 63.45 ms/it, loss 0.465431
Finished training it 64512/76743 of epoch 5, 63.35 ms/it, loss 0.466928
Finished training it 65536/76743 of epoch 5, 73.44 ms/it, loss 0.465016
Finished training it 65536/76743 of epoch 5, 73.78 ms/it, loss 0.465935
Finished training it 65536/76743 of epoch 5, 73.12 ms/it, loss 0.463415
Finished training it 65536/76743 of epoch 5, 73.32 ms/it, loss 0.464549
Finished training it 66560/76743 of epoch 5, 63.47 ms/it, loss 0.465561
Finished training it 66560/76743 of epoch 5, 63.79 ms/it, loss 0.464575
Finished training it 66560/76743 of epoch 5, 63.62 ms/it, loss 0.463947
Finished training it 66560/76743 of epoch 5, 63.59 ms/it, loss 0.464389
Finished training it 67584/76743 of epoch 5, 63.94 ms/it, loss 0.464728
Finished training it 67584/76743 of epoch 5, 63.70 ms/it, loss 0.467853
Finished training it 67584/76743 of epoch 5, 63.71 ms/it, loss 0.464600
Finished training it 67584/76743 of epoch 5, 63.74 ms/it, loss 0.466651
Finished training it 68608/76743 of epoch 5, 63.27 ms/it, loss 0.464790
Finished training it 68608/76743 of epoch 5, 63.45 ms/it, loss 0.461301
Finished training it 68608/76743 of epoch 5, 63.13 ms/it, loss 0.463786
Finished training it 68608/76743 of epoch 5, 63.18 ms/it, loss 0.465067
Finished training it 69632/76743 of epoch 5, 63.60 ms/it, loss 0.465849
Finished training it 69632/76743 of epoch 5, 63.50 ms/it, loss 0.464812
Finished training it 69632/76743 of epoch 5, 63.30 ms/it, loss 0.464542
Finished training it 69632/76743 of epoch 5, 63.15 ms/it, loss 0.464013
Finished training it 70656/76743 of epoch 5, 63.67 ms/it, loss 0.466453
Finished training it 70656/76743 of epoch 5, 63.55 ms/it, loss 0.465506
Finished training it 70656/76743 of epoch 5, 63.88 ms/it, loss 0.465899
Finished training it 70656/76743 of epoch 5, 63.60 ms/it, loss 0.462006
Finished training it 71680/76743 of epoch 5, 63.59 ms/it, loss 0.463677
Finished training it 71680/76743 of epoch 5, 63.73 ms/it, loss 0.464197
Finished training it 71680/76743 of epoch 5, 63.51 ms/it, loss 0.464226
Finished training it 71680/76743 of epoch 5, 63.46 ms/it, loss 0.467418
Testing at - 71680/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549310.0
get out
0 has test check 2549310.0 and sample count 3274240
 accuracy 77.860 %, best 77.860 %, roc auc score 0.7786, best 0.7786
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549310.0
get out
3 has test check 2549310.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 63.42 ms/it, loss 0.468214
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 5, 63.29 ms/it, loss 0.465216
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549310.0
get out
2 has test check 2549310.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 63.65 ms/it, loss 0.464848
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549310.0
get out
1 has test check 2549310.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 63.33 ms/it, loss 0.463055
Finished training it 73728/76743 of epoch 5, 63.77 ms/it, loss 0.465162
Finished training it 73728/76743 of epoch 5, 63.48 ms/it, loss 0.465377
Finished training it 73728/76743 of epoch 5, 63.29 ms/it, loss 0.464836
Finished training it 73728/76743 of epoch 5, 63.56 ms/it, loss 0.464829
Finished training it 74752/76743 of epoch 5, 63.73 ms/it, loss 0.464436
Finished training it 74752/76743 of epoch 5, 63.87 ms/it, loss 0.464162
Finished training it 74752/76743 of epoch 5, 63.87 ms/it, loss 0.466287
Finished training it 74752/76743 of epoch 5, 64.02 ms/it, loss 0.464181
Finished training it 75776/76743 of epoch 5, 64.03 ms/it, loss 0.464363
Finished training it 75776/76743 of epoch 5, 63.61 ms/it, loss 0.466241
Finished training it 75776/76743 of epoch 5, 63.88 ms/it, loss 0.465650
Finished training it 75776/76743 of epoch 5, 63.73 ms/it, loss 0.462686
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 64.98 ms/it, loss 0.465536
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 65.32 ms/it, loss 0.464598
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 64.86 ms/it, loss 0.464906
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 64.86 ms/it, loss 0.465371
Finished training it 2048/76743 of epoch 6, 63.68 ms/it, loss 0.464758
Finished training it 2048/76743 of epoch 6, 63.89 ms/it, loss 0.466442
Finished training it 2048/76743 of epoch 6, 64.06 ms/it, loss 0.465967
Finished training it 2048/76743 of epoch 6, 63.92 ms/it, loss 0.464031
Finished training it 3072/76743 of epoch 6, 63.39 ms/it, loss 0.466800
Finished training it 3072/76743 of epoch 6, 63.48 ms/it, loss 0.465086
Finished training it 3072/76743 of epoch 6, 63.47 ms/it, loss 0.464394
Finished training it 3072/76743 of epoch 6, 63.70 ms/it, loss 0.467189
Finished training it 4096/76743 of epoch 6, 63.66 ms/it, loss 0.464689
Finished training it 4096/76743 of epoch 6, 63.83 ms/it, loss 0.466387
Finished training it 4096/76743 of epoch 6, 63.68 ms/it, loss 0.464837
Finished training it 4096/76743 of epoch 6, 63.47 ms/it, loss 0.466180
Finished training it 5120/76743 of epoch 6, 63.86 ms/it, loss 0.467061
Finished training it 5120/76743 of epoch 6, 63.42 ms/it, loss 0.467545
Finished training it 5120/76743 of epoch 6, 63.71 ms/it, loss 0.466358
Finished training it 5120/76743 of epoch 6, 63.62 ms/it, loss 0.466083
Finished training it 6144/76743 of epoch 6, 63.31 ms/it, loss 0.467590
Finished training it 6144/76743 of epoch 6, 63.59 ms/it, loss 0.465894
Finished training it 6144/76743 of epoch 6, 63.38 ms/it, loss 0.465077
Finished training it 6144/76743 of epoch 6, 63.29 ms/it, loss 0.464094
Finished training it 7168/76743 of epoch 6, 63.68 ms/it, loss 0.465371
Finished training it 7168/76743 of epoch 6, 63.67 ms/it, loss 0.466808
Finished training it 7168/76743 of epoch 6, 63.96 ms/it, loss 0.466590
Finished training it 7168/76743 of epoch 6, 63.71 ms/it, loss 0.463769
Finished training it 8192/76743 of epoch 6, 63.42 ms/it, loss 0.465338
Finished training it 8192/76743 of epoch 6, 63.44 ms/it, loss 0.464887
Finished training it 8192/76743 of epoch 6, 63.65 ms/it, loss 0.464982
Finished training it 8192/76743 of epoch 6, 63.48 ms/it, loss 0.466302
Finished training it 9216/76743 of epoch 6, 68.37 ms/it, loss 0.466003
Finished training it 9216/76743 of epoch 6, 68.22 ms/it, loss 0.468001
Finished training it 9216/76743 of epoch 6, 68.36 ms/it, loss 0.464993
Finished training it 9216/76743 of epoch 6, 68.15 ms/it, loss 0.463888
Finished training it 10240/76743 of epoch 6, 69.22 ms/it, loss 0.463566
Finished training it 10240/76743 of epoch 6, 68.85 ms/it, loss 0.468168
Finished training it 10240/76743 of epoch 6, 68.58 ms/it, loss 0.465515
Finished training it 10240/76743 of epoch 6, 68.76 ms/it, loss 0.462561
Testing at - 10240/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549134.0
get out
0 has test check 2549134.0 and sample count 3274240
 accuracy 77.854 %, best 77.860 %, roc auc score 0.7794, best 0.7794
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549134.0
get out
2 has test check 2549134.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 63.92 ms/it, loss 0.468259
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549134.0
get out
1 has test check 2549134.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 63.50 ms/it, loss 0.461961
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549134.0
get out
3 has test check 2549134.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 63.68 ms/it, loss 0.462809
Finished training it 11264/76743 of epoch 6, 63.62 ms/it, loss 0.465399
Finished training it 12288/76743 of epoch 6, 68.37 ms/it, loss 0.466595
Finished training it 12288/76743 of epoch 6, 68.03 ms/it, loss 0.464946
Finished training it 12288/76743 of epoch 6, 68.10 ms/it, loss 0.463354
Finished training it 12288/76743 of epoch 6, 68.20 ms/it, loss 0.465584
Finished training it 13312/76743 of epoch 6, 69.63 ms/it, loss 0.463782
Finished training it 13312/76743 of epoch 6, 69.80 ms/it, loss 0.465991
Finished training it 13312/76743 of epoch 6, 70.16 ms/it, loss 0.464866
Finished training it 13312/76743 of epoch 6, 69.98 ms/it, loss 0.467493
Finished training it 14336/76743 of epoch 6, 63.43 ms/it, loss 0.464234
Finished training it 14336/76743 of epoch 6, 63.74 ms/it, loss 0.464896
Finished training it 14336/76743 of epoch 6, 63.60 ms/it, loss 0.467399
Finished training it 14336/76743 of epoch 6, 63.59 ms/it, loss 0.464613
Finished training it 15360/76743 of epoch 6, 63.77 ms/it, loss 0.466396
Finished training it 15360/76743 of epoch 6, 63.53 ms/it, loss 0.461668
Finished training it 15360/76743 of epoch 6, 63.68 ms/it, loss 0.462472
Finished training it 15360/76743 of epoch 6, 63.46 ms/it, loss 0.463717
Finished training it 16384/76743 of epoch 6, 63.76 ms/it, loss 0.464611
Finished training it 16384/76743 of epoch 6, 63.63 ms/it, loss 0.466380
Finished training it 16384/76743 of epoch 6, 63.94 ms/it, loss 0.465521
Finished training it 16384/76743 of epoch 6, 63.73 ms/it, loss 0.465696
Finished training it 17408/76743 of epoch 6, 63.49 ms/it, loss 0.465346
Finished training it 17408/76743 of epoch 6, 63.45 ms/it, loss 0.465044
Finished training it 17408/76743 of epoch 6, 63.48 ms/it, loss 0.463505
Finished training it 17408/76743 of epoch 6, 63.70 ms/it, loss 0.463796
Finished training it 18432/76743 of epoch 6, 63.49 ms/it, loss 0.464281
Finished training it 18432/76743 of epoch 6, 63.72 ms/it, loss 0.464195
Finished training it 18432/76743 of epoch 6, 63.43 ms/it, loss 0.463540
Finished training it 18432/76743 of epoch 6, 63.43 ms/it, loss 0.465946
Finished training it 19456/76743 of epoch 6, 63.27 ms/it, loss 0.467169
Finished training it 19456/76743 of epoch 6, 63.61 ms/it, loss 0.465839
Finished training it 19456/76743 of epoch 6, 63.45 ms/it, loss 0.463873
Finished training it 19456/76743 of epoch 6, 63.36 ms/it, loss 0.465049
Finished training it 20480/76743 of epoch 6, 63.95 ms/it, loss 0.465103
Finished training it 20480/76743 of epoch 6, 63.78 ms/it, loss 0.463688
Finished training it 20480/76743 of epoch 6, 63.76 ms/it, loss 0.462932
Finished training it 20480/76743 of epoch 6, 63.74 ms/it, loss 0.462398
Testing at - 20480/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549437.0
get out
0 has test check 2549437.0 and sample count 3274240
 accuracy 77.863 %, best 77.863 %, roc auc score 0.7790, best 0.7794
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 6, 63.45 ms/it, loss 0.461489
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549437.0
get out
3 has test check 2549437.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 63.53 ms/it, loss 0.464226
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549437.0
get out
1 has test check 2549437.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 63.60 ms/it, loss 0.463205
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549437.0
get out
2 has test check 2549437.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 63.92 ms/it, loss 0.465313
Finished training it 22528/76743 of epoch 6, 63.86 ms/it, loss 0.464381
Finished training it 22528/76743 of epoch 6, 63.76 ms/it, loss 0.466198
Finished training it 22528/76743 of epoch 6, 63.69 ms/it, loss 0.462220
Finished training it 22528/76743 of epoch 6, 63.58 ms/it, loss 0.463230
Finished training it 23552/76743 of epoch 6, 63.31 ms/it, loss 0.464060
Finished training it 23552/76743 of epoch 6, 63.64 ms/it, loss 0.467040
Finished training it 23552/76743 of epoch 6, 63.18 ms/it, loss 0.461793
Finished training it 23552/76743 of epoch 6, 63.30 ms/it, loss 0.465163
Finished training it 24576/76743 of epoch 6, 63.61 ms/it, loss 0.465544
Finished training it 24576/76743 of epoch 6, 63.29 ms/it, loss 0.464934
Finished training it 24576/76743 of epoch 6, 63.37 ms/it, loss 0.463911
Finished training it 24576/76743 of epoch 6, 63.16 ms/it, loss 0.464994
Finished training it 25600/76743 of epoch 6, 63.95 ms/it, loss 0.466870
Finished training it 25600/76743 of epoch 6, 63.63 ms/it, loss 0.464371
Finished training it 25600/76743 of epoch 6, 63.65 ms/it, loss 0.465685
Finished training it 25600/76743 of epoch 6, 63.65 ms/it, loss 0.464548
Finished training it 26624/76743 of epoch 6, 63.28 ms/it, loss 0.463838
Finished training it 26624/76743 of epoch 6, 63.19 ms/it, loss 0.464007
Finished training it 26624/76743 of epoch 6, 63.53 ms/it, loss 0.463963
Finished training it 26624/76743 of epoch 6, 63.26 ms/it, loss 0.465982
Finished training it 27648/76743 of epoch 6, 63.14 ms/it, loss 0.465589
Finished training it 27648/76743 of epoch 6, 63.21 ms/it, loss 0.464152
Finished training it 27648/76743 of epoch 6, 63.39 ms/it, loss 0.468755
Finished training it 27648/76743 of epoch 6, 63.14 ms/it, loss 0.464760
Finished training it 28672/76743 of epoch 6, 63.22 ms/it, loss 0.463858
Finished training it 28672/76743 of epoch 6, 63.62 ms/it, loss 0.463674
Finished training it 28672/76743 of epoch 6, 63.37 ms/it, loss 0.465796
Finished training it 28672/76743 of epoch 6, 63.34 ms/it, loss 0.466379
Finished training it 29696/76743 of epoch 6, 63.50 ms/it, loss 0.465327
Finished training it 29696/76743 of epoch 6, 63.75 ms/it, loss 0.463010
Finished training it 29696/76743 of epoch 6, 63.46 ms/it, loss 0.461806
Finished training it 29696/76743 of epoch 6, 63.51 ms/it, loss 0.462196
Finished training it 30720/76743 of epoch 6, 63.87 ms/it, loss 0.464908
Finished training it 30720/76743 of epoch 6, 63.77 ms/it, loss 0.464244
Finished training it 30720/76743 of epoch 6, 64.03 ms/it, loss 0.462345
Finished training it 30720/76743 of epoch 6, 63.81 ms/it, loss 0.465444
Testing at - 30720/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550087.0
get out
0 has test check 2550087.0 and sample count 3274240
 accuracy 77.883 %, best 77.883 %, roc auc score 0.7792, best 0.7794
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 6, 63.42 ms/it, loss 0.465136
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550087.0
get out
2 has test check 2550087.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 63.78 ms/it, loss 0.464491
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550087.0
get out
1 has test check 2550087.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 63.61 ms/it, loss 0.465290
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550087.0
get out
3 has test check 2550087.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 63.58 ms/it, loss 0.463811
Finished training it 32768/76743 of epoch 6, 63.24 ms/it, loss 0.463165
Finished training it 32768/76743 of epoch 6, 63.27 ms/it, loss 0.462012
Finished training it 32768/76743 of epoch 6, 63.38 ms/it, loss 0.464032
Finished training it 32768/76743 of epoch 6, 63.13 ms/it, loss 0.465918
Finished training it 33792/76743 of epoch 6, 64.01 ms/it, loss 0.461236
Finished training it 33792/76743 of epoch 6, 63.83 ms/it, loss 0.464853
Finished training it 33792/76743 of epoch 6, 63.71 ms/it, loss 0.464489
Finished training it 33792/76743 of epoch 6, 63.79 ms/it, loss 0.461519
Finished training it 34816/76743 of epoch 6, 73.08 ms/it, loss 0.461371
Finished training it 34816/76743 of epoch 6, 72.93 ms/it, loss 0.466083
Finished training it 34816/76743 of epoch 6, 73.18 ms/it, loss 0.464723
Finished training it 34816/76743 of epoch 6, 73.11 ms/it, loss 0.464057
Finished training it 35840/76743 of epoch 6, 63.23 ms/it, loss 0.466111
Finished training it 35840/76743 of epoch 6, 63.60 ms/it, loss 0.463747
Finished training it 35840/76743 of epoch 6, 63.38 ms/it, loss 0.464032
Finished training it 35840/76743 of epoch 6, 63.31 ms/it, loss 0.463555
Finished training it 36864/76743 of epoch 6, 63.82 ms/it, loss 0.462732
Finished training it 36864/76743 of epoch 6, 64.14 ms/it, loss 0.464369
Finished training it 36864/76743 of epoch 6, 63.92 ms/it, loss 0.464924
Finished training it 36864/76743 of epoch 6, 63.90 ms/it, loss 0.462482
Finished training it 37888/76743 of epoch 6, 63.58 ms/it, loss 0.462679
Finished training it 37888/76743 of epoch 6, 63.51 ms/it, loss 0.463696
Finished training it 37888/76743 of epoch 6, 63.64 ms/it, loss 0.465125
Finished training it 37888/76743 of epoch 6, 63.80 ms/it, loss 0.465442
Finished training it 38912/76743 of epoch 6, 64.33 ms/it, loss 0.465298
Finished training it 38912/76743 of epoch 6, 64.24 ms/it, loss 0.466896
Finished training it 38912/76743 of epoch 6, 64.16 ms/it, loss 0.463237
Finished training it 38912/76743 of epoch 6, 64.44 ms/it, loss 0.462699
Finished training it 39936/76743 of epoch 6, 64.24 ms/it, loss 0.465196
Finished training it 39936/76743 of epoch 6, 64.28 ms/it, loss 0.465764
Finished training it 39936/76743 of epoch 6, 64.47 ms/it, loss 0.464306
Finished training it 39936/76743 of epoch 6, 64.13 ms/it, loss 0.464092
Finished training it 40960/76743 of epoch 6, 63.78 ms/it, loss 0.465039
Finished training it 40960/76743 of epoch 6, 64.16 ms/it, loss 0.462264
Finished training it 40960/76743 of epoch 6, 64.02 ms/it, loss 0.463507
Finished training it 40960/76743 of epoch 6, 63.89 ms/it, loss 0.464233
Testing at - 40960/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551526.0
get out
0 has test check 2551526.0 and sample count 3274240
 accuracy 77.927 %, best 77.927 %, roc auc score 0.7802, best 0.7802
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 6, 63.81 ms/it, loss 0.464217
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551526.0
get out
2 has test check 2551526.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 64.09 ms/it, loss 0.464148
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551526.0
get out
3 has test check 2551526.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 63.78 ms/it, loss 0.462341
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551526.0
get out
1 has test check 2551526.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 63.86 ms/it, loss 0.462381
Finished training it 43008/76743 of epoch 6, 63.47 ms/it, loss 0.467248
Finished training it 43008/76743 of epoch 6, 63.54 ms/it, loss 0.463360
Finished training it 43008/76743 of epoch 6, 63.70 ms/it, loss 0.460028
Finished training it 43008/76743 of epoch 6, 63.34 ms/it, loss 0.462700
Finished training it 44032/76743 of epoch 6, 64.16 ms/it, loss 0.467541
Finished training it 44032/76743 of epoch 6, 64.03 ms/it, loss 0.462496
Finished training it 44032/76743 of epoch 6, 63.95 ms/it, loss 0.462138
Finished training it 44032/76743 of epoch 6, 63.89 ms/it, loss 0.463873
Finished training it 45056/76743 of epoch 6, 63.86 ms/it, loss 0.462562
Finished training it 45056/76743 of epoch 6, 63.59 ms/it, loss 0.463895
Finished training it 45056/76743 of epoch 6, 63.52 ms/it, loss 0.463742
Finished training it 45056/76743 of epoch 6, 63.61 ms/it, loss 0.466302
Finished training it 46080/76743 of epoch 6, 63.60 ms/it, loss 0.463919
Finished training it 46080/76743 of epoch 6, 63.82 ms/it, loss 0.464609
Finished training it 46080/76743 of epoch 6, 63.53 ms/it, loss 0.465171
Finished training it 46080/76743 of epoch 6, 63.50 ms/it, loss 0.463558
Finished training it 47104/76743 of epoch 6, 63.85 ms/it, loss 0.466549
Finished training it 47104/76743 of epoch 6, 63.82 ms/it, loss 0.464119
Finished training it 47104/76743 of epoch 6, 63.88 ms/it, loss 0.465812
Finished training it 47104/76743 of epoch 6, 64.14 ms/it, loss 0.462481
Finished training it 48128/76743 of epoch 6, 63.27 ms/it, loss 0.462484
Finished training it 48128/76743 of epoch 6, 63.42 ms/it, loss 0.466601
Finished training it 48128/76743 of epoch 6, 63.18 ms/it, loss 0.462171
Finished training it 48128/76743 of epoch 6, 63.18 ms/it, loss 0.463071
Finished training it 49152/76743 of epoch 6, 63.32 ms/it, loss 0.464605
Finished training it 49152/76743 of epoch 6, 63.57 ms/it, loss 0.464227
Finished training it 49152/76743 of epoch 6, 63.17 ms/it, loss 0.463089
Finished training it 49152/76743 of epoch 6, 63.41 ms/it, loss 0.464351
Finished training it 50176/76743 of epoch 6, 64.12 ms/it, loss 0.461873
Finished training it 50176/76743 of epoch 6, 64.25 ms/it, loss 0.462300
Finished training it 50176/76743 of epoch 6, 64.06 ms/it, loss 0.462890
Finished training it 50176/76743 of epoch 6, 63.98 ms/it, loss 0.461571
Finished training it 51200/76743 of epoch 6, 63.83 ms/it, loss 0.462447
Finished training it 51200/76743 of epoch 6, 63.58 ms/it, loss 0.462839
Finished training it 51200/76743 of epoch 6, 63.66 ms/it, loss 0.464530
Finished training it 51200/76743 of epoch 6, 63.53 ms/it, loss 0.465076
Testing at - 51200/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549613.0
get out
0 has test check 2549613.0 and sample count 3274240
 accuracy 77.869 %, best 77.927 %, roc auc score 0.7802, best 0.7802
Finished training it 52224/76743 of epoch 6, 64.00 ms/it, loss 0.464625
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549613.0
get out
2 has test check 2549613.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 64.17 ms/it, loss 0.462678
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549613.0
get out
1 has test check 2549613.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 64.01 ms/it, loss 0.462556
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549613.0
get out
3 has test check 2549613.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 64.05 ms/it, loss 0.464503
Finished training it 53248/76743 of epoch 6, 63.78 ms/it, loss 0.460741
Finished training it 53248/76743 of epoch 6, 63.53 ms/it, loss 0.464491
Finished training it 53248/76743 of epoch 6, 63.49 ms/it, loss 0.462167
Finished training it 53248/76743 of epoch 6, 63.54 ms/it, loss 0.463580
Finished training it 54272/76743 of epoch 6, 63.63 ms/it, loss 0.466297
Finished training it 54272/76743 of epoch 6, 63.47 ms/it, loss 0.462760
Finished training it 54272/76743 of epoch 6, 63.61 ms/it, loss 0.463961
Finished training it 54272/76743 of epoch 6, 63.86 ms/it, loss 0.463452
Finished training it 55296/76743 of epoch 6, 68.99 ms/it, loss 0.461105
Finished training it 55296/76743 of epoch 6, 69.41 ms/it, loss 0.465161
Finished training it 55296/76743 of epoch 6, 69.17 ms/it, loss 0.468828
Finished training it 55296/76743 of epoch 6, 68.79 ms/it, loss 0.462280
Finished training it 56320/76743 of epoch 6, 68.49 ms/it, loss 0.468297
Finished training it 56320/76743 of epoch 6, 68.66 ms/it, loss 0.462553
Finished training it 56320/76743 of epoch 6, 68.41 ms/it, loss 0.466362
Finished training it 56320/76743 of epoch 6, 68.36 ms/it, loss 0.462903
Finished training it 57344/76743 of epoch 6, 63.60 ms/it, loss 0.464286
Finished training it 57344/76743 of epoch 6, 63.35 ms/it, loss 0.462738
Finished training it 57344/76743 of epoch 6, 63.35 ms/it, loss 0.463042
Finished training it 57344/76743 of epoch 6, 63.37 ms/it, loss 0.462681
Finished training it 58368/76743 of epoch 6, 63.56 ms/it, loss 0.461587
Finished training it 58368/76743 of epoch 6, 63.57 ms/it, loss 0.465776
Finished training it 58368/76743 of epoch 6, 63.57 ms/it, loss 0.463233
Finished training it 58368/76743 of epoch 6, 63.89 ms/it, loss 0.464682
Finished training it 59392/76743 of epoch 6, 63.35 ms/it, loss 0.462740
Finished training it 59392/76743 of epoch 6, 63.58 ms/it, loss 0.464989
Finished training it 59392/76743 of epoch 6, 63.31 ms/it, loss 0.462043
Finished training it 59392/76743 of epoch 6, 63.37 ms/it, loss 0.462622
Finished training it 60416/76743 of epoch 6, 63.94 ms/it, loss 0.464431
Finished training it 60416/76743 of epoch 6, 63.77 ms/it, loss 0.465634
Finished training it 60416/76743 of epoch 6, 63.79 ms/it, loss 0.463671
Finished training it 60416/76743 of epoch 6, 63.68 ms/it, loss 0.461762
Finished training it 61440/76743 of epoch 6, 63.58 ms/it, loss 0.464386
Finished training it 61440/76743 of epoch 6, 63.63 ms/it, loss 0.462828
Finished training it 61440/76743 of epoch 6, 63.91 ms/it, loss 0.465214
Finished training it 61440/76743 of epoch 6, 63.80 ms/it, loss 0.464911
Testing at - 61440/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550378.0
get out
0 has test check 2550378.0 and sample count 3274240
 accuracy 77.892 %, best 77.927 %, roc auc score 0.7797, best 0.7802
Finished training it 62464/76743 of epoch 6, 63.73 ms/it, loss 0.463283
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550378.0
get out
2 has test check 2550378.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 63.95 ms/it, loss 0.465269
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550378.0
get out
1 has test check 2550378.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 63.66 ms/it, loss 0.464811
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550378.0
get out
3 has test check 2550378.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 63.65 ms/it, loss 0.464362
Finished training it 63488/76743 of epoch 6, 63.49 ms/it, loss 0.463092
Finished training it 63488/76743 of epoch 6, 63.54 ms/it, loss 0.462109
Finished training it 63488/76743 of epoch 6, 63.78 ms/it, loss 0.463286
Finished training it 63488/76743 of epoch 6, 63.40 ms/it, loss 0.466974
Finished training it 64512/76743 of epoch 6, 64.18 ms/it, loss 0.461440
Finished training it 64512/76743 of epoch 6, 63.97 ms/it, loss 0.464293
Finished training it 64512/76743 of epoch 6, 63.99 ms/it, loss 0.465479
Finished training it 64512/76743 of epoch 6, 63.90 ms/it, loss 0.466299
Finished training it 65536/76743 of epoch 6, 64.07 ms/it, loss 0.464281
Finished training it 65536/76743 of epoch 6, 63.70 ms/it, loss 0.463820
Finished training it 65536/76743 of epoch 6, 63.85 ms/it, loss 0.463067
Finished training it 65536/76743 of epoch 6, 63.80 ms/it, loss 0.461495
Finished training it 66560/76743 of epoch 6, 63.90 ms/it, loss 0.463107
Finished training it 66560/76743 of epoch 6, 63.77 ms/it, loss 0.462881
Finished training it 66560/76743 of epoch 6, 63.70 ms/it, loss 0.464023
Finished training it 66560/76743 of epoch 6, 63.58 ms/it, loss 0.462567
Finished training it 67584/76743 of epoch 6, 63.64 ms/it, loss 0.463508
Finished training it 67584/76743 of epoch 6, 63.49 ms/it, loss 0.466678
Finished training it 67584/76743 of epoch 6, 63.37 ms/it, loss 0.465252
Finished training it 67584/76743 of epoch 6, 63.36 ms/it, loss 0.463615
Finished training it 68608/76743 of epoch 6, 63.35 ms/it, loss 0.463156
Finished training it 68608/76743 of epoch 6, 63.33 ms/it, loss 0.463459
Finished training it 68608/76743 of epoch 6, 63.14 ms/it, loss 0.462129
Finished training it 68608/76743 of epoch 6, 63.58 ms/it, loss 0.459930
Finished training it 69632/76743 of epoch 6, 64.13 ms/it, loss 0.464013
Finished training it 69632/76743 of epoch 6, 63.80 ms/it, loss 0.463021
Finished training it 69632/76743 of epoch 6, 63.85 ms/it, loss 0.462825
Finished training it 69632/76743 of epoch 6, 63.97 ms/it, loss 0.463080
Finished training it 70656/76743 of epoch 6, 63.54 ms/it, loss 0.465210
Finished training it 70656/76743 of epoch 6, 63.77 ms/it, loss 0.464393
Finished training it 70656/76743 of epoch 6, 63.55 ms/it, loss 0.463839
Finished training it 70656/76743 of epoch 6, 63.50 ms/it, loss 0.460334
Finished training it 71680/76743 of epoch 6, 63.84 ms/it, loss 0.462888
Finished training it 71680/76743 of epoch 6, 63.98 ms/it, loss 0.463163
Finished training it 71680/76743 of epoch 6, 63.83 ms/it, loss 0.466352
Finished training it 71680/76743 of epoch 6, 63.77 ms/it, loss 0.462625
Testing at - 71680/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550359.0
get out
0 has test check 2550359.0 and sample count 3274240
 accuracy 77.892 %, best 77.927 %, roc auc score 0.7798, best 0.7802
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550359.0
get out
1 has test check 2550359.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 63.41 ms/it, loss 0.462360
Finished training it 72704/76743 of epoch 6, 63.41 ms/it, loss 0.464208
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550359.0
get out
3 has test check 2550359.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 63.37 ms/it, loss 0.467240
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550359.0
get out
2 has test check 2550359.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 63.63 ms/it, loss 0.463512
Finished training it 73728/76743 of epoch 6, 63.77 ms/it, loss 0.463925
Finished training it 73728/76743 of epoch 6, 64.04 ms/it, loss 0.463745
Finished training it 73728/76743 of epoch 6, 63.71 ms/it, loss 0.463440
Finished training it 73728/76743 of epoch 6, 63.79 ms/it, loss 0.464069
Finished training it 74752/76743 of epoch 6, 63.58 ms/it, loss 0.463394
Finished training it 74752/76743 of epoch 6, 63.98 ms/it, loss 0.463026
Finished training it 74752/76743 of epoch 6, 63.64 ms/it, loss 0.465590
Finished training it 74752/76743 of epoch 6, 63.67 ms/it, loss 0.463137
Finished training it 75776/76743 of epoch 6, 69.51 ms/it, loss 0.464728
Finished training it 75776/76743 of epoch 6, 69.30 ms/it, loss 0.464395
Finished training it 75776/76743 of epoch 6, 69.64 ms/it, loss 0.462672
Finished training it 75776/76743 of epoch 6, 69.46 ms/it, loss 0.461284
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 68.17 ms/it, loss 0.462652
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 67.80 ms/it, loss 0.463624
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 67.60 ms/it, loss 0.463247
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 67.50 ms/it, loss 0.463747
Finished training it 2048/76743 of epoch 7, 63.54 ms/it, loss 0.464649
Finished training it 2048/76743 of epoch 7, 63.81 ms/it, loss 0.463694
Finished training it 2048/76743 of epoch 7, 63.57 ms/it, loss 0.461959
Finished training it 2048/76743 of epoch 7, 63.59 ms/it, loss 0.463072
Finished training it 3072/76743 of epoch 7, 63.85 ms/it, loss 0.462423
Finished training it 3072/76743 of epoch 7, 63.92 ms/it, loss 0.465416
Finished training it 3072/76743 of epoch 7, 64.23 ms/it, loss 0.465591
Finished training it 3072/76743 of epoch 7, 63.95 ms/it, loss 0.463747
Finished training it 4096/76743 of epoch 7, 63.48 ms/it, loss 0.463460
Finished training it 4096/76743 of epoch 7, 63.42 ms/it, loss 0.464475
Finished training it 4096/76743 of epoch 7, 63.53 ms/it, loss 0.463198
Finished training it 4096/76743 of epoch 7, 63.77 ms/it, loss 0.464770
Finished training it 5120/76743 of epoch 7, 63.68 ms/it, loss 0.465239
Finished training it 5120/76743 of epoch 7, 63.38 ms/it, loss 0.465476
Finished training it 5120/76743 of epoch 7, 63.35 ms/it, loss 0.464610
Finished training it 5120/76743 of epoch 7, 63.40 ms/it, loss 0.464394
Finished training it 6144/76743 of epoch 7, 63.55 ms/it, loss 0.465714
Finished training it 6144/76743 of epoch 7, 63.54 ms/it, loss 0.463834
Finished training it 6144/76743 of epoch 7, 63.83 ms/it, loss 0.463918
Finished training it 6144/76743 of epoch 7, 63.56 ms/it, loss 0.462198
Finished training it 7168/76743 of epoch 7, 63.82 ms/it, loss 0.464613
Finished training it 7168/76743 of epoch 7, 64.06 ms/it, loss 0.464894
Finished training it 7168/76743 of epoch 7, 63.87 ms/it, loss 0.464582
Finished training it 7168/76743 of epoch 7, 63.74 ms/it, loss 0.462168
Finished training it 8192/76743 of epoch 7, 63.35 ms/it, loss 0.464729
Finished training it 8192/76743 of epoch 7, 63.26 ms/it, loss 0.462885
Finished training it 8192/76743 of epoch 7, 63.61 ms/it, loss 0.463457
Finished training it 8192/76743 of epoch 7, 63.34 ms/it, loss 0.463428
Finished training it 9216/76743 of epoch 7, 63.29 ms/it, loss 0.465652
Finished training it 9216/76743 of epoch 7, 63.22 ms/it, loss 0.462225
Finished training it 9216/76743 of epoch 7, 63.55 ms/it, loss 0.464261
Finished training it 9216/76743 of epoch 7, 63.27 ms/it, loss 0.462906
Finished training it 10240/76743 of epoch 7, 63.62 ms/it, loss 0.466364
Finished training it 10240/76743 of epoch 7, 64.07 ms/it, loss 0.462017
Finished training it 10240/76743 of epoch 7, 63.72 ms/it, loss 0.461525
Finished training it 10240/76743 of epoch 7, 63.77 ms/it, loss 0.464046
Testing at - 10240/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550110.0
get out
0 has test check 2550110.0 and sample count 3274240
 accuracy 77.884 %, best 77.927 %, roc auc score 0.7799, best 0.7802
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550110.0
get out
3 has test check 2550110.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 63.24 ms/it, loss 0.461495
Finished training it 11264/76743 of epoch 7, 63.20 ms/it, loss 0.463731
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550110.0
get out
2 has test check 2550110.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 63.44 ms/it, loss 0.466341
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550110.0
get out
1 has test check 2550110.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 63.25 ms/it, loss 0.460537
Finished training it 12288/76743 of epoch 7, 63.76 ms/it, loss 0.461754
Finished training it 12288/76743 of epoch 7, 63.60 ms/it, loss 0.463724
Finished training it 12288/76743 of epoch 7, 63.82 ms/it, loss 0.462923
Finished training it 12288/76743 of epoch 7, 63.96 ms/it, loss 0.464238
Finished training it 13312/76743 of epoch 7, 63.89 ms/it, loss 0.463131
Finished training it 13312/76743 of epoch 7, 63.51 ms/it, loss 0.462029
Finished training it 13312/76743 of epoch 7, 63.50 ms/it, loss 0.463751
Finished training it 13312/76743 of epoch 7, 63.54 ms/it, loss 0.465426
Finished training it 14336/76743 of epoch 7, 63.68 ms/it, loss 0.462406
Finished training it 14336/76743 of epoch 7, 63.95 ms/it, loss 0.462905
Finished training it 14336/76743 of epoch 7, 63.70 ms/it, loss 0.462626
Finished training it 14336/76743 of epoch 7, 63.70 ms/it, loss 0.464967
Finished training it 15360/76743 of epoch 7, 63.06 ms/it, loss 0.460238
Finished training it 15360/76743 of epoch 7, 63.41 ms/it, loss 0.465043
Finished training it 15360/76743 of epoch 7, 63.18 ms/it, loss 0.460977
Finished training it 15360/76743 of epoch 7, 63.18 ms/it, loss 0.462132
Finished training it 16384/76743 of epoch 7, 63.85 ms/it, loss 0.464149
Finished training it 16384/76743 of epoch 7, 63.78 ms/it, loss 0.464192
Finished training it 16384/76743 of epoch 7, 64.13 ms/it, loss 0.463772
Finished training it 16384/76743 of epoch 7, 63.74 ms/it, loss 0.462510
Finished training it 17408/76743 of epoch 7, 63.79 ms/it, loss 0.462016
Finished training it 17408/76743 of epoch 7, 63.62 ms/it, loss 0.463609
Finished training it 17408/76743 of epoch 7, 63.42 ms/it, loss 0.463223
Finished training it 17408/76743 of epoch 7, 63.60 ms/it, loss 0.461971
Finished training it 18432/76743 of epoch 7, 63.66 ms/it, loss 0.461777
Finished training it 18432/76743 of epoch 7, 63.68 ms/it, loss 0.462759
Finished training it 18432/76743 of epoch 7, 63.93 ms/it, loss 0.462958
Finished training it 18432/76743 of epoch 7, 63.56 ms/it, loss 0.464441
Finished training it 19456/76743 of epoch 7, 63.32 ms/it, loss 0.465897
Finished training it 19456/76743 of epoch 7, 63.22 ms/it, loss 0.462473
Finished training it 19456/76743 of epoch 7, 63.32 ms/it, loss 0.463187
Finished training it 19456/76743 of epoch 7, 63.55 ms/it, loss 0.463491
Finished training it 20480/76743 of epoch 7, 63.34 ms/it, loss 0.461689
Finished training it 20480/76743 of epoch 7, 63.43 ms/it, loss 0.461197
Finished training it 20480/76743 of epoch 7, 63.66 ms/it, loss 0.463469
Finished training it 20480/76743 of epoch 7, 63.45 ms/it, loss 0.462626
Testing at - 20480/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551634.0
get out
0 has test check 2551634.0 and sample count 3274240
 accuracy 77.931 %, best 77.931 %, roc auc score 0.7799, best 0.7802
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551634.0
get out
2 has test check 2551634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 63.67 ms/it, loss 0.464097
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 7, 63.51 ms/it, loss 0.460114
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551634.0
get out
1 has test check 2551634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 63.41 ms/it, loss 0.461471
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551634.0
get out
3 has test check 2551634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 63.39 ms/it, loss 0.462564
Finished training it 22528/76743 of epoch 7, 63.64 ms/it, loss 0.460517
Finished training it 22528/76743 of epoch 7, 63.73 ms/it, loss 0.464628
Finished training it 22528/76743 of epoch 7, 63.72 ms/it, loss 0.461178
Finished training it 22528/76743 of epoch 7, 63.96 ms/it, loss 0.462854
Finished training it 23552/76743 of epoch 7, 63.55 ms/it, loss 0.463414
Finished training it 23552/76743 of epoch 7, 63.67 ms/it, loss 0.459969
Finished training it 23552/76743 of epoch 7, 63.94 ms/it, loss 0.465119
Finished training it 23552/76743 of epoch 7, 63.70 ms/it, loss 0.462222
Finished training it 24576/76743 of epoch 7, 73.71 ms/it, loss 0.463283
Finished training it 24576/76743 of epoch 7, 73.20 ms/it, loss 0.462969
Finished training it 24576/76743 of epoch 7, 73.27 ms/it, loss 0.463245
Finished training it 24576/76743 of epoch 7, 73.44 ms/it, loss 0.462295
Finished training it 25600/76743 of epoch 7, 63.99 ms/it, loss 0.463057
Finished training it 25600/76743 of epoch 7, 64.06 ms/it, loss 0.463530
Finished training it 25600/76743 of epoch 7, 64.28 ms/it, loss 0.465181
Finished training it 25600/76743 of epoch 7, 64.00 ms/it, loss 0.462682
Finished training it 26624/76743 of epoch 7, 63.97 ms/it, loss 0.462395
Finished training it 26624/76743 of epoch 7, 63.79 ms/it, loss 0.462273
Finished training it 26624/76743 of epoch 7, 63.71 ms/it, loss 0.463065
Finished training it 26624/76743 of epoch 7, 63.75 ms/it, loss 0.464607
Finished training it 27648/76743 of epoch 7, 63.48 ms/it, loss 0.462995
Finished training it 27648/76743 of epoch 7, 63.45 ms/it, loss 0.464590
Finished training it 27648/76743 of epoch 7, 63.46 ms/it, loss 0.462342
Finished training it 27648/76743 of epoch 7, 63.80 ms/it, loss 0.467462
Finished training it 28672/76743 of epoch 7, 63.88 ms/it, loss 0.462355
Finished training it 28672/76743 of epoch 7, 64.10 ms/it, loss 0.462105
Finished training it 28672/76743 of epoch 7, 63.94 ms/it, loss 0.465058
Finished training it 28672/76743 of epoch 7, 63.91 ms/it, loss 0.464128
Finished training it 29696/76743 of epoch 7, 63.45 ms/it, loss 0.461116
Finished training it 29696/76743 of epoch 7, 63.35 ms/it, loss 0.460346
Finished training it 29696/76743 of epoch 7, 63.26 ms/it, loss 0.463497
Finished training it 29696/76743 of epoch 7, 63.30 ms/it, loss 0.460578
Finished training it 30720/76743 of epoch 7, 63.62 ms/it, loss 0.463764
Finished training it 30720/76743 of epoch 7, 63.96 ms/it, loss 0.460413
Finished training it 30720/76743 of epoch 7, 63.51 ms/it, loss 0.462714
Finished training it 30720/76743 of epoch 7, 63.70 ms/it, loss 0.463641
Testing at - 30720/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553218.0
get out
0 has test check 2553218.0 and sample count 3274240
 accuracy 77.979 %, best 77.979 %, roc auc score 0.7814, best 0.7814
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 7, 63.47 ms/it, loss 0.463357
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553218.0
get out
1 has test check 2553218.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 63.64 ms/it, loss 0.463676
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553218.0
get out
2 has test check 2553218.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 63.80 ms/it, loss 0.463382
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553218.0
get out
3 has test check 2553218.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 63.75 ms/it, loss 0.462420
Finished training it 32768/76743 of epoch 7, 63.31 ms/it, loss 0.461863
Finished training it 32768/76743 of epoch 7, 63.74 ms/it, loss 0.462302
Finished training it 32768/76743 of epoch 7, 63.52 ms/it, loss 0.460124
Finished training it 32768/76743 of epoch 7, 63.48 ms/it, loss 0.463942
Finished training it 33792/76743 of epoch 7, 63.59 ms/it, loss 0.459810
Finished training it 33792/76743 of epoch 7, 63.98 ms/it, loss 0.459588
Finished training it 33792/76743 of epoch 7, 63.69 ms/it, loss 0.463283
Finished training it 33792/76743 of epoch 7, 63.74 ms/it, loss 0.463060
Finished training it 34816/76743 of epoch 7, 63.53 ms/it, loss 0.462375
Finished training it 34816/76743 of epoch 7, 63.55 ms/it, loss 0.464985
Finished training it 34816/76743 of epoch 7, 63.50 ms/it, loss 0.460079
Finished training it 34816/76743 of epoch 7, 63.78 ms/it, loss 0.463050
Finished training it 35840/76743 of epoch 7, 63.69 ms/it, loss 0.461714
Finished training it 35840/76743 of epoch 7, 63.55 ms/it, loss 0.464339
Finished training it 35840/76743 of epoch 7, 63.40 ms/it, loss 0.462313
Finished training it 35840/76743 of epoch 7, 63.40 ms/it, loss 0.462459
Finished training it 36864/76743 of epoch 7, 63.39 ms/it, loss 0.463436
Finished training it 36864/76743 of epoch 7, 63.29 ms/it, loss 0.461595
Finished training it 36864/76743 of epoch 7, 63.41 ms/it, loss 0.461156
Finished training it 36864/76743 of epoch 7, 63.57 ms/it, loss 0.462696
Finished training it 37888/76743 of epoch 7, 63.81 ms/it, loss 0.462111
Finished training it 37888/76743 of epoch 7, 63.98 ms/it, loss 0.464110
Finished training it 37888/76743 of epoch 7, 63.88 ms/it, loss 0.461241
Finished training it 37888/76743 of epoch 7, 64.17 ms/it, loss 0.463849
Finished training it 38912/76743 of epoch 7, 63.83 ms/it, loss 0.461938
Finished training it 38912/76743 of epoch 7, 63.98 ms/it, loss 0.461152
Finished training it 38912/76743 of epoch 7, 63.78 ms/it, loss 0.465402
Finished training it 38912/76743 of epoch 7, 63.83 ms/it, loss 0.464043
Finished training it 39936/76743 of epoch 7, 63.70 ms/it, loss 0.462252
Finished training it 39936/76743 of epoch 7, 64.16 ms/it, loss 0.463108
Finished training it 39936/76743 of epoch 7, 64.08 ms/it, loss 0.463572
Finished training it 39936/76743 of epoch 7, 64.10 ms/it, loss 0.464119
Finished training it 40960/76743 of epoch 7, 63.42 ms/it, loss 0.462753
Finished training it 40960/76743 of epoch 7, 63.70 ms/it, loss 0.461354
Finished training it 40960/76743 of epoch 7, 63.35 ms/it, loss 0.464090
Finished training it 40960/76743 of epoch 7, 63.38 ms/it, loss 0.461769
Testing at - 40960/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553179.0
get out
0 has test check 2553179.0 and sample count 3274240
 accuracy 77.978 %, best 77.979 %, roc auc score 0.7814, best 0.7814
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553179.0
get out
3 has test check 2553179.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 63.86 ms/it, loss 0.461273
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553179.0
get out
1 has test check 2553179.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 63.99 ms/it, loss 0.461532
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553179.0
get out
2 has test check 2553179.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 64.14 ms/it, loss 0.462796
Finished training it 41984/76743 of epoch 7, 64.00 ms/it, loss 0.462728
Finished training it 43008/76743 of epoch 7, 63.21 ms/it, loss 0.461634
Finished training it 43008/76743 of epoch 7, 63.25 ms/it, loss 0.461816
Finished training it 43008/76743 of epoch 7, 63.59 ms/it, loss 0.458979
Finished training it 43008/76743 of epoch 7, 63.44 ms/it, loss 0.465679
Finished training it 44032/76743 of epoch 7, 63.17 ms/it, loss 0.462394
Finished training it 44032/76743 of epoch 7, 63.12 ms/it, loss 0.461343
Finished training it 44032/76743 of epoch 7, 63.44 ms/it, loss 0.465839
Finished training it 44032/76743 of epoch 7, 63.26 ms/it, loss 0.460820
Finished training it 45056/76743 of epoch 7, 73.61 ms/it, loss 0.462167
Finished training it 45056/76743 of epoch 7, 73.65 ms/it, loss 0.460861
Finished training it 45056/76743 of epoch 7, 73.43 ms/it, loss 0.462050
Finished training it 45056/76743 of epoch 7, 73.54 ms/it, loss 0.465264
Finished training it 46080/76743 of epoch 7, 63.96 ms/it, loss 0.462467
Finished training it 46080/76743 of epoch 7, 63.98 ms/it, loss 0.463676
Finished training it 46080/76743 of epoch 7, 64.22 ms/it, loss 0.462764
Finished training it 46080/76743 of epoch 7, 63.97 ms/it, loss 0.462651
Finished training it 47104/76743 of epoch 7, 63.51 ms/it, loss 0.461512
Finished training it 47104/76743 of epoch 7, 63.21 ms/it, loss 0.464755
Finished training it 47104/76743 of epoch 7, 63.29 ms/it, loss 0.465084
Finished training it 47104/76743 of epoch 7, 63.34 ms/it, loss 0.462648
Finished training it 48128/76743 of epoch 7, 63.62 ms/it, loss 0.461349
Finished training it 48128/76743 of epoch 7, 63.98 ms/it, loss 0.465280
Finished training it 48128/76743 of epoch 7, 63.63 ms/it, loss 0.462286
Finished training it 48128/76743 of epoch 7, 63.79 ms/it, loss 0.460834
Finished training it 49152/76743 of epoch 7, 63.54 ms/it, loss 0.463222
Finished training it 49152/76743 of epoch 7, 63.61 ms/it, loss 0.462740
Finished training it 49152/76743 of epoch 7, 63.84 ms/it, loss 0.463239
Finished training it 49152/76743 of epoch 7, 63.60 ms/it, loss 0.461488
Finished training it 50176/76743 of epoch 7, 63.59 ms/it, loss 0.460609
Finished training it 50176/76743 of epoch 7, 63.34 ms/it, loss 0.460424
Finished training it 50176/76743 of epoch 7, 63.36 ms/it, loss 0.461882
Finished training it 50176/76743 of epoch 7, 63.21 ms/it, loss 0.460631
Finished training it 51200/76743 of epoch 7, 63.58 ms/it, loss 0.463327
Finished training it 51200/76743 of epoch 7, 63.70 ms/it, loss 0.461512
Finished training it 51200/76743 of epoch 7, 63.73 ms/it, loss 0.463473
Finished training it 51200/76743 of epoch 7, 63.95 ms/it, loss 0.461808
Testing at - 51200/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551207.0
get out
0 has test check 2551207.0 and sample count 3274240
 accuracy 77.918 %, best 77.979 %, roc auc score 0.7817, best 0.7817
Finished training it 52224/76743 of epoch 7, 63.56 ms/it, loss 0.462777
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551207.0
get out
2 has test check 2551207.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 63.77 ms/it, loss 0.461214
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551207.0
get out
3 has test check 2551207.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 63.47 ms/it, loss 0.463119
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551207.0
get out
1 has test check 2551207.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 63.54 ms/it, loss 0.460857
Finished training it 53248/76743 of epoch 7, 64.07 ms/it, loss 0.459803
Finished training it 53248/76743 of epoch 7, 63.74 ms/it, loss 0.462282
Finished training it 53248/76743 of epoch 7, 63.84 ms/it, loss 0.462962
Finished training it 53248/76743 of epoch 7, 63.85 ms/it, loss 0.461487
Finished training it 54272/76743 of epoch 7, 63.17 ms/it, loss 0.461460
Finished training it 54272/76743 of epoch 7, 63.23 ms/it, loss 0.462988
Finished training it 54272/76743 of epoch 7, 63.58 ms/it, loss 0.462341
Finished training it 54272/76743 of epoch 7, 63.36 ms/it, loss 0.465162
Finished training it 55296/76743 of epoch 7, 63.34 ms/it, loss 0.460961
Finished training it 55296/76743 of epoch 7, 63.78 ms/it, loss 0.463979
Finished training it 55296/76743 of epoch 7, 63.53 ms/it, loss 0.466839
Finished training it 55296/76743 of epoch 7, 63.65 ms/it, loss 0.460119
Finished training it 56320/76743 of epoch 7, 63.45 ms/it, loss 0.464895
Finished training it 56320/76743 of epoch 7, 63.62 ms/it, loss 0.461466
Finished training it 56320/76743 of epoch 7, 63.39 ms/it, loss 0.467094
Finished training it 56320/76743 of epoch 7, 63.50 ms/it, loss 0.461076
Finished training it 57344/76743 of epoch 7, 63.40 ms/it, loss 0.461345
Finished training it 57344/76743 of epoch 7, 63.46 ms/it, loss 0.461370
Finished training it 57344/76743 of epoch 7, 63.58 ms/it, loss 0.462511
Finished training it 57344/76743 of epoch 7, 63.44 ms/it, loss 0.461226
Finished training it 58368/76743 of epoch 7, 63.51 ms/it, loss 0.460505
Finished training it 58368/76743 of epoch 7, 63.44 ms/it, loss 0.464011
Finished training it 58368/76743 of epoch 7, 63.74 ms/it, loss 0.463173
Finished training it 58368/76743 of epoch 7, 63.47 ms/it, loss 0.461802
Finished training it 59392/76743 of epoch 7, 63.42 ms/it, loss 0.460799
Finished training it 59392/76743 of epoch 7, 63.55 ms/it, loss 0.460431
Finished training it 59392/76743 of epoch 7, 63.49 ms/it, loss 0.461043
Finished training it 59392/76743 of epoch 7, 63.71 ms/it, loss 0.463117
Finished training it 60416/76743 of epoch 7, 63.69 ms/it, loss 0.464337
Finished training it 60416/76743 of epoch 7, 63.73 ms/it, loss 0.462286
Finished training it 60416/76743 of epoch 7, 63.64 ms/it, loss 0.460093
Finished training it 60416/76743 of epoch 7, 63.95 ms/it, loss 0.462946
Finished training it 61440/76743 of epoch 7, 63.32 ms/it, loss 0.462593
Finished training it 61440/76743 of epoch 7, 63.46 ms/it, loss 0.463501
Finished training it 61440/76743 of epoch 7, 63.23 ms/it, loss 0.461570
Finished training it 61440/76743 of epoch 7, 63.18 ms/it, loss 0.463625
Testing at - 61440/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553202.0
get out
0 has test check 2553202.0 and sample count 3274240
 accuracy 77.978 %, best 77.979 %, roc auc score 0.7815, best 0.7817
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553202.0
get out
3 has test check 2553202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 63.32 ms/it, loss 0.463411
Finished training it 62464/76743 of epoch 7, 63.56 ms/it, loss 0.461717
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553202.0
get out
1 has test check 2553202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 63.45 ms/it, loss 0.463309
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553202.0
get out
2 has test check 2553202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 63.76 ms/it, loss 0.463758
Finished training it 63488/76743 of epoch 7, 63.44 ms/it, loss 0.460506
Finished training it 63488/76743 of epoch 7, 63.56 ms/it, loss 0.465672
Finished training it 63488/76743 of epoch 7, 63.55 ms/it, loss 0.461766
Finished training it 63488/76743 of epoch 7, 63.77 ms/it, loss 0.462163
Finished training it 64512/76743 of epoch 7, 63.45 ms/it, loss 0.464982
Finished training it 64512/76743 of epoch 7, 63.47 ms/it, loss 0.464148
Finished training it 64512/76743 of epoch 7, 63.41 ms/it, loss 0.463125
Finished training it 64512/76743 of epoch 7, 63.67 ms/it, loss 0.460041
Finished training it 65536/76743 of epoch 7, 70.47 ms/it, loss 0.463005
Finished training it 65536/76743 of epoch 7, 70.23 ms/it, loss 0.460176
Finished training it 65536/76743 of epoch 7, 70.14 ms/it, loss 0.462522
Finished training it 65536/76743 of epoch 7, 69.93 ms/it, loss 0.461962
Finished training it 66560/76743 of epoch 7, 68.26 ms/it, loss 0.462699
Finished training it 66560/76743 of epoch 7, 68.18 ms/it, loss 0.460926
Finished training it 66560/76743 of epoch 7, 68.31 ms/it, loss 0.461501
Finished training it 66560/76743 of epoch 7, 68.57 ms/it, loss 0.461506
Finished training it 67584/76743 of epoch 7, 64.03 ms/it, loss 0.462012
Finished training it 67584/76743 of epoch 7, 63.74 ms/it, loss 0.464008
Finished training it 67584/76743 of epoch 7, 63.78 ms/it, loss 0.465246
Finished training it 67584/76743 of epoch 7, 63.86 ms/it, loss 0.461993
Finished training it 68608/76743 of epoch 7, 63.53 ms/it, loss 0.462043
Finished training it 68608/76743 of epoch 7, 63.57 ms/it, loss 0.461151
Finished training it 68608/76743 of epoch 7, 63.85 ms/it, loss 0.458765
Finished training it 68608/76743 of epoch 7, 63.50 ms/it, loss 0.462309
Finished training it 69632/76743 of epoch 7, 63.22 ms/it, loss 0.461996
Finished training it 69632/76743 of epoch 7, 63.47 ms/it, loss 0.463054
Finished training it 69632/76743 of epoch 7, 63.36 ms/it, loss 0.462058
Finished training it 69632/76743 of epoch 7, 63.15 ms/it, loss 0.461611
Finished training it 70656/76743 of epoch 7, 64.16 ms/it, loss 0.463387
Finished training it 70656/76743 of epoch 7, 64.30 ms/it, loss 0.463099
Finished training it 70656/76743 of epoch 7, 64.08 ms/it, loss 0.459153
Finished training it 70656/76743 of epoch 7, 64.09 ms/it, loss 0.462941
Finished training it 71680/76743 of epoch 7, 63.28 ms/it, loss 0.461213
Finished training it 71680/76743 of epoch 7, 63.34 ms/it, loss 0.464879
Finished training it 71680/76743 of epoch 7, 63.22 ms/it, loss 0.461169
Finished training it 71680/76743 of epoch 7, 63.58 ms/it, loss 0.461458
Testing at - 71680/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553555.0
get out
0 has test check 2553555.0 and sample count 3274240
 accuracy 77.989 %, best 77.989 %, roc auc score 0.7817, best 0.7817
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553555.0
get out
2 has test check 2553555.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 63.86 ms/it, loss 0.461841
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553555.0
get out
3 has test check 2553555.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 63.47 ms/it, loss 0.465428
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 7, 63.52 ms/it, loss 0.462365
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553555.0
get out
1 has test check 2553555.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 63.61 ms/it, loss 0.460429
Finished training it 73728/76743 of epoch 7, 64.06 ms/it, loss 0.462598
Finished training it 73728/76743 of epoch 7, 63.84 ms/it, loss 0.462452
Finished training it 73728/76743 of epoch 7, 63.79 ms/it, loss 0.462844
Finished training it 73728/76743 of epoch 7, 63.75 ms/it, loss 0.462073
Finished training it 74752/76743 of epoch 7, 64.40 ms/it, loss 0.461724
Finished training it 74752/76743 of epoch 7, 64.17 ms/it, loss 0.461935
Finished training it 74752/76743 of epoch 7, 64.23 ms/it, loss 0.461312
Finished training it 74752/76743 of epoch 7, 64.22 ms/it, loss 0.464464
Finished training it 75776/76743 of epoch 7, 64.01 ms/it, loss 0.463494
Finished training it 75776/76743 of epoch 7, 64.19 ms/it, loss 0.461263
Finished training it 75776/76743 of epoch 7, 63.98 ms/it, loss 0.463730
Finished training it 75776/76743 of epoch 7, 63.97 ms/it, loss 0.460139
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 64.85 ms/it, loss 0.461869
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 64.95 ms/it, loss 0.462185
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 64.82 ms/it, loss 0.462417
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 65.27 ms/it, loss 0.461539
Finished training it 2048/76743 of epoch 8, 63.90 ms/it, loss 0.462024
Finished training it 2048/76743 of epoch 8, 64.14 ms/it, loss 0.463062
Finished training it 2048/76743 of epoch 8, 63.95 ms/it, loss 0.464363
Finished training it 2048/76743 of epoch 8, 64.01 ms/it, loss 0.460840
Finished training it 3072/76743 of epoch 8, 63.69 ms/it, loss 0.464167
Finished training it 3072/76743 of epoch 8, 63.33 ms/it, loss 0.462776
Finished training it 3072/76743 of epoch 8, 63.39 ms/it, loss 0.463722
Finished training it 3072/76743 of epoch 8, 63.40 ms/it, loss 0.461547
Finished training it 4096/76743 of epoch 8, 63.36 ms/it, loss 0.463633
Finished training it 4096/76743 of epoch 8, 63.72 ms/it, loss 0.463583
Finished training it 4096/76743 of epoch 8, 63.48 ms/it, loss 0.462364
Finished training it 4096/76743 of epoch 8, 63.33 ms/it, loss 0.461636
Finished training it 5120/76743 of epoch 8, 64.09 ms/it, loss 0.463250
Finished training it 5120/76743 of epoch 8, 63.92 ms/it, loss 0.464123
Finished training it 5120/76743 of epoch 8, 64.21 ms/it, loss 0.463878
Finished training it 5120/76743 of epoch 8, 63.94 ms/it, loss 0.462795
Finished training it 6144/76743 of epoch 8, 63.16 ms/it, loss 0.460956
Finished training it 6144/76743 of epoch 8, 63.25 ms/it, loss 0.462660
Finished training it 6144/76743 of epoch 8, 63.47 ms/it, loss 0.462903
Finished training it 6144/76743 of epoch 8, 63.23 ms/it, loss 0.464746
Finished training it 7168/76743 of epoch 8, 63.65 ms/it, loss 0.463994
Finished training it 7168/76743 of epoch 8, 63.43 ms/it, loss 0.461334
Finished training it 7168/76743 of epoch 8, 63.34 ms/it, loss 0.463658
Finished training it 7168/76743 of epoch 8, 63.42 ms/it, loss 0.463085
Finished training it 8192/76743 of epoch 8, 63.78 ms/it, loss 0.461553
Finished training it 8192/76743 of epoch 8, 63.52 ms/it, loss 0.463152
Finished training it 8192/76743 of epoch 8, 63.43 ms/it, loss 0.461475
Finished training it 8192/76743 of epoch 8, 63.47 ms/it, loss 0.462319
Finished training it 9216/76743 of epoch 8, 68.65 ms/it, loss 0.461081
Finished training it 9216/76743 of epoch 8, 68.60 ms/it, loss 0.465238
Finished training it 9216/76743 of epoch 8, 68.35 ms/it, loss 0.461956
Finished training it 9216/76743 of epoch 8, 68.79 ms/it, loss 0.463048
Finished training it 10240/76743 of epoch 8, 63.59 ms/it, loss 0.462694
Finished training it 10240/76743 of epoch 8, 63.66 ms/it, loss 0.460793
Finished training it 10240/76743 of epoch 8, 63.52 ms/it, loss 0.459777
Finished training it 10240/76743 of epoch 8, 63.36 ms/it, loss 0.464620
Testing at - 10240/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553797.0
get out
0 has test check 2553797.0 and sample count 3274240
 accuracy 77.997 %, best 77.997 %, roc auc score 0.7821, best 0.7821
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553797.0
get out
3 has test check 2553797.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 63.66 ms/it, loss 0.460056
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 8, 63.38 ms/it, loss 0.462350
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553797.0
get out
1 has test check 2553797.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 63.59 ms/it, loss 0.458773
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553797.0
get out
2 has test check 2553797.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 63.82 ms/it, loss 0.464584
Finished training it 12288/76743 of epoch 8, 68.47 ms/it, loss 0.462820
Finished training it 12288/76743 of epoch 8, 68.64 ms/it, loss 0.461576
Finished training it 12288/76743 of epoch 8, 68.85 ms/it, loss 0.463087
Finished training it 12288/76743 of epoch 8, 68.68 ms/it, loss 0.460511
Finished training it 13312/76743 of epoch 8, 64.26 ms/it, loss 0.461964
Finished training it 13312/76743 of epoch 8, 64.11 ms/it, loss 0.464355
Finished training it 13312/76743 of epoch 8, 63.87 ms/it, loss 0.462881
Finished training it 13312/76743 of epoch 8, 63.93 ms/it, loss 0.460710
Finished training it 14336/76743 of epoch 8, 63.22 ms/it, loss 0.461507
Finished training it 14336/76743 of epoch 8, 63.58 ms/it, loss 0.461693
Finished training it 14336/76743 of epoch 8, 63.19 ms/it, loss 0.461577
Finished training it 14336/76743 of epoch 8, 63.26 ms/it, loss 0.464574
Finished training it 15360/76743 of epoch 8, 68.49 ms/it, loss 0.463497
Finished training it 15360/76743 of epoch 8, 68.17 ms/it, loss 0.458584
Finished training it 15360/76743 of epoch 8, 68.36 ms/it, loss 0.460577
Finished training it 15360/76743 of epoch 8, 68.05 ms/it, loss 0.459921
Finished training it 16384/76743 of epoch 8, 63.91 ms/it, loss 0.462482
Finished training it 16384/76743 of epoch 8, 63.65 ms/it, loss 0.461832
Finished training it 16384/76743 of epoch 8, 63.47 ms/it, loss 0.463174
Finished training it 16384/76743 of epoch 8, 63.50 ms/it, loss 0.463505
Finished training it 17408/76743 of epoch 8, 64.34 ms/it, loss 0.461584
Finished training it 17408/76743 of epoch 8, 64.18 ms/it, loss 0.462045
Finished training it 17408/76743 of epoch 8, 64.05 ms/it, loss 0.460639
Finished training it 17408/76743 of epoch 8, 64.14 ms/it, loss 0.462458
Finished training it 18432/76743 of epoch 8, 63.96 ms/it, loss 0.461497
Finished training it 18432/76743 of epoch 8, 63.86 ms/it, loss 0.460250
Finished training it 18432/76743 of epoch 8, 63.73 ms/it, loss 0.463178
Finished training it 18432/76743 of epoch 8, 63.53 ms/it, loss 0.461472
Finished training it 19456/76743 of epoch 8, 63.87 ms/it, loss 0.462715
Finished training it 19456/76743 of epoch 8, 63.52 ms/it, loss 0.461975
Finished training it 19456/76743 of epoch 8, 63.54 ms/it, loss 0.464452
Finished training it 19456/76743 of epoch 8, 63.54 ms/it, loss 0.461136
Finished training it 20480/76743 of epoch 8, 63.95 ms/it, loss 0.462710
Finished training it 20480/76743 of epoch 8, 63.78 ms/it, loss 0.459772
Finished training it 20480/76743 of epoch 8, 63.65 ms/it, loss 0.461270
Finished training it 20480/76743 of epoch 8, 63.66 ms/it, loss 0.460342
Testing at - 20480/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553663.0
get out
0 has test check 2553663.0 and sample count 3274240
 accuracy 77.993 %, best 77.997 %, roc auc score 0.7820, best 0.7821
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553663.0
get out
3 has test check 2553663.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 63.41 ms/it, loss 0.461164
Finished training it 21504/76743 of epoch 8, 63.33 ms/it, loss 0.458981
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553663.0
get out
2 has test check 2553663.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 63.71 ms/it, loss 0.463130
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553663.0
get out
1 has test check 2553663.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 63.45 ms/it, loss 0.460353
Finished training it 22528/76743 of epoch 8, 63.84 ms/it, loss 0.461472
Finished training it 22528/76743 of epoch 8, 63.57 ms/it, loss 0.459609
Finished training it 22528/76743 of epoch 8, 63.39 ms/it, loss 0.460043
Finished training it 22528/76743 of epoch 8, 63.67 ms/it, loss 0.462782
Finished training it 23552/76743 of epoch 8, 63.50 ms/it, loss 0.461197
Finished training it 23552/76743 of epoch 8, 63.71 ms/it, loss 0.464228
Finished training it 23552/76743 of epoch 8, 63.45 ms/it, loss 0.462097
Finished training it 23552/76743 of epoch 8, 63.51 ms/it, loss 0.459025
Finished training it 24576/76743 of epoch 8, 63.70 ms/it, loss 0.461896
Finished training it 24576/76743 of epoch 8, 63.86 ms/it, loss 0.462752
Finished training it 24576/76743 of epoch 8, 63.74 ms/it, loss 0.462154
Finished training it 24576/76743 of epoch 8, 63.55 ms/it, loss 0.461330
Finished training it 25600/76743 of epoch 8, 63.95 ms/it, loss 0.463999
Finished training it 25600/76743 of epoch 8, 63.58 ms/it, loss 0.461034
Finished training it 25600/76743 of epoch 8, 63.59 ms/it, loss 0.462505
Finished training it 25600/76743 of epoch 8, 63.65 ms/it, loss 0.461777
Finished training it 26624/76743 of epoch 8, 64.06 ms/it, loss 0.461065
Finished training it 26624/76743 of epoch 8, 63.87 ms/it, loss 0.463306
Finished training it 26624/76743 of epoch 8, 63.80 ms/it, loss 0.461202
Finished training it 26624/76743 of epoch 8, 63.71 ms/it, loss 0.461848
Finished training it 27648/76743 of epoch 8, 63.50 ms/it, loss 0.461386
Finished training it 27648/76743 of epoch 8, 63.44 ms/it, loss 0.462946
Finished training it 27648/76743 of epoch 8, 63.34 ms/it, loss 0.461488
Finished training it 27648/76743 of epoch 8, 63.73 ms/it, loss 0.466193
Finished training it 28672/76743 of epoch 8, 63.83 ms/it, loss 0.463884
Finished training it 28672/76743 of epoch 8, 64.11 ms/it, loss 0.461080
Finished training it 28672/76743 of epoch 8, 63.94 ms/it, loss 0.461094
Finished training it 28672/76743 of epoch 8, 63.88 ms/it, loss 0.463001
Finished training it 29696/76743 of epoch 8, 63.20 ms/it, loss 0.462695
Finished training it 29696/76743 of epoch 8, 63.23 ms/it, loss 0.459159
Finished training it 29696/76743 of epoch 8, 63.18 ms/it, loss 0.459673
Finished training it 29696/76743 of epoch 8, 63.43 ms/it, loss 0.459936
Finished training it 30720/76743 of epoch 8, 63.98 ms/it, loss 0.459535
Finished training it 30720/76743 of epoch 8, 63.96 ms/it, loss 0.462774
Finished training it 30720/76743 of epoch 8, 63.61 ms/it, loss 0.462568
Finished training it 30720/76743 of epoch 8, 63.74 ms/it, loss 0.461785
Testing at - 30720/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553928.0
get out
0 has test check 2553928.0 and sample count 3274240
 accuracy 78.001 %, best 78.001 %, roc auc score 0.7823, best 0.7823
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553928.0
get out
1 has test check 2553928.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 63.48 ms/it, loss 0.463143
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553928.0
get out
3 has test check 2553928.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 63.68 ms/it, loss 0.461395
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 8, 63.65 ms/it, loss 0.462399
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553928.0
get out
2 has test check 2553928.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 63.99 ms/it, loss 0.462435
Finished training it 32768/76743 of epoch 8, 63.64 ms/it, loss 0.459663
Finished training it 32768/76743 of epoch 8, 63.93 ms/it, loss 0.460689
Finished training it 32768/76743 of epoch 8, 63.46 ms/it, loss 0.460788
Finished training it 32768/76743 of epoch 8, 63.57 ms/it, loss 0.462720
Finished training it 33792/76743 of epoch 8, 63.48 ms/it, loss 0.459104
Finished training it 33792/76743 of epoch 8, 63.64 ms/it, loss 0.462213
Finished training it 33792/76743 of epoch 8, 63.79 ms/it, loss 0.458646
Finished training it 33792/76743 of epoch 8, 63.57 ms/it, loss 0.462567
Finished training it 34816/76743 of epoch 8, 69.50 ms/it, loss 0.462107
Finished training it 34816/76743 of epoch 8, 68.93 ms/it, loss 0.463813
Finished training it 34816/76743 of epoch 8, 69.09 ms/it, loss 0.459105
Finished training it 34816/76743 of epoch 8, 69.26 ms/it, loss 0.461694
Finished training it 35840/76743 of epoch 8, 69.29 ms/it, loss 0.463318
Finished training it 35840/76743 of epoch 8, 69.30 ms/it, loss 0.460982
Finished training it 35840/76743 of epoch 8, 69.58 ms/it, loss 0.460909
Finished training it 35840/76743 of epoch 8, 69.54 ms/it, loss 0.461316
Finished training it 36864/76743 of epoch 8, 63.84 ms/it, loss 0.461628
Finished training it 36864/76743 of epoch 8, 63.67 ms/it, loss 0.462547
Finished training it 36864/76743 of epoch 8, 63.65 ms/it, loss 0.460704
Finished training it 36864/76743 of epoch 8, 63.65 ms/it, loss 0.460724
Finished training it 37888/76743 of epoch 8, 63.47 ms/it, loss 0.462584
Finished training it 37888/76743 of epoch 8, 63.56 ms/it, loss 0.461323
Finished training it 37888/76743 of epoch 8, 63.47 ms/it, loss 0.460316
Finished training it 37888/76743 of epoch 8, 63.72 ms/it, loss 0.463009
Finished training it 38912/76743 of epoch 8, 63.34 ms/it, loss 0.464767
Finished training it 38912/76743 of epoch 8, 63.56 ms/it, loss 0.460520
Finished training it 38912/76743 of epoch 8, 63.53 ms/it, loss 0.460956
Finished training it 38912/76743 of epoch 8, 63.30 ms/it, loss 0.462953
Finished training it 39936/76743 of epoch 8, 63.52 ms/it, loss 0.461420
Finished training it 39936/76743 of epoch 8, 63.45 ms/it, loss 0.462613
Finished training it 39936/76743 of epoch 8, 63.73 ms/it, loss 0.462251
Finished training it 39936/76743 of epoch 8, 63.39 ms/it, loss 0.463524
Finished training it 40960/76743 of epoch 8, 63.99 ms/it, loss 0.460182
Finished training it 40960/76743 of epoch 8, 63.68 ms/it, loss 0.463164
Finished training it 40960/76743 of epoch 8, 63.66 ms/it, loss 0.461614
Finished training it 40960/76743 of epoch 8, 63.80 ms/it, loss 0.460860
Testing at - 40960/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554671.0
get out
0 has test check 2554671.0 and sample count 3274240
 accuracy 78.023 %, best 78.023 %, roc auc score 0.7826, best 0.7826
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554671.0
get out
2 has test check 2554671.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 64.12 ms/it, loss 0.461786
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554671.0
get out
3 has test check 2554671.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 63.97 ms/it, loss 0.460441
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 8, 63.72 ms/it, loss 0.461966
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554671.0
get out
1 has test check 2554671.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 63.86 ms/it, loss 0.460391
Finished training it 43008/76743 of epoch 8, 63.30 ms/it, loss 0.460892
Finished training it 43008/76743 of epoch 8, 63.43 ms/it, loss 0.464564
Finished training it 43008/76743 of epoch 8, 63.69 ms/it, loss 0.457857
Finished training it 43008/76743 of epoch 8, 63.34 ms/it, loss 0.460123
Finished training it 44032/76743 of epoch 8, 63.23 ms/it, loss 0.461534
Finished training it 44032/76743 of epoch 8, 63.17 ms/it, loss 0.459802
Finished training it 44032/76743 of epoch 8, 63.50 ms/it, loss 0.464665
Finished training it 44032/76743 of epoch 8, 63.27 ms/it, loss 0.460508
Finished training it 45056/76743 of epoch 8, 64.05 ms/it, loss 0.459867
Finished training it 45056/76743 of epoch 8, 63.88 ms/it, loss 0.461342
Finished training it 45056/76743 of epoch 8, 63.90 ms/it, loss 0.463500
Finished training it 45056/76743 of epoch 8, 63.65 ms/it, loss 0.461114
Finished training it 46080/76743 of epoch 8, 63.78 ms/it, loss 0.461411
Finished training it 46080/76743 of epoch 8, 63.49 ms/it, loss 0.461466
Finished training it 46080/76743 of epoch 8, 63.48 ms/it, loss 0.462360
Finished training it 46080/76743 of epoch 8, 63.47 ms/it, loss 0.460754
Finished training it 47104/76743 of epoch 8, 64.45 ms/it, loss 0.460296
Finished training it 47104/76743 of epoch 8, 64.22 ms/it, loss 0.463919
Finished training it 47104/76743 of epoch 8, 64.12 ms/it, loss 0.463155
Finished training it 47104/76743 of epoch 8, 64.26 ms/it, loss 0.461279
Finished training it 48128/76743 of epoch 8, 63.42 ms/it, loss 0.461219
Finished training it 48128/76743 of epoch 8, 63.43 ms/it, loss 0.459745
Finished training it 48128/76743 of epoch 8, 63.38 ms/it, loss 0.460438
Finished training it 48128/76743 of epoch 8, 63.73 ms/it, loss 0.464138
Finished training it 49152/76743 of epoch 8, 63.40 ms/it, loss 0.461617
Finished training it 49152/76743 of epoch 8, 63.25 ms/it, loss 0.462167
Finished training it 49152/76743 of epoch 8, 63.13 ms/it, loss 0.461811
Finished training it 49152/76743 of epoch 8, 63.19 ms/it, loss 0.460259
Finished training it 50176/76743 of epoch 8, 63.69 ms/it, loss 0.460918
Finished training it 50176/76743 of epoch 8, 63.70 ms/it, loss 0.459585
Finished training it 50176/76743 of epoch 8, 63.49 ms/it, loss 0.459546
Finished training it 50176/76743 of epoch 8, 63.91 ms/it, loss 0.459668
Finished training it 51200/76743 of epoch 8, 63.84 ms/it, loss 0.461020
Finished training it 51200/76743 of epoch 8, 63.62 ms/it, loss 0.462881
Finished training it 51200/76743 of epoch 8, 63.53 ms/it, loss 0.460767
Finished training it 51200/76743 of epoch 8, 63.60 ms/it, loss 0.462563
Testing at - 51200/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552912.0
get out
0 has test check 2552912.0 and sample count 3274240
 accuracy 77.970 %, best 78.023 %, roc auc score 0.7825, best 0.7826
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552912.0
get out
1 has test check 2552912.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 63.67 ms/it, loss 0.460308
Finished training it 52224/76743 of epoch 8, 63.29 ms/it, loss 0.462008
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552912.0
get out
2 has test check 2552912.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 63.93 ms/it, loss 0.460316
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552912.0
get out
3 has test check 2552912.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 63.55 ms/it, loss 0.462020
Finished training it 53248/76743 of epoch 8, 63.34 ms/it, loss 0.461640
Finished training it 53248/76743 of epoch 8, 63.71 ms/it, loss 0.458394
Finished training it 53248/76743 of epoch 8, 63.50 ms/it, loss 0.461220
Finished training it 53248/76743 of epoch 8, 63.50 ms/it, loss 0.459865
Finished training it 54272/76743 of epoch 8, 63.57 ms/it, loss 0.460147
Finished training it 54272/76743 of epoch 8, 63.84 ms/it, loss 0.461238
Finished training it 54272/76743 of epoch 8, 63.51 ms/it, loss 0.461522
Finished training it 54272/76743 of epoch 8, 63.58 ms/it, loss 0.463685
Finished training it 55296/76743 of epoch 8, 68.35 ms/it, loss 0.459827
Finished training it 55296/76743 of epoch 8, 68.21 ms/it, loss 0.465587
Finished training it 55296/76743 of epoch 8, 68.21 ms/it, loss 0.458615
Finished training it 55296/76743 of epoch 8, 68.60 ms/it, loss 0.462522
Finished training it 56320/76743 of epoch 8, 68.38 ms/it, loss 0.463693
Finished training it 56320/76743 of epoch 8, 69.24 ms/it, loss 0.459886
Finished training it 56320/76743 of epoch 8, 68.76 ms/it, loss 0.460094
Finished training it 56320/76743 of epoch 8, 68.58 ms/it, loss 0.465669
Finished training it 57344/76743 of epoch 8, 63.50 ms/it, loss 0.460151
Finished training it 57344/76743 of epoch 8, 63.78 ms/it, loss 0.461525
Finished training it 57344/76743 of epoch 8, 63.47 ms/it, loss 0.460226
Finished training it 57344/76743 of epoch 8, 63.50 ms/it, loss 0.460019
Finished training it 58368/76743 of epoch 8, 64.22 ms/it, loss 0.462739
Finished training it 58368/76743 of epoch 8, 64.15 ms/it, loss 0.460574
Finished training it 58368/76743 of epoch 8, 64.48 ms/it, loss 0.462263
Finished training it 58368/76743 of epoch 8, 64.26 ms/it, loss 0.459408
Finished training it 59392/76743 of epoch 8, 64.04 ms/it, loss 0.461737
Finished training it 59392/76743 of epoch 8, 63.77 ms/it, loss 0.459787
Finished training it 59392/76743 of epoch 8, 63.79 ms/it, loss 0.459190
Finished training it 59392/76743 of epoch 8, 63.59 ms/it, loss 0.459887
Finished training it 60416/76743 of epoch 8, 63.42 ms/it, loss 0.462841
Finished training it 60416/76743 of epoch 8, 63.63 ms/it, loss 0.461445
Finished training it 60416/76743 of epoch 8, 63.37 ms/it, loss 0.460987
Finished training it 60416/76743 of epoch 8, 63.33 ms/it, loss 0.458963
Finished training it 61440/76743 of epoch 8, 64.36 ms/it, loss 0.462420
Finished training it 61440/76743 of epoch 8, 64.30 ms/it, loss 0.461394
Finished training it 61440/76743 of epoch 8, 64.23 ms/it, loss 0.460661
Finished training it 61440/76743 of epoch 8, 64.64 ms/it, loss 0.462412
Testing at - 61440/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553914.0
get out
0 has test check 2553914.0 and sample count 3274240
 accuracy 78.000 %, best 78.023 %, roc auc score 0.7820, best 0.7826
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553914.0
get out
1 has test check 2553914.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 64.20 ms/it, loss 0.462086
Finished training it 62464/76743 of epoch 8, 64.16 ms/it, loss 0.460802
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553914.0
get out
2 has test check 2553914.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 64.42 ms/it, loss 0.462901
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553914.0
get out
3 has test check 2553914.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 64.18 ms/it, loss 0.462220
Finished training it 63488/76743 of epoch 8, 63.40 ms/it, loss 0.460768
Finished training it 63488/76743 of epoch 8, 63.37 ms/it, loss 0.459942
Finished training it 63488/76743 of epoch 8, 63.40 ms/it, loss 0.464979
Finished training it 63488/76743 of epoch 8, 63.65 ms/it, loss 0.461524
Finished training it 64512/76743 of epoch 8, 63.92 ms/it, loss 0.459078
Finished training it 64512/76743 of epoch 8, 63.56 ms/it, loss 0.461833
Finished training it 64512/76743 of epoch 8, 63.62 ms/it, loss 0.462098
Finished training it 64512/76743 of epoch 8, 63.73 ms/it, loss 0.464097
Finished training it 65536/76743 of epoch 8, 63.45 ms/it, loss 0.459082
Finished training it 65536/76743 of epoch 8, 63.68 ms/it, loss 0.462216
Finished training it 65536/76743 of epoch 8, 63.37 ms/it, loss 0.460491
Finished training it 65536/76743 of epoch 8, 63.31 ms/it, loss 0.461609
Finished training it 66560/76743 of epoch 8, 63.25 ms/it, loss 0.461872
Finished training it 66560/76743 of epoch 8, 63.57 ms/it, loss 0.460734
Finished training it 66560/76743 of epoch 8, 63.29 ms/it, loss 0.460956
Finished training it 66560/76743 of epoch 8, 63.33 ms/it, loss 0.459938
Finished training it 67584/76743 of epoch 8, 64.08 ms/it, loss 0.460544
Finished training it 67584/76743 of epoch 8, 63.66 ms/it, loss 0.464167
Finished training it 67584/76743 of epoch 8, 63.59 ms/it, loss 0.462690
Finished training it 67584/76743 of epoch 8, 63.72 ms/it, loss 0.461210
Finished training it 68608/76743 of epoch 8, 63.95 ms/it, loss 0.457505
Finished training it 68608/76743 of epoch 8, 63.70 ms/it, loss 0.460434
Finished training it 68608/76743 of epoch 8, 63.65 ms/it, loss 0.459942
Finished training it 68608/76743 of epoch 8, 63.43 ms/it, loss 0.461414
Finished training it 69632/76743 of epoch 8, 63.29 ms/it, loss 0.460299
Finished training it 69632/76743 of epoch 8, 63.66 ms/it, loss 0.461663
Finished training it 69632/76743 of epoch 8, 63.32 ms/it, loss 0.460415
Finished training it 69632/76743 of epoch 8, 63.43 ms/it, loss 0.460922
Finished training it 70656/76743 of epoch 8, 63.68 ms/it, loss 0.462223
Finished training it 70656/76743 of epoch 8, 63.65 ms/it, loss 0.458304
Finished training it 70656/76743 of epoch 8, 63.90 ms/it, loss 0.462042
Finished training it 70656/76743 of epoch 8, 63.54 ms/it, loss 0.462263
Finished training it 71680/76743 of epoch 8, 64.00 ms/it, loss 0.460392
Finished training it 71680/76743 of epoch 8, 63.69 ms/it, loss 0.460251
Finished training it 71680/76743 of epoch 8, 63.73 ms/it, loss 0.463590
Finished training it 71680/76743 of epoch 8, 63.58 ms/it, loss 0.459833
Testing at - 71680/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556212.0
get out
0 has test check 2556212.0 and sample count 3274240
 accuracy 78.070 %, best 78.070 %, roc auc score 0.7834, best 0.7834
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556212.0
get out
1 has test check 2556212.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 63.69 ms/it, loss 0.459454
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556212.0
get out
2 has test check 2556212.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 63.89 ms/it, loss 0.461223
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556212.0
get out
3 has test check 2556212.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 63.61 ms/it, loss 0.464079
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 8, 63.33 ms/it, loss 0.461443
Finished training it 73728/76743 of epoch 8, 63.53 ms/it, loss 0.460959
Finished training it 73728/76743 of epoch 8, 63.59 ms/it, loss 0.461542
Finished training it 73728/76743 of epoch 8, 63.97 ms/it, loss 0.461333
Finished training it 73728/76743 of epoch 8, 63.74 ms/it, loss 0.461239
Finished training it 74752/76743 of epoch 8, 63.81 ms/it, loss 0.460669
Finished training it 74752/76743 of epoch 8, 63.51 ms/it, loss 0.460839
Finished training it 74752/76743 of epoch 8, 63.30 ms/it, loss 0.463377
Finished training it 74752/76743 of epoch 8, 63.46 ms/it, loss 0.460183
Finished training it 75776/76743 of epoch 8, 76.66 ms/it, loss 0.462690
Finished training it 75776/76743 of epoch 8, 76.83 ms/it, loss 0.460175
Finished training it 75776/76743 of epoch 8, 76.61 ms/it, loss 0.462551
Finished training it 75776/76743 of epoch 8, 76.53 ms/it, loss 0.459541
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 64.56 ms/it, loss 0.460605
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 65.05 ms/it, loss 0.460090
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 64.44 ms/it, loss 0.461285
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 64.46 ms/it, loss 0.460800
Finished training it 2048/76743 of epoch 9, 63.63 ms/it, loss 0.461676
Finished training it 2048/76743 of epoch 9, 63.39 ms/it, loss 0.462816
Finished training it 2048/76743 of epoch 9, 63.49 ms/it, loss 0.460737
Finished training it 2048/76743 of epoch 9, 63.38 ms/it, loss 0.459695
Finished training it 3072/76743 of epoch 9, 63.68 ms/it, loss 0.460245
Finished training it 3072/76743 of epoch 9, 63.77 ms/it, loss 0.462631
Finished training it 3072/76743 of epoch 9, 63.97 ms/it, loss 0.462961
Finished training it 3072/76743 of epoch 9, 63.74 ms/it, loss 0.461176
Finished training it 4096/76743 of epoch 9, 63.83 ms/it, loss 0.462556
Finished training it 4096/76743 of epoch 9, 63.59 ms/it, loss 0.462124
Finished training it 4096/76743 of epoch 9, 63.62 ms/it, loss 0.461088
Finished training it 4096/76743 of epoch 9, 63.42 ms/it, loss 0.460571
Finished training it 5120/76743 of epoch 9, 63.55 ms/it, loss 0.463391
Finished training it 5120/76743 of epoch 9, 63.49 ms/it, loss 0.462388
Finished training it 5120/76743 of epoch 9, 63.42 ms/it, loss 0.461618
Finished training it 5120/76743 of epoch 9, 63.76 ms/it, loss 0.462670
Finished training it 6144/76743 of epoch 9, 63.46 ms/it, loss 0.463624
Finished training it 6144/76743 of epoch 9, 63.73 ms/it, loss 0.461331
Finished training it 6144/76743 of epoch 9, 63.38 ms/it, loss 0.461308
Finished training it 6144/76743 of epoch 9, 63.50 ms/it, loss 0.460211
Finished training it 7168/76743 of epoch 9, 63.81 ms/it, loss 0.462995
Finished training it 7168/76743 of epoch 9, 63.54 ms/it, loss 0.462097
Finished training it 7168/76743 of epoch 9, 63.48 ms/it, loss 0.460397
Finished training it 7168/76743 of epoch 9, 63.55 ms/it, loss 0.462579
Finished training it 8192/76743 of epoch 9, 63.65 ms/it, loss 0.462082
Finished training it 8192/76743 of epoch 9, 63.59 ms/it, loss 0.461163
Finished training it 8192/76743 of epoch 9, 63.84 ms/it, loss 0.460870
Finished training it 8192/76743 of epoch 9, 63.50 ms/it, loss 0.460098
Finished training it 9216/76743 of epoch 9, 63.43 ms/it, loss 0.459786
Finished training it 9216/76743 of epoch 9, 63.29 ms/it, loss 0.460560
Finished training it 9216/76743 of epoch 9, 63.70 ms/it, loss 0.462033
Finished training it 9216/76743 of epoch 9, 63.51 ms/it, loss 0.464102
Finished training it 10240/76743 of epoch 9, 63.45 ms/it, loss 0.463250
Finished training it 10240/76743 of epoch 9, 63.38 ms/it, loss 0.461406
Finished training it 10240/76743 of epoch 9, 63.75 ms/it, loss 0.459120
Finished training it 10240/76743 of epoch 9, 63.51 ms/it, loss 0.458880
Testing at - 10240/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554713.0
get out
0 has test check 2554713.0 and sample count 3274240
 accuracy 78.025 %, best 78.070 %, roc auc score 0.7830, best 0.7834
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554713.0
get out
2 has test check 2554713.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 63.56 ms/it, loss 0.463360
Finished training it 11264/76743 of epoch 9, 63.29 ms/it, loss 0.461156
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554713.0
get out
3 has test check 2554713.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 63.45 ms/it, loss 0.458593
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554713.0
get out
1 has test check 2554713.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 63.25 ms/it, loss 0.457619
Finished training it 12288/76743 of epoch 9, 63.67 ms/it, loss 0.461822
Finished training it 12288/76743 of epoch 9, 63.43 ms/it, loss 0.460377
Finished training it 12288/76743 of epoch 9, 63.51 ms/it, loss 0.459071
Finished training it 12288/76743 of epoch 9, 63.47 ms/it, loss 0.460824
Finished training it 13312/76743 of epoch 9, 63.80 ms/it, loss 0.460288
Finished training it 13312/76743 of epoch 9, 63.27 ms/it, loss 0.459389
Finished training it 13312/76743 of epoch 9, 63.62 ms/it, loss 0.462790
Finished training it 13312/76743 of epoch 9, 63.55 ms/it, loss 0.460975
Finished training it 14336/76743 of epoch 9, 63.80 ms/it, loss 0.460243
Finished training it 14336/76743 of epoch 9, 64.00 ms/it, loss 0.460735
Finished training it 14336/76743 of epoch 9, 63.75 ms/it, loss 0.460473
Finished training it 14336/76743 of epoch 9, 63.62 ms/it, loss 0.463041
Finished training it 15360/76743 of epoch 9, 63.46 ms/it, loss 0.459328
Finished training it 15360/76743 of epoch 9, 63.41 ms/it, loss 0.457079
Finished training it 15360/76743 of epoch 9, 63.71 ms/it, loss 0.462362
Finished training it 15360/76743 of epoch 9, 63.38 ms/it, loss 0.458983
Finished training it 16384/76743 of epoch 9, 63.39 ms/it, loss 0.461733
Finished training it 16384/76743 of epoch 9, 63.66 ms/it, loss 0.460875
Finished training it 16384/76743 of epoch 9, 63.49 ms/it, loss 0.460337
Finished training it 16384/76743 of epoch 9, 63.49 ms/it, loss 0.461911
Finished training it 17408/76743 of epoch 9, 63.66 ms/it, loss 0.460116
Finished training it 17408/76743 of epoch 9, 63.44 ms/it, loss 0.459332
Finished training it 17408/76743 of epoch 9, 63.43 ms/it, loss 0.460796
Finished training it 17408/76743 of epoch 9, 63.26 ms/it, loss 0.461216
Finished training it 18432/76743 of epoch 9, 63.79 ms/it, loss 0.458970
Finished training it 18432/76743 of epoch 9, 63.68 ms/it, loss 0.461934
Finished training it 18432/76743 of epoch 9, 63.75 ms/it, loss 0.460147
Finished training it 18432/76743 of epoch 9, 63.99 ms/it, loss 0.460399
Finished training it 19456/76743 of epoch 9, 63.76 ms/it, loss 0.460569
Finished training it 19456/76743 of epoch 9, 63.99 ms/it, loss 0.461080
Finished training it 19456/76743 of epoch 9, 63.71 ms/it, loss 0.459690
Finished training it 19456/76743 of epoch 9, 63.59 ms/it, loss 0.463490
Finished training it 20480/76743 of epoch 9, 63.51 ms/it, loss 0.461202
Finished training it 20480/76743 of epoch 9, 63.18 ms/it, loss 0.458382
Finished training it 20480/76743 of epoch 9, 63.29 ms/it, loss 0.459575
Finished training it 20480/76743 of epoch 9, 63.25 ms/it, loss 0.458992
Testing at - 20480/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554479.0
get out
0 has test check 2554479.0 and sample count 3274240
 accuracy 78.017 %, best 78.070 %, roc auc score 0.7829, best 0.7834
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554479.0
get out
3 has test check 2554479.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 64.29 ms/it, loss 0.459900
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554479.0
get out
1 has test check 2554479.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 64.19 ms/it, loss 0.459178
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554479.0
get out
2 has test check 2554479.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 64.48 ms/it, loss 0.461430
Finished training it 21504/76743 of epoch 9, 64.26 ms/it, loss 0.457337
Finished training it 22528/76743 of epoch 9, 63.47 ms/it, loss 0.458417
Finished training it 22528/76743 of epoch 9, 63.38 ms/it, loss 0.458596
Finished training it 22528/76743 of epoch 9, 63.70 ms/it, loss 0.460189
Finished training it 22528/76743 of epoch 9, 63.34 ms/it, loss 0.461731
Finished training it 23552/76743 of epoch 9, 63.44 ms/it, loss 0.457662
Finished training it 23552/76743 of epoch 9, 63.51 ms/it, loss 0.460283
Finished training it 23552/76743 of epoch 9, 63.47 ms/it, loss 0.461247
Finished training it 23552/76743 of epoch 9, 63.69 ms/it, loss 0.462824
Finished training it 24576/76743 of epoch 9, 73.42 ms/it, loss 0.460371
Finished training it 24576/76743 of epoch 9, 73.58 ms/it, loss 0.461553
Finished training it 24576/76743 of epoch 9, 73.23 ms/it, loss 0.461428
Finished training it 24576/76743 of epoch 9, 73.30 ms/it, loss 0.460923
Finished training it 25600/76743 of epoch 9, 63.50 ms/it, loss 0.461338
Finished training it 25600/76743 of epoch 9, 63.41 ms/it, loss 0.460559
Finished training it 25600/76743 of epoch 9, 63.72 ms/it, loss 0.462327
Finished training it 25600/76743 of epoch 9, 63.38 ms/it, loss 0.460968
Finished training it 26624/76743 of epoch 9, 63.42 ms/it, loss 0.460849
Finished training it 26624/76743 of epoch 9, 63.46 ms/it, loss 0.462179
Finished training it 26624/76743 of epoch 9, 63.26 ms/it, loss 0.460101
Finished training it 26624/76743 of epoch 9, 63.64 ms/it, loss 0.459885
Finished training it 27648/76743 of epoch 9, 63.32 ms/it, loss 0.460547
Finished training it 27648/76743 of epoch 9, 63.22 ms/it, loss 0.462050
Finished training it 27648/76743 of epoch 9, 63.56 ms/it, loss 0.464913
Finished training it 27648/76743 of epoch 9, 63.24 ms/it, loss 0.460265
Finished training it 28672/76743 of epoch 9, 63.89 ms/it, loss 0.461700
Finished training it 28672/76743 of epoch 9, 64.16 ms/it, loss 0.459573
Finished training it 28672/76743 of epoch 9, 63.80 ms/it, loss 0.459659
Finished training it 28672/76743 of epoch 9, 63.93 ms/it, loss 0.462753
Finished training it 29696/76743 of epoch 9, 63.85 ms/it, loss 0.458711
Finished training it 29696/76743 of epoch 9, 63.56 ms/it, loss 0.461510
Finished training it 29696/76743 of epoch 9, 63.64 ms/it, loss 0.458171
Finished training it 29696/76743 of epoch 9, 63.59 ms/it, loss 0.457750
Finished training it 30720/76743 of epoch 9, 63.69 ms/it, loss 0.461194
Finished training it 30720/76743 of epoch 9, 63.75 ms/it, loss 0.460296
Finished training it 30720/76743 of epoch 9, 63.96 ms/it, loss 0.458186
Finished training it 30720/76743 of epoch 9, 63.70 ms/it, loss 0.461246
Testing at - 30720/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555435.0
get out
0 has test check 2555435.0 and sample count 3274240
 accuracy 78.047 %, best 78.070 %, roc auc score 0.7830, best 0.7834
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555435.0
get out
3 has test check 2555435.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 63.50 ms/it, loss 0.460242
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555435.0
get out
2 has test check 2555435.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 63.76 ms/it, loss 0.460970
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555435.0
get out
1 has test check 2555435.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 63.45 ms/it, loss 0.461677
Finished training it 31744/76743 of epoch 9, 63.41 ms/it, loss 0.461226
Finished training it 32768/76743 of epoch 9, 63.31 ms/it, loss 0.459452
Finished training it 32768/76743 of epoch 9, 63.53 ms/it, loss 0.459567
Finished training it 32768/76743 of epoch 9, 63.26 ms/it, loss 0.461453
Finished training it 32768/76743 of epoch 9, 63.30 ms/it, loss 0.458773
Finished training it 33792/76743 of epoch 9, 64.01 ms/it, loss 0.457424
Finished training it 33792/76743 of epoch 9, 63.68 ms/it, loss 0.461300
Finished training it 33792/76743 of epoch 9, 63.61 ms/it, loss 0.458002
Finished training it 33792/76743 of epoch 9, 63.58 ms/it, loss 0.461024
Finished training it 34816/76743 of epoch 9, 64.00 ms/it, loss 0.462870
Finished training it 34816/76743 of epoch 9, 64.17 ms/it, loss 0.460862
Finished training it 34816/76743 of epoch 9, 63.86 ms/it, loss 0.460594
Finished training it 34816/76743 of epoch 9, 63.98 ms/it, loss 0.457876
Finished training it 35840/76743 of epoch 9, 64.00 ms/it, loss 0.459782
Finished training it 35840/76743 of epoch 9, 63.64 ms/it, loss 0.460293
Finished training it 35840/76743 of epoch 9, 63.74 ms/it, loss 0.460287
Finished training it 35840/76743 of epoch 9, 63.65 ms/it, loss 0.461941
Finished training it 36864/76743 of epoch 9, 63.93 ms/it, loss 0.461098
Finished training it 36864/76743 of epoch 9, 63.62 ms/it, loss 0.458958
Finished training it 36864/76743 of epoch 9, 63.62 ms/it, loss 0.459636
Finished training it 36864/76743 of epoch 9, 63.45 ms/it, loss 0.461656
Finished training it 37888/76743 of epoch 9, 63.68 ms/it, loss 0.461537
Finished training it 37888/76743 of epoch 9, 63.37 ms/it, loss 0.459323
Finished training it 37888/76743 of epoch 9, 63.37 ms/it, loss 0.461758
Finished training it 37888/76743 of epoch 9, 63.20 ms/it, loss 0.460177
Finished training it 38912/76743 of epoch 9, 63.69 ms/it, loss 0.461914
Finished training it 38912/76743 of epoch 9, 63.94 ms/it, loss 0.459256
Finished training it 38912/76743 of epoch 9, 63.75 ms/it, loss 0.463871
Finished training it 38912/76743 of epoch 9, 63.57 ms/it, loss 0.460168
Finished training it 39936/76743 of epoch 9, 63.19 ms/it, loss 0.461936
Finished training it 39936/76743 of epoch 9, 63.68 ms/it, loss 0.461247
Finished training it 39936/76743 of epoch 9, 63.39 ms/it, loss 0.460553
Finished training it 39936/76743 of epoch 9, 63.46 ms/it, loss 0.462514
Finished training it 40960/76743 of epoch 9, 63.30 ms/it, loss 0.460494
Finished training it 40960/76743 of epoch 9, 63.63 ms/it, loss 0.459355
Finished training it 40960/76743 of epoch 9, 63.36 ms/it, loss 0.462477
Finished training it 40960/76743 of epoch 9, 63.34 ms/it, loss 0.460086
Testing at - 40960/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553870.0
get out
0 has test check 2553870.0 and sample count 3274240
 accuracy 77.999 %, best 78.070 %, roc auc score 0.7823, best 0.7834
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553870.0
get out
2 has test check 2553870.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 63.75 ms/it, loss 0.461052
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553870.0
get out
1 has test check 2553870.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 63.63 ms/it, loss 0.459799
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553870.0
get out
3 has test check 2553870.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 63.54 ms/it, loss 0.459769
Finished training it 41984/76743 of epoch 9, 63.40 ms/it, loss 0.461323
Finished training it 43008/76743 of epoch 9, 63.79 ms/it, loss 0.460033
Finished training it 43008/76743 of epoch 9, 63.73 ms/it, loss 0.463797
Finished training it 43008/76743 of epoch 9, 63.67 ms/it, loss 0.459511
Finished training it 43008/76743 of epoch 9, 64.02 ms/it, loss 0.456812
Finished training it 44032/76743 of epoch 9, 63.77 ms/it, loss 0.463564
Finished training it 44032/76743 of epoch 9, 63.51 ms/it, loss 0.459596
Finished training it 44032/76743 of epoch 9, 63.46 ms/it, loss 0.460588
Finished training it 44032/76743 of epoch 9, 63.57 ms/it, loss 0.459056
Finished training it 45056/76743 of epoch 9, 68.51 ms/it, loss 0.458949
Finished training it 45056/76743 of epoch 9, 68.13 ms/it, loss 0.460251
Finished training it 45056/76743 of epoch 9, 68.23 ms/it, loss 0.463177
Finished training it 45056/76743 of epoch 9, 68.34 ms/it, loss 0.460146
Finished training it 46080/76743 of epoch 9, 68.54 ms/it, loss 0.460535
Finished training it 46080/76743 of epoch 9, 68.35 ms/it, loss 0.461527
Finished training it 46080/76743 of epoch 9, 68.52 ms/it, loss 0.460729
Finished training it 46080/76743 of epoch 9, 68.18 ms/it, loss 0.460078
Finished training it 47104/76743 of epoch 9, 63.71 ms/it, loss 0.460583
Finished training it 47104/76743 of epoch 9, 64.02 ms/it, loss 0.459668
Finished training it 47104/76743 of epoch 9, 63.85 ms/it, loss 0.462681
Finished training it 47104/76743 of epoch 9, 63.74 ms/it, loss 0.462681
Finished training it 48128/76743 of epoch 9, 63.61 ms/it, loss 0.458811
Finished training it 48128/76743 of epoch 9, 63.65 ms/it, loss 0.459588
Finished training it 48128/76743 of epoch 9, 63.98 ms/it, loss 0.462864
Finished training it 48128/76743 of epoch 9, 63.74 ms/it, loss 0.459609
Finished training it 49152/76743 of epoch 9, 63.16 ms/it, loss 0.461068
Finished training it 49152/76743 of epoch 9, 63.56 ms/it, loss 0.460652
Finished training it 49152/76743 of epoch 9, 63.26 ms/it, loss 0.459374
Finished training it 49152/76743 of epoch 9, 63.31 ms/it, loss 0.461102
Finished training it 50176/76743 of epoch 9, 63.56 ms/it, loss 0.459765
Finished training it 50176/76743 of epoch 9, 63.66 ms/it, loss 0.458472
Finished training it 50176/76743 of epoch 9, 63.58 ms/it, loss 0.458050
Finished training it 50176/76743 of epoch 9, 63.85 ms/it, loss 0.458585
Finished training it 51200/76743 of epoch 9, 63.47 ms/it, loss 0.461724
Finished training it 51200/76743 of epoch 9, 63.64 ms/it, loss 0.459418
Finished training it 51200/76743 of epoch 9, 63.25 ms/it, loss 0.461202
Finished training it 51200/76743 of epoch 9, 63.38 ms/it, loss 0.459713
Testing at - 51200/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555751.0
get out
0 has test check 2555751.0 and sample count 3274240
 accuracy 78.056 %, best 78.070 %, roc auc score 0.7834, best 0.7834
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555751.0
get out
2 has test check 2555751.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 63.79 ms/it, loss 0.459729
Finished training it 52224/76743 of epoch 9, 63.63 ms/it, loss 0.461024
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555751.0
get out
1 has test check 2555751.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 63.60 ms/it, loss 0.459244
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555751.0
get out
3 has test check 2555751.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 63.59 ms/it, loss 0.460867
Finished training it 53248/76743 of epoch 9, 63.95 ms/it, loss 0.461264
Finished training it 53248/76743 of epoch 9, 64.16 ms/it, loss 0.457774
Finished training it 53248/76743 of epoch 9, 64.03 ms/it, loss 0.458910
Finished training it 53248/76743 of epoch 9, 63.87 ms/it, loss 0.460729
Finished training it 54272/76743 of epoch 9, 63.46 ms/it, loss 0.462974
Finished training it 54272/76743 of epoch 9, 63.69 ms/it, loss 0.460728
Finished training it 54272/76743 of epoch 9, 63.43 ms/it, loss 0.459527
Finished training it 54272/76743 of epoch 9, 63.49 ms/it, loss 0.460233
Finished training it 55296/76743 of epoch 9, 63.32 ms/it, loss 0.457322
Finished training it 55296/76743 of epoch 9, 63.56 ms/it, loss 0.461695
Finished training it 55296/76743 of epoch 9, 63.34 ms/it, loss 0.459218
Finished training it 55296/76743 of epoch 9, 63.31 ms/it, loss 0.464419
Finished training it 56320/76743 of epoch 9, 63.34 ms/it, loss 0.465085
Finished training it 56320/76743 of epoch 9, 63.36 ms/it, loss 0.459078
Finished training it 56320/76743 of epoch 9, 63.50 ms/it, loss 0.462774
Finished training it 56320/76743 of epoch 9, 63.69 ms/it, loss 0.459045
Finished training it 57344/76743 of epoch 9, 63.20 ms/it, loss 0.459322
Finished training it 57344/76743 of epoch 9, 63.13 ms/it, loss 0.459171
Finished training it 57344/76743 of epoch 9, 63.05 ms/it, loss 0.458803
Finished training it 57344/76743 of epoch 9, 63.40 ms/it, loss 0.460459
Finished training it 58368/76743 of epoch 9, 63.65 ms/it, loss 0.460004
Finished training it 58368/76743 of epoch 9, 63.86 ms/it, loss 0.461289
Finished training it 58368/76743 of epoch 9, 63.55 ms/it, loss 0.458555
Finished training it 58368/76743 of epoch 9, 63.60 ms/it, loss 0.462207
Finished training it 59392/76743 of epoch 9, 63.21 ms/it, loss 0.458381
Finished training it 59392/76743 of epoch 9, 63.18 ms/it, loss 0.458549
Finished training it 59392/76743 of epoch 9, 63.40 ms/it, loss 0.460529
Finished training it 59392/76743 of epoch 9, 63.22 ms/it, loss 0.458517
Finished training it 60416/76743 of epoch 9, 63.14 ms/it, loss 0.460230
Finished training it 60416/76743 of epoch 9, 63.12 ms/it, loss 0.457862
Finished training it 60416/76743 of epoch 9, 63.35 ms/it, loss 0.460789
Finished training it 60416/76743 of epoch 9, 63.06 ms/it, loss 0.462151
Finished training it 61440/76743 of epoch 9, 63.37 ms/it, loss 0.461813
Finished training it 61440/76743 of epoch 9, 63.31 ms/it, loss 0.459663
Finished training it 61440/76743 of epoch 9, 63.33 ms/it, loss 0.460562
Finished training it 61440/76743 of epoch 9, 63.69 ms/it, loss 0.461443
Testing at - 61440/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556132.0
get out
0 has test check 2556132.0 and sample count 3274240
 accuracy 78.068 %, best 78.070 %, roc auc score 0.7835, best 0.7835
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556132.0
get out
1 has test check 2556132.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 63.50 ms/it, loss 0.461370
Finished training it 62464/76743 of epoch 9, 63.44 ms/it, loss 0.459894
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556132.0
get out
2 has test check 2556132.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 63.76 ms/it, loss 0.462048
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556132.0
get out
3 has test check 2556132.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 63.30 ms/it, loss 0.461504
Finished training it 63488/76743 of epoch 9, 63.91 ms/it, loss 0.458747
Finished training it 63488/76743 of epoch 9, 64.09 ms/it, loss 0.460805
Finished training it 63488/76743 of epoch 9, 63.80 ms/it, loss 0.463661
Finished training it 63488/76743 of epoch 9, 63.93 ms/it, loss 0.459581
Finished training it 64512/76743 of epoch 9, 63.91 ms/it, loss 0.457723
Finished training it 64512/76743 of epoch 9, 63.56 ms/it, loss 0.463016
Finished training it 64512/76743 of epoch 9, 63.75 ms/it, loss 0.460979
Finished training it 64512/76743 of epoch 9, 63.60 ms/it, loss 0.461333
Finished training it 65536/76743 of epoch 9, 74.43 ms/it, loss 0.461162
Finished training it 65536/76743 of epoch 9, 74.01 ms/it, loss 0.458202
Finished training it 65536/76743 of epoch 9, 74.27 ms/it, loss 0.459500
Finished training it 65536/76743 of epoch 9, 74.23 ms/it, loss 0.460626
Finished training it 66560/76743 of epoch 9, 63.62 ms/it, loss 0.461005
Finished training it 66560/76743 of epoch 9, 63.83 ms/it, loss 0.459786
Finished training it 66560/76743 of epoch 9, 63.54 ms/it, loss 0.459671
Finished training it 66560/76743 of epoch 9, 63.73 ms/it, loss 0.458773
Finished training it 67584/76743 of epoch 9, 63.96 ms/it, loss 0.459604
Finished training it 67584/76743 of epoch 9, 63.75 ms/it, loss 0.461908
Finished training it 67584/76743 of epoch 9, 63.63 ms/it, loss 0.460051
Finished training it 67584/76743 of epoch 9, 63.67 ms/it, loss 0.463223
Finished training it 68608/76743 of epoch 9, 63.91 ms/it, loss 0.456876
Finished training it 68608/76743 of epoch 9, 63.67 ms/it, loss 0.458349
Finished training it 68608/76743 of epoch 9, 63.61 ms/it, loss 0.459523
Finished training it 68608/76743 of epoch 9, 63.62 ms/it, loss 0.460506
Finished training it 69632/76743 of epoch 9, 63.50 ms/it, loss 0.459268
Finished training it 69632/76743 of epoch 9, 63.46 ms/it, loss 0.459661
Finished training it 69632/76743 of epoch 9, 63.51 ms/it, loss 0.459604
Finished training it 69632/76743 of epoch 9, 63.66 ms/it, loss 0.460649
Finished training it 70656/76743 of epoch 9, 64.15 ms/it, loss 0.461192
Finished training it 70656/76743 of epoch 9, 63.91 ms/it, loss 0.457453
Finished training it 70656/76743 of epoch 9, 63.85 ms/it, loss 0.461028
Finished training it 70656/76743 of epoch 9, 63.81 ms/it, loss 0.460626
Finished training it 71680/76743 of epoch 9, 63.76 ms/it, loss 0.462742
Finished training it 71680/76743 of epoch 9, 63.81 ms/it, loss 0.459207
Finished training it 71680/76743 of epoch 9, 63.63 ms/it, loss 0.459189
Finished training it 71680/76743 of epoch 9, 64.03 ms/it, loss 0.459581
Testing at - 71680/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556725.0
get out
0 has test check 2556725.0 and sample count 3274240
 accuracy 78.086 %, best 78.086 %, roc auc score 0.7842, best 0.7842
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556725.0
get out
3 has test check 2556725.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 63.32 ms/it, loss 0.463115
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 9, 63.37 ms/it, loss 0.460175
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556725.0
get out
1 has test check 2556725.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 63.30 ms/it, loss 0.457957
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556725.0
get out
2 has test check 2556725.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 63.59 ms/it, loss 0.459956
Finished training it 73728/76743 of epoch 9, 63.41 ms/it, loss 0.460740
Finished training it 73728/76743 of epoch 9, 63.61 ms/it, loss 0.460638
Finished training it 73728/76743 of epoch 9, 63.33 ms/it, loss 0.460163
Finished training it 73728/76743 of epoch 9, 63.45 ms/it, loss 0.460648
Finished training it 74752/76743 of epoch 9, 63.71 ms/it, loss 0.459719
Finished training it 74752/76743 of epoch 9, 63.55 ms/it, loss 0.459367
Finished training it 74752/76743 of epoch 9, 63.54 ms/it, loss 0.459820
Finished training it 74752/76743 of epoch 9, 63.27 ms/it, loss 0.462401
Finished training it 75776/76743 of epoch 9, 63.77 ms/it, loss 0.461632
Finished training it 75776/76743 of epoch 9, 64.09 ms/it, loss 0.459402
Finished training it 75776/76743 of epoch 9, 63.91 ms/it, loss 0.458550
Finished training it 75776/76743 of epoch 9, 63.83 ms/it, loss 0.461738
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556105.0
get out
0 has test check 2556105.0 and sample count 3274240
 accuracy 78.067 %, best 78.086 %, roc auc score 0.7837, best 0.7842
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556105.0
get out
2 has test check 2556105.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556105.0
get out
1 has test check 2556105.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556105.0
get out
3 has test check 2556105.0 and sample count 3274240
