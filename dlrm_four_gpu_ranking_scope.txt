Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
---------- embedding bag gradient quantized in 4 bits
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 64, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 64, output 16, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 367, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 1, quantization bit width 4, use full precision quantized and channelwise status channelwise
not quantize activations, quantize weights
before training, checking models
tensor([ 0.0222, -0.0005, -0.0043, -0.0073,  0.0185, -0.0040, -0.0199, -0.0135,
        -0.0046,  0.0253,  0.0205,  0.0158, -0.0037,  0.0069, -0.0146, -0.0155])
log path is written: /home/yz25672/dlrm_criteo_kaggle/
optimizer selected is  sgd
ranking from least wide range to the widest range [ 2 14 25  9 15  6 17 23 12 11 10  1  5  3  0 20 24  4  7 22 18 19 21 13
  8 16]
Finished training it 1024/76743 of epoch 0, 92.68 ms/it, loss 0.549130
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
---------- embedding bag gradient quantized in 4 bits
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 64, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 64, output 16, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 367, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 1, quantization bit width 4, use full precision quantized and channelwise status channelwise
not quantize activations, quantize weights
before training, checking models
tensor([ 0.0097, -0.0029, -0.0083,  0.0219,  0.0060, -0.0126,  0.0013, -0.0053,
        -0.0039, -0.0042,  0.0190,  0.0193,  0.0064, -0.0085,  0.0188, -0.0062])
log path is written: /home/yz25672/dlrm_criteo_kaggle/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 92.49 ms/it, loss 0.548326
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
---------- embedding bag gradient quantized in 4 bits
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 64, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 64, output 16, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 367, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 1, quantization bit width 4, use full precision quantized and channelwise status channelwise
not quantize activations, quantize weights
before training, checking models
tensor([-0.0068, -0.0155,  0.0231, -0.0228, -0.0146, -0.0208,  0.0161, -0.0009,
        -0.0058, -0.0102,  0.0196, -0.0106,  0.0221,  0.0036,  0.0234, -0.0066])
log path is written: /home/yz25672/dlrm_criteo_kaggle/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 92.23 ms/it, loss 0.549534
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
---------- embedding bag gradient quantized in 4 bits
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/home/yz25672/dlrm_criteo_kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 64, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 64, output 16, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 367, output 512, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 512, output 256, quantization bit width 4, use full precision quantized and channelwise status channelwise
use quant linear, input 256, output 1, quantization bit width 4, use full precision quantized and channelwise status channelwise
not quantize activations, quantize weights
before training, checking models
tensor([ 0.0142,  0.0239, -0.0213,  0.0071,  0.0203, -0.0122,  0.0225,  0.0153,
         0.0252, -0.0194,  0.0111, -0.0237, -0.0015,  0.0167,  0.0171,  0.0050])
log path is written: /home/yz25672/dlrm_criteo_kaggle/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 92.40 ms/it, loss 0.549157
ranking from least wide range to the widest range [ 2  6 14 23 15  9 12 11 17 10  0  5  3 20 25  7  1  4 24 18 21  8 19 22
 13 16]
Finished training it 2048/76743 of epoch 0, 116.12 ms/it, loss 0.515821
Finished training it 2048/76743 of epoch 0, 115.85 ms/it, loss 0.516989
Finished training it 2048/76743 of epoch 0, 116.19 ms/it, loss 0.517516
Finished training it 2048/76743 of epoch 0, 116.03 ms/it, loss 0.515474
ranking from least wide range to the widest range [15  6  2 14 11 17  9 12 20 10 23  3  7 25  0  5  4  1 18 24 21  8 19 16
 13 22]
Finished training it 3072/76743 of epoch 0, 120.19 ms/it, loss 0.507738
Finished training it 3072/76743 of epoch 0, 120.33 ms/it, loss 0.508448
Finished training it 3072/76743 of epoch 0, 120.26 ms/it, loss 0.507850
Finished training it 3072/76743 of epoch 0, 116.23 ms/it, loss 0.510006
Finished training it 4096/76743 of epoch 0, 120.46 ms/it, loss 0.504538
Finished training it 4096/76743 of epoch 0, 116.97 ms/it, loss 0.503213
ranking from least wide range to the widest range [15 14  9 17  2  6 12 23 11 20 25 10  7  0  3  5  4 18 21 19 24  8  1 13
 22 16]
Finished training it 4096/76743 of epoch 0, 120.51 ms/it, loss 0.504116
Finished training it 4096/76743 of epoch 0, 120.95 ms/it, loss 0.505195
Finished training it 5120/76743 of epoch 0, 119.18 ms/it, loss 0.496406
Finished training it 5120/76743 of epoch 0, 121.43 ms/it, loss 0.500645
ranking from least wide range to the widest range [15 17  2  9 14  6 11 23 12 20  7 10  3  4  0 25  5  8 21 18 19 24  1 16
 13 22]
Finished training it 5120/76743 of epoch 0, 121.27 ms/it, loss 0.501277
Finished training it 5120/76743 of epoch 0, 121.42 ms/it, loss 0.500853
ranking from least wide range to the widest range [15 17  2  6 14  9 11 23  7 12 10  0 20  4 25  3  5  8 21 19 18 24 16  1
 22 13]
Finished training it 6144/76743 of epoch 0, 123.66 ms/it, loss 0.494631
Finished training it 6144/76743 of epoch 0, 123.62 ms/it, loss 0.497744
Finished training it 6144/76743 of epoch 0, 121.66 ms/it, loss 0.495905
Finished training it 6144/76743 of epoch 0, 123.75 ms/it, loss 0.496786
ranking from least wide range to the widest range [ 2 15 17  6  9 11 12  7 14 23 20 10  0  4 21 25  5  3  8 16 19 24 18 22
 13  1]
Finished training it 7168/76743 of epoch 0, 122.51 ms/it, loss 0.497616
Finished training it 7168/76743 of epoch 0, 120.66 ms/it, loss 0.496827
Finished training it 7168/76743 of epoch 0, 122.48 ms/it, loss 0.498148
Finished training it 7168/76743 of epoch 0, 122.41 ms/it, loss 0.494122
ranking from least wide range to the widest range [17 15  2 11  9  6 12 14  7 23  0 20 10  4 21  8 25 19  5 24  3 16 18 22
  1 13]
Finished training it 8192/76743 of epoch 0, 110.53 ms/it, loss 0.492634
Finished training it 8192/76743 of epoch 0, 110.96 ms/it, loss 0.493792
Finished training it 8192/76743 of epoch 0, 110.40 ms/it, loss 0.492062
Finished training it 8192/76743 of epoch 0, 109.06 ms/it, loss 0.495345
ranking from least wide range to the widest range [17  2 15 11  9  6 12  7  0 23 14 20 10  4 21  8 25 19 16 24  5 18 22  3
  1 13]
Finished training it 9216/76743 of epoch 0, 119.01 ms/it, loss 0.491176
Finished training it 9216/76743 of epoch 0, 119.31 ms/it, loss 0.495116
Finished training it 9216/76743 of epoch 0, 117.80 ms/it, loss 0.491965
Finished training it 9216/76743 of epoch 0, 119.52 ms/it, loss 0.491455
Finished training it 10240/76743 of epoch 0, 121.14 ms/it, loss 0.490880
Finished training it 10240/76743 of epoch 0, 122.29 ms/it, loss 0.491242
Finished training it 10240/76743 of epoch 0, 122.29 ms/it, loss 0.489793
ranking from least wide range to the widest range [ 2 17 15 11 12 23  6  9  0 14  7 20 10  4 21  8 19  5 16 24 22 25  3 18
  1 13]
Finished training it 10240/76743 of epoch 0, 122.20 ms/it, loss 0.490171
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.06253908692933083steps testing: 0.12507817385866166steps testing: 0.18761726078799248steps testing: 0.2501563477173233steps testing: 0.31269543464665417steps testing: 0.37523452157598497steps testing: 0.4377736085053158steps testing: 0.5003126954346466steps testing: 0.5628517823639775steps testing: 0.6253908692933083steps testing: 0.6879299562226392steps testing: 0.7504690431519699steps testing: 0.8130081300813008steps testing: 0.8755472170106317steps testing: 0.9380863039399625Warning: Skipping the batch 3197 with size 602
rank: 0 test_accu: 2510599.0
get out
0 has test check 2510599.0 and sample count 3273728
 accuracy 76.689 %, best 76.689 %, roc auc score 0.7517, best 0.7517
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 2 test_accu: 2510599.0
get out
2 has test check 2510599.0 and sample count 3273728
Finished training it 11264/76743 of epoch 0, 120.92 ms/it, loss 0.487896
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 3 test_accu: 2510599.0
get out
3 has test check 2510599.0 and sample count 3273728
Finished training it 11264/76743 of epoch 0, 119.58 ms/it, loss 0.490824
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 1 test_accu: 2510599.0
get out
1 has test check 2510599.0 and sample count 3273728
Finished training it 11264/76743 of epoch 0, 121.60 ms/it, loss 0.488723
Saving model to /home/yz25672/dlrm_criteo_kaggle/save_model_after_training_one1.0.pt
ranking from least wide range to the widest range [ 2 15 17 11 12  7  9 23  6  4 14  0 10 20 21  8 19 24  5 25 16 18 22  3
  1 13]
Finished training it 11264/76743 of epoch 0, 121.60 ms/it, loss 0.488959
ranking from least wide range to the widest range [15  2 17  9 11 23  7 14  0 12  6 20  4 10 21  8 19  5 22 24 16 25 18  1
  3 13]
Finished training it 12288/76743 of epoch 0, 121.04 ms/it, loss 0.486716
Finished training it 12288/76743 of epoch 0, 120.53 ms/it, loss 0.488152
Finished training it 12288/76743 of epoch 0, 119.79 ms/it, loss 0.488500
Finished training it 12288/76743 of epoch 0, 121.22 ms/it, loss 0.487205
Finished training it 13312/76743 of epoch 0, 120.14 ms/it, loss 0.486307
Finished training it 13312/76743 of epoch 0, 119.76 ms/it, loss 0.487622
Finished training it 13312/76743 of epoch 0, 120.75 ms/it, loss 0.487219
ranking from least wide range to the widest range [15  2 23 17 11 12  9  0 14  7  4  6 20 10 21  8 19  5 24 16 22 25 18  3
  1 13]
Finished training it 13312/76743 of epoch 0, 120.57 ms/it, loss 0.487031
Finished training it 14336/76743 of epoch 0, 119.84 ms/it, loss 0.489339
ranking from least wide range to the widest range [15  2 17 23 11  9  0 12 14  7 20  6  4 10 21  8 19  5 16 24 22  1 18  3
 13 25]
Finished training it 14336/76743 of epoch 0, 120.58 ms/it, loss 0.488535
Finished training it 14336/76743 of epoch 0, 120.19 ms/it, loss 0.483930
Finished training it 14336/76743 of epoch 0, 120.74 ms/it, loss 0.485890
Finished training it 15360/76743 of epoch 0, 120.54 ms/it, loss 0.487001
Finished training it 15360/76743 of epoch 0, 120.46 ms/it, loss 0.487681
ranking from least wide range to the widest range [ 2 15 17 23 11  9  7 12 14  0  4 20  6 10 21  8  5 19 22 16 24 18  3 13
  1 25]
Finished training it 15360/76743 of epoch 0, 120.91 ms/it, loss 0.486359
Finished training it 15360/76743 of epoch 0, 121.05 ms/it, loss 0.481860
ranking from least wide range to the widest range [17  2 15 11 23 12  9  7  0 14  4 20  6 10 21  8 19  5 16 24 22 18  1 13
  3 25]
Finished training it 16384/76743 of epoch 0, 128.60 ms/it, loss 0.483874
Finished training it 16384/76743 of epoch 0, 128.11 ms/it, loss 0.484531
Finished training it 16384/76743 of epoch 0, 128.20 ms/it, loss 0.483271
Finished training it 16384/76743 of epoch 0, 128.62 ms/it, loss 0.487356
ranking from least wide range to the widest range [ 2 17 15  9 11  7 12 23 14 20  6  4  0 10 21 19  8  5 22 24 16 18 25  1
  3 13]
Finished training it 17408/76743 of epoch 0, 120.62 ms/it, loss 0.483763
Finished training it 17408/76743 of epoch 0, 120.44 ms/it, loss 0.482280
Finished training it 17408/76743 of epoch 0, 120.04 ms/it, loss 0.480549
Finished training it 17408/76743 of epoch 0, 120.37 ms/it, loss 0.483729
ranking from least wide range to the widest range [17  2 15 11  7  9 12  0 23 20 14  6 10  4  8 21 19  5 24 16 22 18  3  1
 13 25]
Finished training it 18432/76743 of epoch 0, 119.66 ms/it, loss 0.480777
Finished training it 18432/76743 of epoch 0, 119.34 ms/it, loss 0.484471
Finished training it 18432/76743 of epoch 0, 119.48 ms/it, loss 0.483230
Finished training it 18432/76743 of epoch 0, 119.56 ms/it, loss 0.484995
ranking from least wide range to the widest range [17  2  7 11 23 15  0 12 20  4  6  9 14 10 21 19  8  5 24 18 16 22 13 25
  1  3]
Finished training it 19456/76743 of epoch 0, 118.91 ms/it, loss 0.481968
Finished training it 19456/76743 of epoch 0, 118.32 ms/it, loss 0.482243
Finished training it 19456/76743 of epoch 0, 118.90 ms/it, loss 0.485303
Finished training it 19456/76743 of epoch 0, 118.70 ms/it, loss 0.484891
Finished training it 20480/76743 of epoch 0, 117.09 ms/it, loss 0.485372
ranking from least wide range to the widest range [17  2 15 11 23 12  7 20 14  6  9  4  0 10 21  8  5 19 22 16 24 18  1  3
 13 25]
Finished training it 20480/76743 of epoch 0, 117.47 ms/it, loss 0.484023
Finished training it 20480/76743 of epoch 0, 117.57 ms/it, loss 0.482815
Finished training it 20480/76743 of epoch 0, 117.83 ms/it, loss 0.481913
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.06253908692933083steps testing: 0.12507817385866166steps testing: 0.18761726078799248steps testing: 0.2501563477173233steps testing: 0.31269543464665417steps testing: 0.37523452157598497steps testing: 0.4377736085053158steps testing: 0.5003126954346466steps testing: 0.5628517823639775steps testing: 0.6253908692933083steps testing: 0.6879299562226392steps testing: 0.7504690431519699steps testing: 0.8130081300813008steps testing: 0.8755472170106317steps testing: 0.9380863039399625Warning: Skipping the batch 3197 with size 602
rank: 0 test_accu: 2521800.0
get out
0 has test check 2521800.0 and sample count 3273728
 accuracy 77.031 %, best 77.031 %, roc auc score 0.7604, best 0.7604
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 1 test_accu: 2521800.0
get out
1 has test check 2521800.0 and sample count 3273728
Finished training it 21504/76743 of epoch 0, 115.40 ms/it, loss 0.480269
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 2 test_accu: 2521800.0
get out
2 has test check 2521800.0 and sample count 3273728
Finished training it 21504/76743 of epoch 0, 114.97 ms/it, loss 0.480971
Saving model to /home/yz25672/dlrm_criteo_kaggle/save_model_after_training_one0.0.pt
ranking from least wide range to the widest range [17  2 23 15 11 12 20 14  0  7  6  9  4 10  8 21  5 16 19 22 24  1 18  3
 13 25]
Finished training it 21504/76743 of epoch 0, 115.39 ms/it, loss 0.483398
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 3 test_accu: 2521800.0
get out
3 has test check 2521800.0 and sample count 3273728
Finished training it 21504/76743 of epoch 0, 116.11 ms/it, loss 0.480616
Finished training it 22528/76743 of epoch 0, 116.83 ms/it, loss 0.480722
Finished training it 22528/76743 of epoch 0, 116.95 ms/it, loss 0.478458
ranking from least wide range to the widest range [17  2 12 11 15  7 23  9  4 14 20  6  0 10 21  8 19  5 22 16 24  1 18 13
 25  3]
Finished training it 22528/76743 of epoch 0, 116.69 ms/it, loss 0.479779
Finished training it 22528/76743 of epoch 0, 116.35 ms/it, loss 0.479223
Finished training it 23552/76743 of epoch 0, 118.80 ms/it, loss 0.480364
Finished training it 23552/76743 of epoch 0, 119.46 ms/it, loss 0.480196
ranking from least wide range to the widest range [ 2 17 12 11 15 23  4  7  9 20  0  6 14 10  8 19 21  5 22 16 24 18 13 25
  3  1]
Finished training it 23552/76743 of epoch 0, 119.17 ms/it, loss 0.481760
Finished training it 23552/76743 of epoch 0, 119.28 ms/it, loss 0.480933
ranking from least wide range to the widest range [ 2 17 12 11 15 23  7 20  9 14  4  6  0 10 21  5  8 19 16 22 24  1 13 18
  3 25]
Finished training it 24576/76743 of epoch 0, 117.65 ms/it, loss 0.481243
Finished training it 24576/76743 of epoch 0, 118.14 ms/it, loss 0.480950
Finished training it 24576/76743 of epoch 0, 117.60 ms/it, loss 0.480773
Finished training it 24576/76743 of epoch 0, 117.71 ms/it, loss 0.479287
Finished training it 25600/76743 of epoch 0, 113.68 ms/it, loss 0.479022
ranking from least wide range to the widest range [ 2 17 12 11 15  7 23  4 20  9  6 14  0 10 21  8  5 19 24 16 22 18  1 13
  3 25]
Finished training it 25600/76743 of epoch 0, 114.09 ms/it, loss 0.479658
Finished training it 25600/76743 of epoch 0, 114.73 ms/it, loss 0.480341
Finished training it 25600/76743 of epoch 0, 114.19 ms/it, loss 0.476671
Finished training it 26624/76743 of epoch 0, 117.46 ms/it, loss 0.479970
ranking from least wide range to the widest range [ 2 17 12 11 15  7 23  4  9 20  6  0 14 10  8  5 16 19 21 22 24 18 13  1
  3 25]
Finished training it 26624/76743 of epoch 0, 116.56 ms/it, loss 0.479516
Finished training it 26624/76743 of epoch 0, 116.31 ms/it, loss 0.478704
Finished training it 26624/76743 of epoch 0, 116.71 ms/it, loss 0.479312
ranking from least wide range to the widest range [ 2 17 12 15 11 23 20  9  7  4  6 14  0 10 21  8  5 22 16 19 24 13  1 18
  3 25]
Finished training it 27648/76743 of epoch 0, 120.69 ms/it, loss 0.480429
Finished training it 27648/76743 of epoch 0, 120.07 ms/it, loss 0.481373
Finished training it 27648/76743 of epoch 0, 121.00 ms/it, loss 0.481116
Finished training it 27648/76743 of epoch 0, 120.49 ms/it, loss 0.480550
Finished training it 28672/76743 of epoch 0, 120.24 ms/it, loss 0.478392
Finished training it 28672/76743 of epoch 0, 120.70 ms/it, loss 0.478342
ranking from least wide range to the widest range [17  2 11 12 15 23 20  7  6  9 14  4  0 10 21  8  5 19 22 24 16 13 18  3
  1 25]
Finished training it 28672/76743 of epoch 0, 121.38 ms/it, loss 0.480011
Finished training it 28672/76743 of epoch 0, 120.68 ms/it, loss 0.481536
ranking from least wide range to the widest range [ 2 17 11 12 15 23  9  4 20  7 14  0  6 10 19  8 21 24  5 16 22 18 13  1
  3 25]
Finished training it 29696/76743 of epoch 0, 121.76 ms/it, loss 0.480415
Finished training it 29696/76743 of epoch 0, 121.38 ms/it, loss 0.476642
Finished training it 29696/76743 of epoch 0, 121.86 ms/it, loss 0.479982
Finished training it 29696/76743 of epoch 0, 121.91 ms/it, loss 0.478697
ranking from least wide range to the widest range [ 2 17 12 15 11  9 23 20  0  7  6  4 14 10  8 21  5 19 22 16 24 13  1 18
  3 25]
Finished training it 30720/76743 of epoch 0, 117.94 ms/it, loss 0.477604
Finished training it 30720/76743 of epoch 0, 117.20 ms/it, loss 0.478321
Finished training it 30720/76743 of epoch 0, 117.60 ms/it, loss 0.478985
Finished training it 30720/76743 of epoch 0, 117.76 ms/it, loss 0.480800
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.06253908692933083steps testing: 0.12507817385866166steps testing: 0.18761726078799248steps testing: 0.2501563477173233steps testing: 0.31269543464665417steps testing: 0.37523452157598497steps testing: 0.4377736085053158steps testing: 0.5003126954346466steps testing: 0.5628517823639775steps testing: 0.6253908692933083steps testing: 0.6879299562226392steps testing: 0.7504690431519699steps testing: 0.8130081300813008steps testing: 0.8755472170106317steps testing: 0.9380863039399625Warning: Skipping the batch 3197 with size 602
rank: 0 test_accu: 2525222.0
get out
0 has test check 2525222.0 and sample count 3273728
 accuracy 77.136 %, best 77.136 %, roc auc score 0.7631, best 0.7631
Saving model to /home/yz25672/dlrm_criteo_kaggle/save_model_after_training_one1.0.pt
ranking from least wide range to the widest range [ 2 17 12 15 11  7  4 20 23  9  0  6 14 10 19 21  8  5 16 24 22 18 13  1
 25  3]
Finished training it 31744/76743 of epoch 0, 121.22 ms/it, loss 0.480490
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 2 test_accu: 2525222.0
get out
2 has test check 2525222.0 and sample count 3273728
Finished training it 31744/76743 of epoch 0, 121.22 ms/it, loss 0.479694
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 3 test_accu: 2525222.0
get out
3 has test check 2525222.0 and sample count 3273728
Finished training it 31744/76743 of epoch 0, 121.21 ms/it, loss 0.477857
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 3197 with size 602
rank: 1 test_accu: 2525222.0
get out
1 has test check 2525222.0 and sample count 3273728
Finished training it 31744/76743 of epoch 0, 121.43 ms/it, loss 0.480493
