Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 4
---------- Embedding Table 1, quantization used, quantization bit set to 4
---------- Embedding Table 2, quantization used, quantization bit set to 4
---------- Embedding Table 3, quantization used, quantization bit set to 4
---------- Embedding Table 4, quantization used, quantization bit set to 4
---------- Embedding Table 5, quantization used, quantization bit set to 4
---------- Embedding Table 6, quantization used, quantization bit set to 4
---------- Embedding Table 7, quantization used, quantization bit set to 4
---------- Embedding Table 8, quantization used, quantization bit set to 4
---------- Embedding Table 9, quantization used, quantization bit set to 4
---------- Embedding Table 10, quantization used, quantization bit set to 4
---------- Embedding Table 11, quantization used, quantization bit set to 4
---------- Embedding Table 12, quantization used, quantization bit set to 4
---------- Embedding Table 13, quantization used, quantization bit set to 4
---------- Embedding Table 14, quantization used, quantization bit set to 4
---------- Embedding Table 15, quantization used, quantization bit set to 4
---------- Embedding Table 16, quantization used, quantization bit set to 4
---------- Embedding Table 17, quantization used, quantization bit set to 4
---------- Embedding Table 18, quantization used, quantization bit set to 4
---------- Embedding Table 19, quantization used, quantization bit set to 4
---------- Embedding Table 20, quantization used, quantization bit set to 4
---------- Embedding Table 21, quantization used, quantization bit set to 4
---------- Embedding Table 22, quantization used, quantization bit set to 4
---------- Embedding Table 23, quantization used, quantization bit set to 4
---------- Embedding Table 24, quantization used, quantization bit set to 4
---------- Embedding Table 25, quantization used, quantization bit set to 4
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 98.32 ms/it, loss 0.522322
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 4
---------- Embedding Table 1, quantization used, quantization bit set to 4
---------- Embedding Table 2, quantization used, quantization bit set to 4
---------- Embedding Table 3, quantization used, quantization bit set to 4
---------- Embedding Table 4, quantization used, quantization bit set to 4
---------- Embedding Table 5, quantization used, quantization bit set to 4
---------- Embedding Table 6, quantization used, quantization bit set to 4
---------- Embedding Table 7, quantization used, quantization bit set to 4
---------- Embedding Table 8, quantization used, quantization bit set to 4
---------- Embedding Table 9, quantization used, quantization bit set to 4
---------- Embedding Table 10, quantization used, quantization bit set to 4
---------- Embedding Table 11, quantization used, quantization bit set to 4
---------- Embedding Table 12, quantization used, quantization bit set to 4
---------- Embedding Table 13, quantization used, quantization bit set to 4
---------- Embedding Table 14, quantization used, quantization bit set to 4
---------- Embedding Table 15, quantization used, quantization bit set to 4
---------- Embedding Table 16, quantization used, quantization bit set to 4
---------- Embedding Table 17, quantization used, quantization bit set to 4
---------- Embedding Table 18, quantization used, quantization bit set to 4
---------- Embedding Table 19, quantization used, quantization bit set to 4
---------- Embedding Table 20, quantization used, quantization bit set to 4
---------- Embedding Table 21, quantization used, quantization bit set to 4
---------- Embedding Table 22, quantization used, quantization bit set to 4
---------- Embedding Table 23, quantization used, quantization bit set to 4
---------- Embedding Table 24, quantization used, quantization bit set to 4
---------- Embedding Table 25, quantization used, quantization bit set to 4
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 99.20 ms/it, loss 0.519576
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 4
---------- Embedding Table 1, quantization used, quantization bit set to 4
---------- Embedding Table 2, quantization used, quantization bit set to 4
---------- Embedding Table 3, quantization used, quantization bit set to 4
---------- Embedding Table 4, quantization used, quantization bit set to 4
---------- Embedding Table 5, quantization used, quantization bit set to 4
---------- Embedding Table 6, quantization used, quantization bit set to 4
---------- Embedding Table 7, quantization used, quantization bit set to 4
---------- Embedding Table 8, quantization used, quantization bit set to 4
---------- Embedding Table 9, quantization used, quantization bit set to 4
---------- Embedding Table 10, quantization used, quantization bit set to 4
---------- Embedding Table 11, quantization used, quantization bit set to 4
---------- Embedding Table 12, quantization used, quantization bit set to 4
---------- Embedding Table 13, quantization used, quantization bit set to 4
---------- Embedding Table 14, quantization used, quantization bit set to 4
---------- Embedding Table 15, quantization used, quantization bit set to 4
---------- Embedding Table 16, quantization used, quantization bit set to 4
---------- Embedding Table 17, quantization used, quantization bit set to 4
---------- Embedding Table 18, quantization used, quantization bit set to 4
---------- Embedding Table 19, quantization used, quantization bit set to 4
---------- Embedding Table 20, quantization used, quantization bit set to 4
---------- Embedding Table 21, quantization used, quantization bit set to 4
---------- Embedding Table 22, quantization used, quantization bit set to 4
---------- Embedding Table 23, quantization used, quantization bit set to 4
---------- Embedding Table 24, quantization used, quantization bit set to 4
---------- Embedding Table 25, quantization used, quantization bit set to 4
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 97.05 ms/it, loss 0.520228
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, quantization bit set to 4
---------- Embedding Table 1, quantization used, quantization bit set to 4
---------- Embedding Table 2, quantization used, quantization bit set to 4
---------- Embedding Table 3, quantization used, quantization bit set to 4
---------- Embedding Table 4, quantization used, quantization bit set to 4
---------- Embedding Table 5, quantization used, quantization bit set to 4
---------- Embedding Table 6, quantization used, quantization bit set to 4
---------- Embedding Table 7, quantization used, quantization bit set to 4
---------- Embedding Table 8, quantization used, quantization bit set to 4
---------- Embedding Table 9, quantization used, quantization bit set to 4
---------- Embedding Table 10, quantization used, quantization bit set to 4
---------- Embedding Table 11, quantization used, quantization bit set to 4
---------- Embedding Table 12, quantization used, quantization bit set to 4
---------- Embedding Table 13, quantization used, quantization bit set to 4
---------- Embedding Table 14, quantization used, quantization bit set to 4
---------- Embedding Table 15, quantization used, quantization bit set to 4
---------- Embedding Table 16, quantization used, quantization bit set to 4
---------- Embedding Table 17, quantization used, quantization bit set to 4
---------- Embedding Table 18, quantization used, quantization bit set to 4
---------- Embedding Table 19, quantization used, quantization bit set to 4
---------- Embedding Table 20, quantization used, quantization bit set to 4
---------- Embedding Table 21, quantization used, quantization bit set to 4
---------- Embedding Table 22, quantization used, quantization bit set to 4
---------- Embedding Table 23, quantization used, quantization bit set to 4
---------- Embedding Table 24, quantization used, quantization bit set to 4
---------- Embedding Table 25, quantization used, quantization bit set to 4
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 98.15 ms/it, loss 0.520081
Finished training it 2048/76743 of epoch 0, 86.78 ms/it, loss 0.514609
Finished training it 2048/76743 of epoch 0, 87.04 ms/it, loss 0.508949
Finished training it 2048/76743 of epoch 0, 86.92 ms/it, loss 0.512671
Finished training it 2048/76743 of epoch 0, 87.22 ms/it, loss 0.511606
Finished training it 3072/76743 of epoch 0, 86.74 ms/it, loss 0.514269
Finished training it 3072/76743 of epoch 0, 86.09 ms/it, loss 0.511410
Finished training it 3072/76743 of epoch 0, 86.66 ms/it, loss 0.510563
Finished training it 3072/76743 of epoch 0, 86.57 ms/it, loss 0.510157
Finished training it 4096/76743 of epoch 0, 86.33 ms/it, loss 0.509560
Finished training it 4096/76743 of epoch 0, 85.59 ms/it, loss 0.512377
Finished training it 4096/76743 of epoch 0, 86.05 ms/it, loss 0.510961
Finished training it 4096/76743 of epoch 0, 85.35 ms/it, loss 0.510749
Finished training it 5120/76743 of epoch 0, 83.40 ms/it, loss 0.507401
Finished training it 5120/76743 of epoch 0, 83.29 ms/it, loss 0.511334
Finished training it 5120/76743 of epoch 0, 83.66 ms/it, loss 0.509918
Finished training it 5120/76743 of epoch 0, 83.25 ms/it, loss 0.505366
Finished training it 6144/76743 of epoch 0, 85.36 ms/it, loss 0.506234
Finished training it 6144/76743 of epoch 0, 84.65 ms/it, loss 0.506300
Finished training it 6144/76743 of epoch 0, 85.11 ms/it, loss 0.507024
Finished training it 6144/76743 of epoch 0, 85.01 ms/it, loss 0.505363
Finished training it 7168/76743 of epoch 0, 83.79 ms/it, loss 0.507652
Finished training it 7168/76743 of epoch 0, 83.74 ms/it, loss 0.504158
Finished training it 7168/76743 of epoch 0, 83.67 ms/it, loss 0.505668
Finished training it 7168/76743 of epoch 0, 83.37 ms/it, loss 0.505663
Finished training it 8192/76743 of epoch 0, 84.33 ms/it, loss 0.506402
Finished training it 8192/76743 of epoch 0, 84.85 ms/it, loss 0.504517
Finished training it 8192/76743 of epoch 0, 84.08 ms/it, loss 0.503739
Finished training it 8192/76743 of epoch 0, 84.37 ms/it, loss 0.502390
Finished training it 9216/76743 of epoch 0, 85.35 ms/it, loss 0.504781
Finished training it 9216/76743 of epoch 0, 84.96 ms/it, loss 0.503336
Finished training it 9216/76743 of epoch 0, 85.35 ms/it, loss 0.503528
Finished training it 9216/76743 of epoch 0, 85.54 ms/it, loss 0.507045
Finished training it 10240/76743 of epoch 0, 83.92 ms/it, loss 0.506070
Finished training it 10240/76743 of epoch 0, 83.63 ms/it, loss 0.503316
Finished training it 10240/76743 of epoch 0, 83.83 ms/it, loss 0.504795
Finished training it 10240/76743 of epoch 0, 84.41 ms/it, loss 0.502110
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2487497.0
get out
0 has test check 2487497.0 and sample count 3274240
 accuracy 75.972 %, best 75.972 %, roc auc score 0.7291, best 0.7291
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 88.35 ms/it, loss 0.504497
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2487497.0
get out
3 has test check 2487497.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 86.52 ms/it, loss 0.506358
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2487497.0
get out
1 has test check 2487497.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 86.20 ms/it, loss 0.505354
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2487497.0
get out
2 has test check 2487497.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 87.95 ms/it, loss 0.504726
Finished training it 12288/76743 of epoch 0, 86.48 ms/it, loss 0.500691
Finished training it 12288/76743 of epoch 0, 87.44 ms/it, loss 0.503452
Finished training it 12288/76743 of epoch 0, 87.17 ms/it, loss 0.503961
Finished training it 12288/76743 of epoch 0, 86.04 ms/it, loss 0.503045
Finished training it 13312/76743 of epoch 0, 86.46 ms/it, loss 0.500803
Finished training it 13312/76743 of epoch 0, 87.04 ms/it, loss 0.500857
Finished training it 13312/76743 of epoch 0, 86.81 ms/it, loss 0.501588
Finished training it 13312/76743 of epoch 0, 85.91 ms/it, loss 0.502228
Finished training it 14336/76743 of epoch 0, 87.40 ms/it, loss 0.498713
Finished training it 14336/76743 of epoch 0, 86.78 ms/it, loss 0.502237
Finished training it 14336/76743 of epoch 0, 86.32 ms/it, loss 0.499168
Finished training it 14336/76743 of epoch 0, 87.06 ms/it, loss 0.502520
Finished training it 15360/76743 of epoch 0, 100.25 ms/it, loss 0.499929
Finished training it 15360/76743 of epoch 0, 99.60 ms/it, loss 0.501879
Finished training it 15360/76743 of epoch 0, 99.80 ms/it, loss 0.500274
Finished training it 15360/76743 of epoch 0, 99.11 ms/it, loss 0.499264
Finished training it 16384/76743 of epoch 0, 85.66 ms/it, loss 0.496554
Finished training it 16384/76743 of epoch 0, 85.90 ms/it, loss 0.501080
Finished training it 16384/76743 of epoch 0, 85.33 ms/it, loss 0.499017
Finished training it 16384/76743 of epoch 0, 85.55 ms/it, loss 0.499404
Finished training it 17408/76743 of epoch 0, 85.67 ms/it, loss 0.499982
Finished training it 17408/76743 of epoch 0, 85.94 ms/it, loss 0.500162
Finished training it 17408/76743 of epoch 0, 85.57 ms/it, loss 0.501305
Finished training it 17408/76743 of epoch 0, 85.93 ms/it, loss 0.498882
Finished training it 18432/76743 of epoch 0, 85.77 ms/it, loss 0.499735
Finished training it 18432/76743 of epoch 0, 85.33 ms/it, loss 0.499398
Finished training it 18432/76743 of epoch 0, 85.56 ms/it, loss 0.497670
Finished training it 18432/76743 of epoch 0, 86.06 ms/it, loss 0.498013
Finished training it 19456/76743 of epoch 0, 86.05 ms/it, loss 0.498074
Finished training it 19456/76743 of epoch 0, 85.77 ms/it, loss 0.498308
Finished training it 19456/76743 of epoch 0, 85.67 ms/it, loss 0.498071
Finished training it 19456/76743 of epoch 0, 85.76 ms/it, loss 0.498732
Finished training it 20480/76743 of epoch 0, 85.90 ms/it, loss 0.501250
Finished training it 20480/76743 of epoch 0, 85.93 ms/it, loss 0.496355
Finished training it 20480/76743 of epoch 0, 86.19 ms/it, loss 0.496992
Finished training it 20480/76743 of epoch 0, 85.82 ms/it, loss 0.500533
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2492634.0
get out
0 has test check 2492634.0 and sample count 3274240
 accuracy 76.129 %, best 76.129 %, roc auc score 0.7391, best 0.7391
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2492634.0
get out
2 has test check 2492634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 87.31 ms/it, loss 0.497151
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2492634.0
get out
1 has test check 2492634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 86.80 ms/it, loss 0.496729
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2492634.0
get out
3 has test check 2492634.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 86.08 ms/it, loss 0.498317
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 87.49 ms/it, loss 0.494953
Finished training it 22528/76743 of epoch 0, 86.80 ms/it, loss 0.495545
Finished training it 22528/76743 of epoch 0, 86.10 ms/it, loss 0.498505
Finished training it 22528/76743 of epoch 0, 86.99 ms/it, loss 0.496883
Finished training it 22528/76743 of epoch 0, 86.60 ms/it, loss 0.498073
Finished training it 23552/76743 of epoch 0, 86.41 ms/it, loss 0.497273
Finished training it 23552/76743 of epoch 0, 86.79 ms/it, loss 0.497661
Finished training it 23552/76743 of epoch 0, 86.05 ms/it, loss 0.497264
Finished training it 23552/76743 of epoch 0, 86.23 ms/it, loss 0.498165
Finished training it 24576/76743 of epoch 0, 85.81 ms/it, loss 0.495487
Finished training it 24576/76743 of epoch 0, 86.16 ms/it, loss 0.500187
Finished training it 24576/76743 of epoch 0, 86.19 ms/it, loss 0.494288
Finished training it 24576/76743 of epoch 0, 86.57 ms/it, loss 0.497126
Finished training it 25600/76743 of epoch 0, 85.66 ms/it, loss 0.495804
Finished training it 25600/76743 of epoch 0, 85.45 ms/it, loss 0.497494
Finished training it 25600/76743 of epoch 0, 85.23 ms/it, loss 0.496195
Finished training it 25600/76743 of epoch 0, 85.53 ms/it, loss 0.495403
Finished training it 26624/76743 of epoch 0, 86.50 ms/it, loss 0.495527
Finished training it 26624/76743 of epoch 0, 86.21 ms/it, loss 0.496232
Finished training it 26624/76743 of epoch 0, 86.64 ms/it, loss 0.496156
Finished training it 26624/76743 of epoch 0, 86.11 ms/it, loss 0.495651
Finished training it 27648/76743 of epoch 0, 85.89 ms/it, loss 0.495823
Finished training it 27648/76743 of epoch 0, 86.10 ms/it, loss 0.495783
Finished training it 27648/76743 of epoch 0, 85.58 ms/it, loss 0.494859
Finished training it 27648/76743 of epoch 0, 85.63 ms/it, loss 0.493786
Finished training it 28672/76743 of epoch 0, 85.22 ms/it, loss 0.495708
Finished training it 28672/76743 of epoch 0, 85.80 ms/it, loss 0.494372
Finished training it 28672/76743 of epoch 0, 85.46 ms/it, loss 0.494373
Finished training it 28672/76743 of epoch 0, 85.17 ms/it, loss 0.493758
Finished training it 29696/76743 of epoch 0, 85.96 ms/it, loss 0.494997
Finished training it 29696/76743 of epoch 0, 85.47 ms/it, loss 0.493457
Finished training it 29696/76743 of epoch 0, 85.76 ms/it, loss 0.495243
Finished training it 29696/76743 of epoch 0, 85.78 ms/it, loss 0.494735
Finished training it 30720/76743 of epoch 0, 86.98 ms/it, loss 0.492536
Finished training it 30720/76743 of epoch 0, 87.21 ms/it, loss 0.497392
Finished training it 30720/76743 of epoch 0, 86.71 ms/it, loss 0.493310
Finished training it 30720/76743 of epoch 0, 86.76 ms/it, loss 0.492784
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2501237.0
get out
0 has test check 2501237.0 and sample count 3274240
 accuracy 76.391 %, best 76.391 %, roc auc score 0.7419, best 0.7419
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2501237.0
get out
1 has test check 2501237.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 86.77 ms/it, loss 0.494003
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2501237.0
get out
3 has test check 2501237.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 86.19 ms/it, loss 0.494932
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2501237.0
get out
2 has test check 2501237.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 87.31 ms/it, loss 0.493099
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 87.34 ms/it, loss 0.493940
Finished training it 32768/76743 of epoch 0, 87.03 ms/it, loss 0.491448
Finished training it 32768/76743 of epoch 0, 87.18 ms/it, loss 0.495052
Finished training it 32768/76743 of epoch 0, 87.22 ms/it, loss 0.493869
Finished training it 32768/76743 of epoch 0, 86.91 ms/it, loss 0.491257
Finished training it 33792/76743 of epoch 0, 86.97 ms/it, loss 0.490806
Finished training it 33792/76743 of epoch 0, 87.23 ms/it, loss 0.491745
Finished training it 33792/76743 of epoch 0, 86.81 ms/it, loss 0.492882
Finished training it 33792/76743 of epoch 0, 86.56 ms/it, loss 0.493649
Finished training it 34816/76743 of epoch 0, 92.06 ms/it, loss 0.492133
Finished training it 34816/76743 of epoch 0, 91.67 ms/it, loss 0.492222
Finished training it 34816/76743 of epoch 0, 91.65 ms/it, loss 0.495432
Finished training it 34816/76743 of epoch 0, 91.39 ms/it, loss 0.489890
Finished training it 35840/76743 of epoch 0, 92.87 ms/it, loss 0.493143
Finished training it 35840/76743 of epoch 0, 92.75 ms/it, loss 0.491978
Finished training it 35840/76743 of epoch 0, 92.63 ms/it, loss 0.491409
Finished training it 35840/76743 of epoch 0, 92.44 ms/it, loss 0.491972
Finished training it 36864/76743 of epoch 0, 85.98 ms/it, loss 0.493222
Finished training it 36864/76743 of epoch 0, 85.51 ms/it, loss 0.493286
Finished training it 36864/76743 of epoch 0, 85.44 ms/it, loss 0.492748
Finished training it 36864/76743 of epoch 0, 85.60 ms/it, loss 0.491923
Finished training it 37888/76743 of epoch 0, 85.88 ms/it, loss 0.491335
Finished training it 37888/76743 of epoch 0, 85.78 ms/it, loss 0.493060
Finished training it 37888/76743 of epoch 0, 86.10 ms/it, loss 0.492207
Finished training it 37888/76743 of epoch 0, 85.59 ms/it, loss 0.491077
Finished training it 38912/76743 of epoch 0, 85.90 ms/it, loss 0.493042
Finished training it 38912/76743 of epoch 0, 86.22 ms/it, loss 0.494834
Finished training it 38912/76743 of epoch 0, 86.10 ms/it, loss 0.492131
Finished training it 38912/76743 of epoch 0, 86.05 ms/it, loss 0.491233
Finished training it 39936/76743 of epoch 0, 86.50 ms/it, loss 0.489570
Finished training it 39936/76743 of epoch 0, 86.74 ms/it, loss 0.491364
Finished training it 39936/76743 of epoch 0, 86.30 ms/it, loss 0.493106
Finished training it 39936/76743 of epoch 0, 86.32 ms/it, loss 0.490750
Finished training it 40960/76743 of epoch 0, 86.16 ms/it, loss 0.493400
Finished training it 40960/76743 of epoch 0, 86.29 ms/it, loss 0.492275
Finished training it 40960/76743 of epoch 0, 86.54 ms/it, loss 0.490231
Finished training it 40960/76743 of epoch 0, 86.13 ms/it, loss 0.491719
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2505636.0
get out
0 has test check 2505636.0 and sample count 3274240
 accuracy 76.526 %, best 76.526 %, roc auc score 0.7451, best 0.7451
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2505636.0
get out
2 has test check 2505636.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 87.56 ms/it, loss 0.489608
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2505636.0
get out
3 has test check 2505636.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 86.39 ms/it, loss 0.492338
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2505636.0
get out
1 has test check 2505636.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 87.47 ms/it, loss 0.490568
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 87.72 ms/it, loss 0.489246
Finished training it 43008/76743 of epoch 0, 87.14 ms/it, loss 0.492110
Finished training it 43008/76743 of epoch 0, 85.91 ms/it, loss 0.488599
Finished training it 43008/76743 of epoch 0, 87.30 ms/it, loss 0.491078
Finished training it 43008/76743 of epoch 0, 86.96 ms/it, loss 0.493431
Finished training it 44032/76743 of epoch 0, 86.63 ms/it, loss 0.492571
Finished training it 44032/76743 of epoch 0, 85.80 ms/it, loss 0.488414
Finished training it 44032/76743 of epoch 0, 87.10 ms/it, loss 0.490443
Finished training it 44032/76743 of epoch 0, 86.98 ms/it, loss 0.492184
Finished training it 45056/76743 of epoch 0, 86.00 ms/it, loss 0.488735
Finished training it 45056/76743 of epoch 0, 86.47 ms/it, loss 0.489740
Finished training it 45056/76743 of epoch 0, 86.29 ms/it, loss 0.486978
Finished training it 45056/76743 of epoch 0, 85.54 ms/it, loss 0.489563
Finished training it 46080/76743 of epoch 0, 86.18 ms/it, loss 0.489119
Finished training it 46080/76743 of epoch 0, 86.23 ms/it, loss 0.492119
Finished training it 46080/76743 of epoch 0, 85.96 ms/it, loss 0.488361
Finished training it 46080/76743 of epoch 0, 86.47 ms/it, loss 0.489661
Finished training it 47104/76743 of epoch 0, 86.82 ms/it, loss 0.488505
Finished training it 47104/76743 of epoch 0, 86.51 ms/it, loss 0.492192
Finished training it 47104/76743 of epoch 0, 86.72 ms/it, loss 0.488952
Finished training it 47104/76743 of epoch 0, 86.44 ms/it, loss 0.489701
Finished training it 48128/76743 of epoch 0, 85.10 ms/it, loss 0.489842
Finished training it 48128/76743 of epoch 0, 85.44 ms/it, loss 0.492428
Finished training it 48128/76743 of epoch 0, 84.80 ms/it, loss 0.491756
Finished training it 48128/76743 of epoch 0, 85.20 ms/it, loss 0.489236
Finished training it 49152/76743 of epoch 0, 85.57 ms/it, loss 0.489505
Finished training it 49152/76743 of epoch 0, 85.80 ms/it, loss 0.490362
Finished training it 49152/76743 of epoch 0, 85.20 ms/it, loss 0.488449
Finished training it 49152/76743 of epoch 0, 85.37 ms/it, loss 0.489521
Finished training it 50176/76743 of epoch 0, 86.17 ms/it, loss 0.486583
Finished training it 50176/76743 of epoch 0, 86.37 ms/it, loss 0.490803
Finished training it 50176/76743 of epoch 0, 86.30 ms/it, loss 0.490310
Finished training it 50176/76743 of epoch 0, 85.98 ms/it, loss 0.488183
Finished training it 51200/76743 of epoch 0, 86.09 ms/it, loss 0.489297
Finished training it 51200/76743 of epoch 0, 85.82 ms/it, loss 0.490131
Finished training it 51200/76743 of epoch 0, 86.08 ms/it, loss 0.490046
Finished training it 51200/76743 of epoch 0, 86.04 ms/it, loss 0.488926
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2510238.0
get out
0 has test check 2510238.0 and sample count 3274240
 accuracy 76.666 %, best 76.666 %, roc auc score 0.7484, best 0.7484
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 87.94 ms/it, loss 0.486605
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2510238.0
get out
2 has test check 2510238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 87.69 ms/it, loss 0.489409
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2510238.0
get out
1 has test check 2510238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 87.79 ms/it, loss 0.486687
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2510238.0
get out
3 has test check 2510238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 86.62 ms/it, loss 0.487694
Finished training it 53248/76743 of epoch 0, 86.84 ms/it, loss 0.491757
Finished training it 53248/76743 of epoch 0, 86.62 ms/it, loss 0.488188
Finished training it 53248/76743 of epoch 0, 86.21 ms/it, loss 0.488573
Finished training it 53248/76743 of epoch 0, 86.30 ms/it, loss 0.486256
Finished training it 54272/76743 of epoch 0, 86.19 ms/it, loss 0.490925
Finished training it 54272/76743 of epoch 0, 86.79 ms/it, loss 0.489118
Finished training it 54272/76743 of epoch 0, 86.69 ms/it, loss 0.486973
Finished training it 54272/76743 of epoch 0, 86.73 ms/it, loss 0.490917
Finished training it 55296/76743 of epoch 0, 98.88 ms/it, loss 0.488558
Finished training it 55296/76743 of epoch 0, 98.86 ms/it, loss 0.487324
Finished training it 55296/76743 of epoch 0, 98.08 ms/it, loss 0.488489
Finished training it 55296/76743 of epoch 0, 98.96 ms/it, loss 0.490255
Finished training it 56320/76743 of epoch 0, 86.89 ms/it, loss 0.490562
Finished training it 56320/76743 of epoch 0, 86.98 ms/it, loss 0.488727
Finished training it 56320/76743 of epoch 0, 87.00 ms/it, loss 0.485726
Finished training it 56320/76743 of epoch 0, 87.20 ms/it, loss 0.487097
Finished training it 57344/76743 of epoch 0, 86.29 ms/it, loss 0.485026
Finished training it 57344/76743 of epoch 0, 86.71 ms/it, loss 0.488594
Finished training it 57344/76743 of epoch 0, 86.82 ms/it, loss 0.487006
Finished training it 57344/76743 of epoch 0, 86.97 ms/it, loss 0.488027
Finished training it 58368/76743 of epoch 0, 87.19 ms/it, loss 0.487110
Finished training it 58368/76743 of epoch 0, 86.58 ms/it, loss 0.485177
Finished training it 58368/76743 of epoch 0, 87.01 ms/it, loss 0.486535
Finished training it 58368/76743 of epoch 0, 87.15 ms/it, loss 0.489421
Finished training it 59392/76743 of epoch 0, 86.85 ms/it, loss 0.484668
Finished training it 59392/76743 of epoch 0, 87.05 ms/it, loss 0.489333
Finished training it 59392/76743 of epoch 0, 86.61 ms/it, loss 0.488329
Finished training it 59392/76743 of epoch 0, 87.20 ms/it, loss 0.487999
Finished training it 60416/76743 of epoch 0, 85.96 ms/it, loss 0.486210
Finished training it 60416/76743 of epoch 0, 86.08 ms/it, loss 0.487841
Finished training it 60416/76743 of epoch 0, 86.31 ms/it, loss 0.485492
Finished training it 60416/76743 of epoch 0, 86.59 ms/it, loss 0.489036
Finished training it 61440/76743 of epoch 0, 85.73 ms/it, loss 0.487894
Finished training it 61440/76743 of epoch 0, 85.36 ms/it, loss 0.486485
Finished training it 61440/76743 of epoch 0, 85.44 ms/it, loss 0.487511
Finished training it 61440/76743 of epoch 0, 85.39 ms/it, loss 0.487599
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2513855.0
get out
0 has test check 2513855.0 and sample count 3274240
 accuracy 76.777 %, best 76.777 %, roc auc score 0.7520, best 0.7520
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2513855.0
get out
2 has test check 2513855.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 87.40 ms/it, loss 0.487344
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2513855.0
get out
1 has test check 2513855.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 87.46 ms/it, loss 0.485080
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2513855.0
get out
3 has test check 2513855.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 86.32 ms/it, loss 0.486095
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 87.60 ms/it, loss 0.485117
Finished training it 63488/76743 of epoch 0, 87.18 ms/it, loss 0.485466
Finished training it 63488/76743 of epoch 0, 86.19 ms/it, loss 0.485727
Finished training it 63488/76743 of epoch 0, 87.29 ms/it, loss 0.486027
Finished training it 63488/76743 of epoch 0, 86.93 ms/it, loss 0.485183
Finished training it 64512/76743 of epoch 0, 86.08 ms/it, loss 0.483655
Finished training it 64512/76743 of epoch 0, 86.82 ms/it, loss 0.484530
Finished training it 64512/76743 of epoch 0, 87.11 ms/it, loss 0.485805
Finished training it 64512/76743 of epoch 0, 86.93 ms/it, loss 0.486958
Finished training it 65536/76743 of epoch 0, 86.57 ms/it, loss 0.489251
Finished training it 65536/76743 of epoch 0, 87.05 ms/it, loss 0.484048
Finished training it 65536/76743 of epoch 0, 87.26 ms/it, loss 0.484339
Finished training it 65536/76743 of epoch 0, 86.10 ms/it, loss 0.484783
Finished training it 66560/76743 of epoch 0, 86.71 ms/it, loss 0.486633
Finished training it 66560/76743 of epoch 0, 86.98 ms/it, loss 0.483918
Finished training it 66560/76743 of epoch 0, 86.42 ms/it, loss 0.484650
Finished training it 66560/76743 of epoch 0, 86.77 ms/it, loss 0.485950
Finished training it 67584/76743 of epoch 0, 86.72 ms/it, loss 0.484982
Finished training it 67584/76743 of epoch 0, 86.36 ms/it, loss 0.487162
Finished training it 67584/76743 of epoch 0, 86.83 ms/it, loss 0.484676
Finished training it 67584/76743 of epoch 0, 86.96 ms/it, loss 0.484764
Finished training it 68608/76743 of epoch 0, 86.71 ms/it, loss 0.483240
Finished training it 68608/76743 of epoch 0, 87.11 ms/it, loss 0.487357
Finished training it 68608/76743 of epoch 0, 86.35 ms/it, loss 0.484900
Finished training it 68608/76743 of epoch 0, 86.58 ms/it, loss 0.486284
Finished training it 69632/76743 of epoch 0, 86.84 ms/it, loss 0.486534
Finished training it 69632/76743 of epoch 0, 87.09 ms/it, loss 0.481352
Finished training it 69632/76743 of epoch 0, 87.01 ms/it, loss 0.482959
Finished training it 69632/76743 of epoch 0, 86.62 ms/it, loss 0.485895
Finished training it 70656/76743 of epoch 0, 86.60 ms/it, loss 0.484819
Finished training it 70656/76743 of epoch 0, 86.30 ms/it, loss 0.483251
Finished training it 70656/76743 of epoch 0, 86.60 ms/it, loss 0.484150
Finished training it 70656/76743 of epoch 0, 86.95 ms/it, loss 0.487195
Finished training it 71680/76743 of epoch 0, 85.46 ms/it, loss 0.488451
Finished training it 71680/76743 of epoch 0, 85.80 ms/it, loss 0.484396
Finished training it 71680/76743 of epoch 0, 85.51 ms/it, loss 0.483571
Finished training it 71680/76743 of epoch 0, 85.31 ms/it, loss 0.483899
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2517560.0
get out
0 has test check 2517560.0 and sample count 3274240
 accuracy 76.890 %, best 76.890 %, roc auc score 0.7547, best 0.7547
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 87.46 ms/it, loss 0.484797
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2517560.0
get out
1 has test check 2517560.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 87.05 ms/it, loss 0.484789
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2517560.0
get out
2 has test check 2517560.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 87.39 ms/it, loss 0.484666
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2517560.0
get out
3 has test check 2517560.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 86.26 ms/it, loss 0.484156
Finished training it 73728/76743 of epoch 0, 86.78 ms/it, loss 0.485649
Finished training it 73728/76743 of epoch 0, 87.05 ms/it, loss 0.483900
Finished training it 73728/76743 of epoch 0, 86.66 ms/it, loss 0.482397
Finished training it 73728/76743 of epoch 0, 86.89 ms/it, loss 0.483745
Finished training it 74752/76743 of epoch 0, 87.13 ms/it, loss 0.482434
Finished training it 74752/76743 of epoch 0, 87.13 ms/it, loss 0.485998
Finished training it 74752/76743 of epoch 0, 87.50 ms/it, loss 0.484994
Finished training it 74752/76743 of epoch 0, 86.90 ms/it, loss 0.481515
Finished training it 75776/76743 of epoch 0, 93.77 ms/it, loss 0.480888
Finished training it 75776/76743 of epoch 0, 93.77 ms/it, loss 0.482012
Finished training it 75776/76743 of epoch 0, 94.02 ms/it, loss 0.487008
Finished training it 75776/76743 of epoch 0, 93.87 ms/it, loss 0.482298
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 91.01 ms/it, loss 0.483399
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 90.55 ms/it, loss 0.483453
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 90.06 ms/it, loss 0.484863
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 90.42 ms/it, loss 0.483979
Finished training it 2048/76743 of epoch 1, 86.58 ms/it, loss 0.485487
Finished training it 2048/76743 of epoch 1, 86.89 ms/it, loss 0.481105
Finished training it 2048/76743 of epoch 1, 86.75 ms/it, loss 0.482744
Finished training it 2048/76743 of epoch 1, 86.52 ms/it, loss 0.481918
Finished training it 3072/76743 of epoch 1, 87.32 ms/it, loss 0.485764
Finished training it 3072/76743 of epoch 1, 87.02 ms/it, loss 0.481717
Finished training it 3072/76743 of epoch 1, 86.91 ms/it, loss 0.483476
Finished training it 3072/76743 of epoch 1, 86.60 ms/it, loss 0.483105
Finished training it 4096/76743 of epoch 1, 86.66 ms/it, loss 0.485041
Finished training it 4096/76743 of epoch 1, 87.08 ms/it, loss 0.482643
Finished training it 4096/76743 of epoch 1, 86.35 ms/it, loss 0.484519
Finished training it 4096/76743 of epoch 1, 86.65 ms/it, loss 0.482818
Finished training it 5120/76743 of epoch 1, 86.05 ms/it, loss 0.486177
Finished training it 5120/76743 of epoch 1, 85.85 ms/it, loss 0.479155
Finished training it 5120/76743 of epoch 1, 85.77 ms/it, loss 0.481620
Finished training it 5120/76743 of epoch 1, 86.24 ms/it, loss 0.482771
Finished training it 6144/76743 of epoch 1, 86.07 ms/it, loss 0.481477
Finished training it 6144/76743 of epoch 1, 85.70 ms/it, loss 0.481176
Finished training it 6144/76743 of epoch 1, 85.28 ms/it, loss 0.481541
Finished training it 6144/76743 of epoch 1, 85.58 ms/it, loss 0.481815
Finished training it 7168/76743 of epoch 1, 86.35 ms/it, loss 0.485344
Finished training it 7168/76743 of epoch 1, 86.11 ms/it, loss 0.482258
Finished training it 7168/76743 of epoch 1, 85.78 ms/it, loss 0.482358
Finished training it 7168/76743 of epoch 1, 86.01 ms/it, loss 0.481417
Finished training it 8192/76743 of epoch 1, 86.66 ms/it, loss 0.481688
Finished training it 8192/76743 of epoch 1, 86.47 ms/it, loss 0.480713
Finished training it 8192/76743 of epoch 1, 86.35 ms/it, loss 0.484857
Finished training it 8192/76743 of epoch 1, 86.78 ms/it, loss 0.481648
Finished training it 9216/76743 of epoch 1, 86.24 ms/it, loss 0.481537
Finished training it 9216/76743 of epoch 1, 85.87 ms/it, loss 0.482131
Finished training it 9216/76743 of epoch 1, 86.28 ms/it, loss 0.484575
Finished training it 9216/76743 of epoch 1, 86.10 ms/it, loss 0.483248
Finished training it 10240/76743 of epoch 1, 84.97 ms/it, loss 0.483888
Finished training it 10240/76743 of epoch 1, 84.99 ms/it, loss 0.481357
Finished training it 10240/76743 of epoch 1, 85.27 ms/it, loss 0.480474
Finished training it 10240/76743 of epoch 1, 84.63 ms/it, loss 0.482721
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2517549.0
get out
0 has test check 2517549.0 and sample count 3274240
 accuracy 76.890 %, best 76.890 %, roc auc score 0.7562, best 0.7562
Finished training it 11264/76743 of epoch 1, 87.38 ms/it, loss 0.483750
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2517549.0
get out
1 has test check 2517549.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.17 ms/it, loss 0.482776
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2517549.0
get out
2 has test check 2517549.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 87.32 ms/it, loss 0.483115
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2517549.0
get out
3 has test check 2517549.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 86.14 ms/it, loss 0.484142
Finished training it 12288/76743 of epoch 1, 86.80 ms/it, loss 0.482826
Finished training it 12288/76743 of epoch 1, 85.42 ms/it, loss 0.483062
Finished training it 12288/76743 of epoch 1, 86.67 ms/it, loss 0.479966
Finished training it 12288/76743 of epoch 1, 87.10 ms/it, loss 0.482777
Finished training it 13312/76743 of epoch 1, 86.20 ms/it, loss 0.481837
Finished training it 13312/76743 of epoch 1, 86.78 ms/it, loss 0.481217
Finished training it 13312/76743 of epoch 1, 86.60 ms/it, loss 0.481708
Finished training it 13312/76743 of epoch 1, 85.15 ms/it, loss 0.482698
Finished training it 14336/76743 of epoch 1, 85.10 ms/it, loss 0.483176
Finished training it 14336/76743 of epoch 1, 85.36 ms/it, loss 0.483873
Finished training it 14336/76743 of epoch 1, 85.81 ms/it, loss 0.479900
Finished training it 14336/76743 of epoch 1, 84.46 ms/it, loss 0.480713
Finished training it 15360/76743 of epoch 1, 84.45 ms/it, loss 0.480679
Finished training it 15360/76743 of epoch 1, 84.91 ms/it, loss 0.481703
Finished training it 15360/76743 of epoch 1, 84.54 ms/it, loss 0.482593
Finished training it 15360/76743 of epoch 1, 84.54 ms/it, loss 0.484564
Finished training it 16384/76743 of epoch 1, 85.62 ms/it, loss 0.480070
Finished training it 16384/76743 of epoch 1, 86.06 ms/it, loss 0.483349
Finished training it 16384/76743 of epoch 1, 85.51 ms/it, loss 0.482523
Finished training it 16384/76743 of epoch 1, 85.32 ms/it, loss 0.480970
Finished training it 17408/76743 of epoch 1, 85.29 ms/it, loss 0.483285
Finished training it 17408/76743 of epoch 1, 85.87 ms/it, loss 0.483116
Finished training it 17408/76743 of epoch 1, 85.60 ms/it, loss 0.482584
Finished training it 17408/76743 of epoch 1, 85.34 ms/it, loss 0.484140
Finished training it 18432/76743 of epoch 1, 86.09 ms/it, loss 0.480404
Finished training it 18432/76743 of epoch 1, 85.85 ms/it, loss 0.483041
Finished training it 18432/76743 of epoch 1, 85.93 ms/it, loss 0.480563
Finished training it 18432/76743 of epoch 1, 85.80 ms/it, loss 0.482314
Finished training it 19456/76743 of epoch 1, 86.49 ms/it, loss 0.482700
Finished training it 19456/76743 of epoch 1, 86.28 ms/it, loss 0.482198
Finished training it 19456/76743 of epoch 1, 86.54 ms/it, loss 0.480741
Finished training it 19456/76743 of epoch 1, 86.67 ms/it, loss 0.481743
Finished training it 20480/76743 of epoch 1, 85.43 ms/it, loss 0.480722
Finished training it 20480/76743 of epoch 1, 85.25 ms/it, loss 0.479681
Finished training it 20480/76743 of epoch 1, 85.21 ms/it, loss 0.484042
Finished training it 20480/76743 of epoch 1, 85.31 ms/it, loss 0.483938
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2520256.0
get out
0 has test check 2520256.0 and sample count 3274240
 accuracy 76.972 %, best 76.972 %, roc auc score 0.7570, best 0.7570
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 87.56 ms/it, loss 0.478175
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2520256.0
get out
1 has test check 2520256.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.42 ms/it, loss 0.480325
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2520256.0
get out
2 has test check 2520256.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 87.34 ms/it, loss 0.480631
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2520256.0
get out
3 has test check 2520256.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 86.03 ms/it, loss 0.482122
Finished training it 22528/76743 of epoch 1, 85.47 ms/it, loss 0.482443
Finished training it 22528/76743 of epoch 1, 86.22 ms/it, loss 0.479629
Finished training it 22528/76743 of epoch 1, 86.03 ms/it, loss 0.482001
Finished training it 22528/76743 of epoch 1, 86.48 ms/it, loss 0.481125
Finished training it 23552/76743 of epoch 1, 86.71 ms/it, loss 0.481950
Finished training it 23552/76743 of epoch 1, 86.41 ms/it, loss 0.481807
Finished training it 23552/76743 of epoch 1, 86.41 ms/it, loss 0.482725
Finished training it 23552/76743 of epoch 1, 86.30 ms/it, loss 0.481835
Finished training it 24576/76743 of epoch 1, 98.62 ms/it, loss 0.484739
Finished training it 24576/76743 of epoch 1, 99.07 ms/it, loss 0.481318
Finished training it 24576/76743 of epoch 1, 98.74 ms/it, loss 0.478580
Finished training it 24576/76743 of epoch 1, 98.35 ms/it, loss 0.479765
Finished training it 25600/76743 of epoch 1, 86.38 ms/it, loss 0.481143
Finished training it 25600/76743 of epoch 1, 86.49 ms/it, loss 0.481786
Finished training it 25600/76743 of epoch 1, 86.64 ms/it, loss 0.480008
Finished training it 25600/76743 of epoch 1, 86.68 ms/it, loss 0.481395
Finished training it 26624/76743 of epoch 1, 86.31 ms/it, loss 0.480879
Finished training it 26624/76743 of epoch 1, 86.69 ms/it, loss 0.481797
Finished training it 26624/76743 of epoch 1, 86.39 ms/it, loss 0.480316
Finished training it 26624/76743 of epoch 1, 86.49 ms/it, loss 0.480923
Finished training it 27648/76743 of epoch 1, 86.13 ms/it, loss 0.480272
Finished training it 27648/76743 of epoch 1, 86.15 ms/it, loss 0.482401
Finished training it 27648/76743 of epoch 1, 86.55 ms/it, loss 0.482244
Finished training it 27648/76743 of epoch 1, 86.47 ms/it, loss 0.479248
Finished training it 28672/76743 of epoch 1, 86.18 ms/it, loss 0.480442
Finished training it 28672/76743 of epoch 1, 86.38 ms/it, loss 0.481124
Finished training it 28672/76743 of epoch 1, 86.12 ms/it, loss 0.479683
Finished training it 28672/76743 of epoch 1, 86.31 ms/it, loss 0.481778
Finished training it 29696/76743 of epoch 1, 86.02 ms/it, loss 0.480652
Finished training it 29696/76743 of epoch 1, 86.08 ms/it, loss 0.479777
Finished training it 29696/76743 of epoch 1, 86.29 ms/it, loss 0.481021
Finished training it 29696/76743 of epoch 1, 86.43 ms/it, loss 0.481079
Finished training it 30720/76743 of epoch 1, 86.82 ms/it, loss 0.483438
Finished training it 30720/76743 of epoch 1, 86.58 ms/it, loss 0.480298
Finished training it 30720/76743 of epoch 1, 86.77 ms/it, loss 0.479114
Finished training it 30720/76743 of epoch 1, 86.41 ms/it, loss 0.479604
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2524200.0
get out
0 has test check 2524200.0 and sample count 3274240
 accuracy 77.093 %, best 77.093 %, roc auc score 0.7598, best 0.7598
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 87.27 ms/it, loss 0.480391
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2524200.0
get out
2 has test check 2524200.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 87.11 ms/it, loss 0.479488
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2524200.0
get out
3 has test check 2524200.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.01 ms/it, loss 0.481508
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2524200.0
get out
1 has test check 2524200.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 86.92 ms/it, loss 0.480738
Finished training it 32768/76743 of epoch 1, 86.57 ms/it, loss 0.477769
Finished training it 32768/76743 of epoch 1, 86.30 ms/it, loss 0.478593
Finished training it 32768/76743 of epoch 1, 86.82 ms/it, loss 0.480638
Finished training it 32768/76743 of epoch 1, 86.51 ms/it, loss 0.481663
Finished training it 33792/76743 of epoch 1, 86.71 ms/it, loss 0.478867
Finished training it 33792/76743 of epoch 1, 86.54 ms/it, loss 0.480196
Finished training it 33792/76743 of epoch 1, 86.42 ms/it, loss 0.478260
Finished training it 33792/76743 of epoch 1, 86.43 ms/it, loss 0.480703
Finished training it 34816/76743 of epoch 1, 86.04 ms/it, loss 0.479937
Finished training it 34816/76743 of epoch 1, 86.11 ms/it, loss 0.483454
Finished training it 34816/76743 of epoch 1, 85.93 ms/it, loss 0.477530
Finished training it 34816/76743 of epoch 1, 86.22 ms/it, loss 0.480154
Finished training it 35840/76743 of epoch 1, 86.66 ms/it, loss 0.480268
Finished training it 35840/76743 of epoch 1, 86.57 ms/it, loss 0.480036
Finished training it 35840/76743 of epoch 1, 86.45 ms/it, loss 0.479198
Finished training it 35840/76743 of epoch 1, 86.21 ms/it, loss 0.478990
Finished training it 36864/76743 of epoch 1, 86.30 ms/it, loss 0.480775
Finished training it 36864/76743 of epoch 1, 86.50 ms/it, loss 0.482159
Finished training it 36864/76743 of epoch 1, 86.18 ms/it, loss 0.481410
Finished training it 36864/76743 of epoch 1, 86.46 ms/it, loss 0.481813
Finished training it 37888/76743 of epoch 1, 85.77 ms/it, loss 0.479544
Finished training it 37888/76743 of epoch 1, 86.07 ms/it, loss 0.480471
Finished training it 37888/76743 of epoch 1, 85.98 ms/it, loss 0.481355
Finished training it 37888/76743 of epoch 1, 85.87 ms/it, loss 0.479948
Finished training it 38912/76743 of epoch 1, 87.12 ms/it, loss 0.481202
Finished training it 38912/76743 of epoch 1, 87.17 ms/it, loss 0.483821
Finished training it 38912/76743 of epoch 1, 86.97 ms/it, loss 0.480400
Finished training it 38912/76743 of epoch 1, 86.82 ms/it, loss 0.481129
Finished training it 39936/76743 of epoch 1, 87.11 ms/it, loss 0.478222
Finished training it 39936/76743 of epoch 1, 87.11 ms/it, loss 0.480241
Finished training it 39936/76743 of epoch 1, 86.77 ms/it, loss 0.478940
Finished training it 39936/76743 of epoch 1, 86.93 ms/it, loss 0.482088
Finished training it 40960/76743 of epoch 1, 87.45 ms/it, loss 0.478467
Finished training it 40960/76743 of epoch 1, 87.78 ms/it, loss 0.480428
Finished training it 40960/76743 of epoch 1, 87.60 ms/it, loss 0.481893
Finished training it 40960/76743 of epoch 1, 87.25 ms/it, loss 0.479767
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2522420.0
get out
0 has test check 2522420.0 and sample count 3274240
 accuracy 77.038 %, best 77.093 %, roc auc score 0.7591, best 0.7598
Finished training it 41984/76743 of epoch 1, 86.78 ms/it, loss 0.477101
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2522420.0
get out
1 has test check 2522420.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 86.55 ms/it, loss 0.478232
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2522420.0
get out
3 has test check 2522420.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 85.93 ms/it, loss 0.479422
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2522420.0
get out
2 has test check 2522420.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 86.62 ms/it, loss 0.477189
Finished training it 43008/76743 of epoch 1, 86.91 ms/it, loss 0.478781
Finished training it 43008/76743 of epoch 1, 86.80 ms/it, loss 0.481514
Finished training it 43008/76743 of epoch 1, 86.55 ms/it, loss 0.480155
Finished training it 43008/76743 of epoch 1, 86.11 ms/it, loss 0.476887
Finished training it 44032/76743 of epoch 1, 86.47 ms/it, loss 0.481057
Finished training it 44032/76743 of epoch 1, 86.50 ms/it, loss 0.480227
Finished training it 44032/76743 of epoch 1, 86.06 ms/it, loss 0.476719
Finished training it 44032/76743 of epoch 1, 86.71 ms/it, loss 0.478844
Finished training it 45056/76743 of epoch 1, 98.06 ms/it, loss 0.478911
Finished training it 45056/76743 of epoch 1, 97.84 ms/it, loss 0.478857
Finished training it 45056/76743 of epoch 1, 98.22 ms/it, loss 0.476472
Finished training it 45056/76743 of epoch 1, 98.63 ms/it, loss 0.477991
Finished training it 46080/76743 of epoch 1, 86.62 ms/it, loss 0.481396
Finished training it 46080/76743 of epoch 1, 86.80 ms/it, loss 0.477990
Finished training it 46080/76743 of epoch 1, 86.38 ms/it, loss 0.477781
Finished training it 46080/76743 of epoch 1, 86.31 ms/it, loss 0.477204
Finished training it 47104/76743 of epoch 1, 86.86 ms/it, loss 0.476498
Finished training it 47104/76743 of epoch 1, 86.55 ms/it, loss 0.477973
Finished training it 47104/76743 of epoch 1, 86.58 ms/it, loss 0.480675
Finished training it 47104/76743 of epoch 1, 86.79 ms/it, loss 0.477750
Finished training it 48128/76743 of epoch 1, 86.46 ms/it, loss 0.480834
Finished training it 48128/76743 of epoch 1, 86.24 ms/it, loss 0.480208
Finished training it 48128/76743 of epoch 1, 86.22 ms/it, loss 0.478010
Finished training it 48128/76743 of epoch 1, 86.29 ms/it, loss 0.477323
Finished training it 49152/76743 of epoch 1, 86.92 ms/it, loss 0.477490
Finished training it 49152/76743 of epoch 1, 87.26 ms/it, loss 0.477321
Finished training it 49152/76743 of epoch 1, 87.19 ms/it, loss 0.478648
Finished training it 49152/76743 of epoch 1, 87.11 ms/it, loss 0.476728
Finished training it 50176/76743 of epoch 1, 86.48 ms/it, loss 0.474919
Finished training it 50176/76743 of epoch 1, 86.77 ms/it, loss 0.476755
Finished training it 50176/76743 of epoch 1, 86.74 ms/it, loss 0.478797
Finished training it 50176/76743 of epoch 1, 86.80 ms/it, loss 0.478525
Finished training it 51200/76743 of epoch 1, 87.09 ms/it, loss 0.477868
Finished training it 51200/76743 of epoch 1, 86.77 ms/it, loss 0.478295
Finished training it 51200/76743 of epoch 1, 86.92 ms/it, loss 0.477041
Finished training it 51200/76743 of epoch 1, 87.13 ms/it, loss 0.478330
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2528121.0
get out
0 has test check 2528121.0 and sample count 3274240
 accuracy 77.212 %, best 77.212 %, roc auc score 0.7624, best 0.7624
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2528121.0
get out
2 has test check 2528121.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.10 ms/it, loss 0.477895
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 86.44 ms/it, loss 0.475413
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2528121.0
get out
1 has test check 2528121.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.31 ms/it, loss 0.475215
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2528121.0
get out
3 has test check 2528121.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 86.26 ms/it, loss 0.476627
Finished training it 53248/76743 of epoch 1, 86.15 ms/it, loss 0.475066
Finished training it 53248/76743 of epoch 1, 86.37 ms/it, loss 0.480236
Finished training it 53248/76743 of epoch 1, 86.08 ms/it, loss 0.476014
Finished training it 53248/76743 of epoch 1, 85.96 ms/it, loss 0.476382
Finished training it 54272/76743 of epoch 1, 86.50 ms/it, loss 0.477329
Finished training it 54272/76743 of epoch 1, 86.11 ms/it, loss 0.475403
Finished training it 54272/76743 of epoch 1, 86.34 ms/it, loss 0.479766
Finished training it 54272/76743 of epoch 1, 86.24 ms/it, loss 0.478459
Finished training it 55296/76743 of epoch 1, 86.77 ms/it, loss 0.476921
Finished training it 55296/76743 of epoch 1, 86.95 ms/it, loss 0.475926
Finished training it 55296/76743 of epoch 1, 86.78 ms/it, loss 0.477483
Finished training it 55296/76743 of epoch 1, 86.75 ms/it, loss 0.478658
Finished training it 56320/76743 of epoch 1, 86.83 ms/it, loss 0.476721
Finished training it 56320/76743 of epoch 1, 86.66 ms/it, loss 0.477386
Finished training it 56320/76743 of epoch 1, 86.42 ms/it, loss 0.479570
Finished training it 56320/76743 of epoch 1, 86.38 ms/it, loss 0.475003
Finished training it 57344/76743 of epoch 1, 86.56 ms/it, loss 0.477355
Finished training it 57344/76743 of epoch 1, 86.36 ms/it, loss 0.477007
Finished training it 57344/76743 of epoch 1, 86.32 ms/it, loss 0.475255
Finished training it 57344/76743 of epoch 1, 86.39 ms/it, loss 0.478268
Finished training it 58368/76743 of epoch 1, 86.91 ms/it, loss 0.477098
Finished training it 58368/76743 of epoch 1, 86.62 ms/it, loss 0.479187
Finished training it 58368/76743 of epoch 1, 86.65 ms/it, loss 0.474651
Finished training it 58368/76743 of epoch 1, 86.71 ms/it, loss 0.475640
Finished training it 59392/76743 of epoch 1, 86.91 ms/it, loss 0.474307
Finished training it 59392/76743 of epoch 1, 86.82 ms/it, loss 0.478138
Finished training it 59392/76743 of epoch 1, 86.74 ms/it, loss 0.479108
Finished training it 59392/76743 of epoch 1, 86.61 ms/it, loss 0.479168
Finished training it 60416/76743 of epoch 1, 86.49 ms/it, loss 0.477831
Finished training it 60416/76743 of epoch 1, 86.40 ms/it, loss 0.475366
Finished training it 60416/76743 of epoch 1, 86.45 ms/it, loss 0.475651
Finished training it 60416/76743 of epoch 1, 86.77 ms/it, loss 0.478661
Finished training it 61440/76743 of epoch 1, 86.23 ms/it, loss 0.478393
Finished training it 61440/76743 of epoch 1, 85.93 ms/it, loss 0.477452
Finished training it 61440/76743 of epoch 1, 86.12 ms/it, loss 0.478245
Finished training it 61440/76743 of epoch 1, 86.07 ms/it, loss 0.477735
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2529486.0
get out
0 has test check 2529486.0 and sample count 3274240
 accuracy 77.254 %, best 77.254 %, roc auc score 0.7639, best 0.7639
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 86.48 ms/it, loss 0.475406
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2529486.0
get out
2 has test check 2529486.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 86.46 ms/it, loss 0.477329
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2529486.0
get out
3 has test check 2529486.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 86.44 ms/it, loss 0.475945
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2529486.0
get out
1 has test check 2529486.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 86.33 ms/it, loss 0.475426
Finished training it 63488/76743 of epoch 1, 86.26 ms/it, loss 0.476416
Finished training it 63488/76743 of epoch 1, 86.45 ms/it, loss 0.476320
Finished training it 63488/76743 of epoch 1, 86.15 ms/it, loss 0.476806
Finished training it 63488/76743 of epoch 1, 86.35 ms/it, loss 0.476110
Finished training it 64512/76743 of epoch 1, 86.69 ms/it, loss 0.478027
Finished training it 64512/76743 of epoch 1, 86.94 ms/it, loss 0.474921
Finished training it 64512/76743 of epoch 1, 86.86 ms/it, loss 0.477722
Finished training it 64512/76743 of epoch 1, 86.92 ms/it, loss 0.476177
Finished training it 65536/76743 of epoch 1, 92.94 ms/it, loss 0.474890
Finished training it 65536/76743 of epoch 1, 92.17 ms/it, loss 0.475085
Finished training it 65536/76743 of epoch 1, 92.45 ms/it, loss 0.480377
Finished training it 65536/76743 of epoch 1, 92.39 ms/it, loss 0.476267
Finished training it 66560/76743 of epoch 1, 92.77 ms/it, loss 0.475465
Finished training it 66560/76743 of epoch 1, 92.70 ms/it, loss 0.476696
Finished training it 66560/76743 of epoch 1, 92.30 ms/it, loss 0.475923
Finished training it 66560/76743 of epoch 1, 92.43 ms/it, loss 0.477545
Finished training it 67584/76743 of epoch 1, 86.64 ms/it, loss 0.479591
Finished training it 67584/76743 of epoch 1, 86.45 ms/it, loss 0.476444
Finished training it 67584/76743 of epoch 1, 86.73 ms/it, loss 0.477130
Finished training it 67584/76743 of epoch 1, 86.76 ms/it, loss 0.476274
Finished training it 68608/76743 of epoch 1, 86.72 ms/it, loss 0.476215
Finished training it 68608/76743 of epoch 1, 86.71 ms/it, loss 0.478557
Finished training it 68608/76743 of epoch 1, 86.70 ms/it, loss 0.479094
Finished training it 68608/76743 of epoch 1, 86.55 ms/it, loss 0.474443
Finished training it 69632/76743 of epoch 1, 86.70 ms/it, loss 0.477725
Finished training it 69632/76743 of epoch 1, 86.81 ms/it, loss 0.478119
Finished training it 69632/76743 of epoch 1, 86.77 ms/it, loss 0.473954
Finished training it 69632/76743 of epoch 1, 86.64 ms/it, loss 0.474980
Finished training it 70656/76743 of epoch 1, 86.53 ms/it, loss 0.475671
Finished training it 70656/76743 of epoch 1, 86.73 ms/it, loss 0.479142
Finished training it 70656/76743 of epoch 1, 86.57 ms/it, loss 0.474998
Finished training it 70656/76743 of epoch 1, 86.41 ms/it, loss 0.476458
Finished training it 71680/76743 of epoch 1, 86.16 ms/it, loss 0.480979
Finished training it 71680/76743 of epoch 1, 85.98 ms/it, loss 0.476547
Finished training it 71680/76743 of epoch 1, 86.25 ms/it, loss 0.477777
Finished training it 71680/76743 of epoch 1, 86.01 ms/it, loss 0.476197
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2531350.0
get out
0 has test check 2531350.0 and sample count 3274240
 accuracy 77.311 %, best 77.311 %, roc auc score 0.7647, best 0.7647
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 86.54 ms/it, loss 0.477702
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2531350.0
get out
2 has test check 2531350.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 86.46 ms/it, loss 0.477435
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2531350.0
get out
1 has test check 2531350.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 86.43 ms/it, loss 0.477351
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2531350.0
get out
3 has test check 2531350.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 86.40 ms/it, loss 0.477232
Finished training it 73728/76743 of epoch 1, 86.42 ms/it, loss 0.476599
Finished training it 73728/76743 of epoch 1, 86.24 ms/it, loss 0.476622
Finished training it 73728/76743 of epoch 1, 86.14 ms/it, loss 0.474660
Finished training it 73728/76743 of epoch 1, 86.13 ms/it, loss 0.478354
Finished training it 74752/76743 of epoch 1, 86.38 ms/it, loss 0.475586
Finished training it 74752/76743 of epoch 1, 86.38 ms/it, loss 0.477756
Finished training it 74752/76743 of epoch 1, 86.28 ms/it, loss 0.479569
Finished training it 74752/76743 of epoch 1, 86.24 ms/it, loss 0.474117
Finished training it 75776/76743 of epoch 1, 86.75 ms/it, loss 0.475523
Finished training it 75776/76743 of epoch 1, 86.89 ms/it, loss 0.479917
Finished training it 75776/76743 of epoch 1, 86.70 ms/it, loss 0.475246
Finished training it 75776/76743 of epoch 1, 86.98 ms/it, loss 0.473223
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.76 ms/it, loss 0.475987
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.66 ms/it, loss 0.477155
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.66 ms/it, loss 0.477579
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 87.91 ms/it, loss 0.476177
Finished training it 2048/76743 of epoch 2, 86.73 ms/it, loss 0.474029
Finished training it 2048/76743 of epoch 2, 86.59 ms/it, loss 0.475413
Finished training it 2048/76743 of epoch 2, 86.56 ms/it, loss 0.475253
Finished training it 2048/76743 of epoch 2, 86.64 ms/it, loss 0.478468
Finished training it 3072/76743 of epoch 2, 86.60 ms/it, loss 0.476762
Finished training it 3072/76743 of epoch 2, 86.45 ms/it, loss 0.475129
Finished training it 3072/76743 of epoch 2, 86.60 ms/it, loss 0.478665
Finished training it 3072/76743 of epoch 2, 86.40 ms/it, loss 0.476557
Finished training it 4096/76743 of epoch 2, 86.17 ms/it, loss 0.478179
Finished training it 4096/76743 of epoch 2, 86.14 ms/it, loss 0.476263
Finished training it 4096/76743 of epoch 2, 86.28 ms/it, loss 0.475542
Finished training it 4096/76743 of epoch 2, 86.17 ms/it, loss 0.478152
Finished training it 5120/76743 of epoch 2, 86.75 ms/it, loss 0.475948
Finished training it 5120/76743 of epoch 2, 86.56 ms/it, loss 0.479860
Finished training it 5120/76743 of epoch 2, 86.65 ms/it, loss 0.475399
Finished training it 5120/76743 of epoch 2, 86.64 ms/it, loss 0.472225
Finished training it 6144/76743 of epoch 2, 86.19 ms/it, loss 0.474516
Finished training it 6144/76743 of epoch 2, 86.24 ms/it, loss 0.475045
Finished training it 6144/76743 of epoch 2, 85.98 ms/it, loss 0.474613
Finished training it 6144/76743 of epoch 2, 86.35 ms/it, loss 0.474808
Finished training it 7168/76743 of epoch 2, 86.30 ms/it, loss 0.474851
Finished training it 7168/76743 of epoch 2, 86.15 ms/it, loss 0.474432
Finished training it 7168/76743 of epoch 2, 86.31 ms/it, loss 0.475662
Finished training it 7168/76743 of epoch 2, 86.48 ms/it, loss 0.477671
Finished training it 8192/76743 of epoch 2, 86.43 ms/it, loss 0.474125
Finished training it 8192/76743 of epoch 2, 86.67 ms/it, loss 0.474822
Finished training it 8192/76743 of epoch 2, 86.39 ms/it, loss 0.475257
Finished training it 8192/76743 of epoch 2, 86.36 ms/it, loss 0.478522
Finished training it 9216/76743 of epoch 2, 86.09 ms/it, loss 0.475494
Finished training it 9216/76743 of epoch 2, 86.29 ms/it, loss 0.477669
Finished training it 9216/76743 of epoch 2, 86.12 ms/it, loss 0.476741
Finished training it 9216/76743 of epoch 2, 86.30 ms/it, loss 0.474109
Finished training it 10240/76743 of epoch 2, 86.42 ms/it, loss 0.476776
Finished training it 10240/76743 of epoch 2, 86.32 ms/it, loss 0.476834
Finished training it 10240/76743 of epoch 2, 86.53 ms/it, loss 0.473941
Finished training it 10240/76743 of epoch 2, 86.47 ms/it, loss 0.474800
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2532395.0
get out
0 has test check 2532395.0 and sample count 3274240
 accuracy 77.343 %, best 77.343 %, roc auc score 0.7657, best 0.7657
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2532395.0
get out
3 has test check 2532395.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 86.57 ms/it, loss 0.477702
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 86.85 ms/it, loss 0.477501
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2532395.0
get out
2 has test check 2532395.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 86.66 ms/it, loss 0.476696
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2532395.0
get out
1 has test check 2532395.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 86.57 ms/it, loss 0.476229
Finished training it 12288/76743 of epoch 2, 86.50 ms/it, loss 0.475563
Finished training it 12288/76743 of epoch 2, 86.19 ms/it, loss 0.473759
Finished training it 12288/76743 of epoch 2, 86.15 ms/it, loss 0.476815
Finished training it 12288/76743 of epoch 2, 86.33 ms/it, loss 0.477005
Finished training it 13312/76743 of epoch 2, 86.56 ms/it, loss 0.475992
Finished training it 13312/76743 of epoch 2, 86.50 ms/it, loss 0.476658
Finished training it 13312/76743 of epoch 2, 86.66 ms/it, loss 0.475053
Finished training it 13312/76743 of epoch 2, 86.46 ms/it, loss 0.475563
Finished training it 14336/76743 of epoch 2, 97.71 ms/it, loss 0.476928
Finished training it 14336/76743 of epoch 2, 97.07 ms/it, loss 0.477024
Finished training it 14336/76743 of epoch 2, 96.83 ms/it, loss 0.474392
Finished training it 14336/76743 of epoch 2, 97.50 ms/it, loss 0.473407
Finished training it 15360/76743 of epoch 2, 85.84 ms/it, loss 0.474156
Finished training it 15360/76743 of epoch 2, 85.83 ms/it, loss 0.477222
Finished training it 15360/76743 of epoch 2, 86.03 ms/it, loss 0.475729
Finished training it 15360/76743 of epoch 2, 86.03 ms/it, loss 0.474404
Finished training it 16384/76743 of epoch 2, 86.06 ms/it, loss 0.473202
Finished training it 16384/76743 of epoch 2, 86.11 ms/it, loss 0.474689
Finished training it 16384/76743 of epoch 2, 86.12 ms/it, loss 0.475541
Finished training it 16384/76743 of epoch 2, 86.22 ms/it, loss 0.476467
Finished training it 17408/76743 of epoch 2, 86.49 ms/it, loss 0.475640
Finished training it 17408/76743 of epoch 2, 86.59 ms/it, loss 0.476709
Finished training it 17408/76743 of epoch 2, 86.62 ms/it, loss 0.476646
Finished training it 17408/76743 of epoch 2, 86.47 ms/it, loss 0.477774
Finished training it 18432/76743 of epoch 2, 86.80 ms/it, loss 0.475321
Finished training it 18432/76743 of epoch 2, 86.91 ms/it, loss 0.473576
Finished training it 18432/76743 of epoch 2, 86.80 ms/it, loss 0.473352
Finished training it 18432/76743 of epoch 2, 86.67 ms/it, loss 0.476189
Finished training it 19456/76743 of epoch 2, 87.36 ms/it, loss 0.474508
Finished training it 19456/76743 of epoch 2, 87.20 ms/it, loss 0.474607
Finished training it 19456/76743 of epoch 2, 87.32 ms/it, loss 0.476264
Finished training it 19456/76743 of epoch 2, 87.34 ms/it, loss 0.473596
Finished training it 20480/76743 of epoch 2, 86.54 ms/it, loss 0.476910
Finished training it 20480/76743 of epoch 2, 86.41 ms/it, loss 0.473167
Finished training it 20480/76743 of epoch 2, 86.53 ms/it, loss 0.473822
Finished training it 20480/76743 of epoch 2, 86.52 ms/it, loss 0.476852
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2532098.0
get out
0 has test check 2532098.0 and sample count 3274240
 accuracy 77.334 %, best 77.343 %, roc auc score 0.7663, best 0.7663
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2532098.0
get out
3 has test check 2532098.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 86.55 ms/it, loss 0.476786
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2532098.0
get out
2 has test check 2532098.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 86.84 ms/it, loss 0.474899
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2532098.0
get out
1 has test check 2532098.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 86.62 ms/it, loss 0.474644
Finished training it 21504/76743 of epoch 2, 86.93 ms/it, loss 0.472457
Finished training it 22528/76743 of epoch 2, 86.48 ms/it, loss 0.477284
Finished training it 22528/76743 of epoch 2, 86.46 ms/it, loss 0.477188
Finished training it 22528/76743 of epoch 2, 86.41 ms/it, loss 0.474700
Finished training it 22528/76743 of epoch 2, 86.45 ms/it, loss 0.475967
Finished training it 23552/76743 of epoch 2, 86.59 ms/it, loss 0.476289
Finished training it 23552/76743 of epoch 2, 86.55 ms/it, loss 0.475636
Finished training it 23552/76743 of epoch 2, 86.62 ms/it, loss 0.476167
Finished training it 23552/76743 of epoch 2, 86.54 ms/it, loss 0.477445
Finished training it 24576/76743 of epoch 2, 86.83 ms/it, loss 0.475189
Finished training it 24576/76743 of epoch 2, 86.72 ms/it, loss 0.473895
Finished training it 24576/76743 of epoch 2, 86.65 ms/it, loss 0.478192
Finished training it 24576/76743 of epoch 2, 86.73 ms/it, loss 0.472038
Finished training it 25600/76743 of epoch 2, 86.96 ms/it, loss 0.475808
Finished training it 25600/76743 of epoch 2, 87.14 ms/it, loss 0.474445
Finished training it 25600/76743 of epoch 2, 87.04 ms/it, loss 0.475071
Finished training it 25600/76743 of epoch 2, 87.07 ms/it, loss 0.475162
Finished training it 26624/76743 of epoch 2, 86.38 ms/it, loss 0.475053
Finished training it 26624/76743 of epoch 2, 86.18 ms/it, loss 0.474575
Finished training it 26624/76743 of epoch 2, 86.17 ms/it, loss 0.475226
Finished training it 26624/76743 of epoch 2, 86.10 ms/it, loss 0.474800
Finished training it 27648/76743 of epoch 2, 86.46 ms/it, loss 0.476687
Finished training it 27648/76743 of epoch 2, 86.40 ms/it, loss 0.476423
Finished training it 27648/76743 of epoch 2, 86.17 ms/it, loss 0.473910
Finished training it 27648/76743 of epoch 2, 86.42 ms/it, loss 0.473111
Finished training it 28672/76743 of epoch 2, 86.87 ms/it, loss 0.475463
Finished training it 28672/76743 of epoch 2, 87.01 ms/it, loss 0.475190
Finished training it 28672/76743 of epoch 2, 86.88 ms/it, loss 0.473324
Finished training it 28672/76743 of epoch 2, 86.75 ms/it, loss 0.473591
Finished training it 29696/76743 of epoch 2, 86.14 ms/it, loss 0.475480
Finished training it 29696/76743 of epoch 2, 86.44 ms/it, loss 0.475113
Finished training it 29696/76743 of epoch 2, 86.32 ms/it, loss 0.475917
Finished training it 29696/76743 of epoch 2, 86.27 ms/it, loss 0.474059
Finished training it 30720/76743 of epoch 2, 86.81 ms/it, loss 0.478079
Finished training it 30720/76743 of epoch 2, 86.67 ms/it, loss 0.474280
Finished training it 30720/76743 of epoch 2, 86.91 ms/it, loss 0.473453
Finished training it 30720/76743 of epoch 2, 86.85 ms/it, loss 0.473300
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534926.0
get out
0 has test check 2534926.0 and sample count 3274240
 accuracy 77.420 %, best 77.420 %, roc auc score 0.7668, best 0.7668
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 86.70 ms/it, loss 0.474583
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534926.0
get out
2 has test check 2534926.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.59 ms/it, loss 0.473277
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534926.0
get out
3 has test check 2534926.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.60 ms/it, loss 0.475483
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534926.0
get out
1 has test check 2534926.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 86.48 ms/it, loss 0.474753
Finished training it 32768/76743 of epoch 2, 86.68 ms/it, loss 0.472314
Finished training it 32768/76743 of epoch 2, 86.67 ms/it, loss 0.472609
Finished training it 32768/76743 of epoch 2, 86.65 ms/it, loss 0.476206
Finished training it 32768/76743 of epoch 2, 86.77 ms/it, loss 0.474653
Finished training it 33792/76743 of epoch 2, 87.01 ms/it, loss 0.472267
Finished training it 33792/76743 of epoch 2, 86.93 ms/it, loss 0.472427
Finished training it 33792/76743 of epoch 2, 87.09 ms/it, loss 0.474448
Finished training it 33792/76743 of epoch 2, 86.99 ms/it, loss 0.474927
Finished training it 34816/76743 of epoch 2, 92.51 ms/it, loss 0.476705
Finished training it 34816/76743 of epoch 2, 92.33 ms/it, loss 0.470979
Finished training it 34816/76743 of epoch 2, 92.21 ms/it, loss 0.473604
Finished training it 34816/76743 of epoch 2, 92.36 ms/it, loss 0.473694
Finished training it 35840/76743 of epoch 2, 91.96 ms/it, loss 0.472994
Finished training it 35840/76743 of epoch 2, 92.41 ms/it, loss 0.474635
Finished training it 35840/76743 of epoch 2, 92.05 ms/it, loss 0.474309
Finished training it 35840/76743 of epoch 2, 91.86 ms/it, loss 0.473610
Finished training it 36864/76743 of epoch 2, 86.52 ms/it, loss 0.475385
Finished training it 36864/76743 of epoch 2, 86.45 ms/it, loss 0.475830
Finished training it 36864/76743 of epoch 2, 86.30 ms/it, loss 0.475168
Finished training it 36864/76743 of epoch 2, 86.63 ms/it, loss 0.476115
Finished training it 37888/76743 of epoch 2, 86.93 ms/it, loss 0.475037
Finished training it 37888/76743 of epoch 2, 86.94 ms/it, loss 0.474236
Finished training it 37888/76743 of epoch 2, 86.85 ms/it, loss 0.474200
Finished training it 37888/76743 of epoch 2, 87.17 ms/it, loss 0.475862
Finished training it 38912/76743 of epoch 2, 86.77 ms/it, loss 0.475298
Finished training it 38912/76743 of epoch 2, 86.89 ms/it, loss 0.474489
Finished training it 38912/76743 of epoch 2, 86.93 ms/it, loss 0.478061
Finished training it 38912/76743 of epoch 2, 86.78 ms/it, loss 0.475778
Finished training it 39936/76743 of epoch 2, 86.67 ms/it, loss 0.476853
Finished training it 39936/76743 of epoch 2, 86.60 ms/it, loss 0.473219
Finished training it 39936/76743 of epoch 2, 86.75 ms/it, loss 0.474681
Finished training it 39936/76743 of epoch 2, 86.73 ms/it, loss 0.472280
Finished training it 40960/76743 of epoch 2, 86.88 ms/it, loss 0.473199
Finished training it 40960/76743 of epoch 2, 86.86 ms/it, loss 0.473883
Finished training it 40960/76743 of epoch 2, 86.96 ms/it, loss 0.474745
Finished training it 40960/76743 of epoch 2, 86.92 ms/it, loss 0.476372
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533471.0
get out
0 has test check 2533471.0 and sample count 3274240
 accuracy 77.376 %, best 77.420 %, roc auc score 0.7667, best 0.7668
Finished training it 41984/76743 of epoch 2, 85.88 ms/it, loss 0.471899
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533471.0
get out
2 has test check 2533471.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 85.89 ms/it, loss 0.471758
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533471.0
get out
3 has test check 2533471.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 85.61 ms/it, loss 0.474199
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533471.0
get out
1 has test check 2533471.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 85.62 ms/it, loss 0.472422
Finished training it 43008/76743 of epoch 2, 86.88 ms/it, loss 0.476381
Finished training it 43008/76743 of epoch 2, 86.73 ms/it, loss 0.471425
Finished training it 43008/76743 of epoch 2, 86.65 ms/it, loss 0.473168
Finished training it 43008/76743 of epoch 2, 86.63 ms/it, loss 0.474615
Finished training it 44032/76743 of epoch 2, 86.53 ms/it, loss 0.472176
Finished training it 44032/76743 of epoch 2, 86.70 ms/it, loss 0.475527
Finished training it 44032/76743 of epoch 2, 86.72 ms/it, loss 0.473451
Finished training it 44032/76743 of epoch 2, 86.65 ms/it, loss 0.475868
Finished training it 45056/76743 of epoch 2, 86.76 ms/it, loss 0.473611
Finished training it 45056/76743 of epoch 2, 86.85 ms/it, loss 0.471139
Finished training it 45056/76743 of epoch 2, 86.93 ms/it, loss 0.472801
Finished training it 45056/76743 of epoch 2, 86.85 ms/it, loss 0.473546
Finished training it 46080/76743 of epoch 2, 86.25 ms/it, loss 0.472839
Finished training it 46080/76743 of epoch 2, 86.25 ms/it, loss 0.473315
Finished training it 46080/76743 of epoch 2, 86.29 ms/it, loss 0.476196
Finished training it 46080/76743 of epoch 2, 86.34 ms/it, loss 0.473300
Finished training it 47104/76743 of epoch 2, 87.11 ms/it, loss 0.471916
Finished training it 47104/76743 of epoch 2, 86.95 ms/it, loss 0.475883
Finished training it 47104/76743 of epoch 2, 86.95 ms/it, loss 0.473862
Finished training it 47104/76743 of epoch 2, 87.12 ms/it, loss 0.473098
Finished training it 48128/76743 of epoch 2, 86.87 ms/it, loss 0.473457
Finished training it 48128/76743 of epoch 2, 86.83 ms/it, loss 0.473065
Finished training it 48128/76743 of epoch 2, 86.84 ms/it, loss 0.476781
Finished training it 48128/76743 of epoch 2, 86.79 ms/it, loss 0.475987
Finished training it 49152/76743 of epoch 2, 86.49 ms/it, loss 0.473270
Finished training it 49152/76743 of epoch 2, 86.55 ms/it, loss 0.472518
Finished training it 49152/76743 of epoch 2, 86.74 ms/it, loss 0.473222
Finished training it 49152/76743 of epoch 2, 86.57 ms/it, loss 0.474088
Finished training it 50176/76743 of epoch 2, 86.50 ms/it, loss 0.475186
Finished training it 50176/76743 of epoch 2, 86.40 ms/it, loss 0.471529
Finished training it 50176/76743 of epoch 2, 86.46 ms/it, loss 0.472986
Finished training it 50176/76743 of epoch 2, 86.58 ms/it, loss 0.475225
Finished training it 51200/76743 of epoch 2, 87.03 ms/it, loss 0.474431
Finished training it 51200/76743 of epoch 2, 86.99 ms/it, loss 0.474227
Finished training it 51200/76743 of epoch 2, 87.07 ms/it, loss 0.474481
Finished training it 51200/76743 of epoch 2, 86.88 ms/it, loss 0.473638
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535181.0
get out
0 has test check 2535181.0 and sample count 3274240
 accuracy 77.428 %, best 77.428 %, roc auc score 0.7675, best 0.7675
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535181.0
get out
3 has test check 2535181.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 86.51 ms/it, loss 0.472296
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535181.0
get out
2 has test check 2535181.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 86.64 ms/it, loss 0.474454
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 86.60 ms/it, loss 0.471586
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535181.0
get out
1 has test check 2535181.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 86.60 ms/it, loss 0.471862
Finished training it 53248/76743 of epoch 2, 86.83 ms/it, loss 0.472816
Finished training it 53248/76743 of epoch 2, 86.96 ms/it, loss 0.472313
Finished training it 53248/76743 of epoch 2, 86.90 ms/it, loss 0.476204
Finished training it 53248/76743 of epoch 2, 86.96 ms/it, loss 0.472027
Finished training it 54272/76743 of epoch 2, 86.87 ms/it, loss 0.471521
Finished training it 54272/76743 of epoch 2, 87.12 ms/it, loss 0.475071
Finished training it 54272/76743 of epoch 2, 86.99 ms/it, loss 0.476320
Finished training it 54272/76743 of epoch 2, 87.06 ms/it, loss 0.473793
Finished training it 55296/76743 of epoch 2, 92.49 ms/it, loss 0.474919
Finished training it 55296/76743 of epoch 2, 92.04 ms/it, loss 0.473156
Finished training it 55296/76743 of epoch 2, 92.12 ms/it, loss 0.471962
Finished training it 55296/76743 of epoch 2, 92.39 ms/it, loss 0.473818
Finished training it 56320/76743 of epoch 2, 91.92 ms/it, loss 0.472473
Finished training it 56320/76743 of epoch 2, 91.74 ms/it, loss 0.473073
Finished training it 56320/76743 of epoch 2, 92.00 ms/it, loss 0.470667
Finished training it 56320/76743 of epoch 2, 91.74 ms/it, loss 0.475262
Finished training it 57344/76743 of epoch 2, 86.33 ms/it, loss 0.474078
Finished training it 57344/76743 of epoch 2, 86.41 ms/it, loss 0.473302
Finished training it 57344/76743 of epoch 2, 86.21 ms/it, loss 0.472820
Finished training it 57344/76743 of epoch 2, 86.24 ms/it, loss 0.471479
Finished training it 58368/76743 of epoch 2, 86.98 ms/it, loss 0.471648
Finished training it 58368/76743 of epoch 2, 86.73 ms/it, loss 0.475023
Finished training it 58368/76743 of epoch 2, 86.86 ms/it, loss 0.470225
Finished training it 58368/76743 of epoch 2, 86.70 ms/it, loss 0.473274
Finished training it 59392/76743 of epoch 2, 86.81 ms/it, loss 0.475603
Finished training it 59392/76743 of epoch 2, 86.93 ms/it, loss 0.470912
Finished training it 59392/76743 of epoch 2, 86.98 ms/it, loss 0.473824
Finished training it 59392/76743 of epoch 2, 86.96 ms/it, loss 0.475344
Finished training it 60416/76743 of epoch 2, 86.67 ms/it, loss 0.474368
Finished training it 60416/76743 of epoch 2, 86.54 ms/it, loss 0.473665
Finished training it 60416/76743 of epoch 2, 86.49 ms/it, loss 0.471865
Finished training it 60416/76743 of epoch 2, 86.74 ms/it, loss 0.472209
Finished training it 61440/76743 of epoch 2, 87.08 ms/it, loss 0.474237
Finished training it 61440/76743 of epoch 2, 87.05 ms/it, loss 0.474104
Finished training it 61440/76743 of epoch 2, 87.15 ms/it, loss 0.473446
Finished training it 61440/76743 of epoch 2, 87.19 ms/it, loss 0.474200
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2536237.0
get out
0 has test check 2536237.0 and sample count 3274240
 accuracy 77.460 %, best 77.460 %, roc auc score 0.7691, best 0.7691
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 86.94 ms/it, loss 0.471910
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2536237.0
get out
1 has test check 2536237.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.71 ms/it, loss 0.471410
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2536237.0
get out
3 has test check 2536237.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.80 ms/it, loss 0.471637
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2536237.0
get out
2 has test check 2536237.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 86.82 ms/it, loss 0.473177
Finished training it 63488/76743 of epoch 2, 87.11 ms/it, loss 0.472914
Finished training it 63488/76743 of epoch 2, 87.08 ms/it, loss 0.472883
Finished training it 63488/76743 of epoch 2, 87.06 ms/it, loss 0.472635
Finished training it 63488/76743 of epoch 2, 87.05 ms/it, loss 0.472113
Finished training it 64512/76743 of epoch 2, 87.13 ms/it, loss 0.473850
Finished training it 64512/76743 of epoch 2, 87.32 ms/it, loss 0.471866
Finished training it 64512/76743 of epoch 2, 87.06 ms/it, loss 0.471047
Finished training it 64512/76743 of epoch 2, 87.06 ms/it, loss 0.474747
Finished training it 65536/76743 of epoch 2, 87.07 ms/it, loss 0.471205
Finished training it 65536/76743 of epoch 2, 86.95 ms/it, loss 0.471519
Finished training it 65536/76743 of epoch 2, 87.07 ms/it, loss 0.476874
Finished training it 65536/76743 of epoch 2, 86.95 ms/it, loss 0.472059
Finished training it 66560/76743 of epoch 2, 86.92 ms/it, loss 0.473167
Finished training it 66560/76743 of epoch 2, 86.63 ms/it, loss 0.472121
Finished training it 66560/76743 of epoch 2, 86.56 ms/it, loss 0.471940
Finished training it 66560/76743 of epoch 2, 86.89 ms/it, loss 0.471052
Finished training it 67584/76743 of epoch 2, 86.92 ms/it, loss 0.472082
Finished training it 67584/76743 of epoch 2, 86.69 ms/it, loss 0.471887
Finished training it 67584/76743 of epoch 2, 86.66 ms/it, loss 0.475105
Finished training it 67584/76743 of epoch 2, 86.95 ms/it, loss 0.473429
Finished training it 68608/76743 of epoch 2, 86.60 ms/it, loss 0.472106
Finished training it 68608/76743 of epoch 2, 86.78 ms/it, loss 0.470256
Finished training it 68608/76743 of epoch 2, 86.74 ms/it, loss 0.475363
Finished training it 68608/76743 of epoch 2, 86.59 ms/it, loss 0.474709
Finished training it 69632/76743 of epoch 2, 86.91 ms/it, loss 0.470110
Finished training it 69632/76743 of epoch 2, 87.04 ms/it, loss 0.473369
Finished training it 69632/76743 of epoch 2, 86.99 ms/it, loss 0.469150
Finished training it 69632/76743 of epoch 2, 86.76 ms/it, loss 0.473700
Finished training it 70656/76743 of epoch 2, 86.56 ms/it, loss 0.475194
Finished training it 70656/76743 of epoch 2, 86.45 ms/it, loss 0.472100
Finished training it 70656/76743 of epoch 2, 86.47 ms/it, loss 0.471298
Finished training it 70656/76743 of epoch 2, 86.61 ms/it, loss 0.472702
Finished training it 71680/76743 of epoch 2, 86.00 ms/it, loss 0.471740
Finished training it 71680/76743 of epoch 2, 86.09 ms/it, loss 0.476783
Finished training it 71680/76743 of epoch 2, 86.06 ms/it, loss 0.471945
Finished training it 71680/76743 of epoch 2, 86.12 ms/it, loss 0.473998
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537365.0
get out
0 has test check 2537365.0 and sample count 3274240
 accuracy 77.495 %, best 77.495 %, roc auc score 0.7695, best 0.7695
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 87.02 ms/it, loss 0.473259
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537365.0
get out
1 has test check 2537365.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.90 ms/it, loss 0.473315
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537365.0
get out
3 has test check 2537365.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.92 ms/it, loss 0.472968
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537365.0
get out
2 has test check 2537365.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 86.98 ms/it, loss 0.473487
Finished training it 73728/76743 of epoch 2, 86.71 ms/it, loss 0.472366
Finished training it 73728/76743 of epoch 2, 86.65 ms/it, loss 0.470690
Finished training it 73728/76743 of epoch 2, 86.63 ms/it, loss 0.474097
Finished training it 73728/76743 of epoch 2, 86.67 ms/it, loss 0.472335
Finished training it 74752/76743 of epoch 2, 92.70 ms/it, loss 0.473950
Finished training it 74752/76743 of epoch 2, 92.43 ms/it, loss 0.472141
Finished training it 74752/76743 of epoch 2, 92.22 ms/it, loss 0.470726
Finished training it 74752/76743 of epoch 2, 92.48 ms/it, loss 0.475318
Finished training it 75776/76743 of epoch 2, 92.43 ms/it, loss 0.469560
Finished training it 75776/76743 of epoch 2, 92.29 ms/it, loss 0.471489
Finished training it 75776/76743 of epoch 2, 92.13 ms/it, loss 0.471623
Finished training it 75776/76743 of epoch 2, 92.55 ms/it, loss 0.476275
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.26 ms/it, loss 0.473012
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.45 ms/it, loss 0.473916
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.46 ms/it, loss 0.472336
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.56 ms/it, loss 0.472359
Finished training it 2048/76743 of epoch 3, 86.50 ms/it, loss 0.470323
Finished training it 2048/76743 of epoch 3, 86.36 ms/it, loss 0.474622
Finished training it 2048/76743 of epoch 3, 86.35 ms/it, loss 0.471215
Finished training it 2048/76743 of epoch 3, 86.29 ms/it, loss 0.471882
Finished training it 3072/76743 of epoch 3, 86.79 ms/it, loss 0.472925
Finished training it 3072/76743 of epoch 3, 86.71 ms/it, loss 0.475429
Finished training it 3072/76743 of epoch 3, 86.46 ms/it, loss 0.471443
Finished training it 3072/76743 of epoch 3, 86.75 ms/it, loss 0.472897
Finished training it 4096/76743 of epoch 3, 86.71 ms/it, loss 0.474360
Finished training it 4096/76743 of epoch 3, 86.71 ms/it, loss 0.471969
Finished training it 4096/76743 of epoch 3, 86.68 ms/it, loss 0.474906
Finished training it 4096/76743 of epoch 3, 86.52 ms/it, loss 0.472865
Finished training it 5120/76743 of epoch 3, 87.31 ms/it, loss 0.476256
Finished training it 5120/76743 of epoch 3, 87.00 ms/it, loss 0.468465
Finished training it 5120/76743 of epoch 3, 87.20 ms/it, loss 0.471368
Finished training it 5120/76743 of epoch 3, 87.30 ms/it, loss 0.472569
Finished training it 6144/76743 of epoch 3, 87.01 ms/it, loss 0.470982
Finished training it 6144/76743 of epoch 3, 86.88 ms/it, loss 0.471931
Finished training it 6144/76743 of epoch 3, 87.23 ms/it, loss 0.470518
Finished training it 6144/76743 of epoch 3, 87.17 ms/it, loss 0.471142
Finished training it 7168/76743 of epoch 3, 86.54 ms/it, loss 0.470736
Finished training it 7168/76743 of epoch 3, 86.39 ms/it, loss 0.470775
Finished training it 7168/76743 of epoch 3, 86.60 ms/it, loss 0.473726
Finished training it 7168/76743 of epoch 3, 86.58 ms/it, loss 0.471913
Finished training it 8192/76743 of epoch 3, 87.06 ms/it, loss 0.470696
Finished training it 8192/76743 of epoch 3, 86.85 ms/it, loss 0.470000
Finished training it 8192/76743 of epoch 3, 86.52 ms/it, loss 0.471260
Finished training it 8192/76743 of epoch 3, 86.75 ms/it, loss 0.474322
Finished training it 9216/76743 of epoch 3, 86.88 ms/it, loss 0.473799
Finished training it 9216/76743 of epoch 3, 86.64 ms/it, loss 0.471213
Finished training it 9216/76743 of epoch 3, 86.74 ms/it, loss 0.471786
Finished training it 9216/76743 of epoch 3, 86.86 ms/it, loss 0.473829
Finished training it 10240/76743 of epoch 3, 86.46 ms/it, loss 0.472718
Finished training it 10240/76743 of epoch 3, 86.37 ms/it, loss 0.471295
Finished training it 10240/76743 of epoch 3, 86.63 ms/it, loss 0.470431
Finished training it 10240/76743 of epoch 3, 86.52 ms/it, loss 0.473605
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2537444.0
get out
0 has test check 2537444.0 and sample count 3274240
 accuracy 77.497 %, best 77.497 %, roc auc score 0.7701, best 0.7701
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2537444.0
get out
3 has test check 2537444.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 87.19 ms/it, loss 0.474225
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2537444.0
get out
2 has test check 2537444.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 87.26 ms/it, loss 0.473044
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 87.35 ms/it, loss 0.473809
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2537444.0
get out
1 has test check 2537444.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 87.12 ms/it, loss 0.472424
Finished training it 12288/76743 of epoch 3, 86.81 ms/it, loss 0.473305
Finished training it 12288/76743 of epoch 3, 86.82 ms/it, loss 0.472082
Finished training it 12288/76743 of epoch 3, 86.55 ms/it, loss 0.470006
Finished training it 12288/76743 of epoch 3, 86.69 ms/it, loss 0.472929
Finished training it 13312/76743 of epoch 3, 87.20 ms/it, loss 0.471232
Finished training it 13312/76743 of epoch 3, 86.93 ms/it, loss 0.472299
Finished training it 13312/76743 of epoch 3, 87.13 ms/it, loss 0.472256
Finished training it 13312/76743 of epoch 3, 87.16 ms/it, loss 0.471374
Finished training it 14336/76743 of epoch 3, 86.96 ms/it, loss 0.470403
Finished training it 14336/76743 of epoch 3, 87.08 ms/it, loss 0.469406
Finished training it 14336/76743 of epoch 3, 86.92 ms/it, loss 0.473595
Finished training it 14336/76743 of epoch 3, 86.81 ms/it, loss 0.473290
Finished training it 15360/76743 of epoch 3, 87.02 ms/it, loss 0.472166
Finished training it 15360/76743 of epoch 3, 87.21 ms/it, loss 0.473674
Finished training it 15360/76743 of epoch 3, 87.19 ms/it, loss 0.470498
Finished training it 15360/76743 of epoch 3, 87.28 ms/it, loss 0.470933
Finished training it 16384/76743 of epoch 3, 86.90 ms/it, loss 0.472890
Finished training it 16384/76743 of epoch 3, 86.59 ms/it, loss 0.472380
Finished training it 16384/76743 of epoch 3, 86.81 ms/it, loss 0.469983
Finished training it 16384/76743 of epoch 3, 86.84 ms/it, loss 0.470941
Finished training it 17408/76743 of epoch 3, 86.26 ms/it, loss 0.473119
Finished training it 17408/76743 of epoch 3, 86.57 ms/it, loss 0.473975
Finished training it 17408/76743 of epoch 3, 86.49 ms/it, loss 0.472757
Finished training it 17408/76743 of epoch 3, 86.69 ms/it, loss 0.473457
Finished training it 18432/76743 of epoch 3, 86.87 ms/it, loss 0.469815
Finished training it 18432/76743 of epoch 3, 87.15 ms/it, loss 0.471682
Finished training it 18432/76743 of epoch 3, 87.09 ms/it, loss 0.470016
Finished training it 18432/76743 of epoch 3, 87.09 ms/it, loss 0.472766
Finished training it 19456/76743 of epoch 3, 87.06 ms/it, loss 0.471091
Finished training it 19456/76743 of epoch 3, 86.86 ms/it, loss 0.471354
Finished training it 19456/76743 of epoch 3, 87.09 ms/it, loss 0.472318
Finished training it 19456/76743 of epoch 3, 87.14 ms/it, loss 0.470219
Finished training it 20480/76743 of epoch 3, 147.91 ms/it, loss 0.470472
Finished training it 20480/76743 of epoch 3, 147.78 ms/it, loss 0.473136
Finished training it 20480/76743 of epoch 3, 146.67 ms/it, loss 0.473167
Finished training it 20480/76743 of epoch 3, 144.45 ms/it, loss 0.469028
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2536003.0
get out
0 has test check 2536003.0 and sample count 3274240
 accuracy 77.453 %, best 77.497 %, roc auc score 0.7697, best 0.7701
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2536003.0
get out
1 has test check 2536003.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.43 ms/it, loss 0.470411
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2536003.0
get out
3 has test check 2536003.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.58 ms/it, loss 0.473198
Finished training it 21504/76743 of epoch 3, 86.63 ms/it, loss 0.468927
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2536003.0
get out
2 has test check 2536003.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 86.54 ms/it, loss 0.470718
Finished training it 22528/76743 of epoch 3, 87.04 ms/it, loss 0.472850
Finished training it 22528/76743 of epoch 3, 87.23 ms/it, loss 0.472009
Finished training it 22528/76743 of epoch 3, 87.17 ms/it, loss 0.470149
Finished training it 22528/76743 of epoch 3, 86.92 ms/it, loss 0.472891
Finished training it 23552/76743 of epoch 3, 86.68 ms/it, loss 0.471021
Finished training it 23552/76743 of epoch 3, 86.59 ms/it, loss 0.472756
Finished training it 23552/76743 of epoch 3, 86.80 ms/it, loss 0.471948
Finished training it 23552/76743 of epoch 3, 86.83 ms/it, loss 0.471972
Finished training it 24576/76743 of epoch 3, 99.30 ms/it, loss 0.474733
Finished training it 24576/76743 of epoch 3, 99.55 ms/it, loss 0.468136
Finished training it 24576/76743 of epoch 3, 99.58 ms/it, loss 0.469495
Finished training it 24576/76743 of epoch 3, 99.41 ms/it, loss 0.471319
Finished training it 25600/76743 of epoch 3, 87.08 ms/it, loss 0.470272
Finished training it 25600/76743 of epoch 3, 87.22 ms/it, loss 0.471154
Finished training it 25600/76743 of epoch 3, 87.24 ms/it, loss 0.470931
Finished training it 25600/76743 of epoch 3, 87.17 ms/it, loss 0.471799
Finished training it 26624/76743 of epoch 3, 86.97 ms/it, loss 0.471467
Finished training it 26624/76743 of epoch 3, 87.04 ms/it, loss 0.470584
Finished training it 26624/76743 of epoch 3, 87.02 ms/it, loss 0.471080
Finished training it 26624/76743 of epoch 3, 86.80 ms/it, loss 0.471227
Finished training it 27648/76743 of epoch 3, 86.80 ms/it, loss 0.473100
Finished training it 27648/76743 of epoch 3, 86.82 ms/it, loss 0.469415
Finished training it 27648/76743 of epoch 3, 86.93 ms/it, loss 0.470147
Finished training it 27648/76743 of epoch 3, 86.83 ms/it, loss 0.472700
Finished training it 28672/76743 of epoch 3, 87.08 ms/it, loss 0.471007
Finished training it 28672/76743 of epoch 3, 87.07 ms/it, loss 0.469398
Finished training it 28672/76743 of epoch 3, 86.80 ms/it, loss 0.471618
Finished training it 28672/76743 of epoch 3, 86.88 ms/it, loss 0.470006
Finished training it 29696/76743 of epoch 3, 86.64 ms/it, loss 0.471429
Finished training it 29696/76743 of epoch 3, 86.73 ms/it, loss 0.470992
Finished training it 29696/76743 of epoch 3, 86.72 ms/it, loss 0.469258
Finished training it 29696/76743 of epoch 3, 86.51 ms/it, loss 0.470906
Finished training it 30720/76743 of epoch 3, 170.15 ms/it, loss 0.469433
Finished training it 30720/76743 of epoch 3, 172.62 ms/it, loss 0.473836
Finished training it 30720/76743 of epoch 3, 172.27 ms/it, loss 0.469139
Finished training it 30720/76743 of epoch 3, 166.97 ms/it, loss 0.470672
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541216.0
get out
0 has test check 2541216.0 and sample count 3274240
 accuracy 77.612 %, best 77.612 %, roc auc score 0.7724, best 0.7724
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541216.0
get out
3 has test check 2541216.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.66 ms/it, loss 0.471899
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541216.0
get out
2 has test check 2541216.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.83 ms/it, loss 0.469460
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 86.93 ms/it, loss 0.470409
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541216.0
get out
1 has test check 2541216.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 86.68 ms/it, loss 0.471362
Finished training it 32768/76743 of epoch 3, 86.50 ms/it, loss 0.472713
Finished training it 32768/76743 of epoch 3, 86.57 ms/it, loss 0.470890
Finished training it 32768/76743 of epoch 3, 86.36 ms/it, loss 0.468926
Finished training it 32768/76743 of epoch 3, 86.45 ms/it, loss 0.468630
Finished training it 33792/76743 of epoch 3, 87.32 ms/it, loss 0.471094
Finished training it 33792/76743 of epoch 3, 87.32 ms/it, loss 0.468271
Finished training it 33792/76743 of epoch 3, 87.39 ms/it, loss 0.468113
Finished training it 33792/76743 of epoch 3, 87.20 ms/it, loss 0.470429
Finished training it 34816/76743 of epoch 3, 86.99 ms/it, loss 0.466682
Finished training it 34816/76743 of epoch 3, 87.10 ms/it, loss 0.469330
Finished training it 34816/76743 of epoch 3, 86.85 ms/it, loss 0.472314
Finished training it 34816/76743 of epoch 3, 87.22 ms/it, loss 0.469214
Finished training it 35840/76743 of epoch 3, 87.05 ms/it, loss 0.469554
Finished training it 35840/76743 of epoch 3, 87.12 ms/it, loss 0.470551
Finished training it 35840/76743 of epoch 3, 87.15 ms/it, loss 0.468811
Finished training it 35840/76743 of epoch 3, 86.98 ms/it, loss 0.470551
Finished training it 36864/76743 of epoch 3, 86.97 ms/it, loss 0.470965
Finished training it 36864/76743 of epoch 3, 86.89 ms/it, loss 0.470994
Finished training it 36864/76743 of epoch 3, 86.76 ms/it, loss 0.471723
Finished training it 36864/76743 of epoch 3, 86.88 ms/it, loss 0.472016
Finished training it 37888/76743 of epoch 3, 86.84 ms/it, loss 0.470239
Finished training it 37888/76743 of epoch 3, 86.82 ms/it, loss 0.471363
Finished training it 37888/76743 of epoch 3, 86.73 ms/it, loss 0.470221
Finished training it 37888/76743 of epoch 3, 86.69 ms/it, loss 0.471751
Finished training it 38912/76743 of epoch 3, 87.17 ms/it, loss 0.470686
Finished training it 38912/76743 of epoch 3, 87.30 ms/it, loss 0.471487
Finished training it 38912/76743 of epoch 3, 87.58 ms/it, loss 0.473839
Finished training it 38912/76743 of epoch 3, 87.31 ms/it, loss 0.470872
Finished training it 39936/76743 of epoch 3, 120.08 ms/it, loss 0.472488
Finished training it 39936/76743 of epoch 3, 120.85 ms/it, loss 0.470659
Finished training it 39936/76743 of epoch 3, 120.99 ms/it, loss 0.468566
Finished training it 39936/76743 of epoch 3, 118.93 ms/it, loss 0.469270
Finished training it 40960/76743 of epoch 3, 159.60 ms/it, loss 0.472012
Finished training it 40960/76743 of epoch 3, 161.39 ms/it, loss 0.469172
Finished training it 40960/76743 of epoch 3, 156.48 ms/it, loss 0.470577
Finished training it 40960/76743 of epoch 3, 161.35 ms/it, loss 0.470824
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2538656.0
get out
0 has test check 2538656.0 and sample count 3274240
 accuracy 77.534 %, best 77.612 %, roc auc score 0.7717, best 0.7724
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2538656.0
get out
1 has test check 2538656.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 86.95 ms/it, loss 0.468445
Finished training it 41984/76743 of epoch 3, 87.12 ms/it, loss 0.468171
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2538656.0
get out
2 has test check 2538656.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 86.99 ms/it, loss 0.468081
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2538656.0
get out
3 has test check 2538656.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 87.12 ms/it, loss 0.470869
Finished training it 43008/76743 of epoch 3, 87.52 ms/it, loss 0.471197
Finished training it 43008/76743 of epoch 3, 87.61 ms/it, loss 0.469076
Finished training it 43008/76743 of epoch 3, 87.55 ms/it, loss 0.467432
Finished training it 43008/76743 of epoch 3, 87.33 ms/it, loss 0.472850
Finished training it 44032/76743 of epoch 3, 86.46 ms/it, loss 0.469032
Finished training it 44032/76743 of epoch 3, 86.51 ms/it, loss 0.471871
Finished training it 44032/76743 of epoch 3, 86.51 ms/it, loss 0.467471
Finished training it 44032/76743 of epoch 3, 86.20 ms/it, loss 0.471849
Finished training it 45056/76743 of epoch 3, 91.60 ms/it, loss 0.468643
Finished training it 45056/76743 of epoch 3, 91.91 ms/it, loss 0.469606
Finished training it 45056/76743 of epoch 3, 91.71 ms/it, loss 0.467214
Finished training it 45056/76743 of epoch 3, 92.33 ms/it, loss 0.469276
Finished training it 46080/76743 of epoch 3, 92.37 ms/it, loss 0.472320
Finished training it 46080/76743 of epoch 3, 92.74 ms/it, loss 0.468720
Finished training it 46080/76743 of epoch 3, 92.42 ms/it, loss 0.469435
Finished training it 46080/76743 of epoch 3, 92.57 ms/it, loss 0.468887
Finished training it 47104/76743 of epoch 3, 87.33 ms/it, loss 0.469005
Finished training it 47104/76743 of epoch 3, 87.51 ms/it, loss 0.471658
Finished training it 47104/76743 of epoch 3, 87.55 ms/it, loss 0.470026
Finished training it 47104/76743 of epoch 3, 87.46 ms/it, loss 0.467689
Finished training it 48128/76743 of epoch 3, 87.02 ms/it, loss 0.472115
Finished training it 48128/76743 of epoch 3, 86.85 ms/it, loss 0.469028
Finished training it 48128/76743 of epoch 3, 86.86 ms/it, loss 0.472496
Finished training it 48128/76743 of epoch 3, 86.96 ms/it, loss 0.469670
Finished training it 49152/76743 of epoch 3, 107.24 ms/it, loss 0.470509
Finished training it 49152/76743 of epoch 3, 106.79 ms/it, loss 0.469043
Finished training it 49152/76743 of epoch 3, 106.69 ms/it, loss 0.468505
Finished training it 49152/76743 of epoch 3, 105.75 ms/it, loss 0.469892
Finished training it 50176/76743 of epoch 3, 192.51 ms/it, loss 0.471119
Finished training it 50176/76743 of epoch 3, 189.94 ms/it, loss 0.468454
Finished training it 50176/76743 of epoch 3, 192.72 ms/it, loss 0.471118
Finished training it 50176/76743 of epoch 3, 186.18 ms/it, loss 0.467293
Finished training it 51200/76743 of epoch 3, 94.87 ms/it, loss 0.470750
Finished training it 51200/76743 of epoch 3, 93.89 ms/it, loss 0.470616
Finished training it 51200/76743 of epoch 3, 94.42 ms/it, loss 0.469376
Finished training it 51200/76743 of epoch 3, 94.56 ms/it, loss 0.470459
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540921.0
get out
0 has test check 2540921.0 and sample count 3274240
 accuracy 77.603 %, best 77.612 %, roc auc score 0.7727, best 0.7727
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540921.0
get out
3 has test check 2540921.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 91.49 ms/it, loss 0.468193
Finished training it 52224/76743 of epoch 3, 91.28 ms/it, loss 0.467508
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540921.0
get out
2 has test check 2540921.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 91.23 ms/it, loss 0.470579
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540921.0
get out
1 has test check 2540921.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 91.39 ms/it, loss 0.467841
Finished training it 53248/76743 of epoch 3, 89.65 ms/it, loss 0.472399
Finished training it 53248/76743 of epoch 3, 89.55 ms/it, loss 0.469037
Finished training it 53248/76743 of epoch 3, 89.93 ms/it, loss 0.469117
Finished training it 53248/76743 of epoch 3, 89.64 ms/it, loss 0.467603
Finished training it 54272/76743 of epoch 3, 88.61 ms/it, loss 0.470104
Finished training it 54272/76743 of epoch 3, 88.27 ms/it, loss 0.471317
Finished training it 54272/76743 of epoch 3, 88.50 ms/it, loss 0.472788
Finished training it 54272/76743 of epoch 3, 88.45 ms/it, loss 0.468149
Finished training it 55296/76743 of epoch 3, 88.56 ms/it, loss 0.468069
Finished training it 55296/76743 of epoch 3, 88.82 ms/it, loss 0.469781
Finished training it 55296/76743 of epoch 3, 88.63 ms/it, loss 0.471059
Finished training it 55296/76743 of epoch 3, 88.56 ms/it, loss 0.470138
Finished training it 56320/76743 of epoch 3, 88.24 ms/it, loss 0.472059
Finished training it 56320/76743 of epoch 3, 88.40 ms/it, loss 0.467728
Finished training it 56320/76743 of epoch 3, 88.24 ms/it, loss 0.468920
Finished training it 56320/76743 of epoch 3, 88.17 ms/it, loss 0.469488
Finished training it 57344/76743 of epoch 3, 87.97 ms/it, loss 0.470019
Finished training it 57344/76743 of epoch 3, 88.15 ms/it, loss 0.468060
Finished training it 57344/76743 of epoch 3, 88.06 ms/it, loss 0.470600
Finished training it 57344/76743 of epoch 3, 87.98 ms/it, loss 0.469278
Finished training it 58368/76743 of epoch 3, 87.89 ms/it, loss 0.471593
Finished training it 58368/76743 of epoch 3, 88.38 ms/it, loss 0.466712
Finished training it 58368/76743 of epoch 3, 88.20 ms/it, loss 0.468338
Finished training it 58368/76743 of epoch 3, 87.95 ms/it, loss 0.470041
Finished training it 59392/76743 of epoch 3, 87.79 ms/it, loss 0.470162
Finished training it 59392/76743 of epoch 3, 87.77 ms/it, loss 0.467473
Finished training it 59392/76743 of epoch 3, 88.02 ms/it, loss 0.471893
Finished training it 59392/76743 of epoch 3, 87.63 ms/it, loss 0.472124
Finished training it 60416/76743 of epoch 3, 87.92 ms/it, loss 0.468674
Finished training it 60416/76743 of epoch 3, 88.20 ms/it, loss 0.468809
Finished training it 60416/76743 of epoch 3, 87.78 ms/it, loss 0.470476
Finished training it 60416/76743 of epoch 3, 88.21 ms/it, loss 0.471398
Finished training it 61440/76743 of epoch 3, 87.94 ms/it, loss 0.470557
Finished training it 61440/76743 of epoch 3, 88.03 ms/it, loss 0.470390
Finished training it 61440/76743 of epoch 3, 88.06 ms/it, loss 0.471146
Finished training it 61440/76743 of epoch 3, 88.14 ms/it, loss 0.470925
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2540497.0
get out
0 has test check 2540497.0 and sample count 3274240
 accuracy 77.590 %, best 77.612 %, roc auc score 0.7735, best 0.7735
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2540497.0
get out
1 has test check 2540497.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 90.32 ms/it, loss 0.468427
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2540497.0
get out
3 has test check 2540497.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 90.47 ms/it, loss 0.468525
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2540497.0
get out
2 has test check 2540497.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 90.47 ms/it, loss 0.470410
Finished training it 62464/76743 of epoch 3, 90.66 ms/it, loss 0.468540
Finished training it 63488/76743 of epoch 3, 90.44 ms/it, loss 0.469825
Finished training it 63488/76743 of epoch 3, 90.24 ms/it, loss 0.468821
Finished training it 63488/76743 of epoch 3, 90.42 ms/it, loss 0.469237
Finished training it 63488/76743 of epoch 3, 90.41 ms/it, loss 0.469658
Finished training it 64512/76743 of epoch 3, 89.35 ms/it, loss 0.471474
Finished training it 64512/76743 of epoch 3, 89.53 ms/it, loss 0.470676
Finished training it 64512/76743 of epoch 3, 89.14 ms/it, loss 0.468769
Finished training it 64512/76743 of epoch 3, 89.35 ms/it, loss 0.468088
Finished training it 65536/76743 of epoch 3, 89.01 ms/it, loss 0.468582
Finished training it 65536/76743 of epoch 3, 88.89 ms/it, loss 0.468612
Finished training it 65536/76743 of epoch 3, 88.77 ms/it, loss 0.469385
Finished training it 65536/76743 of epoch 3, 88.84 ms/it, loss 0.473365
Finished training it 66560/76743 of epoch 3, 98.74 ms/it, loss 0.469440
Finished training it 66560/76743 of epoch 3, 98.83 ms/it, loss 0.468734
Finished training it 66560/76743 of epoch 3, 99.06 ms/it, loss 0.467674
Finished training it 66560/76743 of epoch 3, 98.85 ms/it, loss 0.469571
Finished training it 67584/76743 of epoch 3, 88.08 ms/it, loss 0.468729
Finished training it 67584/76743 of epoch 3, 87.64 ms/it, loss 0.470219
Finished training it 67584/76743 of epoch 3, 87.92 ms/it, loss 0.468288
Finished training it 67584/76743 of epoch 3, 87.88 ms/it, loss 0.471789
Finished training it 68608/76743 of epoch 3, 87.62 ms/it, loss 0.471475
Finished training it 68608/76743 of epoch 3, 87.71 ms/it, loss 0.468853
Finished training it 68608/76743 of epoch 3, 87.41 ms/it, loss 0.467861
Finished training it 68608/76743 of epoch 3, 87.66 ms/it, loss 0.472710
Finished training it 69632/76743 of epoch 3, 88.19 ms/it, loss 0.470977
Finished training it 69632/76743 of epoch 3, 88.45 ms/it, loss 0.466252
Finished training it 69632/76743 of epoch 3, 88.38 ms/it, loss 0.467044
Finished training it 69632/76743 of epoch 3, 88.16 ms/it, loss 0.470803
Finished training it 70656/76743 of epoch 3, 87.93 ms/it, loss 0.472512
Finished training it 70656/76743 of epoch 3, 87.96 ms/it, loss 0.469219
Finished training it 70656/76743 of epoch 3, 88.00 ms/it, loss 0.468650
Finished training it 70656/76743 of epoch 3, 87.79 ms/it, loss 0.470269
Finished training it 71680/76743 of epoch 3, 88.00 ms/it, loss 0.469034
Finished training it 71680/76743 of epoch 3, 88.47 ms/it, loss 0.471073
Finished training it 71680/76743 of epoch 3, 88.10 ms/it, loss 0.473976
Finished training it 71680/76743 of epoch 3, 88.40 ms/it, loss 0.469160
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541346.0
get out
0 has test check 2541346.0 and sample count 3274240
 accuracy 77.616 %, best 77.616 %, roc auc score 0.7730, best 0.7735
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 91.52 ms/it, loss 0.470173
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541346.0
get out
2 has test check 2541346.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 91.47 ms/it, loss 0.470339
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541346.0
get out
3 has test check 2541346.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 91.72 ms/it, loss 0.470164
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541346.0
get out
1 has test check 2541346.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 91.44 ms/it, loss 0.470566
Finished training it 73728/76743 of epoch 3, 90.79 ms/it, loss 0.467548
Finished training it 73728/76743 of epoch 3, 90.70 ms/it, loss 0.470847
Finished training it 73728/76743 of epoch 3, 90.69 ms/it, loss 0.469681
Finished training it 73728/76743 of epoch 3, 90.61 ms/it, loss 0.469330
Finished training it 74752/76743 of epoch 3, 88.53 ms/it, loss 0.470622
Finished training it 74752/76743 of epoch 3, 88.36 ms/it, loss 0.472039
Finished training it 74752/76743 of epoch 3, 88.56 ms/it, loss 0.467501
Finished training it 74752/76743 of epoch 3, 88.41 ms/it, loss 0.468743
Finished training it 75776/76743 of epoch 3, 88.40 ms/it, loss 0.466155
Finished training it 75776/76743 of epoch 3, 88.28 ms/it, loss 0.468494
Finished training it 75776/76743 of epoch 3, 88.24 ms/it, loss 0.472988
Finished training it 75776/76743 of epoch 3, 88.58 ms/it, loss 0.468273
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 88.28 ms/it, loss 0.470999
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 88.23 ms/it, loss 0.470276
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 88.36 ms/it, loss 0.469455
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 88.28 ms/it, loss 0.469641
Finished training it 2048/76743 of epoch 4, 88.43 ms/it, loss 0.467913
Finished training it 2048/76743 of epoch 4, 88.28 ms/it, loss 0.468716
Finished training it 2048/76743 of epoch 4, 88.56 ms/it, loss 0.467219
Finished training it 2048/76743 of epoch 4, 88.23 ms/it, loss 0.471737
Finished training it 3072/76743 of epoch 4, 87.78 ms/it, loss 0.468383
Finished training it 3072/76743 of epoch 4, 87.71 ms/it, loss 0.472423
Finished training it 3072/76743 of epoch 4, 87.63 ms/it, loss 0.469694
Finished training it 3072/76743 of epoch 4, 87.92 ms/it, loss 0.470245
Finished training it 4096/76743 of epoch 4, 87.96 ms/it, loss 0.471074
Finished training it 4096/76743 of epoch 4, 87.63 ms/it, loss 0.469888
Finished training it 4096/76743 of epoch 4, 87.51 ms/it, loss 0.471588
Finished training it 4096/76743 of epoch 4, 87.55 ms/it, loss 0.468929
Finished training it 5120/76743 of epoch 4, 88.64 ms/it, loss 0.468828
Finished training it 5120/76743 of epoch 4, 88.19 ms/it, loss 0.465819
Finished training it 5120/76743 of epoch 4, 88.30 ms/it, loss 0.469536
Finished training it 5120/76743 of epoch 4, 88.43 ms/it, loss 0.473544
Finished training it 6144/76743 of epoch 4, 87.36 ms/it, loss 0.467827
Finished training it 6144/76743 of epoch 4, 87.63 ms/it, loss 0.468158
Finished training it 6144/76743 of epoch 4, 87.20 ms/it, loss 0.469167
Finished training it 6144/76743 of epoch 4, 87.28 ms/it, loss 0.468448
Finished training it 7168/76743 of epoch 4, 88.34 ms/it, loss 0.467676
Finished training it 7168/76743 of epoch 4, 88.17 ms/it, loss 0.471044
Finished training it 7168/76743 of epoch 4, 87.93 ms/it, loss 0.467745
Finished training it 7168/76743 of epoch 4, 88.46 ms/it, loss 0.469393
Finished training it 8192/76743 of epoch 4, 93.80 ms/it, loss 0.467635
Finished training it 8192/76743 of epoch 4, 93.76 ms/it, loss 0.471632
Finished training it 8192/76743 of epoch 4, 93.13 ms/it, loss 0.466970
Finished training it 8192/76743 of epoch 4, 93.75 ms/it, loss 0.468275
Finished training it 9216/76743 of epoch 4, 91.46 ms/it, loss 0.470728
Finished training it 9216/76743 of epoch 4, 91.53 ms/it, loss 0.469429
Finished training it 9216/76743 of epoch 4, 91.43 ms/it, loss 0.470431
Finished training it 9216/76743 of epoch 4, 91.42 ms/it, loss 0.468368
Finished training it 10240/76743 of epoch 4, 97.17 ms/it, loss 0.468695
Finished training it 10240/76743 of epoch 4, 97.42 ms/it, loss 0.470615
Finished training it 10240/76743 of epoch 4, 97.05 ms/it, loss 0.468121
Finished training it 10240/76743 of epoch 4, 97.37 ms/it, loss 0.469879
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541206.0
get out
0 has test check 2541206.0 and sample count 3274240
 accuracy 77.612 %, best 77.616 %, roc auc score 0.7733, best 0.7735
Finished training it 11264/76743 of epoch 4, 87.85 ms/it, loss 0.470835
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541206.0
get out
1 has test check 2541206.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.88 ms/it, loss 0.469492
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541206.0
get out
3 has test check 2541206.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.85 ms/it, loss 0.471836
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541206.0
get out
2 has test check 2541206.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 87.51 ms/it, loss 0.470836
Finished training it 12288/76743 of epoch 4, 87.94 ms/it, loss 0.470566
Finished training it 12288/76743 of epoch 4, 87.93 ms/it, loss 0.469481
Finished training it 12288/76743 of epoch 4, 88.04 ms/it, loss 0.470029
Finished training it 12288/76743 of epoch 4, 87.86 ms/it, loss 0.467193
Finished training it 13312/76743 of epoch 4, 93.74 ms/it, loss 0.468698
Finished training it 13312/76743 of epoch 4, 93.98 ms/it, loss 0.469921
Finished training it 13312/76743 of epoch 4, 93.69 ms/it, loss 0.469231
Finished training it 13312/76743 of epoch 4, 93.69 ms/it, loss 0.469767
Finished training it 14336/76743 of epoch 4, 92.90 ms/it, loss 0.470629
Finished training it 14336/76743 of epoch 4, 93.01 ms/it, loss 0.470258
Finished training it 14336/76743 of epoch 4, 93.11 ms/it, loss 0.467623
Finished training it 14336/76743 of epoch 4, 93.68 ms/it, loss 0.466650
Finished training it 15360/76743 of epoch 4, 88.16 ms/it, loss 0.470507
Finished training it 15360/76743 of epoch 4, 88.26 ms/it, loss 0.467486
Finished training it 15360/76743 of epoch 4, 88.12 ms/it, loss 0.469243
Finished training it 15360/76743 of epoch 4, 88.27 ms/it, loss 0.467679
Finished training it 16384/76743 of epoch 4, 88.42 ms/it, loss 0.470504
Finished training it 16384/76743 of epoch 4, 88.19 ms/it, loss 0.466976
Finished training it 16384/76743 of epoch 4, 88.12 ms/it, loss 0.469543
Finished training it 16384/76743 of epoch 4, 88.37 ms/it, loss 0.468433
Finished training it 17408/76743 of epoch 4, 88.55 ms/it, loss 0.471172
Finished training it 17408/76743 of epoch 4, 88.52 ms/it, loss 0.469966
Finished training it 17408/76743 of epoch 4, 88.62 ms/it, loss 0.470267
Finished training it 17408/76743 of epoch 4, 88.65 ms/it, loss 0.470600
Finished training it 18432/76743 of epoch 4, 89.09 ms/it, loss 0.469585
Finished training it 18432/76743 of epoch 4, 89.33 ms/it, loss 0.467053
Finished training it 18432/76743 of epoch 4, 89.33 ms/it, loss 0.469545
Finished training it 18432/76743 of epoch 4, 89.21 ms/it, loss 0.467599
Finished training it 19456/76743 of epoch 4, 90.67 ms/it, loss 0.468976
Finished training it 19456/76743 of epoch 4, 90.73 ms/it, loss 0.468016
Finished training it 19456/76743 of epoch 4, 90.76 ms/it, loss 0.466938
Finished training it 19456/76743 of epoch 4, 90.72 ms/it, loss 0.468042
Finished training it 20480/76743 of epoch 4, 91.02 ms/it, loss 0.470855
Finished training it 20480/76743 of epoch 4, 90.94 ms/it, loss 0.468267
Finished training it 20480/76743 of epoch 4, 91.11 ms/it, loss 0.466627
Finished training it 20480/76743 of epoch 4, 91.19 ms/it, loss 0.470417
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543132.0
get out
0 has test check 2543132.0 and sample count 3274240
 accuracy 77.671 %, best 77.671 %, roc auc score 0.7745, best 0.7745
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 88.34 ms/it, loss 0.466072
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543132.0
get out
2 has test check 2543132.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.06 ms/it, loss 0.468509
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543132.0
get out
1 has test check 2543132.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.28 ms/it, loss 0.467970
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543132.0
get out
3 has test check 2543132.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 88.54 ms/it, loss 0.470502
Finished training it 22528/76743 of epoch 4, 88.83 ms/it, loss 0.467461
Finished training it 22528/76743 of epoch 4, 88.57 ms/it, loss 0.470340
Finished training it 22528/76743 of epoch 4, 88.71 ms/it, loss 0.469994
Finished training it 22528/76743 of epoch 4, 88.84 ms/it, loss 0.469638
Finished training it 23552/76743 of epoch 4, 87.66 ms/it, loss 0.470956
Finished training it 23552/76743 of epoch 4, 87.86 ms/it, loss 0.468208
Finished training it 23552/76743 of epoch 4, 87.81 ms/it, loss 0.469967
Finished training it 23552/76743 of epoch 4, 87.82 ms/it, loss 0.469663
Finished training it 24576/76743 of epoch 4, 88.44 ms/it, loss 0.468848
Finished training it 24576/76743 of epoch 4, 88.36 ms/it, loss 0.472341
Finished training it 24576/76743 of epoch 4, 88.33 ms/it, loss 0.467154
Finished training it 24576/76743 of epoch 4, 88.20 ms/it, loss 0.465447
Finished training it 25600/76743 of epoch 4, 88.15 ms/it, loss 0.468293
Finished training it 25600/76743 of epoch 4, 87.91 ms/it, loss 0.467840
Finished training it 25600/76743 of epoch 4, 88.24 ms/it, loss 0.468216
Finished training it 25600/76743 of epoch 4, 88.04 ms/it, loss 0.469156
Finished training it 26624/76743 of epoch 4, 87.99 ms/it, loss 0.468170
Finished training it 26624/76743 of epoch 4, 88.11 ms/it, loss 0.469088
Finished training it 26624/76743 of epoch 4, 88.19 ms/it, loss 0.468488
Finished training it 26624/76743 of epoch 4, 87.90 ms/it, loss 0.468889
Finished training it 27648/76743 of epoch 4, 88.84 ms/it, loss 0.470194
Finished training it 27648/76743 of epoch 4, 88.50 ms/it, loss 0.468174
Finished training it 27648/76743 of epoch 4, 88.70 ms/it, loss 0.470261
Finished training it 27648/76743 of epoch 4, 88.28 ms/it, loss 0.466586
Finished training it 28672/76743 of epoch 4, 89.71 ms/it, loss 0.469663
Finished training it 28672/76743 of epoch 4, 89.88 ms/it, loss 0.467580
Finished training it 28672/76743 of epoch 4, 89.97 ms/it, loss 0.467901
Finished training it 28672/76743 of epoch 4, 89.94 ms/it, loss 0.469073
Finished training it 29696/76743 of epoch 4, 90.70 ms/it, loss 0.468762
Finished training it 29696/76743 of epoch 4, 90.67 ms/it, loss 0.469275
Finished training it 29696/76743 of epoch 4, 90.83 ms/it, loss 0.467498
Finished training it 29696/76743 of epoch 4, 90.51 ms/it, loss 0.469309
Finished training it 30720/76743 of epoch 4, 90.99 ms/it, loss 0.472281
Finished training it 30720/76743 of epoch 4, 90.90 ms/it, loss 0.467270
Finished training it 30720/76743 of epoch 4, 91.08 ms/it, loss 0.467462
Finished training it 30720/76743 of epoch 4, 90.87 ms/it, loss 0.468998
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544510.0
get out
0 has test check 2544510.0 and sample count 3274240
 accuracy 77.713 %, best 77.713 %, roc auc score 0.7747, best 0.7747
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544510.0
get out
1 has test check 2544510.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 87.95 ms/it, loss 0.469134
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544510.0
get out
3 has test check 2544510.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 88.18 ms/it, loss 0.469914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 88.10 ms/it, loss 0.468847
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544510.0
get out
2 has test check 2544510.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 87.88 ms/it, loss 0.467480
Finished training it 32768/76743 of epoch 4, 87.86 ms/it, loss 0.467022
Finished training it 32768/76743 of epoch 4, 87.86 ms/it, loss 0.471136
Finished training it 32768/76743 of epoch 4, 87.94 ms/it, loss 0.469229
Finished training it 32768/76743 of epoch 4, 87.91 ms/it, loss 0.466697
Finished training it 33792/76743 of epoch 4, 94.08 ms/it, loss 0.466622
Finished training it 33792/76743 of epoch 4, 94.06 ms/it, loss 0.468979
Finished training it 33792/76743 of epoch 4, 93.89 ms/it, loss 0.468322
Finished training it 33792/76743 of epoch 4, 93.77 ms/it, loss 0.466700
Finished training it 34816/76743 of epoch 4, 94.00 ms/it, loss 0.467566
Finished training it 34816/76743 of epoch 4, 93.62 ms/it, loss 0.467561
Finished training it 34816/76743 of epoch 4, 93.78 ms/it, loss 0.470755
Finished training it 34816/76743 of epoch 4, 94.11 ms/it, loss 0.464921
Finished training it 35840/76743 of epoch 4, 88.50 ms/it, loss 0.468108
Finished training it 35840/76743 of epoch 4, 88.34 ms/it, loss 0.468976
Finished training it 35840/76743 of epoch 4, 88.23 ms/it, loss 0.468428
Finished training it 35840/76743 of epoch 4, 88.51 ms/it, loss 0.467083
Finished training it 36864/76743 of epoch 4, 88.73 ms/it, loss 0.470078
Finished training it 36864/76743 of epoch 4, 88.34 ms/it, loss 0.469918
Finished training it 36864/76743 of epoch 4, 88.33 ms/it, loss 0.470240
Finished training it 36864/76743 of epoch 4, 88.56 ms/it, loss 0.469013
Finished training it 37888/76743 of epoch 4, 88.61 ms/it, loss 0.468314
Finished training it 37888/76743 of epoch 4, 88.27 ms/it, loss 0.470220
Finished training it 37888/76743 of epoch 4, 88.46 ms/it, loss 0.469763
Finished training it 37888/76743 of epoch 4, 88.48 ms/it, loss 0.468693
Finished training it 38912/76743 of epoch 4, 89.67 ms/it, loss 0.469973
Finished training it 38912/76743 of epoch 4, 89.65 ms/it, loss 0.469283
Finished training it 38912/76743 of epoch 4, 89.68 ms/it, loss 0.471917
Finished training it 38912/76743 of epoch 4, 89.70 ms/it, loss 0.468802
Finished training it 39936/76743 of epoch 4, 90.77 ms/it, loss 0.469124
Finished training it 39936/76743 of epoch 4, 90.83 ms/it, loss 0.466726
Finished training it 39936/76743 of epoch 4, 90.70 ms/it, loss 0.468149
Finished training it 39936/76743 of epoch 4, 90.90 ms/it, loss 0.471266
Finished training it 40960/76743 of epoch 4, 90.46 ms/it, loss 0.469054
Finished training it 40960/76743 of epoch 4, 90.72 ms/it, loss 0.470006
Finished training it 40960/76743 of epoch 4, 90.40 ms/it, loss 0.469189
Finished training it 40960/76743 of epoch 4, 90.52 ms/it, loss 0.468060
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2541006.0
get out
0 has test check 2541006.0 and sample count 3274240
 accuracy 77.606 %, best 77.713 %, roc auc score 0.7736, best 0.7747
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2541006.0
get out
1 has test check 2541006.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.13 ms/it, loss 0.467265
Finished training it 41984/76743 of epoch 4, 88.20 ms/it, loss 0.466631
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2541006.0
get out
3 has test check 2541006.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.33 ms/it, loss 0.468969
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2541006.0
get out
2 has test check 2541006.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 88.23 ms/it, loss 0.466704
Finished training it 43008/76743 of epoch 4, 88.12 ms/it, loss 0.469661
Finished training it 43008/76743 of epoch 4, 88.17 ms/it, loss 0.468318
Finished training it 43008/76743 of epoch 4, 87.96 ms/it, loss 0.471337
Finished training it 43008/76743 of epoch 4, 88.13 ms/it, loss 0.465901
Finished training it 44032/76743 of epoch 4, 88.28 ms/it, loss 0.470333
Finished training it 44032/76743 of epoch 4, 88.30 ms/it, loss 0.466493
Finished training it 44032/76743 of epoch 4, 88.43 ms/it, loss 0.470211
Finished training it 44032/76743 of epoch 4, 88.20 ms/it, loss 0.467641
Finished training it 45056/76743 of epoch 4, 87.79 ms/it, loss 0.467558
Finished training it 45056/76743 of epoch 4, 87.90 ms/it, loss 0.465941
Finished training it 45056/76743 of epoch 4, 87.81 ms/it, loss 0.468029
Finished training it 45056/76743 of epoch 4, 88.06 ms/it, loss 0.468155
Finished training it 46080/76743 of epoch 4, 88.66 ms/it, loss 0.468181
Finished training it 46080/76743 of epoch 4, 88.54 ms/it, loss 0.468073
Finished training it 46080/76743 of epoch 4, 88.80 ms/it, loss 0.467440
Finished training it 46080/76743 of epoch 4, 88.71 ms/it, loss 0.471293
Finished training it 47104/76743 of epoch 4, 88.66 ms/it, loss 0.470173
Finished training it 47104/76743 of epoch 4, 88.71 ms/it, loss 0.467720
Finished training it 47104/76743 of epoch 4, 88.84 ms/it, loss 0.466797
Finished training it 47104/76743 of epoch 4, 88.94 ms/it, loss 0.468774
Finished training it 48128/76743 of epoch 4, 88.56 ms/it, loss 0.470616
Finished training it 48128/76743 of epoch 4, 88.57 ms/it, loss 0.471334
Finished training it 48128/76743 of epoch 4, 88.31 ms/it, loss 0.468330
Finished training it 48128/76743 of epoch 4, 88.55 ms/it, loss 0.467923
Finished training it 49152/76743 of epoch 4, 90.07 ms/it, loss 0.467328
Finished training it 49152/76743 of epoch 4, 90.05 ms/it, loss 0.468819
Finished training it 49152/76743 of epoch 4, 90.02 ms/it, loss 0.468223
Finished training it 49152/76743 of epoch 4, 90.00 ms/it, loss 0.469025
Finished training it 50176/76743 of epoch 4, 90.73 ms/it, loss 0.466903
Finished training it 50176/76743 of epoch 4, 90.50 ms/it, loss 0.465808
Finished training it 50176/76743 of epoch 4, 90.66 ms/it, loss 0.469661
Finished training it 50176/76743 of epoch 4, 90.55 ms/it, loss 0.469665
Finished training it 51200/76743 of epoch 4, 90.71 ms/it, loss 0.467976
Finished training it 51200/76743 of epoch 4, 90.68 ms/it, loss 0.469169
Finished training it 51200/76743 of epoch 4, 90.70 ms/it, loss 0.469103
Finished training it 51200/76743 of epoch 4, 90.56 ms/it, loss 0.469457
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543899.0
get out
0 has test check 2543899.0 and sample count 3274240
 accuracy 77.694 %, best 77.713 %, roc auc score 0.7750, best 0.7750
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543899.0
get out
2 has test check 2543899.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.79 ms/it, loss 0.468904
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543899.0
get out
3 has test check 2543899.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.89 ms/it, loss 0.466587
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543899.0
get out
1 has test check 2543899.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 90.66 ms/it, loss 0.466016
Finished training it 52224/76743 of epoch 4, 90.84 ms/it, loss 0.465828
Finished training it 53248/76743 of epoch 4, 89.42 ms/it, loss 0.471533
Finished training it 53248/76743 of epoch 4, 89.21 ms/it, loss 0.468217
Finished training it 53248/76743 of epoch 4, 89.14 ms/it, loss 0.467514
Finished training it 53248/76743 of epoch 4, 89.14 ms/it, loss 0.466726
Finished training it 54272/76743 of epoch 4, 88.15 ms/it, loss 0.468362
Finished training it 54272/76743 of epoch 4, 87.80 ms/it, loss 0.470292
Finished training it 54272/76743 of epoch 4, 88.12 ms/it, loss 0.466533
Finished training it 54272/76743 of epoch 4, 88.01 ms/it, loss 0.470802
Finished training it 55296/76743 of epoch 4, 98.61 ms/it, loss 0.469838
Finished training it 55296/76743 of epoch 4, 99.09 ms/it, loss 0.466530
Finished training it 55296/76743 of epoch 4, 98.83 ms/it, loss 0.468740
Finished training it 55296/76743 of epoch 4, 98.40 ms/it, loss 0.468751
Finished training it 56320/76743 of epoch 4, 88.22 ms/it, loss 0.466214
Finished training it 56320/76743 of epoch 4, 88.35 ms/it, loss 0.468141
Finished training it 56320/76743 of epoch 4, 88.54 ms/it, loss 0.467666
Finished training it 56320/76743 of epoch 4, 88.27 ms/it, loss 0.470704
Finished training it 57344/76743 of epoch 4, 88.43 ms/it, loss 0.468437
Finished training it 57344/76743 of epoch 4, 88.40 ms/it, loss 0.467076
Finished training it 57344/76743 of epoch 4, 88.25 ms/it, loss 0.469172
Finished training it 57344/76743 of epoch 4, 88.42 ms/it, loss 0.467883
Finished training it 58368/76743 of epoch 4, 88.62 ms/it, loss 0.468360
Finished training it 58368/76743 of epoch 4, 88.52 ms/it, loss 0.466716
Finished training it 58368/76743 of epoch 4, 88.55 ms/it, loss 0.464947
Finished training it 58368/76743 of epoch 4, 88.67 ms/it, loss 0.470257
Finished training it 59392/76743 of epoch 4, 88.69 ms/it, loss 0.470412
Finished training it 59392/76743 of epoch 4, 88.48 ms/it, loss 0.465822
Finished training it 59392/76743 of epoch 4, 88.80 ms/it, loss 0.468948
Finished training it 59392/76743 of epoch 4, 88.66 ms/it, loss 0.470333
Finished training it 60416/76743 of epoch 4, 88.13 ms/it, loss 0.468581
Finished training it 60416/76743 of epoch 4, 88.23 ms/it, loss 0.469918
Finished training it 60416/76743 of epoch 4, 88.33 ms/it, loss 0.467535
Finished training it 60416/76743 of epoch 4, 88.45 ms/it, loss 0.467636
Finished training it 61440/76743 of epoch 4, 88.30 ms/it, loss 0.469915
Finished training it 61440/76743 of epoch 4, 88.12 ms/it, loss 0.469552
Finished training it 61440/76743 of epoch 4, 88.09 ms/it, loss 0.469279
Finished training it 61440/76743 of epoch 4, 88.28 ms/it, loss 0.469286
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2542759.0
get out
0 has test check 2542759.0 and sample count 3274240
 accuracy 77.660 %, best 77.713 %, roc auc score 0.7746, best 0.7750
Finished training it 62464/76743 of epoch 4, 90.53 ms/it, loss 0.467106
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2542759.0
get out
1 has test check 2542759.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.55 ms/it, loss 0.467381
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2542759.0
get out
3 has test check 2542759.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.50 ms/it, loss 0.467114
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2542759.0
get out
2 has test check 2542759.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 90.46 ms/it, loss 0.469001
Finished training it 63488/76743 of epoch 4, 89.26 ms/it, loss 0.468058
Finished training it 63488/76743 of epoch 4, 89.44 ms/it, loss 0.468705
Finished training it 63488/76743 of epoch 4, 89.14 ms/it, loss 0.467632
Finished training it 63488/76743 of epoch 4, 89.42 ms/it, loss 0.468794
Finished training it 64512/76743 of epoch 4, 88.62 ms/it, loss 0.469962
Finished training it 64512/76743 of epoch 4, 88.57 ms/it, loss 0.468591
Finished training it 64512/76743 of epoch 4, 88.72 ms/it, loss 0.466619
Finished training it 64512/76743 of epoch 4, 88.47 ms/it, loss 0.467496
Finished training it 65536/76743 of epoch 4, 88.38 ms/it, loss 0.467052
Finished training it 65536/76743 of epoch 4, 88.64 ms/it, loss 0.467337
Finished training it 65536/76743 of epoch 4, 88.49 ms/it, loss 0.471580
Finished training it 65536/76743 of epoch 4, 88.51 ms/it, loss 0.467288
Finished training it 66560/76743 of epoch 4, 88.57 ms/it, loss 0.465873
Finished training it 66560/76743 of epoch 4, 88.20 ms/it, loss 0.467380
Finished training it 66560/76743 of epoch 4, 88.33 ms/it, loss 0.467999
Finished training it 66560/76743 of epoch 4, 88.45 ms/it, loss 0.467281
Finished training it 67584/76743 of epoch 4, 88.59 ms/it, loss 0.467490
Finished training it 67584/76743 of epoch 4, 88.54 ms/it, loss 0.468670
Finished training it 67584/76743 of epoch 4, 88.69 ms/it, loss 0.466779
Finished training it 67584/76743 of epoch 4, 88.91 ms/it, loss 0.469574
Finished training it 68608/76743 of epoch 4, 88.11 ms/it, loss 0.469873
Finished training it 68608/76743 of epoch 4, 88.10 ms/it, loss 0.466118
Finished training it 68608/76743 of epoch 4, 88.23 ms/it, loss 0.467250
Finished training it 68608/76743 of epoch 4, 88.22 ms/it, loss 0.470641
Finished training it 69632/76743 of epoch 4, 88.87 ms/it, loss 0.465724
Finished training it 69632/76743 of epoch 4, 88.79 ms/it, loss 0.464588
Finished training it 69632/76743 of epoch 4, 88.66 ms/it, loss 0.469115
Finished training it 69632/76743 of epoch 4, 88.83 ms/it, loss 0.468933
Finished training it 70656/76743 of epoch 4, 88.32 ms/it, loss 0.470989
Finished training it 70656/76743 of epoch 4, 88.43 ms/it, loss 0.466748
Finished training it 70656/76743 of epoch 4, 88.40 ms/it, loss 0.467940
Finished training it 70656/76743 of epoch 4, 88.16 ms/it, loss 0.468347
Finished training it 71680/76743 of epoch 4, 88.84 ms/it, loss 0.467861
Finished training it 71680/76743 of epoch 4, 88.96 ms/it, loss 0.470143
Finished training it 71680/76743 of epoch 4, 88.75 ms/it, loss 0.467610
Finished training it 71680/76743 of epoch 4, 88.59 ms/it, loss 0.472556
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544257.0
get out
0 has test check 2544257.0 and sample count 3274240
 accuracy 77.705 %, best 77.713 %, roc auc score 0.7749, best 0.7750
Finished training it 72704/76743 of epoch 4, 90.92 ms/it, loss 0.468784
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544257.0
get out
1 has test check 2544257.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 90.88 ms/it, loss 0.468493
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544257.0
get out
2 has test check 2544257.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 90.81 ms/it, loss 0.468706
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544257.0
get out
3 has test check 2544257.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 90.84 ms/it, loss 0.468811
Finished training it 73728/76743 of epoch 4, 88.48 ms/it, loss 0.469254
Finished training it 73728/76743 of epoch 4, 88.73 ms/it, loss 0.468113
Finished training it 73728/76743 of epoch 4, 88.77 ms/it, loss 0.465728
Finished training it 73728/76743 of epoch 4, 88.54 ms/it, loss 0.467793
Finished training it 74752/76743 of epoch 4, 88.11 ms/it, loss 0.469259
Finished training it 74752/76743 of epoch 4, 87.78 ms/it, loss 0.467080
Finished training it 74752/76743 of epoch 4, 88.06 ms/it, loss 0.470213
Finished training it 74752/76743 of epoch 4, 88.07 ms/it, loss 0.466112
Finished training it 75776/76743 of epoch 4, 88.12 ms/it, loss 0.466841
Finished training it 75776/76743 of epoch 4, 88.20 ms/it, loss 0.465011
Finished training it 75776/76743 of epoch 4, 88.07 ms/it, loss 0.471618
Finished training it 75776/76743 of epoch 4, 88.38 ms/it, loss 0.466797
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.14 ms/it, loss 0.468579
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.08 ms/it, loss 0.468146
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.03 ms/it, loss 0.469251
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 5, 94.07 ms/it, loss 0.467711
Finished training it 2048/76743 of epoch 5, 88.35 ms/it, loss 0.467045
Finished training it 2048/76743 of epoch 5, 88.19 ms/it, loss 0.466469
Finished training it 2048/76743 of epoch 5, 88.27 ms/it, loss 0.470248
Finished training it 2048/76743 of epoch 5, 88.28 ms/it, loss 0.465773
Finished training it 3072/76743 of epoch 5, 88.40 ms/it, loss 0.470870
Finished training it 3072/76743 of epoch 5, 88.18 ms/it, loss 0.468321
Finished training it 3072/76743 of epoch 5, 88.22 ms/it, loss 0.466865
Finished training it 3072/76743 of epoch 5, 88.40 ms/it, loss 0.468669
Finished training it 4096/76743 of epoch 5, 88.61 ms/it, loss 0.469089
Finished training it 4096/76743 of epoch 5, 88.60 ms/it, loss 0.470500
Finished training it 4096/76743 of epoch 5, 88.72 ms/it, loss 0.469864
Finished training it 4096/76743 of epoch 5, 88.56 ms/it, loss 0.467893
Finished training it 5120/76743 of epoch 5, 88.28 ms/it, loss 0.468681
Finished training it 5120/76743 of epoch 5, 88.21 ms/it, loss 0.464894
Finished training it 5120/76743 of epoch 5, 88.17 ms/it, loss 0.467780
Finished training it 5120/76743 of epoch 5, 88.14 ms/it, loss 0.472442
Finished training it 6144/76743 of epoch 5, 88.78 ms/it, loss 0.467034
Finished training it 6144/76743 of epoch 5, 89.13 ms/it, loss 0.466790
Finished training it 6144/76743 of epoch 5, 89.06 ms/it, loss 0.466372
Finished training it 6144/76743 of epoch 5, 88.73 ms/it, loss 0.467670
Finished training it 7168/76743 of epoch 5, 88.51 ms/it, loss 0.468028
Finished training it 7168/76743 of epoch 5, 88.76 ms/it, loss 0.469294
Finished training it 7168/76743 of epoch 5, 88.51 ms/it, loss 0.466504
Finished training it 7168/76743 of epoch 5, 88.53 ms/it, loss 0.466661
Finished training it 8192/76743 of epoch 5, 90.12 ms/it, loss 0.466743
Finished training it 8192/76743 of epoch 5, 90.26 ms/it, loss 0.465241
Finished training it 8192/76743 of epoch 5, 90.35 ms/it, loss 0.470119
Finished training it 8192/76743 of epoch 5, 90.33 ms/it, loss 0.466670
Finished training it 9216/76743 of epoch 5, 90.80 ms/it, loss 0.469541
Finished training it 9216/76743 of epoch 5, 90.83 ms/it, loss 0.469211
Finished training it 9216/76743 of epoch 5, 90.77 ms/it, loss 0.467372
Finished training it 9216/76743 of epoch 5, 91.00 ms/it, loss 0.467869
Finished training it 10240/76743 of epoch 5, 91.01 ms/it, loss 0.468348
Finished training it 10240/76743 of epoch 5, 90.86 ms/it, loss 0.466571
Finished training it 10240/76743 of epoch 5, 90.77 ms/it, loss 0.469157
Finished training it 10240/76743 of epoch 5, 90.79 ms/it, loss 0.466526
Testing at - 10240/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544192.0
get out
0 has test check 2544192.0 and sample count 3274240
 accuracy 77.703 %, best 77.713 %, roc auc score 0.7753, best 0.7753
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544192.0
get out
3 has test check 2544192.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.77 ms/it, loss 0.470374
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544192.0
get out
1 has test check 2544192.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.84 ms/it, loss 0.467787
Testing at - 10240/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544192.0
get out
2 has test check 2544192.0 and sample count 3274240
Finished training it 11264/76743 of epoch 5, 87.64 ms/it, loss 0.469058
Finished training it 11264/76743 of epoch 5, 87.79 ms/it, loss 0.469402
Finished training it 12288/76743 of epoch 5, 88.23 ms/it, loss 0.465798
Finished training it 12288/76743 of epoch 5, 88.15 ms/it, loss 0.469127
Finished training it 12288/76743 of epoch 5, 88.44 ms/it, loss 0.468574
Finished training it 12288/76743 of epoch 5, 88.40 ms/it, loss 0.467497
Finished training it 13312/76743 of epoch 5, 88.12 ms/it, loss 0.467390
Finished training it 13312/76743 of epoch 5, 88.01 ms/it, loss 0.467387
Finished training it 13312/76743 of epoch 5, 87.87 ms/it, loss 0.468149
Finished training it 13312/76743 of epoch 5, 88.01 ms/it, loss 0.468379
Finished training it 14336/76743 of epoch 5, 88.60 ms/it, loss 0.466459
Finished training it 14336/76743 of epoch 5, 88.57 ms/it, loss 0.469833
Finished training it 14336/76743 of epoch 5, 88.37 ms/it, loss 0.468781
Finished training it 14336/76743 of epoch 5, 88.76 ms/it, loss 0.465323
Finished training it 15360/76743 of epoch 5, 87.64 ms/it, loss 0.468155
Finished training it 15360/76743 of epoch 5, 87.79 ms/it, loss 0.465596
Finished training it 15360/76743 of epoch 5, 87.67 ms/it, loss 0.469261
Finished training it 15360/76743 of epoch 5, 87.64 ms/it, loss 0.466317
Finished training it 16384/76743 of epoch 5, 88.27 ms/it, loss 0.469147
Finished training it 16384/76743 of epoch 5, 88.17 ms/it, loss 0.465235
Finished training it 16384/76743 of epoch 5, 87.98 ms/it, loss 0.467843
Finished training it 16384/76743 of epoch 5, 88.27 ms/it, loss 0.467101
Finished training it 17408/76743 of epoch 5, 88.43 ms/it, loss 0.469137
Finished training it 17408/76743 of epoch 5, 88.33 ms/it, loss 0.469625
Finished training it 17408/76743 of epoch 5, 88.28 ms/it, loss 0.468795
Finished training it 17408/76743 of epoch 5, 88.29 ms/it, loss 0.468717
Finished training it 18432/76743 of epoch 5, 90.18 ms/it, loss 0.465626
Finished training it 18432/76743 of epoch 5, 90.30 ms/it, loss 0.465469
Finished training it 18432/76743 of epoch 5, 90.44 ms/it, loss 0.468000
Finished training it 18432/76743 of epoch 5, 90.32 ms/it, loss 0.468160
Finished training it 19456/76743 of epoch 5, 91.30 ms/it, loss 0.465452
Finished training it 19456/76743 of epoch 5, 91.12 ms/it, loss 0.467818
Finished training it 19456/76743 of epoch 5, 91.24 ms/it, loss 0.466737
Finished training it 19456/76743 of epoch 5, 91.14 ms/it, loss 0.466867
Finished training it 20480/76743 of epoch 5, 91.13 ms/it, loss 0.467064
Finished training it 20480/76743 of epoch 5, 91.19 ms/it, loss 0.469019
Finished training it 20480/76743 of epoch 5, 91.01 ms/it, loss 0.469475
Finished training it 20480/76743 of epoch 5, 91.00 ms/it, loss 0.465395
Testing at - 20480/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543824.0
get out
0 has test check 2543824.0 and sample count 3274240
 accuracy 77.692 %, best 77.713 %, roc auc score 0.7749, best 0.7753
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543824.0
get out
1 has test check 2543824.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 87.38 ms/it, loss 0.466611
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543824.0
get out
3 has test check 2543824.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 87.69 ms/it, loss 0.469289
Testing at - 20480/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543824.0
get out
2 has test check 2543824.0 and sample count 3274240
Finished training it 21504/76743 of epoch 5, 87.35 ms/it, loss 0.467121
Finished training it 21504/76743 of epoch 5, 87.51 ms/it, loss 0.464698
Finished training it 22528/76743 of epoch 5, 88.20 ms/it, loss 0.468430
Finished training it 22528/76743 of epoch 5, 88.31 ms/it, loss 0.466077
Finished training it 22528/76743 of epoch 5, 88.45 ms/it, loss 0.468729
Finished training it 22528/76743 of epoch 5, 88.39 ms/it, loss 0.468771
Finished training it 23552/76743 of epoch 5, 88.40 ms/it, loss 0.466906
Finished training it 23552/76743 of epoch 5, 88.62 ms/it, loss 0.468786
Finished training it 23552/76743 of epoch 5, 88.46 ms/it, loss 0.468135
Finished training it 23552/76743 of epoch 5, 88.35 ms/it, loss 0.469748
Finished training it 24576/76743 of epoch 5, 98.47 ms/it, loss 0.464019
Finished training it 24576/76743 of epoch 5, 98.57 ms/it, loss 0.471011
Finished training it 24576/76743 of epoch 5, 98.84 ms/it, loss 0.467604
Finished training it 24576/76743 of epoch 5, 98.57 ms/it, loss 0.465329
Finished training it 25600/76743 of epoch 5, 87.98 ms/it, loss 0.466247
Finished training it 25600/76743 of epoch 5, 88.39 ms/it, loss 0.466773
Finished training it 25600/76743 of epoch 5, 88.21 ms/it, loss 0.467962
Finished training it 25600/76743 of epoch 5, 88.31 ms/it, loss 0.466791
Finished training it 26624/76743 of epoch 5, 88.45 ms/it, loss 0.468213
Finished training it 26624/76743 of epoch 5, 88.71 ms/it, loss 0.467291
Finished training it 26624/76743 of epoch 5, 88.51 ms/it, loss 0.467460
Finished training it 26624/76743 of epoch 5, 88.63 ms/it, loss 0.466790
Finished training it 27648/76743 of epoch 5, 88.47 ms/it, loss 0.469292
Finished training it 27648/76743 of epoch 5, 88.55 ms/it, loss 0.467422
Finished training it 27648/76743 of epoch 5, 88.25 ms/it, loss 0.465894
Finished training it 27648/76743 of epoch 5, 88.40 ms/it, loss 0.469367
Finished training it 28672/76743 of epoch 5, 93.19 ms/it, loss 0.466775
Finished training it 28672/76743 of epoch 5, 93.60 ms/it, loss 0.468484
Finished training it 28672/76743 of epoch 5, 93.73 ms/it, loss 0.467225
Finished training it 28672/76743 of epoch 5, 93.65 ms/it, loss 0.468087
Finished training it 29696/76743 of epoch 5, 91.01 ms/it, loss 0.466576
Finished training it 29696/76743 of epoch 5, 90.99 ms/it, loss 0.468103
Finished training it 29696/76743 of epoch 5, 90.84 ms/it, loss 0.468376
Finished training it 29696/76743 of epoch 5, 90.90 ms/it, loss 0.467865
Finished training it 30720/76743 of epoch 5, 90.68 ms/it, loss 0.467501
Finished training it 30720/76743 of epoch 5, 90.77 ms/it, loss 0.471520
Finished training it 30720/76743 of epoch 5, 90.88 ms/it, loss 0.466605
Finished training it 30720/76743 of epoch 5, 90.64 ms/it, loss 0.466325
Testing at - 30720/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2545868.0
get out
0 has test check 2545868.0 and sample count 3274240
 accuracy 77.754 %, best 77.754 %, roc auc score 0.7756, best 0.7756
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2545868.0
get out
2 has test check 2545868.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 88.29 ms/it, loss 0.466136
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 5, 88.30 ms/it, loss 0.467676
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2545868.0
get out
1 has test check 2545868.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 88.19 ms/it, loss 0.467636
Testing at - 30720/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2545868.0
get out
3 has test check 2545868.0 and sample count 3274240
Finished training it 31744/76743 of epoch 5, 88.40 ms/it, loss 0.468350
Finished training it 32768/76743 of epoch 5, 88.42 ms/it, loss 0.465230
Finished training it 32768/76743 of epoch 5, 88.27 ms/it, loss 0.467635
Finished training it 32768/76743 of epoch 5, 88.24 ms/it, loss 0.469455
Finished training it 32768/76743 of epoch 5, 88.22 ms/it, loss 0.465430
Finished training it 33792/76743 of epoch 5, 88.85 ms/it, loss 0.465197
Finished training it 33792/76743 of epoch 5, 88.64 ms/it, loss 0.465171
Finished training it 33792/76743 of epoch 5, 88.51 ms/it, loss 0.466753
Finished training it 33792/76743 of epoch 5, 88.66 ms/it, loss 0.467911
Finished training it 34816/76743 of epoch 5, 88.33 ms/it, loss 0.465731
Finished training it 34816/76743 of epoch 5, 88.49 ms/it, loss 0.463978
Finished training it 34816/76743 of epoch 5, 88.41 ms/it, loss 0.465654
Finished training it 34816/76743 of epoch 5, 88.39 ms/it, loss 0.468767
Finished training it 35840/76743 of epoch 5, 88.23 ms/it, loss 0.466817
Finished training it 35840/76743 of epoch 5, 88.01 ms/it, loss 0.467027
Finished training it 35840/76743 of epoch 5, 88.52 ms/it, loss 0.467282
Finished training it 35840/76743 of epoch 5, 88.30 ms/it, loss 0.465703
Finished training it 36864/76743 of epoch 5, 88.61 ms/it, loss 0.469321
Finished training it 36864/76743 of epoch 5, 88.68 ms/it, loss 0.468999
Finished training it 36864/76743 of epoch 5, 88.82 ms/it, loss 0.467944
Finished training it 36864/76743 of epoch 5, 88.83 ms/it, loss 0.468412
Finished training it 37888/76743 of epoch 5, 88.70 ms/it, loss 0.468073
Finished training it 37888/76743 of epoch 5, 88.30 ms/it, loss 0.468471
Finished training it 37888/76743 of epoch 5, 88.46 ms/it, loss 0.466553
Finished training it 37888/76743 of epoch 5, 88.62 ms/it, loss 0.466830
Finished training it 38912/76743 of epoch 5, 90.38 ms/it, loss 0.468468
Finished training it 38912/76743 of epoch 5, 90.07 ms/it, loss 0.470433
Finished training it 38912/76743 of epoch 5, 90.23 ms/it, loss 0.467885
Finished training it 38912/76743 of epoch 5, 89.94 ms/it, loss 0.466962
Finished training it 39936/76743 of epoch 5, 90.63 ms/it, loss 0.466259
Finished training it 39936/76743 of epoch 5, 90.65 ms/it, loss 0.467824
Finished training it 39936/76743 of epoch 5, 90.80 ms/it, loss 0.469418
Finished training it 39936/76743 of epoch 5, 90.72 ms/it, loss 0.465114
Finished training it 40960/76743 of epoch 5, 90.63 ms/it, loss 0.468502
Finished training it 40960/76743 of epoch 5, 90.68 ms/it, loss 0.465736
Finished training it 40960/76743 of epoch 5, 90.58 ms/it, loss 0.467510
Finished training it 40960/76743 of epoch 5, 90.57 ms/it, loss 0.467213
Testing at - 40960/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2544201.0
get out
0 has test check 2544201.0 and sample count 3274240
 accuracy 77.704 %, best 77.754 %, roc auc score 0.7756, best 0.7756
Finished training it 41984/76743 of epoch 5, 88.75 ms/it, loss 0.464840
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2544201.0
get out
1 has test check 2544201.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 88.36 ms/it, loss 0.465713
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2544201.0
get out
3 has test check 2544201.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 88.71 ms/it, loss 0.467697
Testing at - 40960/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2544201.0
get out
2 has test check 2544201.0 and sample count 3274240
Finished training it 41984/76743 of epoch 5, 88.36 ms/it, loss 0.465325
Finished training it 43008/76743 of epoch 5, 88.41 ms/it, loss 0.466206
Finished training it 43008/76743 of epoch 5, 88.24 ms/it, loss 0.468140
Finished training it 43008/76743 of epoch 5, 88.40 ms/it, loss 0.463744
Finished training it 43008/76743 of epoch 5, 88.16 ms/it, loss 0.469307
Finished training it 44032/76743 of epoch 5, 88.75 ms/it, loss 0.465062
Finished training it 44032/76743 of epoch 5, 88.41 ms/it, loss 0.465921
Finished training it 44032/76743 of epoch 5, 88.54 ms/it, loss 0.467964
Finished training it 44032/76743 of epoch 5, 88.53 ms/it, loss 0.468810
Finished training it 45056/76743 of epoch 5, 88.78 ms/it, loss 0.465967
Finished training it 45056/76743 of epoch 5, 88.93 ms/it, loss 0.464356
Finished training it 45056/76743 of epoch 5, 88.96 ms/it, loss 0.465994
Finished training it 45056/76743 of epoch 5, 89.07 ms/it, loss 0.466458
Finished training it 46080/76743 of epoch 5, 98.68 ms/it, loss 0.469925
Finished training it 46080/76743 of epoch 5, 99.57 ms/it, loss 0.466254
Finished training it 46080/76743 of epoch 5, 99.34 ms/it, loss 0.465910
Finished training it 46080/76743 of epoch 5, 99.39 ms/it, loss 0.466212
Finished training it 47104/76743 of epoch 5, 88.32 ms/it, loss 0.464795
Finished training it 47104/76743 of epoch 5, 88.05 ms/it, loss 0.468553
Finished training it 47104/76743 of epoch 5, 88.20 ms/it, loss 0.467179
Finished training it 47104/76743 of epoch 5, 87.97 ms/it, loss 0.465998
Finished training it 48128/76743 of epoch 5, 88.31 ms/it, loss 0.466668
Finished training it 48128/76743 of epoch 5, 88.36 ms/it, loss 0.469358
Finished training it 48128/76743 of epoch 5, 88.49 ms/it, loss 0.469733
Finished training it 48128/76743 of epoch 5, 88.44 ms/it, loss 0.466166
Finished training it 49152/76743 of epoch 5, 92.44 ms/it, loss 0.467704
Finished training it 49152/76743 of epoch 5, 92.07 ms/it, loss 0.467297
Finished training it 49152/76743 of epoch 5, 92.37 ms/it, loss 0.466773
Finished training it 49152/76743 of epoch 5, 92.36 ms/it, loss 0.465790
Finished training it 50176/76743 of epoch 5, 90.88 ms/it, loss 0.468020
Finished training it 50176/76743 of epoch 5, 90.93 ms/it, loss 0.464141
Finished training it 50176/76743 of epoch 5, 91.01 ms/it, loss 0.465749
Finished training it 50176/76743 of epoch 5, 90.85 ms/it, loss 0.468528
Finished training it 51200/76743 of epoch 5, 90.55 ms/it, loss 0.467795
Finished training it 51200/76743 of epoch 5, 90.72 ms/it, loss 0.466082
Finished training it 51200/76743 of epoch 5, 90.64 ms/it, loss 0.467560
Finished training it 51200/76743 of epoch 5, 90.61 ms/it, loss 0.467924
Testing at - 51200/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546537.0
get out
0 has test check 2546537.0 and sample count 3274240
 accuracy 77.775 %, best 77.775 %, roc auc score 0.7771, best 0.7771
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546537.0
get out
2 has test check 2546537.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.90 ms/it, loss 0.466921
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 5, 91.09 ms/it, loss 0.464262
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546537.0
get out
3 has test check 2546537.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.98 ms/it, loss 0.465009
Testing at - 51200/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546537.0
get out
1 has test check 2546537.0 and sample count 3274240
Finished training it 52224/76743 of epoch 5, 90.85 ms/it, loss 0.464444
Finished training it 53248/76743 of epoch 5, 89.09 ms/it, loss 0.466407
Finished training it 53248/76743 of epoch 5, 89.18 ms/it, loss 0.465758
Finished training it 53248/76743 of epoch 5, 88.83 ms/it, loss 0.464832
Finished training it 53248/76743 of epoch 5, 89.27 ms/it, loss 0.469482
Finished training it 54272/76743 of epoch 5, 88.68 ms/it, loss 0.469298
Finished training it 54272/76743 of epoch 5, 88.71 ms/it, loss 0.466387
Finished training it 54272/76743 of epoch 5, 88.44 ms/it, loss 0.467850
Finished training it 54272/76743 of epoch 5, 88.72 ms/it, loss 0.464817
Finished training it 55296/76743 of epoch 5, 88.54 ms/it, loss 0.468341
Finished training it 55296/76743 of epoch 5, 88.74 ms/it, loss 0.466844
Finished training it 55296/76743 of epoch 5, 88.53 ms/it, loss 0.467157
Finished training it 55296/76743 of epoch 5, 88.58 ms/it, loss 0.464895
Finished training it 56320/76743 of epoch 5, 88.18 ms/it, loss 0.464378
Finished training it 56320/76743 of epoch 5, 88.42 ms/it, loss 0.466192
Finished training it 56320/76743 of epoch 5, 88.35 ms/it, loss 0.468639
Finished training it 56320/76743 of epoch 5, 88.49 ms/it, loss 0.465779
Finished training it 57344/76743 of epoch 5, 88.05 ms/it, loss 0.466380
Finished training it 57344/76743 of epoch 5, 88.05 ms/it, loss 0.467171
Finished training it 57344/76743 of epoch 5, 87.90 ms/it, loss 0.465675
Finished training it 57344/76743 of epoch 5, 88.16 ms/it, loss 0.464976
Finished training it 58368/76743 of epoch 5, 88.51 ms/it, loss 0.463526
Finished training it 58368/76743 of epoch 5, 88.47 ms/it, loss 0.467929
Finished training it 58368/76743 of epoch 5, 88.54 ms/it, loss 0.466921
Finished training it 58368/76743 of epoch 5, 88.35 ms/it, loss 0.464524
Finished training it 59392/76743 of epoch 5, 88.72 ms/it, loss 0.464145
Finished training it 59392/76743 of epoch 5, 88.95 ms/it, loss 0.468341
Finished training it 59392/76743 of epoch 5, 88.87 ms/it, loss 0.466445
Finished training it 59392/76743 of epoch 5, 88.87 ms/it, loss 0.468255
Finished training it 60416/76743 of epoch 5, 88.38 ms/it, loss 0.468039
Finished training it 60416/76743 of epoch 5, 88.19 ms/it, loss 0.467066
Finished training it 60416/76743 of epoch 5, 88.16 ms/it, loss 0.465734
Finished training it 60416/76743 of epoch 5, 88.34 ms/it, loss 0.465355
Finished training it 61440/76743 of epoch 5, 88.23 ms/it, loss 0.467124
Finished training it 61440/76743 of epoch 5, 88.15 ms/it, loss 0.467900
Finished training it 61440/76743 of epoch 5, 88.20 ms/it, loss 0.467124
Finished training it 61440/76743 of epoch 5, 88.14 ms/it, loss 0.467281
Testing at - 61440/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546469.0
get out
0 has test check 2546469.0 and sample count 3274240
 accuracy 77.773 %, best 77.775 %, roc auc score 0.7772, best 0.7772
Finished training it 62464/76743 of epoch 5, 90.68 ms/it, loss 0.465048
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546469.0
get out
2 has test check 2546469.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.65 ms/it, loss 0.467180
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546469.0
get out
3 has test check 2546469.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.76 ms/it, loss 0.465247
Testing at - 61440/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546469.0
get out
1 has test check 2546469.0 and sample count 3274240
Finished training it 62464/76743 of epoch 5, 90.57 ms/it, loss 0.465057
Finished training it 63488/76743 of epoch 5, 89.00 ms/it, loss 0.465864
Finished training it 63488/76743 of epoch 5, 89.17 ms/it, loss 0.465778
Finished training it 63488/76743 of epoch 5, 89.21 ms/it, loss 0.466511
Finished training it 63488/76743 of epoch 5, 89.02 ms/it, loss 0.466115
Finished training it 64512/76743 of epoch 5, 88.00 ms/it, loss 0.468104
Finished training it 64512/76743 of epoch 5, 88.04 ms/it, loss 0.464708
Finished training it 64512/76743 of epoch 5, 87.88 ms/it, loss 0.465241
Finished training it 64512/76743 of epoch 5, 88.12 ms/it, loss 0.466793
Finished training it 65536/76743 of epoch 5, 99.82 ms/it, loss 0.465398
Finished training it 65536/76743 of epoch 5, 99.75 ms/it, loss 0.465131
Finished training it 65536/76743 of epoch 5, 99.58 ms/it, loss 0.465410
Finished training it 65536/76743 of epoch 5, 99.31 ms/it, loss 0.470105
Finished training it 66560/76743 of epoch 5, 88.54 ms/it, loss 0.466404
Finished training it 66560/76743 of epoch 5, 88.71 ms/it, loss 0.464294
Finished training it 66560/76743 of epoch 5, 88.47 ms/it, loss 0.465739
Finished training it 66560/76743 of epoch 5, 88.66 ms/it, loss 0.465603
Finished training it 67584/76743 of epoch 5, 88.30 ms/it, loss 0.466868
Finished training it 67584/76743 of epoch 5, 88.51 ms/it, loss 0.467781
Finished training it 67584/76743 of epoch 5, 88.32 ms/it, loss 0.465219
Finished training it 67584/76743 of epoch 5, 88.44 ms/it, loss 0.465044
Finished training it 68608/76743 of epoch 5, 88.17 ms/it, loss 0.463982
Finished training it 68608/76743 of epoch 5, 88.28 ms/it, loss 0.468454
Finished training it 68608/76743 of epoch 5, 88.31 ms/it, loss 0.465460
Finished training it 68608/76743 of epoch 5, 87.98 ms/it, loss 0.467775
Finished training it 69632/76743 of epoch 5, 88.40 ms/it, loss 0.463742
Finished training it 69632/76743 of epoch 5, 88.46 ms/it, loss 0.466931
Finished training it 69632/76743 of epoch 5, 88.33 ms/it, loss 0.467215
Finished training it 69632/76743 of epoch 5, 88.50 ms/it, loss 0.462461
Finished training it 70656/76743 of epoch 5, 88.55 ms/it, loss 0.464820
Finished training it 70656/76743 of epoch 5, 88.62 ms/it, loss 0.466751
Finished training it 70656/76743 of epoch 5, 88.66 ms/it, loss 0.468461
Finished training it 70656/76743 of epoch 5, 88.60 ms/it, loss 0.465494
Finished training it 71680/76743 of epoch 5, 88.60 ms/it, loss 0.465841
Finished training it 71680/76743 of epoch 5, 88.56 ms/it, loss 0.468245
Finished training it 71680/76743 of epoch 5, 88.31 ms/it, loss 0.471110
Finished training it 71680/76743 of epoch 5, 88.52 ms/it, loss 0.465029
Testing at - 71680/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546670.0
get out
0 has test check 2546670.0 and sample count 3274240
 accuracy 77.779 %, best 77.779 %, roc auc score 0.7773, best 0.7773
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546670.0
get out
2 has test check 2546670.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.55 ms/it, loss 0.466869
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546670.0
get out
1 has test check 2546670.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.61 ms/it, loss 0.466762
Testing at - 71680/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546670.0
get out
3 has test check 2546670.0 and sample count 3274240
Finished training it 72704/76743 of epoch 5, 90.88 ms/it, loss 0.466718
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 5, 90.76 ms/it, loss 0.466773
Finished training it 73728/76743 of epoch 5, 88.75 ms/it, loss 0.464137
Finished training it 73728/76743 of epoch 5, 88.65 ms/it, loss 0.467597
Finished training it 73728/76743 of epoch 5, 88.66 ms/it, loss 0.466696
Finished training it 73728/76743 of epoch 5, 88.60 ms/it, loss 0.465516
Finished training it 74752/76743 of epoch 5, 88.33 ms/it, loss 0.467821
Finished training it 74752/76743 of epoch 5, 88.12 ms/it, loss 0.469094
Finished training it 74752/76743 of epoch 5, 88.48 ms/it, loss 0.464177
Finished training it 74752/76743 of epoch 5, 88.20 ms/it, loss 0.465230
Finished training it 75776/76743 of epoch 5, 88.03 ms/it, loss 0.464953
Finished training it 75776/76743 of epoch 5, 88.26 ms/it, loss 0.463238
Finished training it 75776/76743 of epoch 5, 87.95 ms/it, loss 0.469803
Finished training it 75776/76743 of epoch 5, 88.10 ms/it, loss 0.464815
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 89.17 ms/it, loss 0.466888
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 89.37 ms/it, loss 0.466230
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 89.31 ms/it, loss 0.467567
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 6, 89.30 ms/it, loss 0.465943
Finished training it 2048/76743 of epoch 6, 88.34 ms/it, loss 0.463545
Finished training it 2048/76743 of epoch 6, 88.08 ms/it, loss 0.464524
Finished training it 2048/76743 of epoch 6, 88.12 ms/it, loss 0.465480
Finished training it 2048/76743 of epoch 6, 87.99 ms/it, loss 0.468385
Finished training it 3072/76743 of epoch 6, 87.96 ms/it, loss 0.466080
Finished training it 3072/76743 of epoch 6, 88.04 ms/it, loss 0.466818
Finished training it 3072/76743 of epoch 6, 88.08 ms/it, loss 0.465106
Finished training it 3072/76743 of epoch 6, 88.34 ms/it, loss 0.468859
Finished training it 4096/76743 of epoch 6, 88.37 ms/it, loss 0.466287
Finished training it 4096/76743 of epoch 6, 88.44 ms/it, loss 0.468730
Finished training it 4096/76743 of epoch 6, 88.36 ms/it, loss 0.467208
Finished training it 4096/76743 of epoch 6, 88.48 ms/it, loss 0.467779
Finished training it 5120/76743 of epoch 6, 88.23 ms/it, loss 0.470023
Finished training it 5120/76743 of epoch 6, 88.16 ms/it, loss 0.465316
Finished training it 5120/76743 of epoch 6, 88.00 ms/it, loss 0.462400
Finished training it 5120/76743 of epoch 6, 88.33 ms/it, loss 0.466783
Finished training it 6144/76743 of epoch 6, 88.21 ms/it, loss 0.463769
Finished training it 6144/76743 of epoch 6, 88.19 ms/it, loss 0.465141
Finished training it 6144/76743 of epoch 6, 88.08 ms/it, loss 0.464597
Finished training it 6144/76743 of epoch 6, 88.15 ms/it, loss 0.465717
Finished training it 7168/76743 of epoch 6, 88.48 ms/it, loss 0.467595
Finished training it 7168/76743 of epoch 6, 88.62 ms/it, loss 0.466133
Finished training it 7168/76743 of epoch 6, 88.42 ms/it, loss 0.465102
Finished training it 7168/76743 of epoch 6, 88.67 ms/it, loss 0.464419
Finished training it 8192/76743 of epoch 6, 91.09 ms/it, loss 0.463345
Finished training it 8192/76743 of epoch 6, 91.29 ms/it, loss 0.468700
Finished training it 8192/76743 of epoch 6, 91.08 ms/it, loss 0.465291
Finished training it 8192/76743 of epoch 6, 91.34 ms/it, loss 0.465002
Finished training it 9216/76743 of epoch 6, 95.71 ms/it, loss 0.465610
Finished training it 9216/76743 of epoch 6, 95.88 ms/it, loss 0.465995
Finished training it 9216/76743 of epoch 6, 96.02 ms/it, loss 0.467572
Finished training it 9216/76743 of epoch 6, 95.75 ms/it, loss 0.467011
Finished training it 10240/76743 of epoch 6, 97.10 ms/it, loss 0.466383
Finished training it 10240/76743 of epoch 6, 97.14 ms/it, loss 0.467395
Finished training it 10240/76743 of epoch 6, 97.07 ms/it, loss 0.464605
Finished training it 10240/76743 of epoch 6, 96.64 ms/it, loss 0.464882
Testing at - 10240/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547441.0
get out
0 has test check 2547441.0 and sample count 3274240
 accuracy 77.803 %, best 77.803 %, roc auc score 0.7773, best 0.7773
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547441.0
get out
3 has test check 2547441.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 88.09 ms/it, loss 0.467631
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547441.0
get out
1 has test check 2547441.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 88.13 ms/it, loss 0.465416
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 6, 88.06 ms/it, loss 0.467059
Testing at - 10240/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547441.0
get out
2 has test check 2547441.0 and sample count 3274240
Finished training it 11264/76743 of epoch 6, 88.11 ms/it, loss 0.466677
Finished training it 12288/76743 of epoch 6, 93.26 ms/it, loss 0.466992
Finished training it 12288/76743 of epoch 6, 93.94 ms/it, loss 0.465403
Finished training it 12288/76743 of epoch 6, 93.53 ms/it, loss 0.466317
Finished training it 12288/76743 of epoch 6, 93.27 ms/it, loss 0.463633
Finished training it 13312/76743 of epoch 6, 94.36 ms/it, loss 0.465906
Finished training it 13312/76743 of epoch 6, 94.03 ms/it, loss 0.465679
Finished training it 13312/76743 of epoch 6, 93.72 ms/it, loss 0.465326
Finished training it 13312/76743 of epoch 6, 93.82 ms/it, loss 0.466288
Finished training it 14336/76743 of epoch 6, 88.66 ms/it, loss 0.464233
Finished training it 14336/76743 of epoch 6, 88.49 ms/it, loss 0.466807
Finished training it 14336/76743 of epoch 6, 88.60 ms/it, loss 0.466734
Finished training it 14336/76743 of epoch 6, 88.76 ms/it, loss 0.463449
Finished training it 15360/76743 of epoch 6, 88.41 ms/it, loss 0.464005
Finished training it 15360/76743 of epoch 6, 88.38 ms/it, loss 0.465938
Finished training it 15360/76743 of epoch 6, 88.41 ms/it, loss 0.467223
Finished training it 15360/76743 of epoch 6, 88.42 ms/it, loss 0.464296
Finished training it 16384/76743 of epoch 6, 88.96 ms/it, loss 0.465877
Finished training it 16384/76743 of epoch 6, 89.09 ms/it, loss 0.465002
Finished training it 16384/76743 of epoch 6, 89.12 ms/it, loss 0.466973
Finished training it 16384/76743 of epoch 6, 88.77 ms/it, loss 0.463415
Finished training it 17408/76743 of epoch 6, 87.89 ms/it, loss 0.466917
Finished training it 17408/76743 of epoch 6, 88.05 ms/it, loss 0.467323
Finished training it 17408/76743 of epoch 6, 87.99 ms/it, loss 0.466317
Finished training it 17408/76743 of epoch 6, 88.24 ms/it, loss 0.467465
Finished training it 18432/76743 of epoch 6, 90.64 ms/it, loss 0.463406
Finished training it 18432/76743 of epoch 6, 90.67 ms/it, loss 0.466016
Finished training it 18432/76743 of epoch 6, 90.87 ms/it, loss 0.466322
Finished training it 18432/76743 of epoch 6, 90.63 ms/it, loss 0.463850
Finished training it 19456/76743 of epoch 6, 90.87 ms/it, loss 0.465739
Finished training it 19456/76743 of epoch 6, 90.95 ms/it, loss 0.463260
Finished training it 19456/76743 of epoch 6, 90.80 ms/it, loss 0.464669
Finished training it 19456/76743 of epoch 6, 90.90 ms/it, loss 0.464828
Finished training it 20480/76743 of epoch 6, 90.78 ms/it, loss 0.463818
Finished training it 20480/76743 of epoch 6, 90.88 ms/it, loss 0.467399
Finished training it 20480/76743 of epoch 6, 90.84 ms/it, loss 0.465135
Finished training it 20480/76743 of epoch 6, 91.08 ms/it, loss 0.466717
Testing at - 20480/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547758.0
get out
0 has test check 2547758.0 and sample count 3274240
 accuracy 77.812 %, best 77.812 %, roc auc score 0.7780, best 0.7780
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 6, 88.40 ms/it, loss 0.463256
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547758.0
get out
1 has test check 2547758.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.07 ms/it, loss 0.464722
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547758.0
get out
3 has test check 2547758.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.40 ms/it, loss 0.467849
Testing at - 20480/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547758.0
get out
2 has test check 2547758.0 and sample count 3274240
Finished training it 21504/76743 of epoch 6, 88.37 ms/it, loss 0.465368
Finished training it 22528/76743 of epoch 6, 87.98 ms/it, loss 0.466846
Finished training it 22528/76743 of epoch 6, 88.17 ms/it, loss 0.466357
Finished training it 22528/76743 of epoch 6, 88.01 ms/it, loss 0.464122
Finished training it 22528/76743 of epoch 6, 88.06 ms/it, loss 0.466217
Finished training it 23552/76743 of epoch 6, 88.33 ms/it, loss 0.467566
Finished training it 23552/76743 of epoch 6, 88.43 ms/it, loss 0.466501
Finished training it 23552/76743 of epoch 6, 88.46 ms/it, loss 0.466204
Finished training it 23552/76743 of epoch 6, 88.40 ms/it, loss 0.464696
Finished training it 24576/76743 of epoch 6, 87.98 ms/it, loss 0.462034
Finished training it 24576/76743 of epoch 6, 88.26 ms/it, loss 0.463498
Finished training it 24576/76743 of epoch 6, 88.00 ms/it, loss 0.465586
Finished training it 24576/76743 of epoch 6, 87.87 ms/it, loss 0.468498
Finished training it 25600/76743 of epoch 6, 88.80 ms/it, loss 0.466082
Finished training it 25600/76743 of epoch 6, 88.76 ms/it, loss 0.465112
Finished training it 25600/76743 of epoch 6, 88.87 ms/it, loss 0.465209
Finished training it 25600/76743 of epoch 6, 88.84 ms/it, loss 0.464612
Finished training it 26624/76743 of epoch 6, 88.65 ms/it, loss 0.465099
Finished training it 26624/76743 of epoch 6, 88.48 ms/it, loss 0.466162
Finished training it 26624/76743 of epoch 6, 88.31 ms/it, loss 0.465978
Finished training it 26624/76743 of epoch 6, 88.41 ms/it, loss 0.465459
Finished training it 27648/76743 of epoch 6, 108.05 ms/it, loss 0.466907
Finished training it 27648/76743 of epoch 6, 108.27 ms/it, loss 0.463377
Finished training it 27648/76743 of epoch 6, 106.84 ms/it, loss 0.466842
Finished training it 27648/76743 of epoch 6, 107.93 ms/it, loss 0.464648
Finished training it 28672/76743 of epoch 6, 90.64 ms/it, loss 0.466081
Finished training it 28672/76743 of epoch 6, 90.71 ms/it, loss 0.465509
Finished training it 28672/76743 of epoch 6, 90.66 ms/it, loss 0.464287
Finished training it 28672/76743 of epoch 6, 90.85 ms/it, loss 0.464784
Finished training it 29696/76743 of epoch 6, 91.03 ms/it, loss 0.463966
Finished training it 29696/76743 of epoch 6, 90.84 ms/it, loss 0.465837
Finished training it 29696/76743 of epoch 6, 90.82 ms/it, loss 0.465572
Finished training it 29696/76743 of epoch 6, 90.82 ms/it, loss 0.465794
Finished training it 30720/76743 of epoch 6, 90.74 ms/it, loss 0.469054
Finished training it 30720/76743 of epoch 6, 90.57 ms/it, loss 0.463978
Finished training it 30720/76743 of epoch 6, 90.61 ms/it, loss 0.465416
Finished training it 30720/76743 of epoch 6, 90.76 ms/it, loss 0.464189
Testing at - 30720/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548194.0
get out
0 has test check 2548194.0 and sample count 3274240
 accuracy 77.826 %, best 77.826 %, roc auc score 0.7777, best 0.7780
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548194.0
get out
1 has test check 2548194.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 88.47 ms/it, loss 0.465989
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548194.0
get out
3 has test check 2548194.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 88.68 ms/it, loss 0.466339
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 6, 88.64 ms/it, loss 0.465680
Testing at - 30720/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548194.0
get out
2 has test check 2548194.0 and sample count 3274240
Finished training it 31744/76743 of epoch 6, 88.46 ms/it, loss 0.464096
Finished training it 32768/76743 of epoch 6, 88.39 ms/it, loss 0.463881
Finished training it 32768/76743 of epoch 6, 88.48 ms/it, loss 0.465059
Finished training it 32768/76743 of epoch 6, 88.50 ms/it, loss 0.467425
Finished training it 32768/76743 of epoch 6, 88.61 ms/it, loss 0.463301
Finished training it 33792/76743 of epoch 6, 88.49 ms/it, loss 0.463571
Finished training it 33792/76743 of epoch 6, 88.20 ms/it, loss 0.463372
Finished training it 33792/76743 of epoch 6, 88.30 ms/it, loss 0.465828
Finished training it 33792/76743 of epoch 6, 88.24 ms/it, loss 0.464893
Finished training it 34816/76743 of epoch 6, 100.87 ms/it, loss 0.461764
Finished training it 34816/76743 of epoch 6, 100.96 ms/it, loss 0.463815
Finished training it 34816/76743 of epoch 6, 101.14 ms/it, loss 0.464180
Finished training it 34816/76743 of epoch 6, 100.70 ms/it, loss 0.466528
Finished training it 35840/76743 of epoch 6, 88.12 ms/it, loss 0.465531
Finished training it 35840/76743 of epoch 6, 88.32 ms/it, loss 0.463786
Finished training it 35840/76743 of epoch 6, 88.38 ms/it, loss 0.465406
Finished training it 35840/76743 of epoch 6, 88.27 ms/it, loss 0.464875
Finished training it 36864/76743 of epoch 6, 88.54 ms/it, loss 0.466848
Finished training it 36864/76743 of epoch 6, 88.30 ms/it, loss 0.465937
Finished training it 36864/76743 of epoch 6, 88.16 ms/it, loss 0.467317
Finished training it 36864/76743 of epoch 6, 88.50 ms/it, loss 0.466241
Finished training it 37888/76743 of epoch 6, 106.31 ms/it, loss 0.465023
Finished training it 37888/76743 of epoch 6, 105.41 ms/it, loss 0.464914
Finished training it 37888/76743 of epoch 6, 106.56 ms/it, loss 0.466518
Finished training it 37888/76743 of epoch 6, 106.39 ms/it, loss 0.466427
Finished training it 38912/76743 of epoch 6, 90.98 ms/it, loss 0.465345
Finished training it 38912/76743 of epoch 6, 91.17 ms/it, loss 0.466288
Finished training it 38912/76743 of epoch 6, 90.94 ms/it, loss 0.465830
Finished training it 38912/76743 of epoch 6, 91.01 ms/it, loss 0.468374
Finished training it 39936/76743 of epoch 6, 91.03 ms/it, loss 0.463829
Finished training it 39936/76743 of epoch 6, 91.02 ms/it, loss 0.464699
Finished training it 39936/76743 of epoch 6, 91.01 ms/it, loss 0.466054
Finished training it 39936/76743 of epoch 6, 91.23 ms/it, loss 0.467597
Finished training it 40960/76743 of epoch 6, 91.02 ms/it, loss 0.465711
Finished training it 40960/76743 of epoch 6, 91.01 ms/it, loss 0.465351
Finished training it 40960/76743 of epoch 6, 91.14 ms/it, loss 0.463809
Finished training it 40960/76743 of epoch 6, 91.10 ms/it, loss 0.466674
Testing at - 40960/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548336.0
get out
0 has test check 2548336.0 and sample count 3274240
 accuracy 77.830 %, best 77.830 %, roc auc score 0.7786, best 0.7786
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548336.0
get out
1 has test check 2548336.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 88.19 ms/it, loss 0.463192
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548336.0
get out
2 has test check 2548336.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 88.16 ms/it, loss 0.463218
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 6, 88.36 ms/it, loss 0.463194
Testing at - 40960/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548336.0
get out
3 has test check 2548336.0 and sample count 3274240
Finished training it 41984/76743 of epoch 6, 88.39 ms/it, loss 0.466020
Finished training it 43008/76743 of epoch 6, 88.09 ms/it, loss 0.464805
Finished training it 43008/76743 of epoch 6, 88.14 ms/it, loss 0.466488
Finished training it 43008/76743 of epoch 6, 88.08 ms/it, loss 0.462065
Finished training it 43008/76743 of epoch 6, 87.97 ms/it, loss 0.467775
Finished training it 44032/76743 of epoch 6, 87.97 ms/it, loss 0.464232
Finished training it 44032/76743 of epoch 6, 87.99 ms/it, loss 0.467102
Finished training it 44032/76743 of epoch 6, 88.01 ms/it, loss 0.462466
Finished training it 44032/76743 of epoch 6, 87.82 ms/it, loss 0.466388
Finished training it 45056/76743 of epoch 6, 88.19 ms/it, loss 0.462722
Finished training it 45056/76743 of epoch 6, 88.18 ms/it, loss 0.464087
Finished training it 45056/76743 of epoch 6, 88.36 ms/it, loss 0.464725
Finished training it 45056/76743 of epoch 6, 88.36 ms/it, loss 0.464259
Finished training it 46080/76743 of epoch 6, 87.98 ms/it, loss 0.468067
Finished training it 46080/76743 of epoch 6, 88.47 ms/it, loss 0.464256
Finished training it 46080/76743 of epoch 6, 88.24 ms/it, loss 0.464593
Finished training it 46080/76743 of epoch 6, 88.21 ms/it, loss 0.463780
Finished training it 47104/76743 of epoch 6, 88.47 ms/it, loss 0.466738
Finished training it 47104/76743 of epoch 6, 88.49 ms/it, loss 0.465334
Finished training it 47104/76743 of epoch 6, 88.40 ms/it, loss 0.463180
Finished training it 47104/76743 of epoch 6, 88.46 ms/it, loss 0.464559
Finished training it 48128/76743 of epoch 6, 110.10 ms/it, loss 0.467711
Finished training it 48128/76743 of epoch 6, 108.92 ms/it, loss 0.464959
Finished training it 48128/76743 of epoch 6, 110.41 ms/it, loss 0.467692
Finished training it 48128/76743 of epoch 6, 110.47 ms/it, loss 0.464328
Finished training it 49152/76743 of epoch 6, 90.61 ms/it, loss 0.464869
Finished training it 49152/76743 of epoch 6, 90.66 ms/it, loss 0.465227
Finished training it 49152/76743 of epoch 6, 90.68 ms/it, loss 0.463430
Finished training it 49152/76743 of epoch 6, 90.77 ms/it, loss 0.465222
Finished training it 50176/76743 of epoch 6, 91.09 ms/it, loss 0.466357
Finished training it 50176/76743 of epoch 6, 90.98 ms/it, loss 0.462936
Finished training it 50176/76743 of epoch 6, 91.16 ms/it, loss 0.466538
Finished training it 50176/76743 of epoch 6, 91.10 ms/it, loss 0.463893
Finished training it 51200/76743 of epoch 6, 90.73 ms/it, loss 0.466198
Finished training it 51200/76743 of epoch 6, 90.84 ms/it, loss 0.466055
Finished training it 51200/76743 of epoch 6, 90.84 ms/it, loss 0.465854
Finished training it 51200/76743 of epoch 6, 90.82 ms/it, loss 0.464345
Testing at - 51200/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549452.0
get out
0 has test check 2549452.0 and sample count 3274240
 accuracy 77.864 %, best 77.864 %, roc auc score 0.7789, best 0.7789
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 6, 88.84 ms/it, loss 0.462837
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549452.0
get out
1 has test check 2549452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 88.74 ms/it, loss 0.462915
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549452.0
get out
2 has test check 2549452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 88.63 ms/it, loss 0.465365
Testing at - 51200/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549452.0
get out
3 has test check 2549452.0 and sample count 3274240
Finished training it 52224/76743 of epoch 6, 88.80 ms/it, loss 0.463263
Finished training it 53248/76743 of epoch 6, 88.13 ms/it, loss 0.464100
Finished training it 53248/76743 of epoch 6, 87.95 ms/it, loss 0.464558
Finished training it 53248/76743 of epoch 6, 87.86 ms/it, loss 0.462827
Finished training it 53248/76743 of epoch 6, 88.02 ms/it, loss 0.467699
Finished training it 54272/76743 of epoch 6, 87.57 ms/it, loss 0.466583
Finished training it 54272/76743 of epoch 6, 87.75 ms/it, loss 0.467854
Finished training it 54272/76743 of epoch 6, 87.65 ms/it, loss 0.463561
Finished training it 54272/76743 of epoch 6, 87.81 ms/it, loss 0.465153
Finished training it 55296/76743 of epoch 6, 93.25 ms/it, loss 0.462826
Finished training it 55296/76743 of epoch 6, 93.38 ms/it, loss 0.465074
Finished training it 55296/76743 of epoch 6, 93.46 ms/it, loss 0.466613
Finished training it 55296/76743 of epoch 6, 93.40 ms/it, loss 0.464560
Finished training it 56320/76743 of epoch 6, 93.82 ms/it, loss 0.464016
Finished training it 56320/76743 of epoch 6, 93.51 ms/it, loss 0.464973
Finished training it 56320/76743 of epoch 6, 93.39 ms/it, loss 0.467150
Finished training it 56320/76743 of epoch 6, 93.29 ms/it, loss 0.462752
Finished training it 57344/76743 of epoch 6, 87.79 ms/it, loss 0.463977
Finished training it 57344/76743 of epoch 6, 87.79 ms/it, loss 0.464979
Finished training it 57344/76743 of epoch 6, 87.49 ms/it, loss 0.465732
Finished training it 57344/76743 of epoch 6, 87.66 ms/it, loss 0.464410
Finished training it 58368/76743 of epoch 6, 88.27 ms/it, loss 0.462260
Finished training it 58368/76743 of epoch 6, 87.82 ms/it, loss 0.463430
Finished training it 58368/76743 of epoch 6, 88.05 ms/it, loss 0.465361
Finished training it 58368/76743 of epoch 6, 87.90 ms/it, loss 0.466825
Finished training it 59392/76743 of epoch 6, 88.01 ms/it, loss 0.465122
Finished training it 59392/76743 of epoch 6, 87.96 ms/it, loss 0.466413
Finished training it 59392/76743 of epoch 6, 87.91 ms/it, loss 0.462719
Finished training it 59392/76743 of epoch 6, 87.72 ms/it, loss 0.467004
Finished training it 60416/76743 of epoch 6, 88.34 ms/it, loss 0.464258
Finished training it 60416/76743 of epoch 6, 88.36 ms/it, loss 0.465974
Finished training it 60416/76743 of epoch 6, 88.44 ms/it, loss 0.466546
Finished training it 60416/76743 of epoch 6, 88.50 ms/it, loss 0.464199
Finished training it 61440/76743 of epoch 6, 88.80 ms/it, loss 0.465955
Finished training it 61440/76743 of epoch 6, 88.92 ms/it, loss 0.466745
Finished training it 61440/76743 of epoch 6, 88.48 ms/it, loss 0.466190
Finished training it 61440/76743 of epoch 6, 88.75 ms/it, loss 0.465713
Testing at - 61440/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547373.0
get out
0 has test check 2547373.0 and sample count 3274240
 accuracy 77.800 %, best 77.864 %, roc auc score 0.7788, best 0.7789
Finished training it 62464/76743 of epoch 6, 87.90 ms/it, loss 0.463683
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547373.0
get out
1 has test check 2547373.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 87.81 ms/it, loss 0.463774
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547373.0
get out
2 has test check 2547373.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 87.91 ms/it, loss 0.466089
Testing at - 61440/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547373.0
get out
3 has test check 2547373.0 and sample count 3274240
Finished training it 62464/76743 of epoch 6, 88.07 ms/it, loss 0.464055
Finished training it 63488/76743 of epoch 6, 88.17 ms/it, loss 0.465326
Finished training it 63488/76743 of epoch 6, 88.13 ms/it, loss 0.464714
Finished training it 63488/76743 of epoch 6, 87.95 ms/it, loss 0.463994
Finished training it 63488/76743 of epoch 6, 88.19 ms/it, loss 0.464065
Finished training it 64512/76743 of epoch 6, 88.23 ms/it, loss 0.463761
Finished training it 64512/76743 of epoch 6, 88.21 ms/it, loss 0.466510
Finished training it 64512/76743 of epoch 6, 88.38 ms/it, loss 0.463142
Finished training it 64512/76743 of epoch 6, 88.41 ms/it, loss 0.465266
Finished training it 65536/76743 of epoch 6, 88.65 ms/it, loss 0.464544
Finished training it 65536/76743 of epoch 6, 88.32 ms/it, loss 0.469199
Finished training it 65536/76743 of epoch 6, 88.41 ms/it, loss 0.464740
Finished training it 65536/76743 of epoch 6, 88.59 ms/it, loss 0.463616
Finished training it 66560/76743 of epoch 6, 88.24 ms/it, loss 0.464479
Finished training it 66560/76743 of epoch 6, 88.48 ms/it, loss 0.465518
Finished training it 66560/76743 of epoch 6, 88.54 ms/it, loss 0.464366
Finished training it 66560/76743 of epoch 6, 88.39 ms/it, loss 0.463398
Finished training it 67584/76743 of epoch 6, 88.47 ms/it, loss 0.466841
Finished training it 67584/76743 of epoch 6, 88.37 ms/it, loss 0.465407
Finished training it 67584/76743 of epoch 6, 88.43 ms/it, loss 0.464120
Finished training it 67584/76743 of epoch 6, 88.30 ms/it, loss 0.464106
Finished training it 68608/76743 of epoch 6, 88.48 ms/it, loss 0.466615
Finished training it 68608/76743 of epoch 6, 88.41 ms/it, loss 0.463071
Finished training it 68608/76743 of epoch 6, 88.60 ms/it, loss 0.463916
Finished training it 68608/76743 of epoch 6, 88.48 ms/it, loss 0.467258
Finished training it 69632/76743 of epoch 6, 88.35 ms/it, loss 0.462805
Finished training it 69632/76743 of epoch 6, 88.14 ms/it, loss 0.466615
Finished training it 69632/76743 of epoch 6, 88.45 ms/it, loss 0.465674
Finished training it 69632/76743 of epoch 6, 88.49 ms/it, loss 0.461468
Finished training it 70656/76743 of epoch 6, 88.07 ms/it, loss 0.465597
Finished training it 70656/76743 of epoch 6, 88.14 ms/it, loss 0.464028
Finished training it 70656/76743 of epoch 6, 88.18 ms/it, loss 0.467637
Finished training it 70656/76743 of epoch 6, 88.33 ms/it, loss 0.465091
Finished training it 71680/76743 of epoch 6, 88.17 ms/it, loss 0.464306
Finished training it 71680/76743 of epoch 6, 88.00 ms/it, loss 0.470024
Finished training it 71680/76743 of epoch 6, 88.16 ms/it, loss 0.464069
Finished training it 71680/76743 of epoch 6, 88.12 ms/it, loss 0.467016
Testing at - 71680/76743 of epoch 6,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549357.0
get out
0 has test check 2549357.0 and sample count 3274240
 accuracy 77.861 %, best 77.864 %, roc auc score 0.7791, best 0.7791
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549357.0
get out
3 has test check 2549357.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 88.43 ms/it, loss 0.465604
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549357.0
get out
1 has test check 2549357.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 88.45 ms/it, loss 0.465740
Testing at - 71680/76743 of epoch 6,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549357.0
get out
2 has test check 2549357.0 and sample count 3274240
Finished training it 72704/76743 of epoch 6, 88.47 ms/it, loss 0.466011
Finished training it 72704/76743 of epoch 6, 88.45 ms/it, loss 0.465586
Finished training it 73728/76743 of epoch 6, 87.89 ms/it, loss 0.466268
Finished training it 73728/76743 of epoch 6, 87.93 ms/it, loss 0.465165
Finished training it 73728/76743 of epoch 6, 87.88 ms/it, loss 0.464330
Finished training it 73728/76743 of epoch 6, 88.16 ms/it, loss 0.463212
Finished training it 74752/76743 of epoch 6, 88.23 ms/it, loss 0.466140
Finished training it 74752/76743 of epoch 6, 88.28 ms/it, loss 0.467233
Finished training it 74752/76743 of epoch 6, 88.26 ms/it, loss 0.464086
Finished training it 74752/76743 of epoch 6, 88.29 ms/it, loss 0.462766
Finished training it 75776/76743 of epoch 6, 93.21 ms/it, loss 0.461829
Finished training it 75776/76743 of epoch 6, 92.91 ms/it, loss 0.463368
Finished training it 75776/76743 of epoch 6, 92.70 ms/it, loss 0.464089
Finished training it 75776/76743 of epoch 6, 92.70 ms/it, loss 0.468272
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 92.41 ms/it, loss 0.464690
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 92.10 ms/it, loss 0.465071
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 92.44 ms/it, loss 0.466373
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 7, 92.27 ms/it, loss 0.465443
Finished training it 2048/76743 of epoch 7, 88.45 ms/it, loss 0.462618
Finished training it 2048/76743 of epoch 7, 88.64 ms/it, loss 0.463224
Finished training it 2048/76743 of epoch 7, 88.41 ms/it, loss 0.463819
Finished training it 2048/76743 of epoch 7, 88.41 ms/it, loss 0.466845
Finished training it 3072/76743 of epoch 7, 88.13 ms/it, loss 0.463668
Finished training it 3072/76743 of epoch 7, 88.19 ms/it, loss 0.464507
Finished training it 3072/76743 of epoch 7, 88.25 ms/it, loss 0.467678
Finished training it 3072/76743 of epoch 7, 88.28 ms/it, loss 0.465002
Finished training it 4096/76743 of epoch 7, 88.34 ms/it, loss 0.466738
Finished training it 4096/76743 of epoch 7, 88.18 ms/it, loss 0.465686
Finished training it 4096/76743 of epoch 7, 88.34 ms/it, loss 0.467304
Finished training it 4096/76743 of epoch 7, 88.26 ms/it, loss 0.464869
Finished training it 5120/76743 of epoch 7, 88.76 ms/it, loss 0.464723
Finished training it 5120/76743 of epoch 7, 88.86 ms/it, loss 0.465602
Finished training it 5120/76743 of epoch 7, 88.55 ms/it, loss 0.461073
Finished training it 5120/76743 of epoch 7, 88.77 ms/it, loss 0.469060
Finished training it 6144/76743 of epoch 7, 145.58 ms/it, loss 0.463243
Finished training it 6144/76743 of epoch 7, 145.69 ms/it, loss 0.463110
Finished training it 6144/76743 of epoch 7, 141.45 ms/it, loss 0.463610
Finished training it 6144/76743 of epoch 7, 146.59 ms/it, loss 0.464192
Finished training it 7168/76743 of epoch 7, 91.03 ms/it, loss 0.464599
Finished training it 7168/76743 of epoch 7, 90.89 ms/it, loss 0.462996
Finished training it 7168/76743 of epoch 7, 90.72 ms/it, loss 0.466284
Finished training it 7168/76743 of epoch 7, 90.86 ms/it, loss 0.463664
Finished training it 8192/76743 of epoch 7, 90.87 ms/it, loss 0.463987
Finished training it 8192/76743 of epoch 7, 90.91 ms/it, loss 0.463560
Finished training it 8192/76743 of epoch 7, 91.00 ms/it, loss 0.462137
Finished training it 8192/76743 of epoch 7, 91.04 ms/it, loss 0.467434
Finished training it 9216/76743 of epoch 7, 90.94 ms/it, loss 0.464524
Finished training it 9216/76743 of epoch 7, 90.81 ms/it, loss 0.463717
Finished training it 9216/76743 of epoch 7, 90.79 ms/it, loss 0.465982
Finished training it 9216/76743 of epoch 7, 90.88 ms/it, loss 0.465935
Finished training it 10240/76743 of epoch 7, 90.81 ms/it, loss 0.463481
Finished training it 10240/76743 of epoch 7, 90.76 ms/it, loss 0.466170
Finished training it 10240/76743 of epoch 7, 90.84 ms/it, loss 0.465189
Finished training it 10240/76743 of epoch 7, 91.07 ms/it, loss 0.463352
Testing at - 10240/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550487.0
get out
0 has test check 2550487.0 and sample count 3274240
 accuracy 77.896 %, best 77.896 %, roc auc score 0.7798, best 0.7798
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 7, 88.24 ms/it, loss 0.465990
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550487.0
get out
2 has test check 2550487.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 88.61 ms/it, loss 0.465465
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550487.0
get out
1 has test check 2550487.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 88.40 ms/it, loss 0.464022
Testing at - 10240/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550487.0
get out
3 has test check 2550487.0 and sample count 3274240
Finished training it 11264/76743 of epoch 7, 88.61 ms/it, loss 0.466387
Finished training it 12288/76743 of epoch 7, 87.79 ms/it, loss 0.465256
Finished training it 12288/76743 of epoch 7, 87.88 ms/it, loss 0.462379
Finished training it 12288/76743 of epoch 7, 88.01 ms/it, loss 0.463838
Finished training it 12288/76743 of epoch 7, 87.89 ms/it, loss 0.464729
Finished training it 13312/76743 of epoch 7, 87.83 ms/it, loss 0.464558
Finished training it 13312/76743 of epoch 7, 87.97 ms/it, loss 0.464420
Finished training it 13312/76743 of epoch 7, 87.91 ms/it, loss 0.463800
Finished training it 13312/76743 of epoch 7, 87.72 ms/it, loss 0.464020
Finished training it 14336/76743 of epoch 7, 88.34 ms/it, loss 0.466006
Finished training it 14336/76743 of epoch 7, 88.05 ms/it, loss 0.465675
Finished training it 14336/76743 of epoch 7, 88.22 ms/it, loss 0.461656
Finished training it 14336/76743 of epoch 7, 88.12 ms/it, loss 0.462609
Finished training it 15360/76743 of epoch 7, 88.52 ms/it, loss 0.462239
Finished training it 15360/76743 of epoch 7, 88.37 ms/it, loss 0.465842
Finished training it 15360/76743 of epoch 7, 88.36 ms/it, loss 0.463044
Finished training it 15360/76743 of epoch 7, 88.27 ms/it, loss 0.464797
Finished training it 16384/76743 of epoch 7, 147.67 ms/it, loss 0.464900
Finished training it 16384/76743 of epoch 7, 146.61 ms/it, loss 0.463374
Finished training it 16384/76743 of epoch 7, 144.17 ms/it, loss 0.461947
Finished training it 16384/76743 of epoch 7, 146.81 ms/it, loss 0.465951
Finished training it 17408/76743 of epoch 7, 90.80 ms/it, loss 0.465484
Finished training it 17408/76743 of epoch 7, 90.65 ms/it, loss 0.464844
Finished training it 17408/76743 of epoch 7, 90.95 ms/it, loss 0.465955
Finished training it 17408/76743 of epoch 7, 90.67 ms/it, loss 0.465033
Finished training it 18432/76743 of epoch 7, 90.77 ms/it, loss 0.462035
Finished training it 18432/76743 of epoch 7, 90.95 ms/it, loss 0.464937
Finished training it 18432/76743 of epoch 7, 90.71 ms/it, loss 0.464671
Finished training it 18432/76743 of epoch 7, 90.79 ms/it, loss 0.462114
Finished training it 19456/76743 of epoch 7, 91.10 ms/it, loss 0.462363
Finished training it 19456/76743 of epoch 7, 90.86 ms/it, loss 0.462909
Finished training it 19456/76743 of epoch 7, 91.01 ms/it, loss 0.463382
Finished training it 19456/76743 of epoch 7, 91.00 ms/it, loss 0.463959
Finished training it 20480/76743 of epoch 7, 90.93 ms/it, loss 0.463374
Finished training it 20480/76743 of epoch 7, 90.79 ms/it, loss 0.465656
Finished training it 20480/76743 of epoch 7, 91.05 ms/it, loss 0.465270
Finished training it 20480/76743 of epoch 7, 90.96 ms/it, loss 0.461992
Testing at - 20480/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550449.0
get out
0 has test check 2550449.0 and sample count 3274240
 accuracy 77.894 %, best 77.896 %, roc auc score 0.7799, best 0.7799
Finished training it 21504/76743 of epoch 7, 88.17 ms/it, loss 0.460885
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550449.0
get out
1 has test check 2550449.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 88.12 ms/it, loss 0.462953
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550449.0
get out
3 has test check 2550449.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 88.44 ms/it, loss 0.465944
Testing at - 20480/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550449.0
get out
2 has test check 2550449.0 and sample count 3274240
Finished training it 21504/76743 of epoch 7, 88.39 ms/it, loss 0.463394
Finished training it 22528/76743 of epoch 7, 88.11 ms/it, loss 0.462562
Finished training it 22528/76743 of epoch 7, 88.16 ms/it, loss 0.464651
Finished training it 22528/76743 of epoch 7, 88.22 ms/it, loss 0.464645
Finished training it 22528/76743 of epoch 7, 88.14 ms/it, loss 0.464964
Finished training it 23552/76743 of epoch 7, 93.32 ms/it, loss 0.465531
Finished training it 23552/76743 of epoch 7, 93.24 ms/it, loss 0.464751
Finished training it 23552/76743 of epoch 7, 93.10 ms/it, loss 0.463375
Finished training it 23552/76743 of epoch 7, 93.18 ms/it, loss 0.464892
Finished training it 24576/76743 of epoch 7, 94.22 ms/it, loss 0.460365
Finished training it 24576/76743 of epoch 7, 94.62 ms/it, loss 0.464107
Finished training it 24576/76743 of epoch 7, 94.73 ms/it, loss 0.461898
Finished training it 24576/76743 of epoch 7, 94.10 ms/it, loss 0.466984
Finished training it 25600/76743 of epoch 7, 146.01 ms/it, loss 0.463653
Finished training it 25600/76743 of epoch 7, 145.59 ms/it, loss 0.463869
Finished training it 25600/76743 of epoch 7, 142.82 ms/it, loss 0.465092
Finished training it 25600/76743 of epoch 7, 146.49 ms/it, loss 0.463484
Finished training it 26624/76743 of epoch 7, 91.97 ms/it, loss 0.464170
Finished training it 26624/76743 of epoch 7, 92.08 ms/it, loss 0.464069
Finished training it 26624/76743 of epoch 7, 91.82 ms/it, loss 0.464276
Finished training it 26624/76743 of epoch 7, 91.74 ms/it, loss 0.463118
Finished training it 27648/76743 of epoch 7, 90.99 ms/it, loss 0.463025
Finished training it 27648/76743 of epoch 7, 90.83 ms/it, loss 0.464883
Finished training it 27648/76743 of epoch 7, 90.91 ms/it, loss 0.464993
Finished training it 27648/76743 of epoch 7, 90.67 ms/it, loss 0.461808
Finished training it 28672/76743 of epoch 7, 90.60 ms/it, loss 0.464141
Finished training it 28672/76743 of epoch 7, 90.67 ms/it, loss 0.463110
Finished training it 28672/76743 of epoch 7, 90.72 ms/it, loss 0.463671
Finished training it 28672/76743 of epoch 7, 90.45 ms/it, loss 0.464594
Finished training it 29696/76743 of epoch 7, 90.96 ms/it, loss 0.464041
Finished training it 29696/76743 of epoch 7, 91.07 ms/it, loss 0.462385
Finished training it 29696/76743 of epoch 7, 90.91 ms/it, loss 0.464402
Finished training it 29696/76743 of epoch 7, 90.93 ms/it, loss 0.464370
Finished training it 30720/76743 of epoch 7, 90.97 ms/it, loss 0.462444
Finished training it 30720/76743 of epoch 7, 90.88 ms/it, loss 0.467270
Finished training it 30720/76743 of epoch 7, 90.83 ms/it, loss 0.463862
Finished training it 30720/76743 of epoch 7, 90.82 ms/it, loss 0.462259
Testing at - 30720/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551063.0
get out
0 has test check 2551063.0 and sample count 3274240
 accuracy 77.913 %, best 77.913 %, roc auc score 0.7801, best 0.7801
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551063.0
get out
1 has test check 2551063.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 87.97 ms/it, loss 0.463833
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551063.0
get out
2 has test check 2551063.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 88.15 ms/it, loss 0.461946
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 7, 88.14 ms/it, loss 0.463769
Testing at - 30720/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551063.0
get out
3 has test check 2551063.0 and sample count 3274240
Finished training it 31744/76743 of epoch 7, 88.13 ms/it, loss 0.464334
Finished training it 32768/76743 of epoch 7, 88.33 ms/it, loss 0.462301
Finished training it 32768/76743 of epoch 7, 88.29 ms/it, loss 0.463247
Finished training it 32768/76743 of epoch 7, 88.29 ms/it, loss 0.466254
Finished training it 32768/76743 of epoch 7, 88.42 ms/it, loss 0.461195
Finished training it 33792/76743 of epoch 7, 88.20 ms/it, loss 0.461847
Finished training it 33792/76743 of epoch 7, 88.22 ms/it, loss 0.463218
Finished training it 33792/76743 of epoch 7, 88.18 ms/it, loss 0.462170
Finished training it 33792/76743 of epoch 7, 88.28 ms/it, loss 0.464360
Finished training it 34816/76743 of epoch 7, 112.14 ms/it, loss 0.465256
Finished training it 34816/76743 of epoch 7, 111.84 ms/it, loss 0.462454
Finished training it 34816/76743 of epoch 7, 111.96 ms/it, loss 0.460331
Finished training it 34816/76743 of epoch 7, 110.88 ms/it, loss 0.462837
Finished training it 35840/76743 of epoch 7, 112.42 ms/it, loss 0.464115
Finished training it 35840/76743 of epoch 7, 112.32 ms/it, loss 0.463844
Finished training it 35840/76743 of epoch 7, 110.72 ms/it, loss 0.463049
Finished training it 35840/76743 of epoch 7, 112.09 ms/it, loss 0.462017
Finished training it 36864/76743 of epoch 7, 90.77 ms/it, loss 0.465028
Finished training it 36864/76743 of epoch 7, 90.86 ms/it, loss 0.464562
Finished training it 36864/76743 of epoch 7, 90.77 ms/it, loss 0.465150
Finished training it 36864/76743 of epoch 7, 90.79 ms/it, loss 0.464097
Finished training it 37888/76743 of epoch 7, 90.70 ms/it, loss 0.463159
Finished training it 37888/76743 of epoch 7, 90.87 ms/it, loss 0.464693
Finished training it 37888/76743 of epoch 7, 90.70 ms/it, loss 0.465150
Finished training it 37888/76743 of epoch 7, 90.79 ms/it, loss 0.463179
Finished training it 38912/76743 of epoch 7, 90.76 ms/it, loss 0.464351
Finished training it 38912/76743 of epoch 7, 90.87 ms/it, loss 0.466940
Finished training it 38912/76743 of epoch 7, 90.79 ms/it, loss 0.463934
Finished training it 38912/76743 of epoch 7, 90.72 ms/it, loss 0.464453
Finished training it 39936/76743 of epoch 7, 90.63 ms/it, loss 0.462062
Finished training it 39936/76743 of epoch 7, 90.66 ms/it, loss 0.463336
Finished training it 39936/76743 of epoch 7, 90.80 ms/it, loss 0.463720
Finished training it 39936/76743 of epoch 7, 90.67 ms/it, loss 0.466231
Finished training it 40960/76743 of epoch 7, 91.10 ms/it, loss 0.462500
Finished training it 40960/76743 of epoch 7, 91.16 ms/it, loss 0.465397
Finished training it 40960/76743 of epoch 7, 90.93 ms/it, loss 0.463439
Finished training it 40960/76743 of epoch 7, 91.10 ms/it, loss 0.464380
Testing at - 40960/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550974.0
get out
0 has test check 2550974.0 and sample count 3274240
 accuracy 77.910 %, best 77.913 %, roc auc score 0.7805, best 0.7805
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550974.0
get out
1 has test check 2550974.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 88.09 ms/it, loss 0.462519
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550974.0
get out
2 has test check 2550974.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 88.27 ms/it, loss 0.461859
Testing at - 40960/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550974.0
get out
3 has test check 2550974.0 and sample count 3274240
Finished training it 41984/76743 of epoch 7, 88.48 ms/it, loss 0.464159
Finished training it 41984/76743 of epoch 7, 88.25 ms/it, loss 0.461512
Finished training it 43008/76743 of epoch 7, 88.13 ms/it, loss 0.465653
Finished training it 43008/76743 of epoch 7, 88.17 ms/it, loss 0.460978
Finished training it 43008/76743 of epoch 7, 88.06 ms/it, loss 0.466558
Finished training it 43008/76743 of epoch 7, 88.18 ms/it, loss 0.463813
Finished training it 44032/76743 of epoch 7, 88.17 ms/it, loss 0.463399
Finished training it 44032/76743 of epoch 7, 88.25 ms/it, loss 0.461538
Finished training it 44032/76743 of epoch 7, 88.06 ms/it, loss 0.465901
Finished training it 44032/76743 of epoch 7, 88.12 ms/it, loss 0.465213
Finished training it 45056/76743 of epoch 7, 139.41 ms/it, loss 0.463459
Finished training it 45056/76743 of epoch 7, 137.73 ms/it, loss 0.461267
Finished training it 45056/76743 of epoch 7, 139.92 ms/it, loss 0.463234
Finished training it 45056/76743 of epoch 7, 139.98 ms/it, loss 0.463066
Finished training it 46080/76743 of epoch 7, 91.14 ms/it, loss 0.462838
Finished training it 46080/76743 of epoch 7, 91.34 ms/it, loss 0.462165
Finished training it 46080/76743 of epoch 7, 91.17 ms/it, loss 0.466196
Finished training it 46080/76743 of epoch 7, 91.17 ms/it, loss 0.463285
Finished training it 47104/76743 of epoch 7, 90.74 ms/it, loss 0.461930
Finished training it 47104/76743 of epoch 7, 90.70 ms/it, loss 0.463596
Finished training it 47104/76743 of epoch 7, 90.76 ms/it, loss 0.465456
Finished training it 47104/76743 of epoch 7, 90.70 ms/it, loss 0.463367
Finished training it 48128/76743 of epoch 7, 90.75 ms/it, loss 0.466132
Finished training it 48128/76743 of epoch 7, 90.94 ms/it, loss 0.466137
Finished training it 48128/76743 of epoch 7, 90.63 ms/it, loss 0.463056
Finished training it 48128/76743 of epoch 7, 90.67 ms/it, loss 0.462973
Finished training it 49152/76743 of epoch 7, 91.02 ms/it, loss 0.461790
Finished training it 49152/76743 of epoch 7, 90.83 ms/it, loss 0.463794
Finished training it 49152/76743 of epoch 7, 90.65 ms/it, loss 0.463766
Finished training it 49152/76743 of epoch 7, 90.83 ms/it, loss 0.463595
Finished training it 50176/76743 of epoch 7, 90.79 ms/it, loss 0.464909
Finished training it 50176/76743 of epoch 7, 91.11 ms/it, loss 0.462221
Finished training it 50176/76743 of epoch 7, 90.93 ms/it, loss 0.464762
Finished training it 50176/76743 of epoch 7, 90.70 ms/it, loss 0.461516
Finished training it 51200/76743 of epoch 7, 90.77 ms/it, loss 0.462596
Finished training it 51200/76743 of epoch 7, 90.70 ms/it, loss 0.464548
Finished training it 51200/76743 of epoch 7, 90.72 ms/it, loss 0.464530
Finished training it 51200/76743 of epoch 7, 90.64 ms/it, loss 0.464566
Testing at - 51200/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551190.0
get out
0 has test check 2551190.0 and sample count 3274240
 accuracy 77.917 %, best 77.917 %, roc auc score 0.7803, best 0.7805
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551190.0
get out
1 has test check 2551190.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 88.27 ms/it, loss 0.461479
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 7, 88.37 ms/it, loss 0.461200
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551190.0
get out
2 has test check 2551190.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 88.38 ms/it, loss 0.464315
Testing at - 51200/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551190.0
get out
3 has test check 2551190.0 and sample count 3274240
Finished training it 52224/76743 of epoch 7, 88.45 ms/it, loss 0.461824
Finished training it 53248/76743 of epoch 7, 87.78 ms/it, loss 0.462210
Finished training it 53248/76743 of epoch 7, 88.00 ms/it, loss 0.466321
Finished training it 53248/76743 of epoch 7, 87.79 ms/it, loss 0.461677
Finished training it 53248/76743 of epoch 7, 87.97 ms/it, loss 0.463549
Finished training it 54272/76743 of epoch 7, 88.07 ms/it, loss 0.466468
Finished training it 54272/76743 of epoch 7, 88.21 ms/it, loss 0.461845
Finished training it 54272/76743 of epoch 7, 87.89 ms/it, loss 0.464107
Finished training it 54272/76743 of epoch 7, 87.95 ms/it, loss 0.465261
Finished training it 55296/76743 of epoch 7, 88.02 ms/it, loss 0.464977
Finished training it 55296/76743 of epoch 7, 88.26 ms/it, loss 0.461417
Finished training it 55296/76743 of epoch 7, 88.12 ms/it, loss 0.463525
Finished training it 55296/76743 of epoch 7, 88.17 ms/it, loss 0.463560
Finished training it 56320/76743 of epoch 7, 88.36 ms/it, loss 0.462786
Finished training it 56320/76743 of epoch 7, 88.24 ms/it, loss 0.463685
Finished training it 56320/76743 of epoch 7, 88.31 ms/it, loss 0.461853
Finished training it 56320/76743 of epoch 7, 88.47 ms/it, loss 0.465652
Finished training it 57344/76743 of epoch 7, 88.63 ms/it, loss 0.464243
Finished training it 57344/76743 of epoch 7, 88.84 ms/it, loss 0.463188
Finished training it 57344/76743 of epoch 7, 88.82 ms/it, loss 0.462430
Finished training it 57344/76743 of epoch 7, 89.03 ms/it, loss 0.462823
Finished training it 58368/76743 of epoch 7, 87.65 ms/it, loss 0.462378
Finished training it 58368/76743 of epoch 7, 87.96 ms/it, loss 0.463906
Finished training it 58368/76743 of epoch 7, 87.67 ms/it, loss 0.460831
Finished training it 58368/76743 of epoch 7, 87.85 ms/it, loss 0.465707
Finished training it 59392/76743 of epoch 7, 88.68 ms/it, loss 0.465062
Finished training it 59392/76743 of epoch 7, 88.62 ms/it, loss 0.461463
Finished training it 59392/76743 of epoch 7, 88.78 ms/it, loss 0.465623
Finished training it 59392/76743 of epoch 7, 88.94 ms/it, loss 0.463754
Finished training it 60416/76743 of epoch 7, 128.19 ms/it, loss 0.463267
Finished training it 60416/76743 of epoch 7, 130.12 ms/it, loss 0.462444
Finished training it 60416/76743 of epoch 7, 130.89 ms/it, loss 0.464077
Finished training it 60416/76743 of epoch 7, 130.78 ms/it, loss 0.465030
Finished training it 61440/76743 of epoch 7, 90.88 ms/it, loss 0.465229
Finished training it 61440/76743 of epoch 7, 90.93 ms/it, loss 0.464103
Finished training it 61440/76743 of epoch 7, 91.08 ms/it, loss 0.464415
Finished training it 61440/76743 of epoch 7, 90.78 ms/it, loss 0.464402
Testing at - 61440/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549633.0
get out
0 has test check 2549633.0 and sample count 3274240
 accuracy 77.869 %, best 77.917 %, roc auc score 0.7806, best 0.7806
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549633.0
get out
3 has test check 2549633.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 88.30 ms/it, loss 0.462482
Finished training it 62464/76743 of epoch 7, 88.41 ms/it, loss 0.462285
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549633.0
get out
2 has test check 2549633.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 88.41 ms/it, loss 0.464074
Testing at - 61440/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549633.0
get out
1 has test check 2549633.0 and sample count 3274240
Finished training it 62464/76743 of epoch 7, 88.14 ms/it, loss 0.462071
Finished training it 63488/76743 of epoch 7, 88.67 ms/it, loss 0.464051
Finished training it 63488/76743 of epoch 7, 88.56 ms/it, loss 0.462933
Finished training it 63488/76743 of epoch 7, 88.56 ms/it, loss 0.462768
Finished training it 63488/76743 of epoch 7, 88.40 ms/it, loss 0.463258
Finished training it 64512/76743 of epoch 7, 89.04 ms/it, loss 0.465269
Finished training it 64512/76743 of epoch 7, 89.21 ms/it, loss 0.461347
Finished training it 64512/76743 of epoch 7, 88.82 ms/it, loss 0.462607
Finished training it 64512/76743 of epoch 7, 88.89 ms/it, loss 0.464334
Finished training it 65536/76743 of epoch 7, 94.25 ms/it, loss 0.462774
Finished training it 65536/76743 of epoch 7, 93.80 ms/it, loss 0.467020
Finished training it 65536/76743 of epoch 7, 93.87 ms/it, loss 0.462446
Finished training it 65536/76743 of epoch 7, 93.73 ms/it, loss 0.462289
Finished training it 66560/76743 of epoch 7, 93.17 ms/it, loss 0.463631
Finished training it 66560/76743 of epoch 7, 93.54 ms/it, loss 0.462533
Finished training it 66560/76743 of epoch 7, 93.57 ms/it, loss 0.462670
Finished training it 66560/76743 of epoch 7, 94.09 ms/it, loss 0.461437
Finished training it 67584/76743 of epoch 7, 88.77 ms/it, loss 0.462510
Finished training it 67584/76743 of epoch 7, 88.72 ms/it, loss 0.462577
Finished training it 67584/76743 of epoch 7, 88.70 ms/it, loss 0.463974
Finished training it 67584/76743 of epoch 7, 88.74 ms/it, loss 0.465134
Finished training it 68608/76743 of epoch 7, 87.86 ms/it, loss 0.464863
Finished training it 68608/76743 of epoch 7, 88.02 ms/it, loss 0.462026
Finished training it 68608/76743 of epoch 7, 87.77 ms/it, loss 0.465566
Finished training it 68608/76743 of epoch 7, 87.92 ms/it, loss 0.461286
Finished training it 69632/76743 of epoch 7, 88.38 ms/it, loss 0.463928
Finished training it 69632/76743 of epoch 7, 88.28 ms/it, loss 0.459687
Finished training it 69632/76743 of epoch 7, 88.32 ms/it, loss 0.460717
Finished training it 69632/76743 of epoch 7, 88.33 ms/it, loss 0.464533
Finished training it 70656/76743 of epoch 7, 128.51 ms/it, loss 0.465780
Finished training it 70656/76743 of epoch 7, 126.26 ms/it, loss 0.463254
Finished training it 70656/76743 of epoch 7, 128.62 ms/it, loss 0.463928
Finished training it 70656/76743 of epoch 7, 127.89 ms/it, loss 0.461913
Finished training it 71680/76743 of epoch 7, 90.78 ms/it, loss 0.462526
Finished training it 71680/76743 of epoch 7, 90.90 ms/it, loss 0.462979
Finished training it 71680/76743 of epoch 7, 90.73 ms/it, loss 0.468639
Finished training it 71680/76743 of epoch 7, 90.72 ms/it, loss 0.465551
Testing at - 71680/76743 of epoch 7,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551899.0
get out
0 has test check 2551899.0 and sample count 3274240
 accuracy 77.939 %, best 77.939 %, roc auc score 0.7809, best 0.7809
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 7, 88.39 ms/it, loss 0.464249
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551899.0
get out
3 has test check 2551899.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 88.28 ms/it, loss 0.464031
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551899.0
get out
2 has test check 2551899.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 88.26 ms/it, loss 0.464288
Testing at - 71680/76743 of epoch 7,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551899.0
get out
1 has test check 2551899.0 and sample count 3274240
Finished training it 72704/76743 of epoch 7, 88.18 ms/it, loss 0.464325
Finished training it 73728/76743 of epoch 7, 88.16 ms/it, loss 0.461468
Finished training it 73728/76743 of epoch 7, 87.94 ms/it, loss 0.464827
Finished training it 73728/76743 of epoch 7, 88.11 ms/it, loss 0.463667
Finished training it 73728/76743 of epoch 7, 87.98 ms/it, loss 0.462644
Finished training it 74752/76743 of epoch 7, 88.18 ms/it, loss 0.462777
Finished training it 74752/76743 of epoch 7, 88.40 ms/it, loss 0.464771
Finished training it 74752/76743 of epoch 7, 88.29 ms/it, loss 0.465828
Finished training it 74752/76743 of epoch 7, 88.29 ms/it, loss 0.461445
Finished training it 75776/76743 of epoch 7, 88.29 ms/it, loss 0.461739
Finished training it 75776/76743 of epoch 7, 88.19 ms/it, loss 0.467102
Finished training it 75776/76743 of epoch 7, 88.14 ms/it, loss 0.460188
Finished training it 75776/76743 of epoch 7, 88.05 ms/it, loss 0.462267
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 89.55 ms/it, loss 0.463589
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 89.53 ms/it, loss 0.463285
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 89.31 ms/it, loss 0.463996
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 8, 89.57 ms/it, loss 0.464883
Finished training it 2048/76743 of epoch 8, 88.17 ms/it, loss 0.461400
Finished training it 2048/76743 of epoch 8, 88.14 ms/it, loss 0.461050
Finished training it 2048/76743 of epoch 8, 88.00 ms/it, loss 0.465282
Finished training it 2048/76743 of epoch 8, 88.07 ms/it, loss 0.462652
Finished training it 3072/76743 of epoch 8, 132.54 ms/it, loss 0.463444
Finished training it 3072/76743 of epoch 8, 133.71 ms/it, loss 0.465571
Finished training it 3072/76743 of epoch 8, 133.59 ms/it, loss 0.462863
Finished training it 3072/76743 of epoch 8, 130.42 ms/it, loss 0.461551
Finished training it 4096/76743 of epoch 8, 91.02 ms/it, loss 0.463118
Finished training it 4096/76743 of epoch 8, 90.95 ms/it, loss 0.464911
Finished training it 4096/76743 of epoch 8, 90.76 ms/it, loss 0.463996
Finished training it 4096/76743 of epoch 8, 90.84 ms/it, loss 0.465647
Finished training it 5120/76743 of epoch 8, 91.17 ms/it, loss 0.463094
Finished training it 5120/76743 of epoch 8, 90.83 ms/it, loss 0.459450
Finished training it 5120/76743 of epoch 8, 91.02 ms/it, loss 0.464159
Finished training it 5120/76743 of epoch 8, 90.89 ms/it, loss 0.467277
Finished training it 6144/76743 of epoch 8, 91.28 ms/it, loss 0.461786
Finished training it 6144/76743 of epoch 8, 91.20 ms/it, loss 0.462009
Finished training it 6144/76743 of epoch 8, 91.17 ms/it, loss 0.462811
Finished training it 6144/76743 of epoch 8, 91.04 ms/it, loss 0.461811
Finished training it 7168/76743 of epoch 8, 90.52 ms/it, loss 0.461937
Finished training it 7168/76743 of epoch 8, 90.67 ms/it, loss 0.461300
Finished training it 7168/76743 of epoch 8, 90.74 ms/it, loss 0.463004
Finished training it 7168/76743 of epoch 8, 90.61 ms/it, loss 0.465032
Finished training it 8192/76743 of epoch 8, 90.66 ms/it, loss 0.466014
Finished training it 8192/76743 of epoch 8, 90.70 ms/it, loss 0.462122
Finished training it 8192/76743 of epoch 8, 90.48 ms/it, loss 0.462620
Finished training it 8192/76743 of epoch 8, 90.52 ms/it, loss 0.460270
Finished training it 9216/76743 of epoch 8, 96.21 ms/it, loss 0.464514
Finished training it 9216/76743 of epoch 8, 96.85 ms/it, loss 0.463488
Finished training it 9216/76743 of epoch 8, 96.83 ms/it, loss 0.462315
Finished training it 9216/76743 of epoch 8, 96.52 ms/it, loss 0.464234
Finished training it 10240/76743 of epoch 8, 91.01 ms/it, loss 0.463810
Finished training it 10240/76743 of epoch 8, 90.86 ms/it, loss 0.461755
Finished training it 10240/76743 of epoch 8, 90.92 ms/it, loss 0.464709
Finished training it 10240/76743 of epoch 8, 90.80 ms/it, loss 0.461912
Testing at - 10240/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551501.0
get out
0 has test check 2551501.0 and sample count 3274240
 accuracy 77.927 %, best 77.939 %, roc auc score 0.7806, best 0.7809
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551501.0
get out
2 has test check 2551501.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 88.37 ms/it, loss 0.464397
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551501.0
get out
1 has test check 2551501.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 88.40 ms/it, loss 0.462462
Finished training it 11264/76743 of epoch 8, 88.48 ms/it, loss 0.464667
Testing at - 10240/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551501.0
get out
3 has test check 2551501.0 and sample count 3274240
Finished training it 11264/76743 of epoch 8, 88.53 ms/it, loss 0.464790
Finished training it 12288/76743 of epoch 8, 93.29 ms/it, loss 0.461356
Finished training it 12288/76743 of epoch 8, 93.78 ms/it, loss 0.463826
Finished training it 12288/76743 of epoch 8, 93.55 ms/it, loss 0.462210
Finished training it 12288/76743 of epoch 8, 93.67 ms/it, loss 0.463854
Finished training it 13312/76743 of epoch 8, 139.60 ms/it, loss 0.462501
Finished training it 13312/76743 of epoch 8, 139.60 ms/it, loss 0.463358
Finished training it 13312/76743 of epoch 8, 137.28 ms/it, loss 0.463066
Finished training it 13312/76743 of epoch 8, 138.78 ms/it, loss 0.463152
Finished training it 14336/76743 of epoch 8, 96.40 ms/it, loss 0.461516
Finished training it 14336/76743 of epoch 8, 96.13 ms/it, loss 0.464322
Finished training it 14336/76743 of epoch 8, 96.95 ms/it, loss 0.460260
Finished training it 14336/76743 of epoch 8, 96.19 ms/it, loss 0.464585
Finished training it 15360/76743 of epoch 8, 90.75 ms/it, loss 0.461069
Finished training it 15360/76743 of epoch 8, 90.81 ms/it, loss 0.462126
Finished training it 15360/76743 of epoch 8, 90.70 ms/it, loss 0.463426
Finished training it 15360/76743 of epoch 8, 90.66 ms/it, loss 0.464229
Finished training it 16384/76743 of epoch 8, 90.34 ms/it, loss 0.463363
Finished training it 16384/76743 of epoch 8, 90.40 ms/it, loss 0.464503
Finished training it 16384/76743 of epoch 8, 90.26 ms/it, loss 0.460586
Finished training it 16384/76743 of epoch 8, 90.40 ms/it, loss 0.461864
Finished training it 17408/76743 of epoch 8, 90.94 ms/it, loss 0.463791
Finished training it 17408/76743 of epoch 8, 91.03 ms/it, loss 0.464406
Finished training it 17408/76743 of epoch 8, 90.97 ms/it, loss 0.464562
Finished training it 17408/76743 of epoch 8, 90.92 ms/it, loss 0.463863
Finished training it 18432/76743 of epoch 8, 91.02 ms/it, loss 0.460935
Finished training it 18432/76743 of epoch 8, 90.97 ms/it, loss 0.463772
Finished training it 18432/76743 of epoch 8, 90.97 ms/it, loss 0.460707
Finished training it 18432/76743 of epoch 8, 91.17 ms/it, loss 0.463710
Finished training it 19456/76743 of epoch 8, 90.80 ms/it, loss 0.460983
Finished training it 19456/76743 of epoch 8, 90.77 ms/it, loss 0.461961
Finished training it 19456/76743 of epoch 8, 90.77 ms/it, loss 0.462994
Finished training it 19456/76743 of epoch 8, 90.77 ms/it, loss 0.462067
Finished training it 20480/76743 of epoch 8, 90.77 ms/it, loss 0.462333
Finished training it 20480/76743 of epoch 8, 90.95 ms/it, loss 0.464276
Finished training it 20480/76743 of epoch 8, 90.81 ms/it, loss 0.460666
Finished training it 20480/76743 of epoch 8, 90.76 ms/it, loss 0.464620
Testing at - 20480/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552705.0
get out
0 has test check 2552705.0 and sample count 3274240
 accuracy 77.963 %, best 77.963 %, roc auc score 0.7817, best 0.7817
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552705.0
get out
2 has test check 2552705.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 88.12 ms/it, loss 0.462486
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 8, 88.04 ms/it, loss 0.460145
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552705.0
get out
1 has test check 2552705.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 87.81 ms/it, loss 0.461814
Testing at - 20480/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552705.0
get out
3 has test check 2552705.0 and sample count 3274240
Finished training it 21504/76743 of epoch 8, 88.01 ms/it, loss 0.464712
Finished training it 22528/76743 of epoch 8, 88.53 ms/it, loss 0.463314
Finished training it 22528/76743 of epoch 8, 88.10 ms/it, loss 0.461038
Finished training it 22528/76743 of epoch 8, 88.44 ms/it, loss 0.463613
Finished training it 22528/76743 of epoch 8, 88.15 ms/it, loss 0.463519
Finished training it 23552/76743 of epoch 8, 127.80 ms/it, loss 0.463321
Finished training it 23552/76743 of epoch 8, 128.69 ms/it, loss 0.463279
Finished training it 23552/76743 of epoch 8, 128.60 ms/it, loss 0.464576
Finished training it 23552/76743 of epoch 8, 126.53 ms/it, loss 0.462271
Finished training it 24576/76743 of epoch 8, 90.72 ms/it, loss 0.461046
Finished training it 24576/76743 of epoch 8, 90.52 ms/it, loss 0.462929
Finished training it 24576/76743 of epoch 8, 90.56 ms/it, loss 0.465745
Finished training it 24576/76743 of epoch 8, 90.46 ms/it, loss 0.459401
Finished training it 25600/76743 of epoch 8, 90.86 ms/it, loss 0.462612
Finished training it 25600/76743 of epoch 8, 91.02 ms/it, loss 0.462606
Finished training it 25600/76743 of epoch 8, 91.13 ms/it, loss 0.462526
Finished training it 25600/76743 of epoch 8, 90.96 ms/it, loss 0.464028
Finished training it 26624/76743 of epoch 8, 90.29 ms/it, loss 0.463113
Finished training it 26624/76743 of epoch 8, 90.41 ms/it, loss 0.462561
Finished training it 26624/76743 of epoch 8, 90.31 ms/it, loss 0.463128
Finished training it 26624/76743 of epoch 8, 90.34 ms/it, loss 0.462140
Finished training it 27648/76743 of epoch 8, 90.38 ms/it, loss 0.464231
Finished training it 27648/76743 of epoch 8, 90.45 ms/it, loss 0.464028
Finished training it 27648/76743 of epoch 8, 90.46 ms/it, loss 0.462101
Finished training it 27648/76743 of epoch 8, 90.52 ms/it, loss 0.460891
Finished training it 28672/76743 of epoch 8, 90.62 ms/it, loss 0.463176
Finished training it 28672/76743 of epoch 8, 90.62 ms/it, loss 0.462077
Finished training it 28672/76743 of epoch 8, 90.93 ms/it, loss 0.462444
Finished training it 28672/76743 of epoch 8, 90.71 ms/it, loss 0.463164
Finished training it 29696/76743 of epoch 8, 90.76 ms/it, loss 0.461197
Finished training it 29696/76743 of epoch 8, 90.57 ms/it, loss 0.462760
Finished training it 29696/76743 of epoch 8, 90.71 ms/it, loss 0.463310
Finished training it 29696/76743 of epoch 8, 90.61 ms/it, loss 0.463588
Finished training it 30720/76743 of epoch 8, 90.69 ms/it, loss 0.462550
Finished training it 30720/76743 of epoch 8, 90.83 ms/it, loss 0.461003
Finished training it 30720/76743 of epoch 8, 91.00 ms/it, loss 0.465994
Finished training it 30720/76743 of epoch 8, 91.05 ms/it, loss 0.461118
Testing at - 30720/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552914.0
get out
0 has test check 2552914.0 and sample count 3274240
 accuracy 77.970 %, best 77.970 %, roc auc score 0.7817, best 0.7817
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552914.0
get out
3 has test check 2552914.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 88.44 ms/it, loss 0.463083
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552914.0
get out
1 has test check 2552914.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 88.42 ms/it, loss 0.463035
Testing at - 30720/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552914.0
get out
2 has test check 2552914.0 and sample count 3274240
Finished training it 31744/76743 of epoch 8, 88.33 ms/it, loss 0.461023
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 8, 88.34 ms/it, loss 0.462385
Finished training it 32768/76743 of epoch 8, 88.35 ms/it, loss 0.464792
Finished training it 32768/76743 of epoch 8, 88.49 ms/it, loss 0.462358
Finished training it 32768/76743 of epoch 8, 88.59 ms/it, loss 0.460408
Finished training it 32768/76743 of epoch 8, 88.43 ms/it, loss 0.461087
Finished training it 33792/76743 of epoch 8, 126.90 ms/it, loss 0.460734
Finished training it 33792/76743 of epoch 8, 125.29 ms/it, loss 0.460906
Finished training it 33792/76743 of epoch 8, 126.99 ms/it, loss 0.462195
Finished training it 33792/76743 of epoch 8, 126.44 ms/it, loss 0.463201
Finished training it 34816/76743 of epoch 8, 97.41 ms/it, loss 0.459576
Finished training it 34816/76743 of epoch 8, 96.82 ms/it, loss 0.464198
Finished training it 34816/76743 of epoch 8, 96.90 ms/it, loss 0.461655
Finished training it 34816/76743 of epoch 8, 96.99 ms/it, loss 0.461487
Finished training it 35840/76743 of epoch 8, 90.85 ms/it, loss 0.461269
Finished training it 35840/76743 of epoch 8, 90.76 ms/it, loss 0.462826
Finished training it 35840/76743 of epoch 8, 90.79 ms/it, loss 0.462576
Finished training it 35840/76743 of epoch 8, 90.76 ms/it, loss 0.461920
Finished training it 36864/76743 of epoch 8, 90.76 ms/it, loss 0.463226
Finished training it 36864/76743 of epoch 8, 90.74 ms/it, loss 0.463088
Finished training it 36864/76743 of epoch 8, 90.65 ms/it, loss 0.464143
Finished training it 36864/76743 of epoch 8, 90.61 ms/it, loss 0.463799
Finished training it 37888/76743 of epoch 8, 90.61 ms/it, loss 0.463611
Finished training it 37888/76743 of epoch 8, 90.83 ms/it, loss 0.461977
Finished training it 37888/76743 of epoch 8, 90.63 ms/it, loss 0.462152
Finished training it 37888/76743 of epoch 8, 90.88 ms/it, loss 0.463306
Finished training it 38912/76743 of epoch 8, 91.07 ms/it, loss 0.465660
Finished training it 38912/76743 of epoch 8, 91.21 ms/it, loss 0.463765
Finished training it 38912/76743 of epoch 8, 91.02 ms/it, loss 0.462749
Finished training it 38912/76743 of epoch 8, 90.98 ms/it, loss 0.463680
Finished training it 39936/76743 of epoch 8, 90.99 ms/it, loss 0.464815
Finished training it 39936/76743 of epoch 8, 90.81 ms/it, loss 0.462227
Finished training it 39936/76743 of epoch 8, 90.74 ms/it, loss 0.462889
Finished training it 39936/76743 of epoch 8, 90.82 ms/it, loss 0.460519
Finished training it 40960/76743 of epoch 8, 90.60 ms/it, loss 0.461255
Finished training it 40960/76743 of epoch 8, 90.49 ms/it, loss 0.462471
Finished training it 40960/76743 of epoch 8, 90.75 ms/it, loss 0.464204
Finished training it 40960/76743 of epoch 8, 90.48 ms/it, loss 0.462847
Testing at - 40960/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553126.0
get out
0 has test check 2553126.0 and sample count 3274240
 accuracy 77.976 %, best 77.976 %, roc auc score 0.7818, best 0.7818
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553126.0
get out
3 has test check 2553126.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 88.83 ms/it, loss 0.462549
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 8, 89.03 ms/it, loss 0.460317
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553126.0
get out
2 has test check 2553126.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 88.96 ms/it, loss 0.460497
Testing at - 40960/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553126.0
get out
1 has test check 2553126.0 and sample count 3274240
Finished training it 41984/76743 of epoch 8, 88.50 ms/it, loss 0.460736
Finished training it 43008/76743 of epoch 8, 130.64 ms/it, loss 0.459592
Finished training it 43008/76743 of epoch 8, 131.54 ms/it, loss 0.465643
Finished training it 43008/76743 of epoch 8, 129.28 ms/it, loss 0.463598
Finished training it 43008/76743 of epoch 8, 131.10 ms/it, loss 0.461994
Finished training it 44032/76743 of epoch 8, 90.92 ms/it, loss 0.461832
Finished training it 44032/76743 of epoch 8, 91.02 ms/it, loss 0.460017
Finished training it 44032/76743 of epoch 8, 90.95 ms/it, loss 0.464470
Finished training it 44032/76743 of epoch 8, 90.85 ms/it, loss 0.464032
Finished training it 45056/76743 of epoch 8, 90.50 ms/it, loss 0.460247
Finished training it 45056/76743 of epoch 8, 90.58 ms/it, loss 0.462008
Finished training it 45056/76743 of epoch 8, 90.63 ms/it, loss 0.461899
Finished training it 45056/76743 of epoch 8, 90.40 ms/it, loss 0.461429
Finished training it 46080/76743 of epoch 8, 90.85 ms/it, loss 0.460692
Finished training it 46080/76743 of epoch 8, 90.79 ms/it, loss 0.464787
Finished training it 46080/76743 of epoch 8, 90.75 ms/it, loss 0.461540
Finished training it 46080/76743 of epoch 8, 90.70 ms/it, loss 0.461354
Finished training it 47104/76743 of epoch 8, 90.88 ms/it, loss 0.464014
Finished training it 47104/76743 of epoch 8, 91.06 ms/it, loss 0.460597
Finished training it 47104/76743 of epoch 8, 90.87 ms/it, loss 0.461997
Finished training it 47104/76743 of epoch 8, 91.11 ms/it, loss 0.462616
Finished training it 48128/76743 of epoch 8, 90.18 ms/it, loss 0.464536
Finished training it 48128/76743 of epoch 8, 90.40 ms/it, loss 0.464714
Finished training it 48128/76743 of epoch 8, 90.02 ms/it, loss 0.461643
Finished training it 48128/76743 of epoch 8, 90.16 ms/it, loss 0.461994
Finished training it 49152/76743 of epoch 8, 90.93 ms/it, loss 0.462243
Finished training it 49152/76743 of epoch 8, 90.95 ms/it, loss 0.461960
Finished training it 49152/76743 of epoch 8, 91.02 ms/it, loss 0.460553
Finished training it 49152/76743 of epoch 8, 91.04 ms/it, loss 0.462810
Finished training it 50176/76743 of epoch 8, 90.44 ms/it, loss 0.460138
Finished training it 50176/76743 of epoch 8, 90.69 ms/it, loss 0.461215
Finished training it 50176/76743 of epoch 8, 90.56 ms/it, loss 0.463569
Finished training it 50176/76743 of epoch 8, 90.54 ms/it, loss 0.463476
Finished training it 51200/76743 of epoch 8, 90.44 ms/it, loss 0.463780
Finished training it 51200/76743 of epoch 8, 90.24 ms/it, loss 0.463255
Finished training it 51200/76743 of epoch 8, 90.44 ms/it, loss 0.461619
Finished training it 51200/76743 of epoch 8, 90.40 ms/it, loss 0.463380
Testing at - 51200/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553408.0
get out
0 has test check 2553408.0 and sample count 3274240
 accuracy 77.985 %, best 77.985 %, roc auc score 0.7821, best 0.7821
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553408.0
get out
3 has test check 2553408.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 88.63 ms/it, loss 0.460781
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553408.0
get out
1 has test check 2553408.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 88.41 ms/it, loss 0.460102
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 8, 88.51 ms/it, loss 0.459905
Testing at - 51200/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553408.0
get out
2 has test check 2553408.0 and sample count 3274240
Finished training it 52224/76743 of epoch 8, 88.40 ms/it, loss 0.462827
Finished training it 53248/76743 of epoch 8, 88.36 ms/it, loss 0.462271
Finished training it 53248/76743 of epoch 8, 88.42 ms/it, loss 0.464792
Finished training it 53248/76743 of epoch 8, 88.21 ms/it, loss 0.461191
Finished training it 53248/76743 of epoch 8, 88.08 ms/it, loss 0.460498
Finished training it 54272/76743 of epoch 8, 88.41 ms/it, loss 0.463974
Finished training it 54272/76743 of epoch 8, 88.62 ms/it, loss 0.465176
Finished training it 54272/76743 of epoch 8, 88.69 ms/it, loss 0.462766
Finished training it 54272/76743 of epoch 8, 88.54 ms/it, loss 0.460941
Finished training it 55296/76743 of epoch 8, 93.35 ms/it, loss 0.462342
Finished training it 55296/76743 of epoch 8, 93.57 ms/it, loss 0.460119
Finished training it 55296/76743 of epoch 8, 93.24 ms/it, loss 0.462667
Finished training it 55296/76743 of epoch 8, 93.18 ms/it, loss 0.463443
Finished training it 56320/76743 of epoch 8, 94.65 ms/it, loss 0.464488
Finished training it 56320/76743 of epoch 8, 94.57 ms/it, loss 0.462461
Finished training it 56320/76743 of epoch 8, 94.52 ms/it, loss 0.459973
Finished training it 56320/76743 of epoch 8, 94.06 ms/it, loss 0.461596
Finished training it 57344/76743 of epoch 8, 88.81 ms/it, loss 0.462016
Finished training it 57344/76743 of epoch 8, 88.52 ms/it, loss 0.461647
Finished training it 57344/76743 of epoch 8, 88.74 ms/it, loss 0.461368
Finished training it 57344/76743 of epoch 8, 88.51 ms/it, loss 0.463079
Finished training it 58368/76743 of epoch 8, 114.16 ms/it, loss 0.463962
Finished training it 58368/76743 of epoch 8, 115.51 ms/it, loss 0.462320
Finished training it 58368/76743 of epoch 8, 115.72 ms/it, loss 0.460676
Finished training it 58368/76743 of epoch 8, 115.33 ms/it, loss 0.459622
Finished training it 59392/76743 of epoch 8, 90.76 ms/it, loss 0.462313
Finished training it 59392/76743 of epoch 8, 90.65 ms/it, loss 0.464293
Finished training it 59392/76743 of epoch 8, 90.66 ms/it, loss 0.460016
Finished training it 59392/76743 of epoch 8, 90.85 ms/it, loss 0.463669
Finished training it 60416/76743 of epoch 8, 90.73 ms/it, loss 0.463497
Finished training it 60416/76743 of epoch 8, 90.94 ms/it, loss 0.461641
Finished training it 60416/76743 of epoch 8, 90.83 ms/it, loss 0.464289
Finished training it 60416/76743 of epoch 8, 90.78 ms/it, loss 0.462081
Finished training it 61440/76743 of epoch 8, 90.50 ms/it, loss 0.463388
Finished training it 61440/76743 of epoch 8, 90.62 ms/it, loss 0.463159
Finished training it 61440/76743 of epoch 8, 90.68 ms/it, loss 0.463287
Finished training it 61440/76743 of epoch 8, 90.62 ms/it, loss 0.464492
Testing at - 61440/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551053.0
get out
0 has test check 2551053.0 and sample count 3274240
 accuracy 77.913 %, best 77.985 %, roc auc score 0.7813, best 0.7821
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551053.0
get out
2 has test check 2551053.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 88.52 ms/it, loss 0.463224
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551053.0
get out
3 has test check 2551053.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 88.53 ms/it, loss 0.462180
Finished training it 62464/76743 of epoch 8, 88.41 ms/it, loss 0.461135
Testing at - 61440/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551053.0
get out
1 has test check 2551053.0 and sample count 3274240
Finished training it 62464/76743 of epoch 8, 88.47 ms/it, loss 0.460881
Finished training it 63488/76743 of epoch 8, 88.41 ms/it, loss 0.463034
Finished training it 63488/76743 of epoch 8, 88.33 ms/it, loss 0.461778
Finished training it 63488/76743 of epoch 8, 88.20 ms/it, loss 0.461424
Finished training it 63488/76743 of epoch 8, 88.08 ms/it, loss 0.461799
Finished training it 64512/76743 of epoch 8, 88.45 ms/it, loss 0.463668
Finished training it 64512/76743 of epoch 8, 88.50 ms/it, loss 0.463307
Finished training it 64512/76743 of epoch 8, 88.27 ms/it, loss 0.461008
Finished training it 64512/76743 of epoch 8, 88.48 ms/it, loss 0.459826
Finished training it 65536/76743 of epoch 8, 88.50 ms/it, loss 0.461401
Finished training it 65536/76743 of epoch 8, 88.44 ms/it, loss 0.461373
Finished training it 65536/76743 of epoch 8, 88.41 ms/it, loss 0.460721
Finished training it 65536/76743 of epoch 8, 88.29 ms/it, loss 0.465664
Finished training it 66560/76743 of epoch 8, 87.83 ms/it, loss 0.461092
Finished training it 66560/76743 of epoch 8, 87.88 ms/it, loss 0.462326
Finished training it 66560/76743 of epoch 8, 88.18 ms/it, loss 0.460192
Finished training it 66560/76743 of epoch 8, 88.05 ms/it, loss 0.461295
Finished training it 67584/76743 of epoch 8, 88.43 ms/it, loss 0.463766
Finished training it 67584/76743 of epoch 8, 88.25 ms/it, loss 0.462707
Finished training it 67584/76743 of epoch 8, 88.39 ms/it, loss 0.461053
Finished training it 67584/76743 of epoch 8, 88.36 ms/it, loss 0.461317
Finished training it 68608/76743 of epoch 8, 153.16 ms/it, loss 0.463736
Finished training it 68608/76743 of epoch 8, 152.36 ms/it, loss 0.464311
Finished training it 68608/76743 of epoch 8, 149.27 ms/it, loss 0.459810
Finished training it 68608/76743 of epoch 8, 151.99 ms/it, loss 0.460929
Finished training it 69632/76743 of epoch 8, 90.82 ms/it, loss 0.463523
Finished training it 69632/76743 of epoch 8, 91.00 ms/it, loss 0.462994
Finished training it 69632/76743 of epoch 8, 90.85 ms/it, loss 0.459034
Finished training it 69632/76743 of epoch 8, 90.68 ms/it, loss 0.459208
Finished training it 70656/76743 of epoch 8, 91.01 ms/it, loss 0.463112
Finished training it 70656/76743 of epoch 8, 90.84 ms/it, loss 0.461565
Finished training it 70656/76743 of epoch 8, 91.10 ms/it, loss 0.460580
Finished training it 70656/76743 of epoch 8, 90.85 ms/it, loss 0.464131
Finished training it 71680/76743 of epoch 8, 90.74 ms/it, loss 0.466929
Finished training it 71680/76743 of epoch 8, 90.78 ms/it, loss 0.461101
Finished training it 71680/76743 of epoch 8, 90.82 ms/it, loss 0.461357
Finished training it 71680/76743 of epoch 8, 90.56 ms/it, loss 0.464120
Testing at - 71680/76743 of epoch 8,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553659.0
get out
0 has test check 2553659.0 and sample count 3274240
 accuracy 77.992 %, best 77.992 %, roc auc score 0.7826, best 0.7826
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553659.0
get out
2 has test check 2553659.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 88.40 ms/it, loss 0.463212
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553659.0
get out
3 has test check 2553659.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 88.61 ms/it, loss 0.462632
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 8, 88.38 ms/it, loss 0.462965
Testing at - 71680/76743 of epoch 8,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553659.0
get out
1 has test check 2553659.0 and sample count 3274240
Finished training it 72704/76743 of epoch 8, 88.43 ms/it, loss 0.462948
Finished training it 73728/76743 of epoch 8, 88.64 ms/it, loss 0.459929
Finished training it 73728/76743 of epoch 8, 88.68 ms/it, loss 0.463236
Finished training it 73728/76743 of epoch 8, 88.73 ms/it, loss 0.462070
Finished training it 73728/76743 of epoch 8, 88.69 ms/it, loss 0.461696
Finished training it 74752/76743 of epoch 8, 88.08 ms/it, loss 0.464383
Finished training it 74752/76743 of epoch 8, 88.05 ms/it, loss 0.460537
Finished training it 74752/76743 of epoch 8, 88.01 ms/it, loss 0.461500
Finished training it 74752/76743 of epoch 8, 88.30 ms/it, loss 0.463777
Finished training it 75776/76743 of epoch 8, 94.43 ms/it, loss 0.459025
Finished training it 75776/76743 of epoch 8, 95.00 ms/it, loss 0.460873
Finished training it 75776/76743 of epoch 8, 94.55 ms/it, loss 0.465486
Finished training it 75776/76743 of epoch 8, 94.58 ms/it, loss 0.460741
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 95.22 ms/it, loss 0.461932
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 95.18 ms/it, loss 0.463543
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 95.32 ms/it, loss 0.462186
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 9, 94.61 ms/it, loss 0.462468
Finished training it 2048/76743 of epoch 9, 115.23 ms/it, loss 0.460201
Finished training it 2048/76743 of epoch 9, 115.44 ms/it, loss 0.463885
Finished training it 2048/76743 of epoch 9, 114.94 ms/it, loss 0.460384
Finished training it 2048/76743 of epoch 9, 114.10 ms/it, loss 0.461511
Finished training it 3072/76743 of epoch 9, 91.00 ms/it, loss 0.462663
Finished training it 3072/76743 of epoch 9, 90.78 ms/it, loss 0.461603
Finished training it 3072/76743 of epoch 9, 90.89 ms/it, loss 0.464604
Finished training it 3072/76743 of epoch 9, 90.71 ms/it, loss 0.460719
Finished training it 4096/76743 of epoch 9, 90.78 ms/it, loss 0.462080
Finished training it 4096/76743 of epoch 9, 90.69 ms/it, loss 0.462609
Finished training it 4096/76743 of epoch 9, 90.84 ms/it, loss 0.463746
Finished training it 4096/76743 of epoch 9, 90.75 ms/it, loss 0.464252
Finished training it 5120/76743 of epoch 9, 90.70 ms/it, loss 0.461345
Finished training it 5120/76743 of epoch 9, 90.52 ms/it, loss 0.462599
Finished training it 5120/76743 of epoch 9, 90.40 ms/it, loss 0.458184
Finished training it 5120/76743 of epoch 9, 90.44 ms/it, loss 0.466002
Finished training it 6144/76743 of epoch 9, 90.66 ms/it, loss 0.460823
Finished training it 6144/76743 of epoch 9, 90.88 ms/it, loss 0.460386
Finished training it 6144/76743 of epoch 9, 90.91 ms/it, loss 0.460332
Finished training it 6144/76743 of epoch 9, 90.82 ms/it, loss 0.461267
Finished training it 7168/76743 of epoch 9, 90.53 ms/it, loss 0.460745
Finished training it 7168/76743 of epoch 9, 90.61 ms/it, loss 0.459957
Finished training it 7168/76743 of epoch 9, 90.69 ms/it, loss 0.463258
Finished training it 7168/76743 of epoch 9, 90.65 ms/it, loss 0.461675
Finished training it 8192/76743 of epoch 9, 90.15 ms/it, loss 0.464718
Finished training it 8192/76743 of epoch 9, 90.03 ms/it, loss 0.461164
Finished training it 8192/76743 of epoch 9, 90.06 ms/it, loss 0.459109
Finished training it 8192/76743 of epoch 9, 90.25 ms/it, loss 0.460900
Finished training it 9216/76743 of epoch 9, 90.67 ms/it, loss 0.463071
Finished training it 9216/76743 of epoch 9, 90.46 ms/it, loss 0.463010
Finished training it 9216/76743 of epoch 9, 90.71 ms/it, loss 0.462126
Finished training it 9216/76743 of epoch 9, 90.55 ms/it, loss 0.460848
Finished training it 10240/76743 of epoch 9, 90.32 ms/it, loss 0.460568
Finished training it 10240/76743 of epoch 9, 90.35 ms/it, loss 0.460395
Finished training it 10240/76743 of epoch 9, 90.34 ms/it, loss 0.463448
Finished training it 10240/76743 of epoch 9, 90.42 ms/it, loss 0.462313
Testing at - 10240/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555044.0
get out
0 has test check 2555044.0 and sample count 3274240
 accuracy 78.035 %, best 78.035 %, roc auc score 0.7831, best 0.7831
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 9, 109.82 ms/it, loss 0.463028
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555044.0
get out
3 has test check 2555044.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 110.11 ms/it, loss 0.463410
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555044.0
get out
2 has test check 2555044.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 109.11 ms/it, loss 0.463143
Testing at - 10240/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555044.0
get out
1 has test check 2555044.0 and sample count 3274240
Finished training it 11264/76743 of epoch 9, 110.23 ms/it, loss 0.461113
Finished training it 12288/76743 of epoch 9, 90.65 ms/it, loss 0.462284
Finished training it 12288/76743 of epoch 9, 90.59 ms/it, loss 0.460972
Finished training it 12288/76743 of epoch 9, 90.44 ms/it, loss 0.460243
Finished training it 12288/76743 of epoch 9, 90.50 ms/it, loss 0.462201
Finished training it 13312/76743 of epoch 9, 91.00 ms/it, loss 0.461550
Finished training it 13312/76743 of epoch 9, 90.95 ms/it, loss 0.462089
Finished training it 13312/76743 of epoch 9, 90.93 ms/it, loss 0.461397
Finished training it 13312/76743 of epoch 9, 90.87 ms/it, loss 0.461102
Finished training it 14336/76743 of epoch 9, 90.91 ms/it, loss 0.459046
Finished training it 14336/76743 of epoch 9, 91.02 ms/it, loss 0.460041
Finished training it 14336/76743 of epoch 9, 90.81 ms/it, loss 0.462823
Finished training it 14336/76743 of epoch 9, 90.84 ms/it, loss 0.462891
Finished training it 15360/76743 of epoch 9, 90.82 ms/it, loss 0.459461
Finished training it 15360/76743 of epoch 9, 90.69 ms/it, loss 0.461507
Finished training it 15360/76743 of epoch 9, 90.97 ms/it, loss 0.460308
Finished training it 15360/76743 of epoch 9, 90.93 ms/it, loss 0.462918
Finished training it 16384/76743 of epoch 9, 91.24 ms/it, loss 0.460409
Finished training it 16384/76743 of epoch 9, 91.09 ms/it, loss 0.463519
Finished training it 16384/76743 of epoch 9, 91.06 ms/it, loss 0.458880
Finished training it 16384/76743 of epoch 9, 90.99 ms/it, loss 0.461975
Finished training it 17408/76743 of epoch 9, 90.55 ms/it, loss 0.462588
Finished training it 17408/76743 of epoch 9, 90.68 ms/it, loss 0.462362
Finished training it 17408/76743 of epoch 9, 90.80 ms/it, loss 0.463141
Finished training it 17408/76743 of epoch 9, 90.55 ms/it, loss 0.462689
Finished training it 18432/76743 of epoch 9, 91.06 ms/it, loss 0.462922
Finished training it 18432/76743 of epoch 9, 91.23 ms/it, loss 0.462585
Finished training it 18432/76743 of epoch 9, 91.14 ms/it, loss 0.459599
Finished training it 18432/76743 of epoch 9, 90.84 ms/it, loss 0.459420
Finished training it 19456/76743 of epoch 9, 90.63 ms/it, loss 0.461775
Finished training it 19456/76743 of epoch 9, 90.44 ms/it, loss 0.460918
Finished training it 19456/76743 of epoch 9, 90.53 ms/it, loss 0.461041
Finished training it 19456/76743 of epoch 9, 90.81 ms/it, loss 0.459451
Finished training it 20480/76743 of epoch 9, 90.77 ms/it, loss 0.460831
Finished training it 20480/76743 of epoch 9, 90.67 ms/it, loss 0.463119
Finished training it 20480/76743 of epoch 9, 90.75 ms/it, loss 0.459469
Finished training it 20480/76743 of epoch 9, 90.80 ms/it, loss 0.463001
Testing at - 20480/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554054.0
get out
0 has test check 2554054.0 and sample count 3274240
 accuracy 78.004 %, best 78.035 %, roc auc score 0.7826, best 0.7831
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554054.0
get out
3 has test check 2554054.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 133.32 ms/it, loss 0.462909
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554054.0
get out
2 has test check 2554054.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 131.87 ms/it, loss 0.461263
Testing at - 20480/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554054.0
get out
1 has test check 2554054.0 and sample count 3274240
Finished training it 21504/76743 of epoch 9, 134.30 ms/it, loss 0.460199
Finished training it 21504/76743 of epoch 9, 134.37 ms/it, loss 0.458983
Finished training it 22528/76743 of epoch 9, 90.42 ms/it, loss 0.462380
Finished training it 22528/76743 of epoch 9, 90.64 ms/it, loss 0.462060
Finished training it 22528/76743 of epoch 9, 90.57 ms/it, loss 0.462607
Finished training it 22528/76743 of epoch 9, 90.45 ms/it, loss 0.459980
Finished training it 23552/76743 of epoch 9, 91.13 ms/it, loss 0.461187
Finished training it 23552/76743 of epoch 9, 91.20 ms/it, loss 0.461928
Finished training it 23552/76743 of epoch 9, 91.25 ms/it, loss 0.462288
Finished training it 23552/76743 of epoch 9, 91.08 ms/it, loss 0.463232
Finished training it 24576/76743 of epoch 9, 102.91 ms/it, loss 0.464999
Finished training it 24576/76743 of epoch 9, 103.06 ms/it, loss 0.459624
Finished training it 24576/76743 of epoch 9, 102.92 ms/it, loss 0.461968
Finished training it 24576/76743 of epoch 9, 102.85 ms/it, loss 0.458419
Finished training it 25600/76743 of epoch 9, 90.74 ms/it, loss 0.461312
Finished training it 25600/76743 of epoch 9, 90.56 ms/it, loss 0.461134
Finished training it 25600/76743 of epoch 9, 90.56 ms/it, loss 0.462440
Finished training it 25600/76743 of epoch 9, 90.66 ms/it, loss 0.460989
Finished training it 26624/76743 of epoch 9, 90.82 ms/it, loss 0.461871
Finished training it 26624/76743 of epoch 9, 90.88 ms/it, loss 0.461336
Finished training it 26624/76743 of epoch 9, 90.76 ms/it, loss 0.461461
Finished training it 26624/76743 of epoch 9, 90.67 ms/it, loss 0.460719
Finished training it 27648/76743 of epoch 9, 90.58 ms/it, loss 0.459567
Finished training it 27648/76743 of epoch 9, 90.89 ms/it, loss 0.462576
Finished training it 27648/76743 of epoch 9, 90.79 ms/it, loss 0.460786
Finished training it 27648/76743 of epoch 9, 90.67 ms/it, loss 0.462771
Finished training it 28672/76743 of epoch 9, 91.29 ms/it, loss 0.460598
Finished training it 28672/76743 of epoch 9, 91.20 ms/it, loss 0.460253
Finished training it 28672/76743 of epoch 9, 91.19 ms/it, loss 0.461932
Finished training it 28672/76743 of epoch 9, 91.38 ms/it, loss 0.461689
Finished training it 29696/76743 of epoch 9, 90.96 ms/it, loss 0.461126
Finished training it 29696/76743 of epoch 9, 91.01 ms/it, loss 0.462219
Finished training it 29696/76743 of epoch 9, 91.02 ms/it, loss 0.461895
Finished training it 29696/76743 of epoch 9, 91.04 ms/it, loss 0.459809
Finished training it 30720/76743 of epoch 9, 90.96 ms/it, loss 0.465055
Finished training it 30720/76743 of epoch 9, 90.75 ms/it, loss 0.461386
Finished training it 30720/76743 of epoch 9, 91.10 ms/it, loss 0.459948
Finished training it 30720/76743 of epoch 9, 90.89 ms/it, loss 0.460011
Testing at - 30720/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555764.0
get out
0 has test check 2555764.0 and sample count 3274240
 accuracy 78.057 %, best 78.057 %, roc auc score 0.7837, best 0.7837
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 9, 90.98 ms/it, loss 0.461795
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555764.0
get out
3 has test check 2555764.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 91.04 ms/it, loss 0.462026
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555764.0
get out
1 has test check 2555764.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 90.76 ms/it, loss 0.461168
Testing at - 30720/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555764.0
get out
2 has test check 2555764.0 and sample count 3274240
Finished training it 31744/76743 of epoch 9, 90.92 ms/it, loss 0.459626
Finished training it 32768/76743 of epoch 9, 90.62 ms/it, loss 0.459843
Finished training it 32768/76743 of epoch 9, 91.02 ms/it, loss 0.458858
Finished training it 32768/76743 of epoch 9, 90.78 ms/it, loss 0.463251
Finished training it 32768/76743 of epoch 9, 90.84 ms/it, loss 0.460516
Finished training it 33792/76743 of epoch 9, 90.83 ms/it, loss 0.461067
Finished training it 33792/76743 of epoch 9, 90.98 ms/it, loss 0.461989
Finished training it 33792/76743 of epoch 9, 90.81 ms/it, loss 0.458983
Finished training it 33792/76743 of epoch 9, 90.76 ms/it, loss 0.459414
Finished training it 34816/76743 of epoch 9, 90.86 ms/it, loss 0.458155
Finished training it 34816/76743 of epoch 9, 90.75 ms/it, loss 0.459890
Finished training it 34816/76743 of epoch 9, 90.77 ms/it, loss 0.462732
Finished training it 34816/76743 of epoch 9, 90.87 ms/it, loss 0.460073
Finished training it 35840/76743 of epoch 9, 90.87 ms/it, loss 0.459630
Finished training it 35840/76743 of epoch 9, 90.77 ms/it, loss 0.461486
Finished training it 35840/76743 of epoch 9, 90.65 ms/it, loss 0.461750
Finished training it 35840/76743 of epoch 9, 90.69 ms/it, loss 0.460561
Finished training it 36864/76743 of epoch 9, 90.52 ms/it, loss 0.462275
Finished training it 36864/76743 of epoch 9, 90.63 ms/it, loss 0.462918
Finished training it 36864/76743 of epoch 9, 90.63 ms/it, loss 0.461598
Finished training it 36864/76743 of epoch 9, 90.76 ms/it, loss 0.462058
Finished training it 37888/76743 of epoch 9, 90.95 ms/it, loss 0.461065
Finished training it 37888/76743 of epoch 9, 90.60 ms/it, loss 0.462509
Finished training it 37888/76743 of epoch 9, 90.82 ms/it, loss 0.460451
Finished training it 37888/76743 of epoch 9, 90.89 ms/it, loss 0.462463
Finished training it 38912/76743 of epoch 9, 91.03 ms/it, loss 0.461961
Finished training it 38912/76743 of epoch 9, 90.82 ms/it, loss 0.464170
Finished training it 38912/76743 of epoch 9, 90.79 ms/it, loss 0.462074
Finished training it 38912/76743 of epoch 9, 90.85 ms/it, loss 0.461078
Finished training it 39936/76743 of epoch 9, 90.81 ms/it, loss 0.459249
Finished training it 39936/76743 of epoch 9, 90.86 ms/it, loss 0.460678
Finished training it 39936/76743 of epoch 9, 90.97 ms/it, loss 0.463648
Finished training it 39936/76743 of epoch 9, 90.86 ms/it, loss 0.461457
Finished training it 40960/76743 of epoch 9, 91.02 ms/it, loss 0.462610
Finished training it 40960/76743 of epoch 9, 90.89 ms/it, loss 0.461452
Finished training it 40960/76743 of epoch 9, 90.89 ms/it, loss 0.459781
Finished training it 40960/76743 of epoch 9, 90.91 ms/it, loss 0.461069
Testing at - 40960/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554772.0
get out
0 has test check 2554772.0 and sample count 3274240
 accuracy 78.026 %, best 78.057 %, roc auc score 0.7837, best 0.7837
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554772.0
get out
3 has test check 2554772.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.61 ms/it, loss 0.461069
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554772.0
get out
1 has test check 2554772.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.32 ms/it, loss 0.458894
Testing at - 40960/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554772.0
get out
2 has test check 2554772.0 and sample count 3274240
Finished training it 41984/76743 of epoch 9, 90.49 ms/it, loss 0.458468
Finished training it 41984/76743 of epoch 9, 90.56 ms/it, loss 0.459049
Finished training it 43008/76743 of epoch 9, 91.09 ms/it, loss 0.458057
Finished training it 43008/76743 of epoch 9, 90.92 ms/it, loss 0.462153
Finished training it 43008/76743 of epoch 9, 90.84 ms/it, loss 0.464011
Finished training it 43008/76743 of epoch 9, 90.92 ms/it, loss 0.460134
Finished training it 44032/76743 of epoch 9, 96.50 ms/it, loss 0.460543
Finished training it 44032/76743 of epoch 9, 96.38 ms/it, loss 0.462134
Finished training it 44032/76743 of epoch 9, 96.64 ms/it, loss 0.458770
Finished training it 44032/76743 of epoch 9, 96.45 ms/it, loss 0.462753
Finished training it 45056/76743 of epoch 9, 95.99 ms/it, loss 0.458495
Finished training it 45056/76743 of epoch 9, 96.40 ms/it, loss 0.460217
Finished training it 45056/76743 of epoch 9, 96.18 ms/it, loss 0.460250
Finished training it 45056/76743 of epoch 9, 96.36 ms/it, loss 0.460724
Finished training it 46080/76743 of epoch 9, 90.64 ms/it, loss 0.463255
Finished training it 46080/76743 of epoch 9, 90.77 ms/it, loss 0.459269
Finished training it 46080/76743 of epoch 9, 90.51 ms/it, loss 0.460107
Finished training it 46080/76743 of epoch 9, 90.57 ms/it, loss 0.460248
Finished training it 47104/76743 of epoch 9, 88.18 ms/it, loss 0.458984
Finished training it 47104/76743 of epoch 9, 88.08 ms/it, loss 0.460899
Finished training it 47104/76743 of epoch 9, 88.18 ms/it, loss 0.461125
Finished training it 47104/76743 of epoch 9, 88.31 ms/it, loss 0.462446
Finished training it 48128/76743 of epoch 9, 88.52 ms/it, loss 0.463676
Finished training it 48128/76743 of epoch 9, 88.62 ms/it, loss 0.463311
Finished training it 48128/76743 of epoch 9, 88.23 ms/it, loss 0.460178
Finished training it 48128/76743 of epoch 9, 88.38 ms/it, loss 0.460758
Finished training it 49152/76743 of epoch 9, 88.26 ms/it, loss 0.461383
Finished training it 49152/76743 of epoch 9, 88.20 ms/it, loss 0.459092
Finished training it 49152/76743 of epoch 9, 88.07 ms/it, loss 0.460733
Finished training it 49152/76743 of epoch 9, 88.04 ms/it, loss 0.460843
Finished training it 50176/76743 of epoch 9, 88.74 ms/it, loss 0.462029
Finished training it 50176/76743 of epoch 9, 88.78 ms/it, loss 0.461892
Finished training it 50176/76743 of epoch 9, 88.83 ms/it, loss 0.459975
Finished training it 50176/76743 of epoch 9, 88.63 ms/it, loss 0.458514
Finished training it 51200/76743 of epoch 9, 87.68 ms/it, loss 0.461560
Finished training it 51200/76743 of epoch 9, 87.87 ms/it, loss 0.459507
Finished training it 51200/76743 of epoch 9, 87.70 ms/it, loss 0.462244
Finished training it 51200/76743 of epoch 9, 87.61 ms/it, loss 0.461770
Testing at - 51200/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555380.0
get out
0 has test check 2555380.0 and sample count 3274240
 accuracy 78.045 %, best 78.057 %, roc auc score 0.7835, best 0.7837
Finished training it 52224/76743 of epoch 9, 88.39 ms/it, loss 0.458766
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555380.0
get out
2 has test check 2555380.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 87.94 ms/it, loss 0.460976
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555380.0
get out
1 has test check 2555380.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 87.72 ms/it, loss 0.458725
Testing at - 51200/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555380.0
get out
3 has test check 2555380.0 and sample count 3274240
Finished training it 52224/76743 of epoch 9, 88.10 ms/it, loss 0.458712
Finished training it 53248/76743 of epoch 9, 87.62 ms/it, loss 0.463813
Finished training it 53248/76743 of epoch 9, 87.33 ms/it, loss 0.459193
Finished training it 53248/76743 of epoch 9, 87.88 ms/it, loss 0.460668
Finished training it 53248/76743 of epoch 9, 87.54 ms/it, loss 0.459797
Finished training it 54272/76743 of epoch 9, 87.99 ms/it, loss 0.461645
Finished training it 54272/76743 of epoch 9, 87.69 ms/it, loss 0.463570
Finished training it 54272/76743 of epoch 9, 87.57 ms/it, loss 0.462731
Finished training it 54272/76743 of epoch 9, 87.84 ms/it, loss 0.459609
Finished training it 55296/76743 of epoch 9, 87.83 ms/it, loss 0.461078
Finished training it 55296/76743 of epoch 9, 87.64 ms/it, loss 0.458768
Finished training it 55296/76743 of epoch 9, 87.78 ms/it, loss 0.460665
Finished training it 55296/76743 of epoch 9, 87.33 ms/it, loss 0.461937
Finished training it 56320/76743 of epoch 9, 88.48 ms/it, loss 0.459883
Finished training it 56320/76743 of epoch 9, 87.66 ms/it, loss 0.458438
Finished training it 56320/76743 of epoch 9, 87.93 ms/it, loss 0.462732
Finished training it 56320/76743 of epoch 9, 88.48 ms/it, loss 0.460642
Finished training it 57344/76743 of epoch 9, 88.91 ms/it, loss 0.459886
Finished training it 57344/76743 of epoch 9, 88.61 ms/it, loss 0.460297
Finished training it 57344/76743 of epoch 9, 88.31 ms/it, loss 0.461742
Finished training it 57344/76743 of epoch 9, 88.32 ms/it, loss 0.460427
Finished training it 58368/76743 of epoch 9, 87.23 ms/it, loss 0.459664
Finished training it 58368/76743 of epoch 9, 87.62 ms/it, loss 0.457956
Finished training it 58368/76743 of epoch 9, 87.40 ms/it, loss 0.460810
Finished training it 58368/76743 of epoch 9, 87.80 ms/it, loss 0.461993
Finished training it 59392/76743 of epoch 9, 87.86 ms/it, loss 0.462388
Finished training it 59392/76743 of epoch 9, 88.05 ms/it, loss 0.460103
Finished training it 59392/76743 of epoch 9, 88.19 ms/it, loss 0.462618
Finished training it 59392/76743 of epoch 9, 87.76 ms/it, loss 0.458617
Finished training it 60416/76743 of epoch 9, 87.68 ms/it, loss 0.460501
Finished training it 60416/76743 of epoch 9, 87.45 ms/it, loss 0.462715
Finished training it 60416/76743 of epoch 9, 87.42 ms/it, loss 0.459758
Finished training it 60416/76743 of epoch 9, 87.00 ms/it, loss 0.461661
Finished training it 61440/76743 of epoch 9, 87.46 ms/it, loss 0.461594
Finished training it 61440/76743 of epoch 9, 86.97 ms/it, loss 0.462588
Finished training it 61440/76743 of epoch 9, 87.00 ms/it, loss 0.460871
Finished training it 61440/76743 of epoch 9, 86.87 ms/it, loss 0.461414
Testing at - 61440/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553032.0
get out
0 has test check 2553032.0 and sample count 3274240
 accuracy 77.973 %, best 78.057 %, roc auc score 0.7836, best 0.7837
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553032.0
get out
1 has test check 2553032.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 86.89 ms/it, loss 0.459424
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553032.0
get out
2 has test check 2553032.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 86.89 ms/it, loss 0.461933
Finished training it 62464/76743 of epoch 9, 87.28 ms/it, loss 0.459339
Testing at - 61440/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553032.0
get out
3 has test check 2553032.0 and sample count 3274240
Finished training it 62464/76743 of epoch 9, 87.14 ms/it, loss 0.459951
Finished training it 63488/76743 of epoch 9, 86.59 ms/it, loss 0.460132
Finished training it 63488/76743 of epoch 9, 86.86 ms/it, loss 0.459859
Finished training it 63488/76743 of epoch 9, 86.72 ms/it, loss 0.461345
Finished training it 63488/76743 of epoch 9, 86.55 ms/it, loss 0.460446
Finished training it 64512/76743 of epoch 9, 87.20 ms/it, loss 0.458365
Finished training it 64512/76743 of epoch 9, 86.76 ms/it, loss 0.459331
Finished training it 64512/76743 of epoch 9, 87.19 ms/it, loss 0.461675
Finished training it 64512/76743 of epoch 9, 87.56 ms/it, loss 0.462029
Finished training it 65536/76743 of epoch 9, 97.24 ms/it, loss 0.460098
Finished training it 65536/76743 of epoch 9, 97.05 ms/it, loss 0.464272
Finished training it 65536/76743 of epoch 9, 96.58 ms/it, loss 0.459310
Finished training it 65536/76743 of epoch 9, 97.63 ms/it, loss 0.459969
Finished training it 66560/76743 of epoch 9, 87.95 ms/it, loss 0.460767
Finished training it 66560/76743 of epoch 9, 88.02 ms/it, loss 0.459757
Finished training it 66560/76743 of epoch 9, 87.82 ms/it, loss 0.458785
Finished training it 66560/76743 of epoch 9, 88.56 ms/it, loss 0.459435
Finished training it 67584/76743 of epoch 9, 88.05 ms/it, loss 0.462828
Finished training it 67584/76743 of epoch 9, 87.89 ms/it, loss 0.459759
Finished training it 67584/76743 of epoch 9, 87.57 ms/it, loss 0.461145
Finished training it 67584/76743 of epoch 9, 88.52 ms/it, loss 0.459834
Finished training it 68608/76743 of epoch 9, 88.11 ms/it, loss 0.458582
Finished training it 68608/76743 of epoch 9, 88.33 ms/it, loss 0.462894
Finished training it 68608/76743 of epoch 9, 87.75 ms/it, loss 0.462194
Finished training it 68608/76743 of epoch 9, 88.05 ms/it, loss 0.459590
Finished training it 69632/76743 of epoch 9, 88.13 ms/it, loss 0.457984
Finished training it 69632/76743 of epoch 9, 87.76 ms/it, loss 0.461475
Finished training it 69632/76743 of epoch 9, 87.92 ms/it, loss 0.462054
Finished training it 69632/76743 of epoch 9, 87.54 ms/it, loss 0.457768
Finished training it 70656/76743 of epoch 9, 88.13 ms/it, loss 0.460320
Finished training it 70656/76743 of epoch 9, 87.54 ms/it, loss 0.461357
Finished training it 70656/76743 of epoch 9, 87.76 ms/it, loss 0.459314
Finished training it 70656/76743 of epoch 9, 87.82 ms/it, loss 0.463157
Finished training it 71680/76743 of epoch 9, 88.22 ms/it, loss 0.462850
Finished training it 71680/76743 of epoch 9, 87.99 ms/it, loss 0.465760
Finished training it 71680/76743 of epoch 9, 87.91 ms/it, loss 0.460315
Finished training it 71680/76743 of epoch 9, 88.15 ms/it, loss 0.460322
Testing at - 71680/76743 of epoch 9,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2556012.0
get out
0 has test check 2556012.0 and sample count 3274240
 accuracy 78.064 %, best 78.064 %, roc auc score 0.7839, best 0.7839
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2556012.0
get out
1 has test check 2556012.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 86.98 ms/it, loss 0.461413
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2556012.0
get out
3 has test check 2556012.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 87.05 ms/it, loss 0.461257
Testing at - 71680/76743 of epoch 9,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2556012.0
get out
2 has test check 2556012.0 and sample count 3274240
Finished training it 72704/76743 of epoch 9, 87.30 ms/it, loss 0.461641
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 9, 87.13 ms/it, loss 0.461773
Finished training it 73728/76743 of epoch 9, 87.39 ms/it, loss 0.460386
Finished training it 73728/76743 of epoch 9, 87.54 ms/it, loss 0.461606
Finished training it 73728/76743 of epoch 9, 87.82 ms/it, loss 0.458653
Finished training it 73728/76743 of epoch 9, 87.53 ms/it, loss 0.462574
Finished training it 74752/76743 of epoch 9, 87.67 ms/it, loss 0.463190
Finished training it 74752/76743 of epoch 9, 87.42 ms/it, loss 0.459050
Finished training it 74752/76743 of epoch 9, 87.38 ms/it, loss 0.462532
Finished training it 74752/76743 of epoch 9, 87.25 ms/it, loss 0.460230
Finished training it 75776/76743 of epoch 9, 87.44 ms/it, loss 0.459702
Finished training it 75776/76743 of epoch 9, 88.24 ms/it, loss 0.460104
Finished training it 75776/76743 of epoch 9, 87.76 ms/it, loss 0.457827
Finished training it 75776/76743 of epoch 9, 87.37 ms/it, loss 0.464685
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555500.0
get out
0 has test check 2555500.0 and sample count 3274240
 accuracy 78.049 %, best 78.064 %, roc auc score 0.7835, best 0.7839
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555500.0
get out
2 has test check 2555500.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555500.0
get out
3 has test check 2555500.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 10,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555500.0
get out
1 has test check 2555500.0 and sample count 3274240
