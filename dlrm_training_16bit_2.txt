Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 16
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 16
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 16
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 16
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 16
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 16
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 16
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 16
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 16
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 16
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 16
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 16
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 16
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 16
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 16
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 16
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 16
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 16
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 16
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 16
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 16
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 16
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 16
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 16
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 16
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.69 ms/it, loss 0.514827
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 16
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 16
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 16
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 16
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 16
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 16
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 16
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 16
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 16
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 16
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 16
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 16
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 16
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 16
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 16
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 16
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 16
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 16
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 16
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 16
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 16
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 16
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 16
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 16
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 16
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 33.26 ms/it, loss 0.513141
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 16
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 16
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 16
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 16
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 16
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 16
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 16
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 16
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 16
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 16
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 16
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 16
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 16
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 16
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 16
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 16
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 16
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 16
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 16
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 16
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 16
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 16
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 16
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 16
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 16
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.22 ms/it, loss 0.510793
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 16
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 16
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 16
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 16
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 16
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 16
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 16
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 16
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 16
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 16
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 16
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 16
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 16
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 16
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 16
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 16
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 16
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 16
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 16
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 16
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 16
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 16
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 16
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 16
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 16
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 33.27 ms/it, loss 0.511781
Finished training it 2048/76743 of epoch 0, 32.28 ms/it, loss 0.500473
Finished training it 2048/76743 of epoch 0, 32.28 ms/it, loss 0.497773
Finished training it 2048/76743 of epoch 0, 32.11 ms/it, loss 0.498249
Finished training it 2048/76743 of epoch 0, 31.98 ms/it, loss 0.499621
Finished training it 3072/76743 of epoch 0, 32.17 ms/it, loss 0.493527
Finished training it 3072/76743 of epoch 0, 32.25 ms/it, loss 0.492125
Finished training it 3072/76743 of epoch 0, 32.29 ms/it, loss 0.491488
Finished training it 3072/76743 of epoch 0, 32.44 ms/it, loss 0.490342
Finished training it 4096/76743 of epoch 0, 32.09 ms/it, loss 0.483112
Finished training it 4096/76743 of epoch 0, 32.07 ms/it, loss 0.483832
Finished training it 4096/76743 of epoch 0, 31.97 ms/it, loss 0.482634
Finished training it 4096/76743 of epoch 0, 31.94 ms/it, loss 0.483014
Finished training it 5120/76743 of epoch 0, 32.27 ms/it, loss 0.478455
Finished training it 5120/76743 of epoch 0, 32.12 ms/it, loss 0.479574
Finished training it 5120/76743 of epoch 0, 32.20 ms/it, loss 0.480420
Finished training it 5120/76743 of epoch 0, 32.26 ms/it, loss 0.479727
Finished training it 6144/76743 of epoch 0, 32.03 ms/it, loss 0.476667
Finished training it 6144/76743 of epoch 0, 31.91 ms/it, loss 0.471358
Finished training it 6144/76743 of epoch 0, 31.93 ms/it, loss 0.474188
Finished training it 6144/76743 of epoch 0, 31.96 ms/it, loss 0.473543
Finished training it 7168/76743 of epoch 0, 32.20 ms/it, loss 0.470325
Finished training it 7168/76743 of epoch 0, 32.26 ms/it, loss 0.472006
Finished training it 7168/76743 of epoch 0, 31.99 ms/it, loss 0.472978
Finished training it 7168/76743 of epoch 0, 31.99 ms/it, loss 0.470245
Finished training it 8192/76743 of epoch 0, 31.86 ms/it, loss 0.468615
Finished training it 8192/76743 of epoch 0, 32.10 ms/it, loss 0.468829
Finished training it 8192/76743 of epoch 0, 32.11 ms/it, loss 0.466063
Finished training it 8192/76743 of epoch 0, 31.80 ms/it, loss 0.468636
Finished training it 9216/76743 of epoch 0, 32.06 ms/it, loss 0.465646
Finished training it 9216/76743 of epoch 0, 32.23 ms/it, loss 0.469111
Finished training it 9216/76743 of epoch 0, 31.92 ms/it, loss 0.465058
Finished training it 9216/76743 of epoch 0, 31.79 ms/it, loss 0.470870
Finished training it 10240/76743 of epoch 0, 32.19 ms/it, loss 0.465105
Finished training it 10240/76743 of epoch 0, 31.77 ms/it, loss 0.466527
Finished training it 10240/76743 of epoch 0, 32.14 ms/it, loss 0.468462
Finished training it 10240/76743 of epoch 0, 31.94 ms/it, loss 0.466651
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550559.0
get out
0 has test check 2550559.0 and sample count 3274240
 accuracy 77.898 %, best 77.898 %, roc auc score 0.7827, best 0.7827
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550559.0
get out
1 has test check 2550559.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 32.41 ms/it, loss 0.466291
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550559.0
get out
2 has test check 2550559.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 32.15 ms/it, loss 0.464037
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 32.26 ms/it, loss 0.464255
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550559.0
get out
3 has test check 2550559.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 32.04 ms/it, loss 0.463863
Finished training it 12288/76743 of epoch 0, 32.15 ms/it, loss 0.463401
Finished training it 12288/76743 of epoch 0, 32.62 ms/it, loss 0.463173
Finished training it 12288/76743 of epoch 0, 32.64 ms/it, loss 0.465775
Finished training it 12288/76743 of epoch 0, 32.25 ms/it, loss 0.463424
Finished training it 13312/76743 of epoch 0, 32.57 ms/it, loss 0.463866
Finished training it 13312/76743 of epoch 0, 32.51 ms/it, loss 0.459543
Finished training it 13312/76743 of epoch 0, 32.51 ms/it, loss 0.463017
Finished training it 13312/76743 of epoch 0, 32.53 ms/it, loss 0.460441
Finished training it 14336/76743 of epoch 0, 32.36 ms/it, loss 0.461009
Finished training it 14336/76743 of epoch 0, 32.38 ms/it, loss 0.461593
Finished training it 14336/76743 of epoch 0, 32.80 ms/it, loss 0.460027
Finished training it 14336/76743 of epoch 0, 32.60 ms/it, loss 0.459282
Finished training it 15360/76743 of epoch 0, 39.62 ms/it, loss 0.461273
Finished training it 15360/76743 of epoch 0, 39.27 ms/it, loss 0.458277
Finished training it 15360/76743 of epoch 0, 40.05 ms/it, loss 0.461250
Finished training it 15360/76743 of epoch 0, 37.63 ms/it, loss 0.460456
Finished training it 16384/76743 of epoch 0, 32.48 ms/it, loss 0.457459
Finished training it 16384/76743 of epoch 0, 32.30 ms/it, loss 0.463366
Finished training it 16384/76743 of epoch 0, 32.48 ms/it, loss 0.459894
Finished training it 16384/76743 of epoch 0, 32.26 ms/it, loss 0.460378
