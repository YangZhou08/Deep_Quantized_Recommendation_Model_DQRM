Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 56.42 ms/it, loss 0.518853
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 53.77 ms/it, loss 0.517802
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 58.34 ms/it, loss 0.518804
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 57.90 ms/it, loss 0.517532
Finished training it 2048/76743 of epoch 0, 52.27 ms/it, loss 0.498865
Finished training it 2048/76743 of epoch 0, 51.63 ms/it, loss 0.499217
Finished training it 2048/76743 of epoch 0, 52.35 ms/it, loss 0.500747
Finished training it 2048/76743 of epoch 0, 51.09 ms/it, loss 0.500275
Finished training it 3072/76743 of epoch 0, 52.56 ms/it, loss 0.491397
Finished training it 3072/76743 of epoch 0, 51.71 ms/it, loss 0.492603
Finished training it 3072/76743 of epoch 0, 52.43 ms/it, loss 0.493585
Finished training it 3072/76743 of epoch 0, 51.02 ms/it, loss 0.492038
Finished training it 4096/76743 of epoch 0, 52.44 ms/it, loss 0.483830
Finished training it 4096/76743 of epoch 0, 52.60 ms/it, loss 0.487045
Finished training it 4096/76743 of epoch 0, 51.94 ms/it, loss 0.484898
Finished training it 4096/76743 of epoch 0, 51.26 ms/it, loss 0.484601
Finished training it 5120/76743 of epoch 0, 51.97 ms/it, loss 0.483103
Finished training it 5120/76743 of epoch 0, 51.10 ms/it, loss 0.480771
Finished training it 5120/76743 of epoch 0, 52.67 ms/it, loss 0.484606
Finished training it 5120/76743 of epoch 0, 52.59 ms/it, loss 0.481647
Finished training it 6144/76743 of epoch 0, 52.07 ms/it, loss 0.475421
Finished training it 6144/76743 of epoch 0, 51.71 ms/it, loss 0.478111
Finished training it 6144/76743 of epoch 0, 51.07 ms/it, loss 0.479630
Finished training it 6144/76743 of epoch 0, 50.43 ms/it, loss 0.479870
Finished training it 7168/76743 of epoch 0, 51.74 ms/it, loss 0.479456
Finished training it 7168/76743 of epoch 0, 50.94 ms/it, loss 0.475221
Finished training it 7168/76743 of epoch 0, 50.21 ms/it, loss 0.477406
Finished training it 7168/76743 of epoch 0, 51.51 ms/it, loss 0.479548
Finished training it 8192/76743 of epoch 0, 49.68 ms/it, loss 0.475687
Finished training it 8192/76743 of epoch 0, 50.99 ms/it, loss 0.475061
Finished training it 8192/76743 of epoch 0, 51.00 ms/it, loss 0.477298
Finished training it 8192/76743 of epoch 0, 50.24 ms/it, loss 0.474246
Finished training it 9216/76743 of epoch 0, 50.59 ms/it, loss 0.474568
Finished training it 9216/76743 of epoch 0, 50.80 ms/it, loss 0.472988
Finished training it 9216/76743 of epoch 0, 51.25 ms/it, loss 0.472663
Finished training it 9216/76743 of epoch 0, 49.51 ms/it, loss 0.475054
Finished training it 10240/76743 of epoch 0, 45.97 ms/it, loss 0.471880
Finished training it 10240/76743 of epoch 0, 46.23 ms/it, loss 0.471446
Finished training it 10240/76743 of epoch 0, 44.98 ms/it, loss 0.473551
Finished training it 10240/76743 of epoch 0, 45.36 ms/it, loss 0.471471
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2534247.0
get out
0 has test check 2534247.0 and sample count 3274240
 accuracy 77.400 %, best 77.400 %, roc auc score 0.7738, best 0.7738
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2534247.0
get out
3 has test check 2534247.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 51.79 ms/it, loss 0.471345
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2534247.0
get out
2 has test check 2534247.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 49.69 ms/it, loss 0.471864
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2534247.0
get out
1 has test check 2534247.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 52.76 ms/it, loss 0.470392
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 52.93 ms/it, loss 0.471498
Finished training it 12288/76743 of epoch 0, 52.87 ms/it, loss 0.469821
Finished training it 12288/76743 of epoch 0, 52.19 ms/it, loss 0.470180
Finished training it 12288/76743 of epoch 0, 52.81 ms/it, loss 0.468985
Finished training it 12288/76743 of epoch 0, 51.11 ms/it, loss 0.468414
Finished training it 13312/76743 of epoch 0, 50.45 ms/it, loss 0.466318
Finished training it 13312/76743 of epoch 0, 51.12 ms/it, loss 0.469017
Finished training it 13312/76743 of epoch 0, 51.59 ms/it, loss 0.468676
Finished training it 13312/76743 of epoch 0, 51.80 ms/it, loss 0.471116
Finished training it 14336/76743 of epoch 0, 56.50 ms/it, loss 0.468607
Finished training it 14336/76743 of epoch 0, 58.49 ms/it, loss 0.469989
Finished training it 14336/76743 of epoch 0, 60.75 ms/it, loss 0.469052
Finished training it 14336/76743 of epoch 0, 59.98 ms/it, loss 0.469372
Finished training it 15360/76743 of epoch 0, 52.93 ms/it, loss 0.466635
Finished training it 15360/76743 of epoch 0, 52.94 ms/it, loss 0.469966
Finished training it 15360/76743 of epoch 0, 52.10 ms/it, loss 0.467863
Finished training it 15360/76743 of epoch 0, 50.69 ms/it, loss 0.468980
Finished training it 16384/76743 of epoch 0, 52.26 ms/it, loss 0.464596
Finished training it 16384/76743 of epoch 0, 51.98 ms/it, loss 0.465808
Finished training it 16384/76743 of epoch 0, 52.23 ms/it, loss 0.467573
Finished training it 16384/76743 of epoch 0, 50.98 ms/it, loss 0.468067
Finished training it 17408/76743 of epoch 0, 52.35 ms/it, loss 0.466869
Finished training it 17408/76743 of epoch 0, 51.71 ms/it, loss 0.468373
Finished training it 17408/76743 of epoch 0, 52.27 ms/it, loss 0.465099
Finished training it 17408/76743 of epoch 0, 50.97 ms/it, loss 0.468058
Finished training it 18432/76743 of epoch 0, 52.24 ms/it, loss 0.465178
Finished training it 18432/76743 of epoch 0, 52.85 ms/it, loss 0.465719
Finished training it 18432/76743 of epoch 0, 52.75 ms/it, loss 0.464602
Finished training it 18432/76743 of epoch 0, 51.15 ms/it, loss 0.464174
Finished training it 19456/76743 of epoch 0, 51.63 ms/it, loss 0.460914
Finished training it 19456/76743 of epoch 0, 49.97 ms/it, loss 0.466068
Finished training it 19456/76743 of epoch 0, 50.79 ms/it, loss 0.465677
Finished training it 19456/76743 of epoch 0, 51.90 ms/it, loss 0.463998
Finished training it 20480/76743 of epoch 0, 45.99 ms/it, loss 0.463254
Finished training it 20480/76743 of epoch 0, 46.59 ms/it, loss 0.465427
Finished training it 20480/76743 of epoch 0, 45.15 ms/it, loss 0.463446
Finished training it 20480/76743 of epoch 0, 46.23 ms/it, loss 0.465795
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2548235.0
get out
0 has test check 2548235.0 and sample count 3274240
 accuracy 77.827 %, best 77.827 %, roc auc score 0.7833, best 0.7833
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2548235.0
get out
3 has test check 2548235.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 51.71 ms/it, loss 0.463399
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 52.50 ms/it, loss 0.463627
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2548235.0
get out
2 has test check 2548235.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 50.91 ms/it, loss 0.463547
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2548235.0
get out
1 has test check 2548235.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 52.74 ms/it, loss 0.464788
Finished training it 22528/76743 of epoch 0, 50.00 ms/it, loss 0.463338
Finished training it 22528/76743 of epoch 0, 51.24 ms/it, loss 0.463633
Finished training it 22528/76743 of epoch 0, 50.68 ms/it, loss 0.463247
Finished training it 22528/76743 of epoch 0, 51.13 ms/it, loss 0.463349
Finished training it 23552/76743 of epoch 0, 51.79 ms/it, loss 0.462069
Finished training it 23552/76743 of epoch 0, 51.37 ms/it, loss 0.463385
Finished training it 23552/76743 of epoch 0, 52.20 ms/it, loss 0.462316
Finished training it 23552/76743 of epoch 0, 50.74 ms/it, loss 0.462071
Finished training it 24576/76743 of epoch 0, 51.28 ms/it, loss 0.463627
Finished training it 24576/76743 of epoch 0, 50.70 ms/it, loss 0.461958
Finished training it 24576/76743 of epoch 0, 51.65 ms/it, loss 0.461146
Finished training it 24576/76743 of epoch 0, 51.83 ms/it, loss 0.461788
Finished training it 25600/76743 of epoch 0, 51.23 ms/it, loss 0.463285
Finished training it 25600/76743 of epoch 0, 51.75 ms/it, loss 0.460956
Finished training it 25600/76743 of epoch 0, 51.77 ms/it, loss 0.462164
Finished training it 25600/76743 of epoch 0, 50.76 ms/it, loss 0.463205
Finished training it 26624/76743 of epoch 0, 52.21 ms/it, loss 0.460774
Finished training it 26624/76743 of epoch 0, 52.53 ms/it, loss 0.461777
Finished training it 26624/76743 of epoch 0, 52.07 ms/it, loss 0.463788
Finished training it 26624/76743 of epoch 0, 51.51 ms/it, loss 0.458609
Finished training it 27648/76743 of epoch 0, 51.06 ms/it, loss 0.461105
Finished training it 27648/76743 of epoch 0, 51.34 ms/it, loss 0.462745
Finished training it 27648/76743 of epoch 0, 52.14 ms/it, loss 0.459562
Finished training it 27648/76743 of epoch 0, 51.70 ms/it, loss 0.462486
Finished training it 28672/76743 of epoch 0, 51.98 ms/it, loss 0.461675
Finished training it 28672/76743 of epoch 0, 51.46 ms/it, loss 0.459076
Finished training it 28672/76743 of epoch 0, 50.96 ms/it, loss 0.459551
Finished training it 28672/76743 of epoch 0, 51.90 ms/it, loss 0.461042
Finished training it 29696/76743 of epoch 0, 51.21 ms/it, loss 0.460754
Finished training it 29696/76743 of epoch 0, 51.96 ms/it, loss 0.462141
Finished training it 29696/76743 of epoch 0, 51.36 ms/it, loss 0.460570
Finished training it 29696/76743 of epoch 0, 51.88 ms/it, loss 0.458175
Finished training it 30720/76743 of epoch 0, 51.88 ms/it, loss 0.463070
Finished training it 30720/76743 of epoch 0, 51.78 ms/it, loss 0.460960
Finished training it 30720/76743 of epoch 0, 51.38 ms/it, loss 0.460904
Finished training it 30720/76743 of epoch 0, 51.67 ms/it, loss 0.460527
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2560007.0
get out
0 has test check 2560007.0 and sample count 3274240
 accuracy 78.186 %, best 78.186 %, roc auc score 0.7877, best 0.7877
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 51.65 ms/it, loss 0.462316
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2560007.0
get out
3 has test check 2560007.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 51.44 ms/it, loss 0.459478
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2560007.0
get out
1 has test check 2560007.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 51.59 ms/it, loss 0.461126
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2560007.0
get out
2 has test check 2560007.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 50.90 ms/it, loss 0.460888
Finished training it 32768/76743 of epoch 0, 49.61 ms/it, loss 0.457934
Finished training it 32768/76743 of epoch 0, 49.57 ms/it, loss 0.460640
Finished training it 32768/76743 of epoch 0, 48.91 ms/it, loss 0.458684
Finished training it 32768/76743 of epoch 0, 49.68 ms/it, loss 0.462285
Finished training it 33792/76743 of epoch 0, 51.10 ms/it, loss 0.459714
Finished training it 33792/76743 of epoch 0, 52.76 ms/it, loss 0.460480
Finished training it 33792/76743 of epoch 0, 51.81 ms/it, loss 0.457608
Finished training it 33792/76743 of epoch 0, 51.64 ms/it, loss 0.456244
Finished training it 34816/76743 of epoch 0, 47.30 ms/it, loss 0.458476
Finished training it 34816/76743 of epoch 0, 46.77 ms/it, loss 0.460686
Finished training it 34816/76743 of epoch 0, 47.04 ms/it, loss 0.458687
Finished training it 34816/76743 of epoch 0, 46.07 ms/it, loss 0.458103
Finished training it 35840/76743 of epoch 0, 51.76 ms/it, loss 0.460215
Finished training it 35840/76743 of epoch 0, 51.33 ms/it, loss 0.460857
Finished training it 35840/76743 of epoch 0, 52.02 ms/it, loss 0.458623
Finished training it 35840/76743 of epoch 0, 51.33 ms/it, loss 0.459162
Finished training it 36864/76743 of epoch 0, 51.91 ms/it, loss 0.456932
Finished training it 36864/76743 of epoch 0, 52.39 ms/it, loss 0.459673
Finished training it 36864/76743 of epoch 0, 52.43 ms/it, loss 0.456989
Finished training it 36864/76743 of epoch 0, 51.62 ms/it, loss 0.459272
Finished training it 37888/76743 of epoch 0, 50.90 ms/it, loss 0.459070
Finished training it 37888/76743 of epoch 0, 50.97 ms/it, loss 0.458670
Finished training it 37888/76743 of epoch 0, 50.57 ms/it, loss 0.456282
Finished training it 37888/76743 of epoch 0, 50.34 ms/it, loss 0.459593
Finished training it 38912/76743 of epoch 0, 51.24 ms/it, loss 0.458151
Finished training it 38912/76743 of epoch 0, 52.20 ms/it, loss 0.456218
Finished training it 38912/76743 of epoch 0, 51.71 ms/it, loss 0.460185
Finished training it 38912/76743 of epoch 0, 51.82 ms/it, loss 0.459865
Finished training it 39936/76743 of epoch 0, 52.14 ms/it, loss 0.458553
Finished training it 39936/76743 of epoch 0, 51.86 ms/it, loss 0.460475
Finished training it 39936/76743 of epoch 0, 51.21 ms/it, loss 0.456543
Finished training it 39936/76743 of epoch 0, 51.66 ms/it, loss 0.457691
Finished training it 40960/76743 of epoch 0, 51.49 ms/it, loss 0.456663
Finished training it 40960/76743 of epoch 0, 51.38 ms/it, loss 0.461408
Finished training it 40960/76743 of epoch 0, 51.82 ms/it, loss 0.459574
Finished training it 40960/76743 of epoch 0, 51.99 ms/it, loss 0.455450
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562628.0
get out
0 has test check 2562628.0 and sample count 3274240
 accuracy 78.266 %, best 78.266 %, roc auc score 0.7903, best 0.7903
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562628.0
get out
2 has test check 2562628.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 51.52 ms/it, loss 0.458824
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562628.0
get out
1 has test check 2562628.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 52.08 ms/it, loss 0.458874
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562628.0
get out
3 has test check 2562628.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 51.88 ms/it, loss 0.458106
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 51.91 ms/it, loss 0.459721
Finished training it 43008/76743 of epoch 0, 50.09 ms/it, loss 0.456735
Finished training it 43008/76743 of epoch 0, 50.03 ms/it, loss 0.455859
Finished training it 43008/76743 of epoch 0, 50.31 ms/it, loss 0.457220
Finished training it 43008/76743 of epoch 0, 49.80 ms/it, loss 0.457081
Finished training it 44032/76743 of epoch 0, 45.26 ms/it, loss 0.458535
Finished training it 44032/76743 of epoch 0, 45.33 ms/it, loss 0.456839
Finished training it 44032/76743 of epoch 0, 45.25 ms/it, loss 0.457197
Finished training it 44032/76743 of epoch 0, 45.41 ms/it, loss 0.456420
Finished training it 45056/76743 of epoch 0, 44.98 ms/it, loss 0.457818
Finished training it 45056/76743 of epoch 0, 45.01 ms/it, loss 0.455958
Finished training it 45056/76743 of epoch 0, 44.97 ms/it, loss 0.458781
Finished training it 45056/76743 of epoch 0, 45.55 ms/it, loss 0.457299
Finished training it 46080/76743 of epoch 0, 49.72 ms/it, loss 0.456543
Finished training it 46080/76743 of epoch 0, 49.94 ms/it, loss 0.457017
Finished training it 46080/76743 of epoch 0, 50.26 ms/it, loss 0.455081
Finished training it 46080/76743 of epoch 0, 50.33 ms/it, loss 0.457766
Finished training it 47104/76743 of epoch 0, 51.54 ms/it, loss 0.457849
Finished training it 47104/76743 of epoch 0, 51.80 ms/it, loss 0.458885
Finished training it 47104/76743 of epoch 0, 51.58 ms/it, loss 0.457683
Finished training it 47104/76743 of epoch 0, 51.20 ms/it, loss 0.455111
Finished training it 48128/76743 of epoch 0, 51.40 ms/it, loss 0.460003
Finished training it 48128/76743 of epoch 0, 51.99 ms/it, loss 0.455914
Finished training it 48128/76743 of epoch 0, 51.81 ms/it, loss 0.456207
Finished training it 48128/76743 of epoch 0, 51.13 ms/it, loss 0.455013
Finished training it 49152/76743 of epoch 0, 51.81 ms/it, loss 0.456524
Finished training it 49152/76743 of epoch 0, 51.10 ms/it, loss 0.452762
Finished training it 49152/76743 of epoch 0, 51.64 ms/it, loss 0.458511
Finished training it 49152/76743 of epoch 0, 51.17 ms/it, loss 0.456863
Finished training it 50176/76743 of epoch 0, 51.23 ms/it, loss 0.454962
Finished training it 50176/76743 of epoch 0, 52.10 ms/it, loss 0.457952
Finished training it 50176/76743 of epoch 0, 51.57 ms/it, loss 0.454474
Finished training it 50176/76743 of epoch 0, 51.86 ms/it, loss 0.454383
Finished training it 51200/76743 of epoch 0, 51.98 ms/it, loss 0.457184
Finished training it 51200/76743 of epoch 0, 51.14 ms/it, loss 0.455134
Finished training it 51200/76743 of epoch 0, 51.25 ms/it, loss 0.455077
Finished training it 51200/76743 of epoch 0, 51.63 ms/it, loss 0.455868
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562873.0
get out
0 has test check 2562873.0 and sample count 3274240
 accuracy 78.274 %, best 78.274 %, roc auc score 0.7919, best 0.7919
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 51.60 ms/it, loss 0.455461
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562873.0
get out
3 has test check 2562873.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 51.16 ms/it, loss 0.455377
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562873.0
get out
1 has test check 2562873.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 51.71 ms/it, loss 0.455512
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562873.0
get out
2 has test check 2562873.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 50.92 ms/it, loss 0.458299
Finished training it 53248/76743 of epoch 0, 51.91 ms/it, loss 0.455518
Finished training it 53248/76743 of epoch 0, 51.29 ms/it, loss 0.452196
Finished training it 53248/76743 of epoch 0, 51.49 ms/it, loss 0.455945
Finished training it 53248/76743 of epoch 0, 52.03 ms/it, loss 0.455989
Finished training it 54272/76743 of epoch 0, 50.95 ms/it, loss 0.455844
Finished training it 54272/76743 of epoch 0, 51.72 ms/it, loss 0.457189
Finished training it 54272/76743 of epoch 0, 51.12 ms/it, loss 0.454893
Finished training it 54272/76743 of epoch 0, 51.26 ms/it, loss 0.456092
Finished training it 55296/76743 of epoch 0, 45.68 ms/it, loss 0.455118
Finished training it 55296/76743 of epoch 0, 45.11 ms/it, loss 0.454453
Finished training it 55296/76743 of epoch 0, 45.69 ms/it, loss 0.457404
Finished training it 55296/76743 of epoch 0, 46.01 ms/it, loss 0.455696
Finished training it 56320/76743 of epoch 0, 48.78 ms/it, loss 0.454769
Finished training it 56320/76743 of epoch 0, 49.14 ms/it, loss 0.457240
Finished training it 56320/76743 of epoch 0, 48.96 ms/it, loss 0.454437
Finished training it 56320/76743 of epoch 0, 48.71 ms/it, loss 0.455156
Finished training it 57344/76743 of epoch 0, 51.05 ms/it, loss 0.454460
Finished training it 57344/76743 of epoch 0, 50.86 ms/it, loss 0.456070
Finished training it 57344/76743 of epoch 0, 51.53 ms/it, loss 0.456448
Finished training it 57344/76743 of epoch 0, 51.45 ms/it, loss 0.456156
Finished training it 58368/76743 of epoch 0, 51.89 ms/it, loss 0.453435
Finished training it 58368/76743 of epoch 0, 51.73 ms/it, loss 0.452989
Finished training it 58368/76743 of epoch 0, 51.20 ms/it, loss 0.454816
Finished training it 58368/76743 of epoch 0, 51.25 ms/it, loss 0.453278
Finished training it 59392/76743 of epoch 0, 51.77 ms/it, loss 0.456655
Finished training it 59392/76743 of epoch 0, 51.00 ms/it, loss 0.455350
Finished training it 59392/76743 of epoch 0, 51.37 ms/it, loss 0.454828
Finished training it 59392/76743 of epoch 0, 51.20 ms/it, loss 0.455216
Finished training it 60416/76743 of epoch 0, 51.82 ms/it, loss 0.455517
Finished training it 60416/76743 of epoch 0, 52.13 ms/it, loss 0.456571
Finished training it 60416/76743 of epoch 0, 52.20 ms/it, loss 0.454163
Finished training it 60416/76743 of epoch 0, 51.52 ms/it, loss 0.454673
Finished training it 61440/76743 of epoch 0, 51.56 ms/it, loss 0.455569
Finished training it 61440/76743 of epoch 0, 51.78 ms/it, loss 0.456755
Finished training it 61440/76743 of epoch 0, 52.09 ms/it, loss 0.456073
Finished training it 61440/76743 of epoch 0, 51.51 ms/it, loss 0.455796
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568764.0
get out
0 has test check 2568764.0 and sample count 3274240
 accuracy 78.454 %, best 78.454 %, roc auc score 0.7936, best 0.7936
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568764.0
get out
1 has test check 2568764.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 51.58 ms/it, loss 0.452464
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568764.0
get out
3 has test check 2568764.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 51.28 ms/it, loss 0.454260
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 51.36 ms/it, loss 0.455139
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568764.0
get out
2 has test check 2568764.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 51.04 ms/it, loss 0.453031
Finished training it 63488/76743 of epoch 0, 51.19 ms/it, loss 0.455851
Finished training it 63488/76743 of epoch 0, 51.79 ms/it, loss 0.454508
Finished training it 63488/76743 of epoch 0, 51.50 ms/it, loss 0.454652
Finished training it 63488/76743 of epoch 0, 51.97 ms/it, loss 0.452370
Finished training it 64512/76743 of epoch 0, 46.43 ms/it, loss 0.451860
Finished training it 64512/76743 of epoch 0, 46.60 ms/it, loss 0.451991
Finished training it 64512/76743 of epoch 0, 47.11 ms/it, loss 0.455048
Finished training it 64512/76743 of epoch 0, 46.82 ms/it, loss 0.455330
Finished training it 65536/76743 of epoch 0, 44.83 ms/it, loss 0.453754
Finished training it 65536/76743 of epoch 0, 45.26 ms/it, loss 0.454885
Finished training it 65536/76743 of epoch 0, 45.65 ms/it, loss 0.450270
Finished training it 65536/76743 of epoch 0, 45.31 ms/it, loss 0.453642
Finished training it 66560/76743 of epoch 0, 46.88 ms/it, loss 0.457530
Finished training it 66560/76743 of epoch 0, 46.50 ms/it, loss 0.456603
Finished training it 66560/76743 of epoch 0, 46.99 ms/it, loss 0.453476
Finished training it 66560/76743 of epoch 0, 46.80 ms/it, loss 0.453934
Finished training it 67584/76743 of epoch 0, 51.74 ms/it, loss 0.453350
Finished training it 67584/76743 of epoch 0, 52.34 ms/it, loss 0.454818
Finished training it 67584/76743 of epoch 0, 52.30 ms/it, loss 0.454450
Finished training it 67584/76743 of epoch 0, 51.58 ms/it, loss 0.455302
Finished training it 68608/76743 of epoch 0, 51.66 ms/it, loss 0.453788
Finished training it 68608/76743 of epoch 0, 51.89 ms/it, loss 0.454429
Finished training it 68608/76743 of epoch 0, 52.16 ms/it, loss 0.454135
Finished training it 68608/76743 of epoch 0, 51.55 ms/it, loss 0.455220
Finished training it 69632/76743 of epoch 0, 50.44 ms/it, loss 0.454440
Finished training it 69632/76743 of epoch 0, 50.66 ms/it, loss 0.452509
Finished training it 69632/76743 of epoch 0, 50.19 ms/it, loss 0.452965
Finished training it 69632/76743 of epoch 0, 51.14 ms/it, loss 0.452365
Finished training it 70656/76743 of epoch 0, 51.21 ms/it, loss 0.455949
Finished training it 70656/76743 of epoch 0, 50.95 ms/it, loss 0.452275
Finished training it 70656/76743 of epoch 0, 51.47 ms/it, loss 0.453577
Finished training it 70656/76743 of epoch 0, 51.74 ms/it, loss 0.454331
Finished training it 71680/76743 of epoch 0, 51.33 ms/it, loss 0.452125
Finished training it 71680/76743 of epoch 0, 52.09 ms/it, loss 0.454529
Finished training it 71680/76743 of epoch 0, 51.43 ms/it, loss 0.454659
Finished training it 71680/76743 of epoch 0, 51.97 ms/it, loss 0.452745
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568334.0
get out
0 has test check 2568334.0 and sample count 3274240
 accuracy 78.441 %, best 78.454 %, roc auc score 0.7942, best 0.7942
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568334.0
get out
1 has test check 2568334.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 51.74 ms/it, loss 0.454430
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568334.0
get out
3 has test check 2568334.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 51.40 ms/it, loss 0.454924
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568334.0
get out
2 has test check 2568334.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 51.18 ms/it, loss 0.451507
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 51.79 ms/it, loss 0.453274
Finished training it 73728/76743 of epoch 0, 51.45 ms/it, loss 0.452446
Finished training it 73728/76743 of epoch 0, 51.39 ms/it, loss 0.453046
Finished training it 73728/76743 of epoch 0, 50.85 ms/it, loss 0.454822
Finished training it 73728/76743 of epoch 0, 51.18 ms/it, loss 0.451574
Finished training it 74752/76743 of epoch 0, 54.43 ms/it, loss 0.452490
Finished training it 74752/76743 of epoch 0, 53.24 ms/it, loss 0.451245
Finished training it 74752/76743 of epoch 0, 54.31 ms/it, loss 0.455739
Finished training it 74752/76743 of epoch 0, 53.60 ms/it, loss 0.451185
Finished training it 75776/76743 of epoch 0, 47.01 ms/it, loss 0.452820
Finished training it 75776/76743 of epoch 0, 45.30 ms/it, loss 0.450817
Finished training it 75776/76743 of epoch 0, 45.73 ms/it, loss 0.452741
Finished training it 75776/76743 of epoch 0, 45.93 ms/it, loss 0.454639
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 49.62 ms/it, loss 0.453984
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 48.94 ms/it, loss 0.452529
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 49.73 ms/it, loss 0.454022
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 49.05 ms/it, loss 0.453377
Finished training it 2048/76743 of epoch 1, 51.71 ms/it, loss 0.455049
Finished training it 2048/76743 of epoch 1, 51.47 ms/it, loss 0.453458
Finished training it 2048/76743 of epoch 1, 51.85 ms/it, loss 0.452807
Finished training it 2048/76743 of epoch 1, 51.22 ms/it, loss 0.454655
Finished training it 3072/76743 of epoch 1, 50.94 ms/it, loss 0.452563
Finished training it 3072/76743 of epoch 1, 50.52 ms/it, loss 0.453585
Finished training it 3072/76743 of epoch 1, 51.28 ms/it, loss 0.455475
Finished training it 3072/76743 of epoch 1, 50.73 ms/it, loss 0.452650
Finished training it 4096/76743 of epoch 1, 51.17 ms/it, loss 0.450363
Finished training it 4096/76743 of epoch 1, 51.45 ms/it, loss 0.450582
Finished training it 4096/76743 of epoch 1, 51.13 ms/it, loss 0.450701
Finished training it 4096/76743 of epoch 1, 51.41 ms/it, loss 0.454012
Finished training it 5120/76743 of epoch 1, 51.26 ms/it, loss 0.450662
Finished training it 5120/76743 of epoch 1, 51.29 ms/it, loss 0.453232
Finished training it 5120/76743 of epoch 1, 51.70 ms/it, loss 0.453245
Finished training it 5120/76743 of epoch 1, 51.13 ms/it, loss 0.451745
Finished training it 6144/76743 of epoch 1, 51.53 ms/it, loss 0.452480
Finished training it 6144/76743 of epoch 1, 52.19 ms/it, loss 0.447489
Finished training it 6144/76743 of epoch 1, 51.82 ms/it, loss 0.451987
Finished training it 6144/76743 of epoch 1, 51.76 ms/it, loss 0.452615
Finished training it 7168/76743 of epoch 1, 52.29 ms/it, loss 0.454309
Finished training it 7168/76743 of epoch 1, 51.93 ms/it, loss 0.454367
Finished training it 7168/76743 of epoch 1, 51.42 ms/it, loss 0.451413
Finished training it 7168/76743 of epoch 1, 51.48 ms/it, loss 0.450339
Finished training it 8192/76743 of epoch 1, 51.71 ms/it, loss 0.453448
Finished training it 8192/76743 of epoch 1, 51.44 ms/it, loss 0.449937
Finished training it 8192/76743 of epoch 1, 51.68 ms/it, loss 0.451773
Finished training it 8192/76743 of epoch 1, 51.44 ms/it, loss 0.451641
Finished training it 9216/76743 of epoch 1, 51.71 ms/it, loss 0.450520
Finished training it 9216/76743 of epoch 1, 51.43 ms/it, loss 0.452762
Finished training it 9216/76743 of epoch 1, 51.57 ms/it, loss 0.451308
Finished training it 9216/76743 of epoch 1, 51.28 ms/it, loss 0.452975
Finished training it 10240/76743 of epoch 1, 51.41 ms/it, loss 0.450856
Finished training it 10240/76743 of epoch 1, 51.50 ms/it, loss 0.449957
Finished training it 10240/76743 of epoch 1, 51.06 ms/it, loss 0.451084
Finished training it 10240/76743 of epoch 1, 51.01 ms/it, loss 0.452573
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568579.0
get out
0 has test check 2568579.0 and sample count 3274240
 accuracy 78.448 %, best 78.454 %, roc auc score 0.7958, best 0.7958
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 51.86 ms/it, loss 0.451496
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568579.0
get out
1 has test check 2568579.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 51.84 ms/it, loss 0.450993
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568579.0
get out
3 has test check 2568579.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 51.51 ms/it, loss 0.450894
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568579.0
get out
2 has test check 2568579.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 51.31 ms/it, loss 0.452389
Finished training it 12288/76743 of epoch 1, 51.24 ms/it, loss 0.449917
Finished training it 12288/76743 of epoch 1, 51.45 ms/it, loss 0.450878
Finished training it 12288/76743 of epoch 1, 51.02 ms/it, loss 0.451924
Finished training it 12288/76743 of epoch 1, 50.83 ms/it, loss 0.449464
Finished training it 13312/76743 of epoch 1, 50.41 ms/it, loss 0.451138
Finished training it 13312/76743 of epoch 1, 50.94 ms/it, loss 0.450239
Finished training it 13312/76743 of epoch 1, 50.55 ms/it, loss 0.452816
Finished training it 13312/76743 of epoch 1, 50.24 ms/it, loss 0.448442
Finished training it 14336/76743 of epoch 1, 52.02 ms/it, loss 0.452795
Finished training it 14336/76743 of epoch 1, 51.55 ms/it, loss 0.452923
Finished training it 14336/76743 of epoch 1, 52.20 ms/it, loss 0.452882
Finished training it 14336/76743 of epoch 1, 51.55 ms/it, loss 0.451835
Finished training it 15360/76743 of epoch 1, 51.33 ms/it, loss 0.452563
Finished training it 15360/76743 of epoch 1, 51.98 ms/it, loss 0.449919
Finished training it 15360/76743 of epoch 1, 51.35 ms/it, loss 0.451635
Finished training it 15360/76743 of epoch 1, 51.77 ms/it, loss 0.453020
Finished training it 16384/76743 of epoch 1, 51.37 ms/it, loss 0.449419
Finished training it 16384/76743 of epoch 1, 51.05 ms/it, loss 0.450283
Finished training it 16384/76743 of epoch 1, 51.56 ms/it, loss 0.451428
Finished training it 16384/76743 of epoch 1, 51.21 ms/it, loss 0.452303
Finished training it 17408/76743 of epoch 1, 51.35 ms/it, loss 0.453029
Finished training it 17408/76743 of epoch 1, 52.04 ms/it, loss 0.451688
Finished training it 17408/76743 of epoch 1, 51.61 ms/it, loss 0.453355
Finished training it 17408/76743 of epoch 1, 51.80 ms/it, loss 0.450463
Finished training it 18432/76743 of epoch 1, 51.64 ms/it, loss 0.450007
Finished training it 18432/76743 of epoch 1, 51.05 ms/it, loss 0.449322
Finished training it 18432/76743 of epoch 1, 51.92 ms/it, loss 0.451490
Finished training it 18432/76743 of epoch 1, 51.39 ms/it, loss 0.450871
Finished training it 19456/76743 of epoch 1, 52.11 ms/it, loss 0.450093
Finished training it 19456/76743 of epoch 1, 51.68 ms/it, loss 0.446846
Finished training it 19456/76743 of epoch 1, 51.34 ms/it, loss 0.451646
Finished training it 19456/76743 of epoch 1, 51.47 ms/it, loss 0.452396
Finished training it 20480/76743 of epoch 1, 52.10 ms/it, loss 0.451528
Finished training it 20480/76743 of epoch 1, 52.03 ms/it, loss 0.452723
Finished training it 20480/76743 of epoch 1, 51.32 ms/it, loss 0.449904
Finished training it 20480/76743 of epoch 1, 51.37 ms/it, loss 0.450418
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2572171.0
get out
0 has test check 2572171.0 and sample count 3274240
 accuracy 78.558 %, best 78.558 %, roc auc score 0.7965, best 0.7965
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2572171.0
get out
2 has test check 2572171.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 48.64 ms/it, loss 0.450475
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2572171.0
get out
1 has test check 2572171.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 48.91 ms/it, loss 0.452359
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2572171.0
get out
3 has test check 2572171.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 48.62 ms/it, loss 0.450576
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 49.29 ms/it, loss 0.450451
Finished training it 22528/76743 of epoch 1, 51.51 ms/it, loss 0.450284
Finished training it 22528/76743 of epoch 1, 51.79 ms/it, loss 0.451007
Finished training it 22528/76743 of epoch 1, 51.43 ms/it, loss 0.450739
Finished training it 22528/76743 of epoch 1, 51.07 ms/it, loss 0.450945
Finished training it 23552/76743 of epoch 1, 57.01 ms/it, loss 0.451794
Finished training it 23552/76743 of epoch 1, 57.61 ms/it, loss 0.450552
Finished training it 23552/76743 of epoch 1, 56.59 ms/it, loss 0.450245
Finished training it 23552/76743 of epoch 1, 57.89 ms/it, loss 0.449134
Finished training it 24576/76743 of epoch 1, 50.80 ms/it, loss 0.451789
Finished training it 24576/76743 of epoch 1, 51.44 ms/it, loss 0.449309
Finished training it 24576/76743 of epoch 1, 51.35 ms/it, loss 0.450264
Finished training it 24576/76743 of epoch 1, 51.48 ms/it, loss 0.449303
Finished training it 25600/76743 of epoch 1, 51.68 ms/it, loss 0.452037
Finished training it 25600/76743 of epoch 1, 52.11 ms/it, loss 0.450151
Finished training it 25600/76743 of epoch 1, 52.12 ms/it, loss 0.449556
Finished training it 25600/76743 of epoch 1, 51.88 ms/it, loss 0.451164
Finished training it 26624/76743 of epoch 1, 51.46 ms/it, loss 0.451111
Finished training it 26624/76743 of epoch 1, 51.33 ms/it, loss 0.449845
Finished training it 26624/76743 of epoch 1, 51.04 ms/it, loss 0.452148
Finished training it 26624/76743 of epoch 1, 51.02 ms/it, loss 0.447751
Finished training it 27648/76743 of epoch 1, 52.13 ms/it, loss 0.448734
Finished training it 27648/76743 of epoch 1, 51.68 ms/it, loss 0.451313
Finished training it 27648/76743 of epoch 1, 52.18 ms/it, loss 0.451793
Finished training it 27648/76743 of epoch 1, 51.62 ms/it, loss 0.450119
Finished training it 28672/76743 of epoch 1, 51.27 ms/it, loss 0.448179
Finished training it 28672/76743 of epoch 1, 51.09 ms/it, loss 0.449502
Finished training it 28672/76743 of epoch 1, 51.90 ms/it, loss 0.450614
Finished training it 28672/76743 of epoch 1, 51.73 ms/it, loss 0.451307
Finished training it 29696/76743 of epoch 1, 51.50 ms/it, loss 0.449544
Finished training it 29696/76743 of epoch 1, 51.77 ms/it, loss 0.451476
Finished training it 29696/76743 of epoch 1, 52.00 ms/it, loss 0.447552
Finished training it 29696/76743 of epoch 1, 51.13 ms/it, loss 0.450649
Finished training it 30720/76743 of epoch 1, 51.32 ms/it, loss 0.451513
Finished training it 30720/76743 of epoch 1, 52.07 ms/it, loss 0.453100
Finished training it 30720/76743 of epoch 1, 52.12 ms/it, loss 0.451322
Finished training it 30720/76743 of epoch 1, 51.68 ms/it, loss 0.450294
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574391.0
get out
0 has test check 2574391.0 and sample count 3274240
 accuracy 78.626 %, best 78.626 %, roc auc score 0.7974, best 0.7974
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574391.0
get out
1 has test check 2574391.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 51.80 ms/it, loss 0.451064
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574391.0
get out
2 has test check 2574391.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 51.29 ms/it, loss 0.451150
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574391.0
get out
3 has test check 2574391.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 51.35 ms/it, loss 0.450000
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 51.71 ms/it, loss 0.452112
Finished training it 32768/76743 of epoch 1, 51.08 ms/it, loss 0.448467
Finished training it 32768/76743 of epoch 1, 51.69 ms/it, loss 0.452957
Finished training it 32768/76743 of epoch 1, 51.22 ms/it, loss 0.448392
Finished training it 32768/76743 of epoch 1, 51.66 ms/it, loss 0.451188
Finished training it 33792/76743 of epoch 1, 51.95 ms/it, loss 0.451098
Finished training it 33792/76743 of epoch 1, 51.80 ms/it, loss 0.450535
Finished training it 33792/76743 of epoch 1, 51.47 ms/it, loss 0.446540
Finished training it 33792/76743 of epoch 1, 51.41 ms/it, loss 0.448028
Finished training it 34816/76743 of epoch 1, 46.59 ms/it, loss 0.451172
Finished training it 34816/76743 of epoch 1, 46.83 ms/it, loss 0.449098
Finished training it 34816/76743 of epoch 1, 46.63 ms/it, loss 0.449143
Finished training it 34816/76743 of epoch 1, 47.18 ms/it, loss 0.448582
Finished training it 35840/76743 of epoch 1, 44.61 ms/it, loss 0.451562
Finished training it 35840/76743 of epoch 1, 45.17 ms/it, loss 0.449614
Finished training it 35840/76743 of epoch 1, 45.08 ms/it, loss 0.451188
Finished training it 35840/76743 of epoch 1, 44.90 ms/it, loss 0.450214
Finished training it 36864/76743 of epoch 1, 46.67 ms/it, loss 0.450584
Finished training it 36864/76743 of epoch 1, 46.20 ms/it, loss 0.447948
Finished training it 36864/76743 of epoch 1, 46.61 ms/it, loss 0.450005
Finished training it 36864/76743 of epoch 1, 46.84 ms/it, loss 0.448245
Finished training it 37888/76743 of epoch 1, 51.76 ms/it, loss 0.450586
Finished training it 37888/76743 of epoch 1, 52.13 ms/it, loss 0.449663
Finished training it 37888/76743 of epoch 1, 51.94 ms/it, loss 0.447638
Finished training it 37888/76743 of epoch 1, 52.12 ms/it, loss 0.450421
Finished training it 38912/76743 of epoch 1, 51.22 ms/it, loss 0.449259
Finished training it 38912/76743 of epoch 1, 51.67 ms/it, loss 0.450915
Finished training it 38912/76743 of epoch 1, 51.40 ms/it, loss 0.451220
Finished training it 38912/76743 of epoch 1, 51.69 ms/it, loss 0.447800
Finished training it 39936/76743 of epoch 1, 51.54 ms/it, loss 0.451341
Finished training it 39936/76743 of epoch 1, 51.32 ms/it, loss 0.449111
Finished training it 39936/76743 of epoch 1, 51.80 ms/it, loss 0.449822
Finished training it 39936/76743 of epoch 1, 51.42 ms/it, loss 0.447703
Finished training it 40960/76743 of epoch 1, 51.48 ms/it, loss 0.451609
Finished training it 40960/76743 of epoch 1, 51.60 ms/it, loss 0.447456
Finished training it 40960/76743 of epoch 1, 51.25 ms/it, loss 0.448409
Finished training it 40960/76743 of epoch 1, 50.88 ms/it, loss 0.453436
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574270.0
get out
0 has test check 2574270.0 and sample count 3274240
 accuracy 78.622 %, best 78.626 %, roc auc score 0.7979, best 0.7979
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574270.0
get out
1 has test check 2574270.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 51.38 ms/it, loss 0.450526
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 51.53 ms/it, loss 0.451043
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574270.0
get out
3 has test check 2574270.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 51.22 ms/it, loss 0.450172
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574270.0
get out
2 has test check 2574270.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 50.82 ms/it, loss 0.450304
Finished training it 43008/76743 of epoch 1, 51.50 ms/it, loss 0.447755
Finished training it 43008/76743 of epoch 1, 51.66 ms/it, loss 0.449185
Finished training it 43008/76743 of epoch 1, 51.26 ms/it, loss 0.449048
Finished training it 43008/76743 of epoch 1, 51.56 ms/it, loss 0.448661
Finished training it 44032/76743 of epoch 1, 52.25 ms/it, loss 0.450611
Finished training it 44032/76743 of epoch 1, 52.26 ms/it, loss 0.448901
Finished training it 44032/76743 of epoch 1, 52.57 ms/it, loss 0.449320
Finished training it 44032/76743 of epoch 1, 57.01 ms/it, loss 0.448606
Finished training it 45056/76743 of epoch 1, 52.78 ms/it, loss 0.449910
Finished training it 45056/76743 of epoch 1, 53.47 ms/it, loss 0.449095
Finished training it 45056/76743 of epoch 1, 48.62 ms/it, loss 0.450785
Finished training it 45056/76743 of epoch 1, 52.70 ms/it, loss 0.448453
Finished training it 46080/76743 of epoch 1, 45.63 ms/it, loss 0.447245
Finished training it 46080/76743 of epoch 1, 45.67 ms/it, loss 0.450159
Finished training it 46080/76743 of epoch 1, 45.26 ms/it, loss 0.449220
Finished training it 46080/76743 of epoch 1, 45.60 ms/it, loss 0.448829
Finished training it 47104/76743 of epoch 1, 46.27 ms/it, loss 0.449881
Finished training it 47104/76743 of epoch 1, 45.93 ms/it, loss 0.451600
Finished training it 47104/76743 of epoch 1, 45.50 ms/it, loss 0.450245
Finished training it 47104/76743 of epoch 1, 45.89 ms/it, loss 0.447789
Finished training it 48128/76743 of epoch 1, 51.59 ms/it, loss 0.448585
Finished training it 48128/76743 of epoch 1, 51.65 ms/it, loss 0.448799
Finished training it 48128/76743 of epoch 1, 50.91 ms/it, loss 0.447378
Finished training it 48128/76743 of epoch 1, 51.41 ms/it, loss 0.452481
Finished training it 49152/76743 of epoch 1, 51.47 ms/it, loss 0.449499
Finished training it 49152/76743 of epoch 1, 51.62 ms/it, loss 0.451055
Finished training it 49152/76743 of epoch 1, 51.85 ms/it, loss 0.449329
Finished training it 49152/76743 of epoch 1, 51.16 ms/it, loss 0.445193
Finished training it 50176/76743 of epoch 1, 51.75 ms/it, loss 0.450467
Finished training it 50176/76743 of epoch 1, 51.43 ms/it, loss 0.447082
Finished training it 50176/76743 of epoch 1, 50.79 ms/it, loss 0.447974
Finished training it 50176/76743 of epoch 1, 51.24 ms/it, loss 0.446954
Finished training it 51200/76743 of epoch 1, 51.56 ms/it, loss 0.447857
Finished training it 51200/76743 of epoch 1, 51.84 ms/it, loss 0.450033
Finished training it 51200/76743 of epoch 1, 51.82 ms/it, loss 0.448795
Finished training it 51200/76743 of epoch 1, 51.16 ms/it, loss 0.447938
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2573478.0
get out
0 has test check 2573478.0 and sample count 3274240
 accuracy 78.598 %, best 78.626 %, roc auc score 0.7983, best 0.7983
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2573478.0
get out
1 has test check 2573478.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 52.07 ms/it, loss 0.448072
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2573478.0
get out
3 has test check 2573478.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 51.97 ms/it, loss 0.448199
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2573478.0
get out
2 has test check 2573478.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 51.62 ms/it, loss 0.451578
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 52.27 ms/it, loss 0.448486
Finished training it 53248/76743 of epoch 1, 51.52 ms/it, loss 0.448654
Finished training it 53248/76743 of epoch 1, 51.62 ms/it, loss 0.448774
Finished training it 53248/76743 of epoch 1, 51.23 ms/it, loss 0.448694
Finished training it 53248/76743 of epoch 1, 51.35 ms/it, loss 0.445675
Finished training it 54272/76743 of epoch 1, 51.99 ms/it, loss 0.450193
Finished training it 54272/76743 of epoch 1, 51.38 ms/it, loss 0.449253
Finished training it 54272/76743 of epoch 1, 51.67 ms/it, loss 0.448559
Finished training it 54272/76743 of epoch 1, 51.68 ms/it, loss 0.448217
Finished training it 55296/76743 of epoch 1, 50.32 ms/it, loss 0.448495
Finished training it 55296/76743 of epoch 1, 50.10 ms/it, loss 0.448765
Finished training it 55296/76743 of epoch 1, 49.84 ms/it, loss 0.447532
Finished training it 55296/76743 of epoch 1, 49.51 ms/it, loss 0.450687
Finished training it 56320/76743 of epoch 1, 45.09 ms/it, loss 0.450685
Finished training it 56320/76743 of epoch 1, 45.12 ms/it, loss 0.448093
Finished training it 56320/76743 of epoch 1, 44.79 ms/it, loss 0.448608
Finished training it 56320/76743 of epoch 1, 45.31 ms/it, loss 0.447801
Finished training it 57344/76743 of epoch 1, 45.07 ms/it, loss 0.447855
Finished training it 57344/76743 of epoch 1, 45.60 ms/it, loss 0.449818
Finished training it 57344/76743 of epoch 1, 45.51 ms/it, loss 0.449989
Finished training it 57344/76743 of epoch 1, 45.46 ms/it, loss 0.449706
Finished training it 58368/76743 of epoch 1, 51.09 ms/it, loss 0.446374
Finished training it 58368/76743 of epoch 1, 50.50 ms/it, loss 0.448077
Finished training it 58368/76743 of epoch 1, 50.89 ms/it, loss 0.446961
Finished training it 58368/76743 of epoch 1, 50.51 ms/it, loss 0.446600
Finished training it 59392/76743 of epoch 1, 51.27 ms/it, loss 0.448708
Finished training it 59392/76743 of epoch 1, 51.50 ms/it, loss 0.448169
Finished training it 59392/76743 of epoch 1, 51.01 ms/it, loss 0.448678
Finished training it 59392/76743 of epoch 1, 51.46 ms/it, loss 0.450045
Finished training it 60416/76743 of epoch 1, 51.98 ms/it, loss 0.449110
Finished training it 60416/76743 of epoch 1, 52.48 ms/it, loss 0.450091
Finished training it 60416/76743 of epoch 1, 52.45 ms/it, loss 0.447424
Finished training it 60416/76743 of epoch 1, 51.93 ms/it, loss 0.448468
Finished training it 61440/76743 of epoch 1, 50.79 ms/it, loss 0.448971
Finished training it 61440/76743 of epoch 1, 50.99 ms/it, loss 0.450425
Finished training it 61440/76743 of epoch 1, 51.23 ms/it, loss 0.449564
Finished training it 61440/76743 of epoch 1, 50.73 ms/it, loss 0.449596
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576802.0
get out
0 has test check 2576802.0 and sample count 3274240
 accuracy 78.699 %, best 78.699 %, roc auc score 0.7989, best 0.7989
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576802.0
get out
1 has test check 2576802.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 51.82 ms/it, loss 0.446021
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 51.66 ms/it, loss 0.449122
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576802.0
get out
3 has test check 2576802.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 51.36 ms/it, loss 0.447686
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576802.0
get out
2 has test check 2576802.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 51.00 ms/it, loss 0.446521
Finished training it 63488/76743 of epoch 1, 51.44 ms/it, loss 0.448133
Finished training it 63488/76743 of epoch 1, 51.64 ms/it, loss 0.448032
Finished training it 63488/76743 of epoch 1, 51.69 ms/it, loss 0.445759
Finished training it 63488/76743 of epoch 1, 51.14 ms/it, loss 0.449900
Finished training it 64512/76743 of epoch 1, 56.84 ms/it, loss 0.445677
Finished training it 64512/76743 of epoch 1, 51.75 ms/it, loss 0.445662
Finished training it 64512/76743 of epoch 1, 52.40 ms/it, loss 0.448925
Finished training it 64512/76743 of epoch 1, 52.10 ms/it, loss 0.448931
Finished training it 65536/76743 of epoch 1, 51.21 ms/it, loss 0.448519
Finished training it 65536/76743 of epoch 1, 55.65 ms/it, loss 0.443933
Finished training it 65536/76743 of epoch 1, 55.38 ms/it, loss 0.447673
Finished training it 65536/76743 of epoch 1, 55.00 ms/it, loss 0.447212
Finished training it 66560/76743 of epoch 1, 45.46 ms/it, loss 0.447478
Finished training it 66560/76743 of epoch 1, 45.12 ms/it, loss 0.447714
Finished training it 66560/76743 of epoch 1, 45.24 ms/it, loss 0.450548
Finished training it 66560/76743 of epoch 1, 45.13 ms/it, loss 0.451537
Finished training it 67584/76743 of epoch 1, 45.44 ms/it, loss 0.448601
Finished training it 67584/76743 of epoch 1, 45.74 ms/it, loss 0.449129
Finished training it 67584/76743 of epoch 1, 45.28 ms/it, loss 0.449357
Finished training it 67584/76743 of epoch 1, 45.04 ms/it, loss 0.447548
Finished training it 68608/76743 of epoch 1, 49.92 ms/it, loss 0.447941
Finished training it 68608/76743 of epoch 1, 50.51 ms/it, loss 0.447824
Finished training it 68608/76743 of epoch 1, 49.95 ms/it, loss 0.448462
Finished training it 68608/76743 of epoch 1, 49.75 ms/it, loss 0.449353
Finished training it 69632/76743 of epoch 1, 51.79 ms/it, loss 0.446270
Finished training it 69632/76743 of epoch 1, 51.50 ms/it, loss 0.448447
Finished training it 69632/76743 of epoch 1, 51.89 ms/it, loss 0.446734
Finished training it 69632/76743 of epoch 1, 51.24 ms/it, loss 0.446752
Finished training it 70656/76743 of epoch 1, 51.73 ms/it, loss 0.450086
Finished training it 70656/76743 of epoch 1, 51.86 ms/it, loss 0.447825
Finished training it 70656/76743 of epoch 1, 51.54 ms/it, loss 0.446443
Finished training it 70656/76743 of epoch 1, 51.96 ms/it, loss 0.448406
Finished training it 71680/76743 of epoch 1, 51.16 ms/it, loss 0.447349
Finished training it 71680/76743 of epoch 1, 50.94 ms/it, loss 0.446257
Finished training it 71680/76743 of epoch 1, 51.19 ms/it, loss 0.448776
Finished training it 71680/76743 of epoch 1, 50.90 ms/it, loss 0.448991
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575516.0
get out
0 has test check 2575516.0 and sample count 3274240
 accuracy 78.660 %, best 78.699 %, roc auc score 0.7988, best 0.7989
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 51.59 ms/it, loss 0.447099
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575516.0
get out
3 has test check 2575516.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 51.48 ms/it, loss 0.449001
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575516.0
get out
2 has test check 2575516.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 51.30 ms/it, loss 0.445669
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575516.0
get out
1 has test check 2575516.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 52.03 ms/it, loss 0.448485
Finished training it 73728/76743 of epoch 1, 52.11 ms/it, loss 0.447007
Finished training it 73728/76743 of epoch 1, 51.53 ms/it, loss 0.446173
Finished training it 73728/76743 of epoch 1, 51.35 ms/it, loss 0.449261
Finished training it 73728/76743 of epoch 1, 51.63 ms/it, loss 0.446899
Finished training it 74752/76743 of epoch 1, 51.81 ms/it, loss 0.445594
Finished training it 74752/76743 of epoch 1, 52.28 ms/it, loss 0.450074
Finished training it 74752/76743 of epoch 1, 51.81 ms/it, loss 0.445670
Finished training it 74752/76743 of epoch 1, 51.98 ms/it, loss 0.446987
Finished training it 75776/76743 of epoch 1, 52.00 ms/it, loss 0.447232
Finished training it 75776/76743 of epoch 1, 51.60 ms/it, loss 0.445574
Finished training it 75776/76743 of epoch 1, 52.16 ms/it, loss 0.447052
Finished training it 75776/76743 of epoch 1, 51.50 ms/it, loss 0.448649
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 45.55 ms/it, loss 0.447630
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 45.71 ms/it, loss 0.448623
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 46.25 ms/it, loss 0.448421
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 45.95 ms/it, loss 0.447134
Finished training it 2048/76743 of epoch 2, 47.74 ms/it, loss 0.449506
Finished training it 2048/76743 of epoch 2, 47.30 ms/it, loss 0.447832
Finished training it 2048/76743 of epoch 2, 47.73 ms/it, loss 0.447609
Finished training it 2048/76743 of epoch 2, 47.49 ms/it, loss 0.449084
Finished training it 3072/76743 of epoch 2, 51.30 ms/it, loss 0.447137
Finished training it 3072/76743 of epoch 2, 51.97 ms/it, loss 0.446967
Finished training it 3072/76743 of epoch 2, 51.91 ms/it, loss 0.450046
Finished training it 3072/76743 of epoch 2, 51.68 ms/it, loss 0.447967
Finished training it 4096/76743 of epoch 2, 51.36 ms/it, loss 0.445438
Finished training it 4096/76743 of epoch 2, 51.59 ms/it, loss 0.448593
Finished training it 4096/76743 of epoch 2, 51.08 ms/it, loss 0.444971
Finished training it 4096/76743 of epoch 2, 51.77 ms/it, loss 0.445188
Finished training it 5120/76743 of epoch 2, 51.13 ms/it, loss 0.447686
Finished training it 5120/76743 of epoch 2, 51.39 ms/it, loss 0.447511
Finished training it 5120/76743 of epoch 2, 51.29 ms/it, loss 0.445159
Finished training it 5120/76743 of epoch 2, 50.63 ms/it, loss 0.446518
Finished training it 6144/76743 of epoch 2, 51.37 ms/it, loss 0.447037
Finished training it 6144/76743 of epoch 2, 51.62 ms/it, loss 0.441945
Finished training it 6144/76743 of epoch 2, 50.94 ms/it, loss 0.447160
Finished training it 6144/76743 of epoch 2, 50.88 ms/it, loss 0.447329
Finished training it 7168/76743 of epoch 2, 52.67 ms/it, loss 0.448974
Finished training it 7168/76743 of epoch 2, 51.81 ms/it, loss 0.446274
Finished training it 7168/76743 of epoch 2, 52.39 ms/it, loss 0.449392
Finished training it 7168/76743 of epoch 2, 52.19 ms/it, loss 0.445339
Finished training it 8192/76743 of epoch 2, 50.95 ms/it, loss 0.448259
Finished training it 8192/76743 of epoch 2, 51.11 ms/it, loss 0.444628
Finished training it 8192/76743 of epoch 2, 51.58 ms/it, loss 0.446845
Finished training it 8192/76743 of epoch 2, 50.83 ms/it, loss 0.446698
Finished training it 9216/76743 of epoch 2, 51.39 ms/it, loss 0.446148
Finished training it 9216/76743 of epoch 2, 50.91 ms/it, loss 0.447689
Finished training it 9216/76743 of epoch 2, 51.27 ms/it, loss 0.447321
Finished training it 9216/76743 of epoch 2, 51.31 ms/it, loss 0.445779
Finished training it 10240/76743 of epoch 2, 51.30 ms/it, loss 0.447539
Finished training it 10240/76743 of epoch 2, 51.51 ms/it, loss 0.446311
Finished training it 10240/76743 of epoch 2, 51.54 ms/it, loss 0.444833
Finished training it 10240/76743 of epoch 2, 52.02 ms/it, loss 0.446046
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574392.0
get out
0 has test check 2574392.0 and sample count 3274240
 accuracy 78.626 %, best 78.699 %, roc auc score 0.7996, best 0.7996
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574392.0
get out
2 has test check 2574392.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 45.63 ms/it, loss 0.447682
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 45.59 ms/it, loss 0.446387
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574392.0
get out
3 has test check 2574392.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 45.36 ms/it, loss 0.445827
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574392.0
get out
1 has test check 2574392.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 45.77 ms/it, loss 0.446052
Finished training it 12288/76743 of epoch 2, 44.85 ms/it, loss 0.446935
Finished training it 12288/76743 of epoch 2, 45.29 ms/it, loss 0.445941
Finished training it 12288/76743 of epoch 2, 45.01 ms/it, loss 0.444403
Finished training it 12288/76743 of epoch 2, 45.29 ms/it, loss 0.444584
Finished training it 13312/76743 of epoch 2, 58.50 ms/it, loss 0.445188
Finished training it 13312/76743 of epoch 2, 57.36 ms/it, loss 0.443331
Finished training it 13312/76743 of epoch 2, 57.16 ms/it, loss 0.446182
Finished training it 13312/76743 of epoch 2, 57.39 ms/it, loss 0.447947
Finished training it 14336/76743 of epoch 2, 52.57 ms/it, loss 0.447895
Finished training it 14336/76743 of epoch 2, 51.95 ms/it, loss 0.448000
Finished training it 14336/76743 of epoch 2, 52.04 ms/it, loss 0.448080
Finished training it 14336/76743 of epoch 2, 51.40 ms/it, loss 0.447042
Finished training it 15360/76743 of epoch 2, 50.60 ms/it, loss 0.447746
Finished training it 15360/76743 of epoch 2, 51.20 ms/it, loss 0.445142
Finished training it 15360/76743 of epoch 2, 50.58 ms/it, loss 0.446795
Finished training it 15360/76743 of epoch 2, 50.88 ms/it, loss 0.448107
Finished training it 16384/76743 of epoch 2, 51.06 ms/it, loss 0.445411
Finished training it 16384/76743 of epoch 2, 51.10 ms/it, loss 0.447478
Finished training it 16384/76743 of epoch 2, 51.74 ms/it, loss 0.446888
Finished training it 16384/76743 of epoch 2, 51.52 ms/it, loss 0.444697
Finished training it 17408/76743 of epoch 2, 52.19 ms/it, loss 0.446933
Finished training it 17408/76743 of epoch 2, 51.91 ms/it, loss 0.445590
Finished training it 17408/76743 of epoch 2, 51.57 ms/it, loss 0.448590
Finished training it 17408/76743 of epoch 2, 51.56 ms/it, loss 0.448371
Finished training it 18432/76743 of epoch 2, 51.30 ms/it, loss 0.444471
Finished training it 18432/76743 of epoch 2, 51.71 ms/it, loss 0.445200
Finished training it 18432/76743 of epoch 2, 51.61 ms/it, loss 0.445990
Finished training it 18432/76743 of epoch 2, 52.13 ms/it, loss 0.446907
Finished training it 19456/76743 of epoch 2, 52.11 ms/it, loss 0.445244
Finished training it 19456/76743 of epoch 2, 51.92 ms/it, loss 0.441892
Finished training it 19456/76743 of epoch 2, 51.52 ms/it, loss 0.446758
Finished training it 19456/76743 of epoch 2, 51.35 ms/it, loss 0.447638
Finished training it 20480/76743 of epoch 2, 52.04 ms/it, loss 0.447570
Finished training it 20480/76743 of epoch 2, 51.89 ms/it, loss 0.445195
Finished training it 20480/76743 of epoch 2, 52.14 ms/it, loss 0.446309
Finished training it 20480/76743 of epoch 2, 51.39 ms/it, loss 0.445616
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577266.0
get out
0 has test check 2577266.0 and sample count 3274240
 accuracy 78.713 %, best 78.713 %, roc auc score 0.8000, best 0.8000
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 45.10 ms/it, loss 0.445673
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577266.0
get out
1 has test check 2577266.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 45.52 ms/it, loss 0.447699
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577266.0
get out
2 has test check 2577266.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 45.29 ms/it, loss 0.445800
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577266.0
get out
3 has test check 2577266.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 44.92 ms/it, loss 0.445911
Finished training it 22528/76743 of epoch 2, 45.61 ms/it, loss 0.445449
Finished training it 22528/76743 of epoch 2, 45.68 ms/it, loss 0.445873
Finished training it 22528/76743 of epoch 2, 45.57 ms/it, loss 0.446031
Finished training it 22528/76743 of epoch 2, 45.81 ms/it, loss 0.446369
Finished training it 23552/76743 of epoch 2, 50.81 ms/it, loss 0.445785
Finished training it 23552/76743 of epoch 2, 51.30 ms/it, loss 0.444365
Finished training it 23552/76743 of epoch 2, 51.45 ms/it, loss 0.445897
Finished training it 23552/76743 of epoch 2, 51.10 ms/it, loss 0.447275
Finished training it 24576/76743 of epoch 2, 52.01 ms/it, loss 0.444599
Finished training it 24576/76743 of epoch 2, 51.32 ms/it, loss 0.445720
Finished training it 24576/76743 of epoch 2, 51.88 ms/it, loss 0.444777
Finished training it 24576/76743 of epoch 2, 51.81 ms/it, loss 0.447198
Finished training it 25600/76743 of epoch 2, 51.51 ms/it, loss 0.446561
Finished training it 25600/76743 of epoch 2, 51.47 ms/it, loss 0.447414
Finished training it 25600/76743 of epoch 2, 51.98 ms/it, loss 0.445323
Finished training it 25600/76743 of epoch 2, 52.05 ms/it, loss 0.444987
Finished training it 26624/76743 of epoch 2, 50.59 ms/it, loss 0.443282
Finished training it 26624/76743 of epoch 2, 51.08 ms/it, loss 0.445487
Finished training it 26624/76743 of epoch 2, 50.77 ms/it, loss 0.447577
Finished training it 26624/76743 of epoch 2, 51.36 ms/it, loss 0.446913
Finished training it 27648/76743 of epoch 2, 52.16 ms/it, loss 0.444295
Finished training it 27648/76743 of epoch 2, 51.78 ms/it, loss 0.446665
Finished training it 27648/76743 of epoch 2, 51.36 ms/it, loss 0.445272
Finished training it 27648/76743 of epoch 2, 51.80 ms/it, loss 0.447399
Finished training it 28672/76743 of epoch 2, 47.22 ms/it, loss 0.446900
Finished training it 28672/76743 of epoch 2, 47.21 ms/it, loss 0.443719
Finished training it 28672/76743 of epoch 2, 46.65 ms/it, loss 0.445032
Finished training it 28672/76743 of epoch 2, 47.48 ms/it, loss 0.446068
Finished training it 29696/76743 of epoch 2, 44.88 ms/it, loss 0.445099
Finished training it 29696/76743 of epoch 2, 44.97 ms/it, loss 0.442872
Finished training it 29696/76743 of epoch 2, 44.86 ms/it, loss 0.446791
Finished training it 29696/76743 of epoch 2, 44.78 ms/it, loss 0.446174
Finished training it 30720/76743 of epoch 2, 47.83 ms/it, loss 0.445784
Finished training it 30720/76743 of epoch 2, 48.22 ms/it, loss 0.446899
Finished training it 30720/76743 of epoch 2, 46.53 ms/it, loss 0.447228
Finished training it 30720/76743 of epoch 2, 48.07 ms/it, loss 0.448608
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578985.0
get out
0 has test check 2578985.0 and sample count 3274240
 accuracy 78.766 %, best 78.766 %, roc auc score 0.8005, best 0.8005
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578985.0
get out
1 has test check 2578985.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 48.31 ms/it, loss 0.446601
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578985.0
get out
2 has test check 2578985.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 47.84 ms/it, loss 0.446600
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578985.0
get out
3 has test check 2578985.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 47.87 ms/it, loss 0.445678
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 48.08 ms/it, loss 0.447485
Finished training it 32768/76743 of epoch 2, 47.41 ms/it, loss 0.448847
Finished training it 32768/76743 of epoch 2, 47.00 ms/it, loss 0.446825
Finished training it 32768/76743 of epoch 2, 47.70 ms/it, loss 0.443935
Finished training it 32768/76743 of epoch 2, 47.11 ms/it, loss 0.444006
Finished training it 33792/76743 of epoch 2, 47.19 ms/it, loss 0.446410
Finished training it 33792/76743 of epoch 2, 51.11 ms/it, loss 0.442209
Finished training it 33792/76743 of epoch 2, 51.96 ms/it, loss 0.443605
Finished training it 33792/76743 of epoch 2, 51.72 ms/it, loss 0.446646
Finished training it 34816/76743 of epoch 2, 48.23 ms/it, loss 0.444665
Finished training it 34816/76743 of epoch 2, 48.47 ms/it, loss 0.444697
Finished training it 34816/76743 of epoch 2, 47.75 ms/it, loss 0.443899
Finished training it 34816/76743 of epoch 2, 47.33 ms/it, loss 0.446868
Finished training it 35840/76743 of epoch 2, 47.52 ms/it, loss 0.445385
Finished training it 35840/76743 of epoch 2, 46.99 ms/it, loss 0.447156
Finished training it 35840/76743 of epoch 2, 47.40 ms/it, loss 0.446735
Finished training it 35840/76743 of epoch 2, 47.84 ms/it, loss 0.445871
Finished training it 36864/76743 of epoch 2, 47.13 ms/it, loss 0.443754
Finished training it 36864/76743 of epoch 2, 47.77 ms/it, loss 0.446221
Finished training it 36864/76743 of epoch 2, 48.30 ms/it, loss 0.445618
Finished training it 36864/76743 of epoch 2, 47.53 ms/it, loss 0.443967
Finished training it 37888/76743 of epoch 2, 47.46 ms/it, loss 0.443396
Finished training it 37888/76743 of epoch 2, 48.03 ms/it, loss 0.445475
Finished training it 37888/76743 of epoch 2, 48.38 ms/it, loss 0.446408
Finished training it 37888/76743 of epoch 2, 47.54 ms/it, loss 0.446216
Finished training it 38912/76743 of epoch 2, 48.33 ms/it, loss 0.444769
Finished training it 38912/76743 of epoch 2, 47.97 ms/it, loss 0.446601
Finished training it 38912/76743 of epoch 2, 48.10 ms/it, loss 0.443445
Finished training it 38912/76743 of epoch 2, 47.55 ms/it, loss 0.446695
Finished training it 39936/76743 of epoch 2, 47.69 ms/it, loss 0.446984
Finished training it 39936/76743 of epoch 2, 47.98 ms/it, loss 0.445567
Finished training it 39936/76743 of epoch 2, 48.63 ms/it, loss 0.443343
Finished training it 39936/76743 of epoch 2, 47.25 ms/it, loss 0.444916
Finished training it 40960/76743 of epoch 2, 48.39 ms/it, loss 0.449219
Finished training it 40960/76743 of epoch 2, 47.21 ms/it, loss 0.444193
Finished training it 40960/76743 of epoch 2, 48.11 ms/it, loss 0.443258
Finished training it 40960/76743 of epoch 2, 47.55 ms/it, loss 0.447402
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578984.0
get out
0 has test check 2578984.0 and sample count 3274240
 accuracy 78.766 %, best 78.766 %, roc auc score 0.8007, best 0.8007
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578984.0
get out
1 has test check 2578984.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 47.71 ms/it, loss 0.446180
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 47.67 ms/it, loss 0.446666
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578984.0
get out
2 has test check 2578984.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 48.08 ms/it, loss 0.445895
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578984.0
get out
3 has test check 2578984.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 47.26 ms/it, loss 0.446227
Finished training it 43008/76743 of epoch 2, 47.24 ms/it, loss 0.443368
Finished training it 43008/76743 of epoch 2, 47.60 ms/it, loss 0.444984
Finished training it 43008/76743 of epoch 2, 47.60 ms/it, loss 0.444936
Finished training it 43008/76743 of epoch 2, 46.92 ms/it, loss 0.444232
Finished training it 44032/76743 of epoch 2, 48.70 ms/it, loss 0.445216
Finished training it 44032/76743 of epoch 2, 49.15 ms/it, loss 0.444771
Finished training it 44032/76743 of epoch 2, 48.13 ms/it, loss 0.446409
Finished training it 44032/76743 of epoch 2, 52.08 ms/it, loss 0.444585
Finished training it 45056/76743 of epoch 2, 47.21 ms/it, loss 0.446617
Finished training it 45056/76743 of epoch 2, 47.12 ms/it, loss 0.444200
Finished training it 45056/76743 of epoch 2, 48.35 ms/it, loss 0.445493
Finished training it 45056/76743 of epoch 2, 47.33 ms/it, loss 0.444949
Finished training it 46080/76743 of epoch 2, 48.23 ms/it, loss 0.443130
Finished training it 46080/76743 of epoch 2, 47.76 ms/it, loss 0.445871
Finished training it 46080/76743 of epoch 2, 47.72 ms/it, loss 0.445094
Finished training it 46080/76743 of epoch 2, 48.57 ms/it, loss 0.444727
Finished training it 47104/76743 of epoch 2, 48.74 ms/it, loss 0.443696
Finished training it 47104/76743 of epoch 2, 48.41 ms/it, loss 0.447567
Finished training it 47104/76743 of epoch 2, 48.20 ms/it, loss 0.445758
Finished training it 47104/76743 of epoch 2, 47.81 ms/it, loss 0.446047
Finished training it 48128/76743 of epoch 2, 46.89 ms/it, loss 0.448363
Finished training it 48128/76743 of epoch 2, 47.46 ms/it, loss 0.444303
Finished training it 48128/76743 of epoch 2, 47.19 ms/it, loss 0.444584
Finished training it 48128/76743 of epoch 2, 47.92 ms/it, loss 0.443425
Finished training it 49152/76743 of epoch 2, 47.34 ms/it, loss 0.441121
Finished training it 49152/76743 of epoch 2, 47.20 ms/it, loss 0.445295
Finished training it 49152/76743 of epoch 2, 46.75 ms/it, loss 0.446890
Finished training it 49152/76743 of epoch 2, 46.14 ms/it, loss 0.445434
Finished training it 50176/76743 of epoch 2, 47.78 ms/it, loss 0.446304
Finished training it 50176/76743 of epoch 2, 47.19 ms/it, loss 0.442838
Finished training it 50176/76743 of epoch 2, 47.85 ms/it, loss 0.443065
Finished training it 50176/76743 of epoch 2, 48.51 ms/it, loss 0.444087
Finished training it 51200/76743 of epoch 2, 47.54 ms/it, loss 0.445973
Finished training it 51200/76743 of epoch 2, 47.21 ms/it, loss 0.443715
Finished training it 51200/76743 of epoch 2, 47.82 ms/it, loss 0.443935
Finished training it 51200/76743 of epoch 2, 47.59 ms/it, loss 0.444889
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577010.0
get out
0 has test check 2577010.0 and sample count 3274240
 accuracy 78.706 %, best 78.766 %, roc auc score 0.8008, best 0.8008
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577010.0
get out
3 has test check 2577010.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 46.99 ms/it, loss 0.444117
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577010.0
get out
1 has test check 2577010.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 47.82 ms/it, loss 0.444016
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577010.0
get out
2 has test check 2577010.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 47.44 ms/it, loss 0.447722
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 47.69 ms/it, loss 0.444538
Finished training it 53248/76743 of epoch 2, 47.38 ms/it, loss 0.444978
Finished training it 53248/76743 of epoch 2, 47.50 ms/it, loss 0.444341
Finished training it 53248/76743 of epoch 2, 47.24 ms/it, loss 0.444548
Finished training it 53248/76743 of epoch 2, 47.67 ms/it, loss 0.441762
Finished training it 54272/76743 of epoch 2, 46.60 ms/it, loss 0.444219
Finished training it 54272/76743 of epoch 2, 51.22 ms/it, loss 0.445158
Finished training it 54272/76743 of epoch 2, 50.59 ms/it, loss 0.444399
Finished training it 54272/76743 of epoch 2, 51.34 ms/it, loss 0.446141
Finished training it 55296/76743 of epoch 2, 48.24 ms/it, loss 0.446668
Finished training it 55296/76743 of epoch 2, 47.95 ms/it, loss 0.444605
Finished training it 55296/76743 of epoch 2, 47.36 ms/it, loss 0.443428
Finished training it 55296/76743 of epoch 2, 47.62 ms/it, loss 0.444627
Finished training it 56320/76743 of epoch 2, 47.85 ms/it, loss 0.446789
Finished training it 56320/76743 of epoch 2, 47.61 ms/it, loss 0.443999
Finished training it 56320/76743 of epoch 2, 48.31 ms/it, loss 0.444511
Finished training it 56320/76743 of epoch 2, 47.91 ms/it, loss 0.443795
Finished training it 57344/76743 of epoch 2, 47.08 ms/it, loss 0.446209
Finished training it 57344/76743 of epoch 2, 46.90 ms/it, loss 0.445903
Finished training it 57344/76743 of epoch 2, 47.49 ms/it, loss 0.445737
Finished training it 57344/76743 of epoch 2, 46.86 ms/it, loss 0.443926
Finished training it 58368/76743 of epoch 2, 47.62 ms/it, loss 0.442515
Finished training it 58368/76743 of epoch 2, 47.20 ms/it, loss 0.442387
Finished training it 58368/76743 of epoch 2, 47.08 ms/it, loss 0.443026
Finished training it 58368/76743 of epoch 2, 46.92 ms/it, loss 0.444000
Finished training it 59392/76743 of epoch 2, 46.55 ms/it, loss 0.444661
Finished training it 59392/76743 of epoch 2, 47.00 ms/it, loss 0.444639
Finished training it 59392/76743 of epoch 2, 46.73 ms/it, loss 0.444074
Finished training it 59392/76743 of epoch 2, 46.95 ms/it, loss 0.446141
Finished training it 60416/76743 of epoch 2, 47.44 ms/it, loss 0.445325
Finished training it 60416/76743 of epoch 2, 47.38 ms/it, loss 0.446070
Finished training it 60416/76743 of epoch 2, 47.55 ms/it, loss 0.444649
Finished training it 60416/76743 of epoch 2, 47.85 ms/it, loss 0.443470
Finished training it 61440/76743 of epoch 2, 46.40 ms/it, loss 0.445130
Finished training it 61440/76743 of epoch 2, 46.89 ms/it, loss 0.445572
Finished training it 61440/76743 of epoch 2, 46.98 ms/it, loss 0.446325
Finished training it 61440/76743 of epoch 2, 46.93 ms/it, loss 0.445789
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580324.0
get out
0 has test check 2580324.0 and sample count 3274240
 accuracy 78.807 %, best 78.807 %, roc auc score 0.8013, best 0.8013
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 47.65 ms/it, loss 0.445423
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580324.0
get out
2 has test check 2580324.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 48.01 ms/it, loss 0.442463
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580324.0
get out
3 has test check 2580324.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 47.14 ms/it, loss 0.443682
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580324.0
get out
1 has test check 2580324.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 47.33 ms/it, loss 0.442120
Finished training it 63488/76743 of epoch 2, 47.44 ms/it, loss 0.443975
Finished training it 63488/76743 of epoch 2, 47.56 ms/it, loss 0.445964
Finished training it 63488/76743 of epoch 2, 46.97 ms/it, loss 0.444126
Finished training it 63488/76743 of epoch 2, 47.25 ms/it, loss 0.441777
Finished training it 64512/76743 of epoch 2, 47.65 ms/it, loss 0.441846
Finished training it 64512/76743 of epoch 2, 47.71 ms/it, loss 0.444887
Finished training it 64512/76743 of epoch 2, 47.28 ms/it, loss 0.444827
Finished training it 64512/76743 of epoch 2, 47.08 ms/it, loss 0.441813
Finished training it 65536/76743 of epoch 2, 48.08 ms/it, loss 0.443795
Finished training it 65536/76743 of epoch 2, 49.32 ms/it, loss 0.443090
Finished training it 65536/76743 of epoch 2, 52.36 ms/it, loss 0.444522
Finished training it 65536/76743 of epoch 2, 48.56 ms/it, loss 0.440041
Finished training it 66560/76743 of epoch 2, 47.71 ms/it, loss 0.447593
Finished training it 66560/76743 of epoch 2, 46.99 ms/it, loss 0.443739
Finished training it 66560/76743 of epoch 2, 47.35 ms/it, loss 0.443749
Finished training it 66560/76743 of epoch 2, 46.70 ms/it, loss 0.446621
Finished training it 67584/76743 of epoch 2, 47.71 ms/it, loss 0.445168
Finished training it 67584/76743 of epoch 2, 47.30 ms/it, loss 0.443835
Finished training it 67584/76743 of epoch 2, 48.07 ms/it, loss 0.445504
Finished training it 67584/76743 of epoch 2, 47.49 ms/it, loss 0.444853
Finished training it 68608/76743 of epoch 2, 47.02 ms/it, loss 0.444621
Finished training it 68608/76743 of epoch 2, 46.95 ms/it, loss 0.444012
Finished training it 68608/76743 of epoch 2, 47.07 ms/it, loss 0.443950
Finished training it 68608/76743 of epoch 2, 47.70 ms/it, loss 0.445361
Finished training it 69632/76743 of epoch 2, 47.67 ms/it, loss 0.442878
Finished training it 69632/76743 of epoch 2, 47.40 ms/it, loss 0.442228
Finished training it 69632/76743 of epoch 2, 47.24 ms/it, loss 0.444531
Finished training it 69632/76743 of epoch 2, 47.59 ms/it, loss 0.442861
Finished training it 70656/76743 of epoch 2, 47.27 ms/it, loss 0.446201
Finished training it 70656/76743 of epoch 2, 47.52 ms/it, loss 0.444668
Finished training it 70656/76743 of epoch 2, 47.80 ms/it, loss 0.443818
Finished training it 70656/76743 of epoch 2, 47.70 ms/it, loss 0.442822
Finished training it 71680/76743 of epoch 2, 47.09 ms/it, loss 0.445158
Finished training it 71680/76743 of epoch 2, 47.34 ms/it, loss 0.443744
Finished training it 71680/76743 of epoch 2, 47.71 ms/it, loss 0.442478
Finished training it 71680/76743 of epoch 2, 47.52 ms/it, loss 0.445050
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579180.0
get out
0 has test check 2579180.0 and sample count 3274240
 accuracy 78.772 %, best 78.807 %, roc auc score 0.8010, best 0.8013
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 47.95 ms/it, loss 0.443265
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579180.0
get out
3 has test check 2579180.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 47.42 ms/it, loss 0.445152
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579180.0
get out
1 has test check 2579180.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 47.79 ms/it, loss 0.444585
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579180.0
get out
2 has test check 2579180.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 47.50 ms/it, loss 0.441937
Finished training it 73728/76743 of epoch 2, 48.34 ms/it, loss 0.442943
Finished training it 73728/76743 of epoch 2, 47.61 ms/it, loss 0.442492
Finished training it 73728/76743 of epoch 2, 47.90 ms/it, loss 0.445564
Finished training it 73728/76743 of epoch 2, 48.29 ms/it, loss 0.443397
Finished training it 74752/76743 of epoch 2, 47.18 ms/it, loss 0.443222
Finished training it 74752/76743 of epoch 2, 52.25 ms/it, loss 0.441758
Finished training it 74752/76743 of epoch 2, 51.89 ms/it, loss 0.442070
Finished training it 74752/76743 of epoch 2, 52.04 ms/it, loss 0.446348
Finished training it 75776/76743 of epoch 2, 47.03 ms/it, loss 0.443299
Finished training it 75776/76743 of epoch 2, 47.37 ms/it, loss 0.444721
Finished training it 75776/76743 of epoch 2, 47.15 ms/it, loss 0.441953
Finished training it 75776/76743 of epoch 2, 47.32 ms/it, loss 0.443121
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 48.17 ms/it, loss 0.443768
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 47.58 ms/it, loss 0.444895
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 48.55 ms/it, loss 0.443493
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 48.84 ms/it, loss 0.444502
Finished training it 2048/76743 of epoch 3, 47.21 ms/it, loss 0.445667
Finished training it 2048/76743 of epoch 3, 47.58 ms/it, loss 0.444025
Finished training it 2048/76743 of epoch 3, 47.21 ms/it, loss 0.443961
Finished training it 2048/76743 of epoch 3, 47.41 ms/it, loss 0.445374
Finished training it 3072/76743 of epoch 3, 47.25 ms/it, loss 0.444024
Finished training it 3072/76743 of epoch 3, 47.21 ms/it, loss 0.443316
Finished training it 3072/76743 of epoch 3, 47.57 ms/it, loss 0.443338
Finished training it 3072/76743 of epoch 3, 47.34 ms/it, loss 0.446254
Finished training it 4096/76743 of epoch 3, 47.28 ms/it, loss 0.444855
Finished training it 4096/76743 of epoch 3, 47.52 ms/it, loss 0.441802
Finished training it 4096/76743 of epoch 3, 47.50 ms/it, loss 0.441118
Finished training it 4096/76743 of epoch 3, 47.64 ms/it, loss 0.441419
Finished training it 5120/76743 of epoch 3, 47.99 ms/it, loss 0.442872
Finished training it 5120/76743 of epoch 3, 47.52 ms/it, loss 0.443468
Finished training it 5120/76743 of epoch 3, 46.99 ms/it, loss 0.443773
Finished training it 5120/76743 of epoch 3, 47.21 ms/it, loss 0.441379
Finished training it 6144/76743 of epoch 3, 47.47 ms/it, loss 0.443416
Finished training it 6144/76743 of epoch 3, 47.83 ms/it, loss 0.443505
Finished training it 6144/76743 of epoch 3, 47.58 ms/it, loss 0.438044
Finished training it 6144/76743 of epoch 3, 47.36 ms/it, loss 0.443539
Finished training it 7168/76743 of epoch 3, 47.60 ms/it, loss 0.441822
Finished training it 7168/76743 of epoch 3, 47.64 ms/it, loss 0.445406
Finished training it 7168/76743 of epoch 3, 47.70 ms/it, loss 0.445623
Finished training it 7168/76743 of epoch 3, 47.90 ms/it, loss 0.442571
Finished training it 8192/76743 of epoch 3, 47.71 ms/it, loss 0.443156
Finished training it 8192/76743 of epoch 3, 47.75 ms/it, loss 0.444504
Finished training it 8192/76743 of epoch 3, 47.82 ms/it, loss 0.443057
Finished training it 8192/76743 of epoch 3, 47.30 ms/it, loss 0.440860
Finished training it 9216/76743 of epoch 3, 47.73 ms/it, loss 0.442412
Finished training it 9216/76743 of epoch 3, 47.10 ms/it, loss 0.443544
Finished training it 9216/76743 of epoch 3, 47.19 ms/it, loss 0.442333
Finished training it 9216/76743 of epoch 3, 47.44 ms/it, loss 0.443905
Finished training it 10240/76743 of epoch 3, 47.87 ms/it, loss 0.443839
Finished training it 10240/76743 of epoch 3, 47.50 ms/it, loss 0.442792
Finished training it 10240/76743 of epoch 3, 47.81 ms/it, loss 0.440968
Finished training it 10240/76743 of epoch 3, 48.10 ms/it, loss 0.442486
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577291.0
get out
0 has test check 2577291.0 and sample count 3274240
 accuracy 78.714 %, best 78.807 %, roc auc score 0.8015, best 0.8015
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577291.0
get out
3 has test check 2577291.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 46.94 ms/it, loss 0.442279
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577291.0
get out
1 has test check 2577291.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 47.57 ms/it, loss 0.442244
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577291.0
get out
2 has test check 2577291.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 47.51 ms/it, loss 0.444377
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 47.90 ms/it, loss 0.442700
Finished training it 12288/76743 of epoch 3, 47.36 ms/it, loss 0.440977
Finished training it 12288/76743 of epoch 3, 47.66 ms/it, loss 0.442302
Finished training it 12288/76743 of epoch 3, 47.60 ms/it, loss 0.440722
Finished training it 12288/76743 of epoch 3, 47.30 ms/it, loss 0.443378
Finished training it 13312/76743 of epoch 3, 47.41 ms/it, loss 0.442610
Finished training it 13312/76743 of epoch 3, 48.09 ms/it, loss 0.439700
Finished training it 13312/76743 of epoch 3, 48.18 ms/it, loss 0.441488
Finished training it 13312/76743 of epoch 3, 52.20 ms/it, loss 0.444493
Finished training it 14336/76743 of epoch 3, 48.09 ms/it, loss 0.444266
Finished training it 14336/76743 of epoch 3, 49.79 ms/it, loss 0.443471
Finished training it 14336/76743 of epoch 3, 48.23 ms/it, loss 0.444225
Finished training it 14336/76743 of epoch 3, 48.47 ms/it, loss 0.444561
Finished training it 15360/76743 of epoch 3, 47.58 ms/it, loss 0.444466
Finished training it 15360/76743 of epoch 3, 47.26 ms/it, loss 0.443131
Finished training it 15360/76743 of epoch 3, 47.78 ms/it, loss 0.441714
Finished training it 15360/76743 of epoch 3, 48.22 ms/it, loss 0.444238
Finished training it 16384/76743 of epoch 3, 46.94 ms/it, loss 0.441348
Finished training it 16384/76743 of epoch 3, 47.22 ms/it, loss 0.443851
Finished training it 16384/76743 of epoch 3, 47.25 ms/it, loss 0.443433
Finished training it 16384/76743 of epoch 3, 46.84 ms/it, loss 0.441876
Finished training it 17408/76743 of epoch 3, 46.55 ms/it, loss 0.444902
Finished training it 17408/76743 of epoch 3, 46.67 ms/it, loss 0.441963
Finished training it 17408/76743 of epoch 3, 47.01 ms/it, loss 0.444998
Finished training it 17408/76743 of epoch 3, 46.81 ms/it, loss 0.443388
Finished training it 18432/76743 of epoch 3, 47.22 ms/it, loss 0.442492
Finished training it 18432/76743 of epoch 3, 47.63 ms/it, loss 0.443393
Finished training it 18432/76743 of epoch 3, 47.84 ms/it, loss 0.441575
Finished training it 18432/76743 of epoch 3, 48.19 ms/it, loss 0.440965
Finished training it 19456/76743 of epoch 3, 47.43 ms/it, loss 0.443065
Finished training it 19456/76743 of epoch 3, 47.97 ms/it, loss 0.441529
Finished training it 19456/76743 of epoch 3, 48.13 ms/it, loss 0.444009
Finished training it 19456/76743 of epoch 3, 47.61 ms/it, loss 0.438251
Finished training it 20480/76743 of epoch 3, 48.03 ms/it, loss 0.443761
Finished training it 20480/76743 of epoch 3, 47.97 ms/it, loss 0.442508
Finished training it 20480/76743 of epoch 3, 47.57 ms/it, loss 0.441769
Finished training it 20480/76743 of epoch 3, 48.06 ms/it, loss 0.441858
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579644.0
get out
0 has test check 2579644.0 and sample count 3274240
 accuracy 78.786 %, best 78.807 %, roc auc score 0.8017, best 0.8017
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579644.0
get out
3 has test check 2579644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 47.16 ms/it, loss 0.442339
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 47.46 ms/it, loss 0.441915
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579644.0
get out
2 has test check 2579644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 47.57 ms/it, loss 0.442346
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579644.0
get out
1 has test check 2579644.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 47.46 ms/it, loss 0.444096
Finished training it 22528/76743 of epoch 3, 46.68 ms/it, loss 0.442034
Finished training it 22528/76743 of epoch 3, 46.58 ms/it, loss 0.442338
Finished training it 22528/76743 of epoch 3, 46.89 ms/it, loss 0.442969
Finished training it 22528/76743 of epoch 3, 46.61 ms/it, loss 0.441870
Finished training it 23552/76743 of epoch 3, 51.99 ms/it, loss 0.442341
Finished training it 23552/76743 of epoch 3, 51.62 ms/it, loss 0.442377
Finished training it 23552/76743 of epoch 3, 47.24 ms/it, loss 0.440859
Finished training it 23552/76743 of epoch 3, 51.43 ms/it, loss 0.443926
Finished training it 24576/76743 of epoch 3, 47.86 ms/it, loss 0.442127
Finished training it 24576/76743 of epoch 3, 47.49 ms/it, loss 0.441425
Finished training it 24576/76743 of epoch 3, 47.30 ms/it, loss 0.441056
Finished training it 24576/76743 of epoch 3, 47.21 ms/it, loss 0.443774
Finished training it 25600/76743 of epoch 3, 47.02 ms/it, loss 0.441360
Finished training it 25600/76743 of epoch 3, 47.04 ms/it, loss 0.444022
Finished training it 25600/76743 of epoch 3, 47.20 ms/it, loss 0.441764
Finished training it 25600/76743 of epoch 3, 47.37 ms/it, loss 0.442888
Finished training it 26624/76743 of epoch 3, 47.51 ms/it, loss 0.444054
Finished training it 26624/76743 of epoch 3, 47.74 ms/it, loss 0.442184
Finished training it 26624/76743 of epoch 3, 47.44 ms/it, loss 0.443750
Finished training it 26624/76743 of epoch 3, 48.06 ms/it, loss 0.439688
Finished training it 27648/76743 of epoch 3, 47.27 ms/it, loss 0.440708
Finished training it 27648/76743 of epoch 3, 47.44 ms/it, loss 0.441502
Finished training it 27648/76743 of epoch 3, 47.25 ms/it, loss 0.443020
Finished training it 27648/76743 of epoch 3, 47.20 ms/it, loss 0.443983
Finished training it 28672/76743 of epoch 3, 46.33 ms/it, loss 0.440209
Finished training it 28672/76743 of epoch 3, 46.49 ms/it, loss 0.442520
Finished training it 28672/76743 of epoch 3, 47.24 ms/it, loss 0.441507
Finished training it 28672/76743 of epoch 3, 46.77 ms/it, loss 0.443523
Finished training it 29696/76743 of epoch 3, 47.46 ms/it, loss 0.443019
Finished training it 29696/76743 of epoch 3, 47.37 ms/it, loss 0.441649
Finished training it 29696/76743 of epoch 3, 47.08 ms/it, loss 0.439282
Finished training it 29696/76743 of epoch 3, 47.90 ms/it, loss 0.442658
Finished training it 30720/76743 of epoch 3, 46.83 ms/it, loss 0.442270
Finished training it 30720/76743 of epoch 3, 47.30 ms/it, loss 0.443950
Finished training it 30720/76743 of epoch 3, 46.87 ms/it, loss 0.445068
Finished training it 30720/76743 of epoch 3, 46.91 ms/it, loss 0.443421
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581257.0
get out
0 has test check 2581257.0 and sample count 3274240
 accuracy 78.835 %, best 78.835 %, roc auc score 0.8021, best 0.8021
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581257.0
get out
3 has test check 2581257.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 47.38 ms/it, loss 0.442203
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 47.58 ms/it, loss 0.443975
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581257.0
get out
2 has test check 2581257.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 47.43 ms/it, loss 0.442863
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581257.0
get out
1 has test check 2581257.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 47.48 ms/it, loss 0.443024
Finished training it 32768/76743 of epoch 3, 47.05 ms/it, loss 0.443264
Finished training it 32768/76743 of epoch 3, 46.80 ms/it, loss 0.440453
Finished training it 32768/76743 of epoch 3, 46.74 ms/it, loss 0.445342
Finished training it 32768/76743 of epoch 3, 47.32 ms/it, loss 0.440551
Finished training it 33792/76743 of epoch 3, 47.36 ms/it, loss 0.443156
Finished training it 33792/76743 of epoch 3, 51.91 ms/it, loss 0.443135
Finished training it 33792/76743 of epoch 3, 48.00 ms/it, loss 0.440063
Finished training it 33792/76743 of epoch 3, 47.43 ms/it, loss 0.438803
Finished training it 34816/76743 of epoch 3, 47.08 ms/it, loss 0.441210
Finished training it 34816/76743 of epoch 3, 47.83 ms/it, loss 0.441018
Finished training it 34816/76743 of epoch 3, 47.65 ms/it, loss 0.440262
Finished training it 34816/76743 of epoch 3, 47.87 ms/it, loss 0.443506
Finished training it 35840/76743 of epoch 3, 46.93 ms/it, loss 0.442039
Finished training it 35840/76743 of epoch 3, 47.06 ms/it, loss 0.443272
Finished training it 35840/76743 of epoch 3, 47.01 ms/it, loss 0.443573
Finished training it 35840/76743 of epoch 3, 47.49 ms/it, loss 0.442452
Finished training it 36864/76743 of epoch 3, 47.45 ms/it, loss 0.440458
Finished training it 36864/76743 of epoch 3, 47.58 ms/it, loss 0.442715
Finished training it 36864/76743 of epoch 3, 47.31 ms/it, loss 0.440404
Finished training it 36864/76743 of epoch 3, 47.90 ms/it, loss 0.442257
Finished training it 37888/76743 of epoch 3, 47.08 ms/it, loss 0.440039
Finished training it 37888/76743 of epoch 3, 47.74 ms/it, loss 0.442904
Finished training it 37888/76743 of epoch 3, 47.27 ms/it, loss 0.442207
Finished training it 37888/76743 of epoch 3, 47.51 ms/it, loss 0.442863
Finished training it 38912/76743 of epoch 3, 47.57 ms/it, loss 0.443199
Finished training it 38912/76743 of epoch 3, 47.66 ms/it, loss 0.441136
Finished training it 38912/76743 of epoch 3, 46.60 ms/it, loss 0.443191
Finished training it 38912/76743 of epoch 3, 47.02 ms/it, loss 0.440030
Finished training it 39936/76743 of epoch 3, 47.03 ms/it, loss 0.443534
Finished training it 39936/76743 of epoch 3, 46.96 ms/it, loss 0.441475
Finished training it 39936/76743 of epoch 3, 47.71 ms/it, loss 0.439746
Finished training it 39936/76743 of epoch 3, 47.12 ms/it, loss 0.442056
Finished training it 40960/76743 of epoch 3, 47.36 ms/it, loss 0.439957
Finished training it 40960/76743 of epoch 3, 47.98 ms/it, loss 0.445874
Finished training it 40960/76743 of epoch 3, 47.39 ms/it, loss 0.443921
Finished training it 40960/76743 of epoch 3, 47.49 ms/it, loss 0.440912
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580913.0
get out
0 has test check 2580913.0 and sample count 3274240
 accuracy 78.825 %, best 78.835 %, roc auc score 0.8021, best 0.8021
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 47.79 ms/it, loss 0.443129
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580913.0
get out
3 has test check 2580913.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 47.39 ms/it, loss 0.443048
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580913.0
get out
1 has test check 2580913.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 47.31 ms/it, loss 0.442591
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580913.0
get out
2 has test check 2580913.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 47.75 ms/it, loss 0.442358
Finished training it 43008/76743 of epoch 3, 47.69 ms/it, loss 0.439758
Finished training it 43008/76743 of epoch 3, 47.86 ms/it, loss 0.441621
Finished training it 43008/76743 of epoch 3, 47.26 ms/it, loss 0.441498
Finished training it 43008/76743 of epoch 3, 47.43 ms/it, loss 0.440637
Finished training it 44032/76743 of epoch 3, 54.10 ms/it, loss 0.443012
Finished training it 44032/76743 of epoch 3, 54.07 ms/it, loss 0.441600
Finished training it 44032/76743 of epoch 3, 54.61 ms/it, loss 0.441927
Finished training it 44032/76743 of epoch 3, 48.43 ms/it, loss 0.441328
Finished training it 45056/76743 of epoch 3, 48.30 ms/it, loss 0.441949
Finished training it 45056/76743 of epoch 3, 47.97 ms/it, loss 0.443272
Finished training it 45056/76743 of epoch 3, 46.89 ms/it, loss 0.440686
Finished training it 45056/76743 of epoch 3, 47.27 ms/it, loss 0.441685
Finished training it 46080/76743 of epoch 3, 47.68 ms/it, loss 0.439624
Finished training it 46080/76743 of epoch 3, 47.44 ms/it, loss 0.441827
Finished training it 46080/76743 of epoch 3, 47.95 ms/it, loss 0.441336
Finished training it 46080/76743 of epoch 3, 47.19 ms/it, loss 0.442358
Finished training it 47104/76743 of epoch 3, 47.54 ms/it, loss 0.442324
Finished training it 47104/76743 of epoch 3, 48.40 ms/it, loss 0.440381
Finished training it 47104/76743 of epoch 3, 47.74 ms/it, loss 0.444382
Finished training it 47104/76743 of epoch 3, 47.52 ms/it, loss 0.442600
Finished training it 48128/76743 of epoch 3, 46.90 ms/it, loss 0.444881
Finished training it 48128/76743 of epoch 3, 47.09 ms/it, loss 0.440915
Finished training it 48128/76743 of epoch 3, 47.29 ms/it, loss 0.441104
Finished training it 48128/76743 of epoch 3, 47.67 ms/it, loss 0.440081
Finished training it 49152/76743 of epoch 3, 47.61 ms/it, loss 0.437746
Finished training it 49152/76743 of epoch 3, 47.73 ms/it, loss 0.442123
Finished training it 49152/76743 of epoch 3, 47.00 ms/it, loss 0.441884
Finished training it 49152/76743 of epoch 3, 47.54 ms/it, loss 0.443514
Finished training it 50176/76743 of epoch 3, 46.88 ms/it, loss 0.439529
Finished training it 50176/76743 of epoch 3, 46.57 ms/it, loss 0.439832
Finished training it 50176/76743 of epoch 3, 46.88 ms/it, loss 0.440845
Finished training it 50176/76743 of epoch 3, 46.73 ms/it, loss 0.442786
Finished training it 51200/76743 of epoch 3, 47.31 ms/it, loss 0.440354
Finished training it 51200/76743 of epoch 3, 47.89 ms/it, loss 0.440485
Finished training it 51200/76743 of epoch 3, 47.48 ms/it, loss 0.441666
Finished training it 51200/76743 of epoch 3, 47.42 ms/it, loss 0.442652
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578625.0
get out
0 has test check 2578625.0 and sample count 3274240
 accuracy 78.755 %, best 78.835 %, roc auc score 0.8021, best 0.8021
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578625.0
get out
1 has test check 2578625.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 47.53 ms/it, loss 0.440655
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578625.0
get out
2 has test check 2578625.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 47.97 ms/it, loss 0.444502
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578625.0
get out
3 has test check 2578625.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 47.53 ms/it, loss 0.440598
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 47.74 ms/it, loss 0.441245
Finished training it 53248/76743 of epoch 3, 47.72 ms/it, loss 0.441765
Finished training it 53248/76743 of epoch 3, 48.00 ms/it, loss 0.438524
Finished training it 53248/76743 of epoch 3, 47.65 ms/it, loss 0.440941
Finished training it 53248/76743 of epoch 3, 47.93 ms/it, loss 0.441016
Finished training it 54272/76743 of epoch 3, 47.24 ms/it, loss 0.442765
Finished training it 54272/76743 of epoch 3, 48.41 ms/it, loss 0.440875
Finished training it 54272/76743 of epoch 3, 53.96 ms/it, loss 0.440834
Finished training it 54272/76743 of epoch 3, 49.06 ms/it, loss 0.441843
Finished training it 55296/76743 of epoch 3, 47.07 ms/it, loss 0.441180
Finished training it 55296/76743 of epoch 3, 48.13 ms/it, loss 0.441431
Finished training it 55296/76743 of epoch 3, 48.11 ms/it, loss 0.439975
Finished training it 55296/76743 of epoch 3, 48.78 ms/it, loss 0.443109
Finished training it 56320/76743 of epoch 3, 48.25 ms/it, loss 0.441069
Finished training it 56320/76743 of epoch 3, 47.48 ms/it, loss 0.440564
Finished training it 56320/76743 of epoch 3, 47.11 ms/it, loss 0.440510
Finished training it 56320/76743 of epoch 3, 47.55 ms/it, loss 0.443390
Finished training it 57344/76743 of epoch 3, 47.10 ms/it, loss 0.440694
Finished training it 57344/76743 of epoch 3, 46.97 ms/it, loss 0.443103
Finished training it 57344/76743 of epoch 3, 47.62 ms/it, loss 0.442757
Finished training it 57344/76743 of epoch 3, 48.19 ms/it, loss 0.442540
Finished training it 58368/76743 of epoch 3, 47.30 ms/it, loss 0.439624
Finished training it 58368/76743 of epoch 3, 47.15 ms/it, loss 0.440591
Finished training it 58368/76743 of epoch 3, 48.13 ms/it, loss 0.439175
Finished training it 58368/76743 of epoch 3, 47.46 ms/it, loss 0.439280
Finished training it 59392/76743 of epoch 3, 47.74 ms/it, loss 0.442786
Finished training it 59392/76743 of epoch 3, 48.15 ms/it, loss 0.441162
Finished training it 59392/76743 of epoch 3, 47.33 ms/it, loss 0.441319
Finished training it 59392/76743 of epoch 3, 47.77 ms/it, loss 0.440682
Finished training it 60416/76743 of epoch 3, 47.30 ms/it, loss 0.441991
Finished training it 60416/76743 of epoch 3, 48.16 ms/it, loss 0.441353
Finished training it 60416/76743 of epoch 3, 47.62 ms/it, loss 0.440196
Finished training it 60416/76743 of epoch 3, 47.49 ms/it, loss 0.442706
Finished training it 61440/76743 of epoch 3, 47.12 ms/it, loss 0.442887
Finished training it 61440/76743 of epoch 3, 47.14 ms/it, loss 0.442261
Finished training it 61440/76743 of epoch 3, 46.90 ms/it, loss 0.441988
Finished training it 61440/76743 of epoch 3, 47.63 ms/it, loss 0.442487
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582252.0
get out
0 has test check 2582252.0 and sample count 3274240
 accuracy 78.866 %, best 78.866 %, roc auc score 0.8025, best 0.8025
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582252.0
get out
3 has test check 2582252.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 47.94 ms/it, loss 0.440344
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582252.0
get out
2 has test check 2582252.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 48.24 ms/it, loss 0.439063
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582252.0
get out
1 has test check 2582252.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 48.21 ms/it, loss 0.438635
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 48.20 ms/it, loss 0.442271
Finished training it 63488/76743 of epoch 3, 46.79 ms/it, loss 0.440700
Finished training it 63488/76743 of epoch 3, 47.43 ms/it, loss 0.442686
Finished training it 63488/76743 of epoch 3, 47.11 ms/it, loss 0.440747
Finished training it 63488/76743 of epoch 3, 47.17 ms/it, loss 0.438457
Finished training it 64512/76743 of epoch 3, 47.56 ms/it, loss 0.438550
Finished training it 64512/76743 of epoch 3, 51.74 ms/it, loss 0.438571
Finished training it 64512/76743 of epoch 3, 52.34 ms/it, loss 0.441427
Finished training it 64512/76743 of epoch 3, 51.57 ms/it, loss 0.441448
Finished training it 65536/76743 of epoch 3, 47.11 ms/it, loss 0.436688
Finished training it 65536/76743 of epoch 3, 47.07 ms/it, loss 0.440934
Finished training it 65536/76743 of epoch 3, 47.73 ms/it, loss 0.439709
Finished training it 65536/76743 of epoch 3, 46.75 ms/it, loss 0.440517
Finished training it 66560/76743 of epoch 3, 47.39 ms/it, loss 0.440360
Finished training it 66560/76743 of epoch 3, 47.12 ms/it, loss 0.440487
Finished training it 66560/76743 of epoch 3, 47.14 ms/it, loss 0.443262
Finished training it 66560/76743 of epoch 3, 47.94 ms/it, loss 0.444486
Finished training it 67584/76743 of epoch 3, 47.09 ms/it, loss 0.442214
Finished training it 67584/76743 of epoch 3, 46.59 ms/it, loss 0.441824
Finished training it 67584/76743 of epoch 3, 46.48 ms/it, loss 0.440774
Finished training it 67584/76743 of epoch 3, 46.74 ms/it, loss 0.441568
Finished training it 68608/76743 of epoch 3, 47.35 ms/it, loss 0.440570
Finished training it 68608/76743 of epoch 3, 46.69 ms/it, loss 0.440744
Finished training it 68608/76743 of epoch 3, 47.34 ms/it, loss 0.442051
Finished training it 68608/76743 of epoch 3, 47.30 ms/it, loss 0.441379
Finished training it 69632/76743 of epoch 3, 47.70 ms/it, loss 0.439651
Finished training it 69632/76743 of epoch 3, 47.28 ms/it, loss 0.441250
Finished training it 69632/76743 of epoch 3, 47.94 ms/it, loss 0.439634
Finished training it 69632/76743 of epoch 3, 47.53 ms/it, loss 0.438910
Finished training it 70656/76743 of epoch 3, 47.08 ms/it, loss 0.442954
Finished training it 70656/76743 of epoch 3, 47.76 ms/it, loss 0.439629
Finished training it 70656/76743 of epoch 3, 47.56 ms/it, loss 0.440626
Finished training it 70656/76743 of epoch 3, 47.16 ms/it, loss 0.441388
Finished training it 71680/76743 of epoch 3, 46.98 ms/it, loss 0.440558
Finished training it 71680/76743 of epoch 3, 47.23 ms/it, loss 0.441850
Finished training it 71680/76743 of epoch 3, 47.74 ms/it, loss 0.441875
Finished training it 71680/76743 of epoch 3, 47.66 ms/it, loss 0.439265
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580831.0
get out
0 has test check 2580831.0 and sample count 3274240
 accuracy 78.822 %, best 78.866 %, roc auc score 0.8022, best 0.8025
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 47.60 ms/it, loss 0.439920
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580831.0
get out
3 has test check 2580831.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 47.94 ms/it, loss 0.441943
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580831.0
get out
2 has test check 2580831.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 47.81 ms/it, loss 0.438763
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580831.0
get out
1 has test check 2580831.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 47.69 ms/it, loss 0.441172
Finished training it 73728/76743 of epoch 3, 47.32 ms/it, loss 0.440358
Finished training it 73728/76743 of epoch 3, 47.39 ms/it, loss 0.439476
Finished training it 73728/76743 of epoch 3, 47.12 ms/it, loss 0.439330
Finished training it 73728/76743 of epoch 3, 47.69 ms/it, loss 0.442423
Finished training it 74752/76743 of epoch 3, 46.84 ms/it, loss 0.443034
Finished training it 74752/76743 of epoch 3, 46.68 ms/it, loss 0.438861
Finished training it 74752/76743 of epoch 3, 47.46 ms/it, loss 0.438399
Finished training it 74752/76743 of epoch 3, 47.02 ms/it, loss 0.439989
Finished training it 75776/76743 of epoch 3, 49.53 ms/it, loss 0.441301
Finished training it 75776/76743 of epoch 3, 52.19 ms/it, loss 0.440048
Finished training it 75776/76743 of epoch 3, 47.87 ms/it, loss 0.438914
Finished training it 75776/76743 of epoch 3, 47.85 ms/it, loss 0.439873
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 48.99 ms/it, loss 0.441124
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 48.34 ms/it, loss 0.440491
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 48.39 ms/it, loss 0.441689
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 49.16 ms/it, loss 0.440208
Finished training it 2048/76743 of epoch 4, 47.71 ms/it, loss 0.442497
Finished training it 2048/76743 of epoch 4, 47.54 ms/it, loss 0.442354
Finished training it 2048/76743 of epoch 4, 47.08 ms/it, loss 0.440575
Finished training it 2048/76743 of epoch 4, 47.28 ms/it, loss 0.440994
Finished training it 3072/76743 of epoch 4, 46.88 ms/it, loss 0.443005
Finished training it 3072/76743 of epoch 4, 47.07 ms/it, loss 0.440803
Finished training it 3072/76743 of epoch 4, 47.39 ms/it, loss 0.439953
Finished training it 3072/76743 of epoch 4, 47.79 ms/it, loss 0.440170
Finished training it 4096/76743 of epoch 4, 47.11 ms/it, loss 0.441455
Finished training it 4096/76743 of epoch 4, 47.05 ms/it, loss 0.438586
Finished training it 4096/76743 of epoch 4, 46.51 ms/it, loss 0.438092
Finished training it 4096/76743 of epoch 4, 46.87 ms/it, loss 0.437769
Finished training it 5120/76743 of epoch 4, 47.96 ms/it, loss 0.439697
Finished training it 5120/76743 of epoch 4, 47.11 ms/it, loss 0.440064
Finished training it 5120/76743 of epoch 4, 47.91 ms/it, loss 0.438117
Finished training it 5120/76743 of epoch 4, 46.80 ms/it, loss 0.440337
Finished training it 6144/76743 of epoch 4, 47.15 ms/it, loss 0.440426
Finished training it 6144/76743 of epoch 4, 46.91 ms/it, loss 0.440162
Finished training it 6144/76743 of epoch 4, 46.51 ms/it, loss 0.434601
Finished training it 6144/76743 of epoch 4, 47.58 ms/it, loss 0.440374
Finished training it 7168/76743 of epoch 4, 47.82 ms/it, loss 0.442264
Finished training it 7168/76743 of epoch 4, 48.28 ms/it, loss 0.439296
Finished training it 7168/76743 of epoch 4, 47.74 ms/it, loss 0.442240
Finished training it 7168/76743 of epoch 4, 47.84 ms/it, loss 0.438653
Finished training it 8192/76743 of epoch 4, 47.04 ms/it, loss 0.441191
Finished training it 8192/76743 of epoch 4, 47.69 ms/it, loss 0.440036
Finished training it 8192/76743 of epoch 4, 46.81 ms/it, loss 0.437675
Finished training it 8192/76743 of epoch 4, 46.82 ms/it, loss 0.439708
Finished training it 9216/76743 of epoch 4, 47.56 ms/it, loss 0.440741
Finished training it 9216/76743 of epoch 4, 47.14 ms/it, loss 0.439381
Finished training it 9216/76743 of epoch 4, 47.61 ms/it, loss 0.439125
Finished training it 9216/76743 of epoch 4, 47.17 ms/it, loss 0.440169
Finished training it 10240/76743 of epoch 4, 47.39 ms/it, loss 0.440596
Finished training it 10240/76743 of epoch 4, 47.41 ms/it, loss 0.439508
Finished training it 10240/76743 of epoch 4, 47.16 ms/it, loss 0.437581
Finished training it 10240/76743 of epoch 4, 47.02 ms/it, loss 0.439212
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578270.0
get out
0 has test check 2578270.0 and sample count 3274240
 accuracy 78.744 %, best 78.866 %, roc auc score 0.8024, best 0.8025
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 47.81 ms/it, loss 0.439453
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578270.0
get out
3 has test check 2578270.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 47.62 ms/it, loss 0.439089
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578270.0
get out
1 has test check 2578270.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 47.72 ms/it, loss 0.438934
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578270.0
get out
2 has test check 2578270.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 48.32 ms/it, loss 0.441284
Finished training it 12288/76743 of epoch 4, 47.77 ms/it, loss 0.439120
Finished training it 12288/76743 of epoch 4, 48.04 ms/it, loss 0.437466
Finished training it 12288/76743 of epoch 4, 47.61 ms/it, loss 0.437756
Finished training it 12288/76743 of epoch 4, 47.44 ms/it, loss 0.440178
Finished training it 13312/76743 of epoch 4, 47.35 ms/it, loss 0.441514
Finished training it 13312/76743 of epoch 4, 46.84 ms/it, loss 0.439350
Finished training it 13312/76743 of epoch 4, 47.50 ms/it, loss 0.436399
Finished training it 13312/76743 of epoch 4, 46.71 ms/it, loss 0.438425
Finished training it 14336/76743 of epoch 4, 52.29 ms/it, loss 0.440312
Finished training it 14336/76743 of epoch 4, 51.89 ms/it, loss 0.440833
Finished training it 14336/76743 of epoch 4, 51.76 ms/it, loss 0.441405
Finished training it 14336/76743 of epoch 4, 48.10 ms/it, loss 0.441146
Finished training it 15360/76743 of epoch 4, 48.09 ms/it, loss 0.441007
Finished training it 15360/76743 of epoch 4, 47.12 ms/it, loss 0.439947
Finished training it 15360/76743 of epoch 4, 47.15 ms/it, loss 0.441090
Finished training it 15360/76743 of epoch 4, 47.12 ms/it, loss 0.438542
Finished training it 16384/76743 of epoch 4, 47.28 ms/it, loss 0.438666
Finished training it 16384/76743 of epoch 4, 47.50 ms/it, loss 0.438135
Finished training it 16384/76743 of epoch 4, 47.56 ms/it, loss 0.440337
Finished training it 16384/76743 of epoch 4, 48.18 ms/it, loss 0.440728
Finished training it 17408/76743 of epoch 4, 47.59 ms/it, loss 0.441928
Finished training it 17408/76743 of epoch 4, 47.45 ms/it, loss 0.440137
Finished training it 17408/76743 of epoch 4, 47.16 ms/it, loss 0.441573
Finished training it 17408/76743 of epoch 4, 47.26 ms/it, loss 0.438554
Finished training it 18432/76743 of epoch 4, 46.86 ms/it, loss 0.439329
Finished training it 18432/76743 of epoch 4, 47.17 ms/it, loss 0.438198
Finished training it 18432/76743 of epoch 4, 47.68 ms/it, loss 0.440115
Finished training it 18432/76743 of epoch 4, 47.52 ms/it, loss 0.437911
Finished training it 19456/76743 of epoch 4, 47.85 ms/it, loss 0.438252
Finished training it 19456/76743 of epoch 4, 47.51 ms/it, loss 0.439842
Finished training it 19456/76743 of epoch 4, 48.07 ms/it, loss 0.440699
Finished training it 19456/76743 of epoch 4, 47.82 ms/it, loss 0.435068
Finished training it 20480/76743 of epoch 4, 47.12 ms/it, loss 0.438695
Finished training it 20480/76743 of epoch 4, 47.82 ms/it, loss 0.438535
Finished training it 20480/76743 of epoch 4, 47.42 ms/it, loss 0.440501
Finished training it 20480/76743 of epoch 4, 47.32 ms/it, loss 0.439296
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580639.0
get out
0 has test check 2580639.0 and sample count 3274240
 accuracy 78.816 %, best 78.866 %, roc auc score 0.8024, best 0.8025
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580639.0
get out
3 has test check 2580639.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 46.96 ms/it, loss 0.439120
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580639.0
get out
2 has test check 2580639.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 46.90 ms/it, loss 0.439274
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 47.33 ms/it, loss 0.438780
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580639.0
get out
1 has test check 2580639.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 47.28 ms/it, loss 0.440941
Finished training it 22528/76743 of epoch 4, 47.84 ms/it, loss 0.438721
Finished training it 22528/76743 of epoch 4, 47.82 ms/it, loss 0.439846
Finished training it 22528/76743 of epoch 4, 47.69 ms/it, loss 0.438987
Finished training it 22528/76743 of epoch 4, 47.78 ms/it, loss 0.438662
Finished training it 23552/76743 of epoch 4, 53.19 ms/it, loss 0.437676
Finished training it 23552/76743 of epoch 4, 48.51 ms/it, loss 0.439174
Finished training it 23552/76743 of epoch 4, 49.13 ms/it, loss 0.439121
Finished training it 23552/76743 of epoch 4, 48.28 ms/it, loss 0.440822
Finished training it 24576/76743 of epoch 4, 48.05 ms/it, loss 0.437812
Finished training it 24576/76743 of epoch 4, 48.33 ms/it, loss 0.440666
Finished training it 24576/76743 of epoch 4, 47.93 ms/it, loss 0.438377
Finished training it 24576/76743 of epoch 4, 48.06 ms/it, loss 0.438872
Finished training it 25600/76743 of epoch 4, 47.70 ms/it, loss 0.440844
Finished training it 25600/76743 of epoch 4, 48.03 ms/it, loss 0.439772
Finished training it 25600/76743 of epoch 4, 47.65 ms/it, loss 0.438081
Finished training it 25600/76743 of epoch 4, 47.55 ms/it, loss 0.438611
Finished training it 26624/76743 of epoch 4, 48.21 ms/it, loss 0.436427
Finished training it 26624/76743 of epoch 4, 48.01 ms/it, loss 0.439198
Finished training it 26624/76743 of epoch 4, 47.58 ms/it, loss 0.440879
Finished training it 26624/76743 of epoch 4, 47.69 ms/it, loss 0.440775
Finished training it 27648/76743 of epoch 4, 47.92 ms/it, loss 0.438116
Finished training it 27648/76743 of epoch 4, 47.12 ms/it, loss 0.439814
Finished training it 27648/76743 of epoch 4, 47.35 ms/it, loss 0.440598
Finished training it 27648/76743 of epoch 4, 47.12 ms/it, loss 0.437494
Finished training it 28672/76743 of epoch 4, 47.15 ms/it, loss 0.439397
Finished training it 28672/76743 of epoch 4, 46.60 ms/it, loss 0.436886
Finished training it 28672/76743 of epoch 4, 47.49 ms/it, loss 0.438292
Finished training it 28672/76743 of epoch 4, 46.88 ms/it, loss 0.440452
Finished training it 29696/76743 of epoch 4, 47.92 ms/it, loss 0.439705
Finished training it 29696/76743 of epoch 4, 47.70 ms/it, loss 0.436039
Finished training it 29696/76743 of epoch 4, 48.12 ms/it, loss 0.439474
Finished training it 29696/76743 of epoch 4, 47.69 ms/it, loss 0.438553
Finished training it 30720/76743 of epoch 4, 47.74 ms/it, loss 0.440240
Finished training it 30720/76743 of epoch 4, 47.65 ms/it, loss 0.438937
Finished training it 30720/76743 of epoch 4, 47.92 ms/it, loss 0.441753
Finished training it 30720/76743 of epoch 4, 48.29 ms/it, loss 0.440683
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582116.0
get out
0 has test check 2582116.0 and sample count 3274240
 accuracy 78.862 %, best 78.866 %, roc auc score 0.8028, best 0.8028
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582116.0
get out
1 has test check 2582116.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 47.80 ms/it, loss 0.439813
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582116.0
get out
3 has test check 2582116.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 47.80 ms/it, loss 0.439081
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 48.19 ms/it, loss 0.440694
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582116.0
get out
2 has test check 2582116.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 48.18 ms/it, loss 0.439550
Finished training it 32768/76743 of epoch 4, 47.84 ms/it, loss 0.440033
Finished training it 32768/76743 of epoch 4, 48.61 ms/it, loss 0.437306
Finished training it 32768/76743 of epoch 4, 47.54 ms/it, loss 0.437099
Finished training it 32768/76743 of epoch 4, 47.59 ms/it, loss 0.442173
Finished training it 33792/76743 of epoch 4, 47.34 ms/it, loss 0.440136
Finished training it 33792/76743 of epoch 4, 47.78 ms/it, loss 0.436815
Finished training it 33792/76743 of epoch 4, 47.22 ms/it, loss 0.439937
Finished training it 33792/76743 of epoch 4, 47.15 ms/it, loss 0.435579
Finished training it 34816/76743 of epoch 4, 52.04 ms/it, loss 0.437725
Finished training it 34816/76743 of epoch 4, 51.21 ms/it, loss 0.440282
Finished training it 34816/76743 of epoch 4, 51.20 ms/it, loss 0.437005
Finished training it 34816/76743 of epoch 4, 47.68 ms/it, loss 0.438064
Finished training it 35840/76743 of epoch 4, 47.34 ms/it, loss 0.439230
Finished training it 35840/76743 of epoch 4, 46.70 ms/it, loss 0.439015
Finished training it 35840/76743 of epoch 4, 46.71 ms/it, loss 0.440319
Finished training it 35840/76743 of epoch 4, 46.83 ms/it, loss 0.439992
Finished training it 36864/76743 of epoch 4, 47.40 ms/it, loss 0.437254
Finished training it 36864/76743 of epoch 4, 47.24 ms/it, loss 0.437302
Finished training it 36864/76743 of epoch 4, 47.38 ms/it, loss 0.439484
Finished training it 36864/76743 of epoch 4, 48.00 ms/it, loss 0.439094
Finished training it 37888/76743 of epoch 4, 47.20 ms/it, loss 0.439698
Finished training it 37888/76743 of epoch 4, 47.10 ms/it, loss 0.439697
Finished training it 37888/76743 of epoch 4, 47.23 ms/it, loss 0.436700
Finished training it 37888/76743 of epoch 4, 47.01 ms/it, loss 0.439139
Finished training it 38912/76743 of epoch 4, 47.04 ms/it, loss 0.440011
Finished training it 38912/76743 of epoch 4, 47.61 ms/it, loss 0.437715
Finished training it 38912/76743 of epoch 4, 47.03 ms/it, loss 0.436921
Finished training it 38912/76743 of epoch 4, 46.98 ms/it, loss 0.440000
Finished training it 39936/76743 of epoch 4, 47.17 ms/it, loss 0.438282
Finished training it 39936/76743 of epoch 4, 47.25 ms/it, loss 0.438789
Finished training it 39936/76743 of epoch 4, 47.70 ms/it, loss 0.436485
Finished training it 39936/76743 of epoch 4, 47.32 ms/it, loss 0.440330
Finished training it 40960/76743 of epoch 4, 47.14 ms/it, loss 0.442797
Finished training it 40960/76743 of epoch 4, 46.79 ms/it, loss 0.440780
Finished training it 40960/76743 of epoch 4, 46.51 ms/it, loss 0.437762
Finished training it 40960/76743 of epoch 4, 46.72 ms/it, loss 0.436865
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582002.0
get out
0 has test check 2582002.0 and sample count 3274240
 accuracy 78.858 %, best 78.866 %, roc auc score 0.8027, best 0.8028
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 47.72 ms/it, loss 0.439917
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582002.0
get out
1 has test check 2582002.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 47.72 ms/it, loss 0.439218
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582002.0
get out
2 has test check 2582002.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 47.83 ms/it, loss 0.439196
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582002.0
get out
3 has test check 2582002.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 47.56 ms/it, loss 0.439934
Finished training it 43008/76743 of epoch 4, 48.01 ms/it, loss 0.436573
Finished training it 43008/76743 of epoch 4, 48.15 ms/it, loss 0.438226
Finished training it 43008/76743 of epoch 4, 47.73 ms/it, loss 0.437251
Finished training it 43008/76743 of epoch 4, 48.40 ms/it, loss 0.438534
Finished training it 44032/76743 of epoch 4, 47.36 ms/it, loss 0.439891
Finished training it 44032/76743 of epoch 4, 48.90 ms/it, loss 0.438531
Finished training it 44032/76743 of epoch 4, 53.03 ms/it, loss 0.438043
Finished training it 44032/76743 of epoch 4, 48.76 ms/it, loss 0.438962
Finished training it 45056/76743 of epoch 4, 47.53 ms/it, loss 0.440155
Finished training it 45056/76743 of epoch 4, 47.58 ms/it, loss 0.438726
Finished training it 45056/76743 of epoch 4, 47.95 ms/it, loss 0.437511
Finished training it 45056/76743 of epoch 4, 48.94 ms/it, loss 0.438847
Finished training it 46080/76743 of epoch 4, 47.64 ms/it, loss 0.436562
Finished training it 46080/76743 of epoch 4, 47.54 ms/it, loss 0.438849
Finished training it 46080/76743 of epoch 4, 47.77 ms/it, loss 0.439087
Finished training it 46080/76743 of epoch 4, 47.92 ms/it, loss 0.438178
Finished training it 47104/76743 of epoch 4, 47.05 ms/it, loss 0.441108
Finished training it 47104/76743 of epoch 4, 47.71 ms/it, loss 0.437147
Finished training it 47104/76743 of epoch 4, 47.26 ms/it, loss 0.439470
Finished training it 47104/76743 of epoch 4, 46.99 ms/it, loss 0.439006
Finished training it 48128/76743 of epoch 4, 47.42 ms/it, loss 0.437907
Finished training it 48128/76743 of epoch 4, 47.20 ms/it, loss 0.441563
Finished training it 48128/76743 of epoch 4, 48.08 ms/it, loss 0.436897
Finished training it 48128/76743 of epoch 4, 47.53 ms/it, loss 0.437585
Finished training it 49152/76743 of epoch 4, 47.91 ms/it, loss 0.440302
Finished training it 49152/76743 of epoch 4, 47.65 ms/it, loss 0.438618
Finished training it 49152/76743 of epoch 4, 47.42 ms/it, loss 0.439023
Finished training it 49152/76743 of epoch 4, 47.93 ms/it, loss 0.434534
Finished training it 50176/76743 of epoch 4, 46.77 ms/it, loss 0.439525
Finished training it 50176/76743 of epoch 4, 47.97 ms/it, loss 0.437742
Finished training it 50176/76743 of epoch 4, 47.21 ms/it, loss 0.436893
Finished training it 50176/76743 of epoch 4, 47.05 ms/it, loss 0.436385
Finished training it 51200/76743 of epoch 4, 47.78 ms/it, loss 0.439516
Finished training it 51200/76743 of epoch 4, 46.99 ms/it, loss 0.438571
Finished training it 51200/76743 of epoch 4, 46.76 ms/it, loss 0.437107
Finished training it 51200/76743 of epoch 4, 47.47 ms/it, loss 0.437264
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579253.0
get out
0 has test check 2579253.0 and sample count 3274240
 accuracy 78.774 %, best 78.866 %, roc auc score 0.8026, best 0.8028
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579253.0
get out
1 has test check 2579253.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 47.62 ms/it, loss 0.437671
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579253.0
get out
2 has test check 2579253.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 47.73 ms/it, loss 0.441497
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 47.89 ms/it, loss 0.438221
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579253.0
get out
3 has test check 2579253.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 47.46 ms/it, loss 0.437382
Finished training it 53248/76743 of epoch 4, 47.11 ms/it, loss 0.437839
Finished training it 53248/76743 of epoch 4, 47.06 ms/it, loss 0.438675
Finished training it 53248/76743 of epoch 4, 47.49 ms/it, loss 0.435529
Finished training it 53248/76743 of epoch 4, 47.03 ms/it, loss 0.437816
Finished training it 54272/76743 of epoch 4, 47.16 ms/it, loss 0.437695
Finished training it 54272/76743 of epoch 4, 47.30 ms/it, loss 0.439526
Finished training it 54272/76743 of epoch 4, 47.04 ms/it, loss 0.437610
Finished training it 54272/76743 of epoch 4, 47.74 ms/it, loss 0.438565
Finished training it 55296/76743 of epoch 4, 48.82 ms/it, loss 0.438026
Finished training it 55296/76743 of epoch 4, 52.89 ms/it, loss 0.436837
Finished training it 55296/76743 of epoch 4, 53.08 ms/it, loss 0.439775
Finished training it 55296/76743 of epoch 4, 52.93 ms/it, loss 0.438240
Finished training it 56320/76743 of epoch 4, 47.54 ms/it, loss 0.437386
Finished training it 56320/76743 of epoch 4, 47.83 ms/it, loss 0.440205
Finished training it 56320/76743 of epoch 4, 47.56 ms/it, loss 0.437288
Finished training it 56320/76743 of epoch 4, 48.11 ms/it, loss 0.437838
Finished training it 57344/76743 of epoch 4, 46.48 ms/it, loss 0.439366
Finished training it 57344/76743 of epoch 4, 46.50 ms/it, loss 0.439620
Finished training it 57344/76743 of epoch 4, 46.24 ms/it, loss 0.440150
Finished training it 57344/76743 of epoch 4, 46.50 ms/it, loss 0.437397
Finished training it 58368/76743 of epoch 4, 48.07 ms/it, loss 0.436041
Finished training it 58368/76743 of epoch 4, 47.65 ms/it, loss 0.436190
Finished training it 58368/76743 of epoch 4, 46.95 ms/it, loss 0.437366
Finished training it 58368/76743 of epoch 4, 47.37 ms/it, loss 0.436376
Finished training it 59392/76743 of epoch 4, 47.51 ms/it, loss 0.439569
Finished training it 59392/76743 of epoch 4, 47.05 ms/it, loss 0.438144
Finished training it 59392/76743 of epoch 4, 47.84 ms/it, loss 0.437469
Finished training it 59392/76743 of epoch 4, 47.96 ms/it, loss 0.437858
Finished training it 60416/76743 of epoch 4, 46.88 ms/it, loss 0.438687
Finished training it 60416/76743 of epoch 4, 47.15 ms/it, loss 0.439520
Finished training it 60416/76743 of epoch 4, 47.30 ms/it, loss 0.438155
Finished training it 60416/76743 of epoch 4, 47.02 ms/it, loss 0.437077
Finished training it 61440/76743 of epoch 4, 47.25 ms/it, loss 0.439014
Finished training it 61440/76743 of epoch 4, 47.49 ms/it, loss 0.439418
Finished training it 61440/76743 of epoch 4, 47.41 ms/it, loss 0.439071
Finished training it 61440/76743 of epoch 4, 47.43 ms/it, loss 0.439595
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582544.0
get out
0 has test check 2582544.0 and sample count 3274240
 accuracy 78.875 %, best 78.875 %, roc auc score 0.8029, best 0.8029
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582544.0
get out
1 has test check 2582544.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 47.68 ms/it, loss 0.435482
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 47.84 ms/it, loss 0.439094
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582544.0
get out
3 has test check 2582544.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 47.72 ms/it, loss 0.437110
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582544.0
get out
2 has test check 2582544.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 48.04 ms/it, loss 0.435768
Finished training it 63488/76743 of epoch 4, 47.72 ms/it, loss 0.435315
Finished training it 63488/76743 of epoch 4, 47.36 ms/it, loss 0.437564
Finished training it 63488/76743 of epoch 4, 47.93 ms/it, loss 0.437660
Finished training it 63488/76743 of epoch 4, 47.82 ms/it, loss 0.439518
Finished training it 64512/76743 of epoch 4, 47.93 ms/it, loss 0.438262
Finished training it 64512/76743 of epoch 4, 47.70 ms/it, loss 0.438190
Finished training it 64512/76743 of epoch 4, 47.72 ms/it, loss 0.435678
Finished training it 64512/76743 of epoch 4, 52.61 ms/it, loss 0.435495
Finished training it 65536/76743 of epoch 4, 48.48 ms/it, loss 0.436454
Finished training it 65536/76743 of epoch 4, 47.29 ms/it, loss 0.437634
Finished training it 65536/76743 of epoch 4, 48.74 ms/it, loss 0.437278
Finished training it 65536/76743 of epoch 4, 47.39 ms/it, loss 0.433634
Finished training it 66560/76743 of epoch 4, 47.11 ms/it, loss 0.437184
Finished training it 66560/76743 of epoch 4, 46.90 ms/it, loss 0.440198
Finished training it 66560/76743 of epoch 4, 47.32 ms/it, loss 0.437306
Finished training it 66560/76743 of epoch 4, 47.63 ms/it, loss 0.441469
Finished training it 67584/76743 of epoch 4, 47.16 ms/it, loss 0.439139
Finished training it 67584/76743 of epoch 4, 46.56 ms/it, loss 0.437743
Finished training it 67584/76743 of epoch 4, 47.13 ms/it, loss 0.438380
Finished training it 67584/76743 of epoch 4, 47.12 ms/it, loss 0.438629
Finished training it 68608/76743 of epoch 4, 45.95 ms/it, loss 0.437519
Finished training it 68608/76743 of epoch 4, 46.31 ms/it, loss 0.437398
Finished training it 68608/76743 of epoch 4, 46.39 ms/it, loss 0.438942
Finished training it 68608/76743 of epoch 4, 46.29 ms/it, loss 0.438349
Finished training it 69632/76743 of epoch 4, 47.35 ms/it, loss 0.436472
Finished training it 69632/76743 of epoch 4, 47.07 ms/it, loss 0.438148
Finished training it 69632/76743 of epoch 4, 47.18 ms/it, loss 0.435796
Finished training it 69632/76743 of epoch 4, 47.50 ms/it, loss 0.436498
Finished training it 70656/76743 of epoch 4, 47.39 ms/it, loss 0.437453
Finished training it 70656/76743 of epoch 4, 47.75 ms/it, loss 0.438215
Finished training it 70656/76743 of epoch 4, 47.53 ms/it, loss 0.436421
Finished training it 70656/76743 of epoch 4, 47.28 ms/it, loss 0.439842
Finished training it 71680/76743 of epoch 4, 47.08 ms/it, loss 0.438584
Finished training it 71680/76743 of epoch 4, 48.03 ms/it, loss 0.436244
Finished training it 71680/76743 of epoch 4, 47.43 ms/it, loss 0.437372
Finished training it 71680/76743 of epoch 4, 47.35 ms/it, loss 0.438872
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581491.0
get out
0 has test check 2581491.0 and sample count 3274240
 accuracy 78.842 %, best 78.875 %, roc auc score 0.8026, best 0.8029
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581491.0
get out
3 has test check 2581491.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 47.45 ms/it, loss 0.438803
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581491.0
get out
2 has test check 2581491.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 47.60 ms/it, loss 0.435613
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 47.78 ms/it, loss 0.436637
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581491.0
get out
1 has test check 2581491.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 47.75 ms/it, loss 0.437877
Finished training it 73728/76743 of epoch 4, 47.39 ms/it, loss 0.436145
Finished training it 73728/76743 of epoch 4, 47.25 ms/it, loss 0.439293
Finished training it 73728/76743 of epoch 4, 47.00 ms/it, loss 0.437420
Finished training it 73728/76743 of epoch 4, 46.94 ms/it, loss 0.436184
Finished training it 74752/76743 of epoch 4, 47.78 ms/it, loss 0.435159
Finished training it 74752/76743 of epoch 4, 47.42 ms/it, loss 0.439809
Finished training it 74752/76743 of epoch 4, 47.17 ms/it, loss 0.435714
Finished training it 74752/76743 of epoch 4, 47.46 ms/it, loss 0.436888
Finished training it 75776/76743 of epoch 4, 53.03 ms/it, loss 0.437971
Finished training it 75776/76743 of epoch 4, 49.06 ms/it, loss 0.436893
Finished training it 75776/76743 of epoch 4, 52.25 ms/it, loss 0.435783
Finished training it 75776/76743 of epoch 4, 52.44 ms/it, loss 0.436727
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581923.0
get out
0 has test check 2581923.0 and sample count 3274240
 accuracy 78.856 %, best 78.875 %, roc auc score 0.8028, best 0.8029
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581923.0
get out
2 has test check 2581923.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581923.0
get out
1 has test check 2581923.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581923.0
get out
3 has test check 2581923.0 and sample count 3274240
