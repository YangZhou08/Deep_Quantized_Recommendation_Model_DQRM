Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 35.76 ms/it, loss 0.513629
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 35.36 ms/it, loss 0.514090
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.98 ms/it, loss 0.514921
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.38 ms/it, loss 0.513336
Finished training it 2048/76743 of epoch 0, 33.14 ms/it, loss 0.496230
Finished training it 2048/76743 of epoch 0, 33.16 ms/it, loss 0.502490
Finished training it 2048/76743 of epoch 0, 33.21 ms/it, loss 0.498130
Finished training it 2048/76743 of epoch 0, 33.22 ms/it, loss 0.498634
Finished training it 3072/76743 of epoch 0, 33.02 ms/it, loss 0.492478
Finished training it 3072/76743 of epoch 0, 33.01 ms/it, loss 0.491327
Finished training it 3072/76743 of epoch 0, 32.89 ms/it, loss 0.492226
Finished training it 3072/76743 of epoch 0, 33.10 ms/it, loss 0.489976
Finished training it 4096/76743 of epoch 0, 33.13 ms/it, loss 0.485188
Finished training it 4096/76743 of epoch 0, 33.23 ms/it, loss 0.484574
Finished training it 4096/76743 of epoch 0, 33.39 ms/it, loss 0.484148
Finished training it 4096/76743 of epoch 0, 33.25 ms/it, loss 0.482445
Finished training it 5120/76743 of epoch 0, 33.18 ms/it, loss 0.474798
Finished training it 5120/76743 of epoch 0, 33.11 ms/it, loss 0.476847
Finished training it 5120/76743 of epoch 0, 33.04 ms/it, loss 0.480445
Finished training it 5120/76743 of epoch 0, 33.21 ms/it, loss 0.479271
Finished training it 6144/76743 of epoch 0, 33.12 ms/it, loss 0.472880
Finished training it 6144/76743 of epoch 0, 33.20 ms/it, loss 0.473886
Finished training it 6144/76743 of epoch 0, 33.02 ms/it, loss 0.472633
Finished training it 6144/76743 of epoch 0, 33.14 ms/it, loss 0.471607
Finished training it 7168/76743 of epoch 0, 32.95 ms/it, loss 0.471551
Finished training it 7168/76743 of epoch 0, 33.02 ms/it, loss 0.470972
Finished training it 7168/76743 of epoch 0, 32.90 ms/it, loss 0.470461
Finished training it 7168/76743 of epoch 0, 32.80 ms/it, loss 0.467571
Finished training it 8192/76743 of epoch 0, 32.77 ms/it, loss 0.468482
Finished training it 8192/76743 of epoch 0, 32.72 ms/it, loss 0.464335
Finished training it 8192/76743 of epoch 0, 32.73 ms/it, loss 0.467981
Finished training it 8192/76743 of epoch 0, 32.65 ms/it, loss 0.466354
Finished training it 9216/76743 of epoch 0, 32.70 ms/it, loss 0.469724
Finished training it 9216/76743 of epoch 0, 32.76 ms/it, loss 0.467817
Finished training it 9216/76743 of epoch 0, 32.52 ms/it, loss 0.466429
Finished training it 9216/76743 of epoch 0, 32.61 ms/it, loss 0.466265
Finished training it 10240/76743 of epoch 0, 32.73 ms/it, loss 0.466015
Finished training it 10240/76743 of epoch 0, 32.67 ms/it, loss 0.464580
Finished training it 10240/76743 of epoch 0, 32.73 ms/it, loss 0.464067
Finished training it 10240/76743 of epoch 0, 32.60 ms/it, loss 0.464458
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2553121.0
get out
0 has test check 2553121.0 and sample count 3274240
 accuracy 77.976 %, best 77.976 %, roc auc score 0.7844, best 0.7844
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2553121.0
get out
2 has test check 2553121.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 35.90 ms/it, loss 0.466045
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2553121.0
get out
3 has test check 2553121.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 35.99 ms/it, loss 0.465441
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2553121.0
get out
1 has test check 2553121.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 36.09 ms/it, loss 0.462058
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 36.16 ms/it, loss 0.462920
Finished training it 12288/76743 of epoch 0, 34.23 ms/it, loss 0.461930
Finished training it 12288/76743 of epoch 0, 33.96 ms/it, loss 0.459639
Finished training it 12288/76743 of epoch 0, 33.94 ms/it, loss 0.464016
Finished training it 12288/76743 of epoch 0, 34.30 ms/it, loss 0.461995
Finished training it 13312/76743 of epoch 0, 35.30 ms/it, loss 0.461153
Finished training it 13312/76743 of epoch 0, 35.48 ms/it, loss 0.461898
Finished training it 13312/76743 of epoch 0, 35.01 ms/it, loss 0.460353
Finished training it 13312/76743 of epoch 0, 35.22 ms/it, loss 0.463599
Finished training it 14336/76743 of epoch 0, 44.54 ms/it, loss 0.460920
Finished training it 14336/76743 of epoch 0, 46.22 ms/it, loss 0.460567
Finished training it 14336/76743 of epoch 0, 45.66 ms/it, loss 0.462138
Finished training it 14336/76743 of epoch 0, 45.67 ms/it, loss 0.462860
Finished training it 15360/76743 of epoch 0, 45.21 ms/it, loss 0.463497
Finished training it 15360/76743 of epoch 0, 44.88 ms/it, loss 0.461288
Finished training it 15360/76743 of epoch 0, 44.86 ms/it, loss 0.459905
Finished training it 15360/76743 of epoch 0, 44.92 ms/it, loss 0.462852
Finished training it 16384/76743 of epoch 0, 40.05 ms/it, loss 0.460739
Finished training it 16384/76743 of epoch 0, 39.93 ms/it, loss 0.456686
Finished training it 16384/76743 of epoch 0, 39.97 ms/it, loss 0.462319
Finished training it 16384/76743 of epoch 0, 39.84 ms/it, loss 0.455881
Finished training it 17408/76743 of epoch 0, 40.16 ms/it, loss 0.458326
Finished training it 17408/76743 of epoch 0, 40.31 ms/it, loss 0.458847
Finished training it 17408/76743 of epoch 0, 40.24 ms/it, loss 0.460567
Finished training it 17408/76743 of epoch 0, 40.21 ms/it, loss 0.459571
Finished training it 18432/76743 of epoch 0, 39.95 ms/it, loss 0.457786
Finished training it 18432/76743 of epoch 0, 40.06 ms/it, loss 0.458977
Finished training it 18432/76743 of epoch 0, 40.04 ms/it, loss 0.458671
Finished training it 18432/76743 of epoch 0, 39.97 ms/it, loss 0.457674
Finished training it 19456/76743 of epoch 0, 40.36 ms/it, loss 0.456635
Finished training it 19456/76743 of epoch 0, 40.40 ms/it, loss 0.458585
Finished training it 19456/76743 of epoch 0, 40.35 ms/it, loss 0.458474
Finished training it 19456/76743 of epoch 0, 40.27 ms/it, loss 0.458789
Finished training it 20480/76743 of epoch 0, 40.54 ms/it, loss 0.456523
Finished training it 20480/76743 of epoch 0, 40.50 ms/it, loss 0.456457
Finished training it 20480/76743 of epoch 0, 40.46 ms/it, loss 0.455946
Finished training it 20480/76743 of epoch 0, 40.50 ms/it, loss 0.457530
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2564684.0
get out
0 has test check 2564684.0 and sample count 3274240
 accuracy 78.329 %, best 78.329 %, roc auc score 0.7909, best 0.7909
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2564684.0
get out
1 has test check 2564684.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 34.94 ms/it, loss 0.456328
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 34.93 ms/it, loss 0.457549
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2564684.0
get out
3 has test check 2564684.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 34.87 ms/it, loss 0.457051
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2564684.0
get out
2 has test check 2564684.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 34.85 ms/it, loss 0.454535
Finished training it 22528/76743 of epoch 0, 35.07 ms/it, loss 0.455057
Finished training it 22528/76743 of epoch 0, 35.05 ms/it, loss 0.458442
Finished training it 22528/76743 of epoch 0, 34.92 ms/it, loss 0.455797
Finished training it 22528/76743 of epoch 0, 34.85 ms/it, loss 0.454916
Finished training it 23552/76743 of epoch 0, 35.10 ms/it, loss 0.452084
Finished training it 23552/76743 of epoch 0, 34.88 ms/it, loss 0.453076
Finished training it 23552/76743 of epoch 0, 35.01 ms/it, loss 0.454376
Finished training it 23552/76743 of epoch 0, 34.94 ms/it, loss 0.457834
Finished training it 24576/76743 of epoch 0, 34.96 ms/it, loss 0.458065
Finished training it 24576/76743 of epoch 0, 34.84 ms/it, loss 0.455693
Finished training it 24576/76743 of epoch 0, 34.94 ms/it, loss 0.453755
Finished training it 24576/76743 of epoch 0, 34.76 ms/it, loss 0.456091
Finished training it 25600/76743 of epoch 0, 34.78 ms/it, loss 0.455676
Finished training it 25600/76743 of epoch 0, 35.02 ms/it, loss 0.457118
Finished training it 25600/76743 of epoch 0, 35.03 ms/it, loss 0.452620
Finished training it 25600/76743 of epoch 0, 34.86 ms/it, loss 0.455905
Finished training it 26624/76743 of epoch 0, 35.01 ms/it, loss 0.454805
Finished training it 26624/76743 of epoch 0, 34.92 ms/it, loss 0.454133
Finished training it 26624/76743 of epoch 0, 35.20 ms/it, loss 0.454398
Finished training it 26624/76743 of epoch 0, 35.13 ms/it, loss 0.457001
Finished training it 27648/76743 of epoch 0, 34.95 ms/it, loss 0.454567
Finished training it 27648/76743 of epoch 0, 35.05 ms/it, loss 0.454469
Finished training it 27648/76743 of epoch 0, 35.07 ms/it, loss 0.455887
Finished training it 27648/76743 of epoch 0, 34.98 ms/it, loss 0.453540
Finished training it 28672/76743 of epoch 0, 34.95 ms/it, loss 0.455408
Finished training it 28672/76743 of epoch 0, 34.85 ms/it, loss 0.454196
Finished training it 28672/76743 of epoch 0, 34.91 ms/it, loss 0.454950
Finished training it 28672/76743 of epoch 0, 34.79 ms/it, loss 0.454499
Finished training it 29696/76743 of epoch 0, 34.93 ms/it, loss 0.455173
Finished training it 29696/76743 of epoch 0, 35.12 ms/it, loss 0.453754
Finished training it 29696/76743 of epoch 0, 35.04 ms/it, loss 0.454451
Finished training it 29696/76743 of epoch 0, 35.10 ms/it, loss 0.454206
Finished training it 30720/76743 of epoch 0, 35.04 ms/it, loss 0.453562
Finished training it 30720/76743 of epoch 0, 35.04 ms/it, loss 0.452103
Finished training it 30720/76743 of epoch 0, 34.94 ms/it, loss 0.454408
Finished training it 30720/76743 of epoch 0, 34.84 ms/it, loss 0.453987
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2566697.0
get out
0 has test check 2566697.0 and sample count 3274240
 accuracy 78.391 %, best 78.391 %, roc auc score 0.7938, best 0.7938
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 39.76 ms/it, loss 0.454329
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2566697.0
get out
1 has test check 2566697.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 39.81 ms/it, loss 0.451593
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2566697.0
get out
2 has test check 2566697.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 39.64 ms/it, loss 0.454328
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2566697.0
get out
3 has test check 2566697.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 39.59 ms/it, loss 0.455720
Finished training it 32768/76743 of epoch 0, 40.75 ms/it, loss 0.452159
Finished training it 32768/76743 of epoch 0, 41.01 ms/it, loss 0.455431
Finished training it 32768/76743 of epoch 0, 40.79 ms/it, loss 0.453011
Finished training it 32768/76743 of epoch 0, 40.99 ms/it, loss 0.450348
Finished training it 33792/76743 of epoch 0, 45.56 ms/it, loss 0.451428
Finished training it 33792/76743 of epoch 0, 45.99 ms/it, loss 0.453534
Finished training it 33792/76743 of epoch 0, 46.27 ms/it, loss 0.453542
Finished training it 33792/76743 of epoch 0, 45.97 ms/it, loss 0.456104
Finished training it 34816/76743 of epoch 0, 41.11 ms/it, loss 0.454643
Finished training it 34816/76743 of epoch 0, 40.73 ms/it, loss 0.454535
Finished training it 34816/76743 of epoch 0, 40.95 ms/it, loss 0.452303
Finished training it 34816/76743 of epoch 0, 40.83 ms/it, loss 0.453278
Finished training it 35840/76743 of epoch 0, 40.32 ms/it, loss 0.455128
Finished training it 35840/76743 of epoch 0, 40.45 ms/it, loss 0.453388
Finished training it 35840/76743 of epoch 0, 40.48 ms/it, loss 0.452350
Finished training it 35840/76743 of epoch 0, 40.26 ms/it, loss 0.452042
Finished training it 36864/76743 of epoch 0, 40.10 ms/it, loss 0.452947
Finished training it 36864/76743 of epoch 0, 40.09 ms/it, loss 0.453054
Finished training it 36864/76743 of epoch 0, 39.97 ms/it, loss 0.453734
Finished training it 36864/76743 of epoch 0, 39.93 ms/it, loss 0.451030
Finished training it 37888/76743 of epoch 0, 36.79 ms/it, loss 0.450750
Finished training it 37888/76743 of epoch 0, 36.69 ms/it, loss 0.451892
Finished training it 37888/76743 of epoch 0, 36.57 ms/it, loss 0.451897
Finished training it 37888/76743 of epoch 0, 36.88 ms/it, loss 0.452654
Finished training it 38912/76743 of epoch 0, 35.29 ms/it, loss 0.451628
Finished training it 38912/76743 of epoch 0, 34.99 ms/it, loss 0.450734
Finished training it 38912/76743 of epoch 0, 35.29 ms/it, loss 0.452998
Finished training it 38912/76743 of epoch 0, 35.11 ms/it, loss 0.452209
Finished training it 39936/76743 of epoch 0, 35.32 ms/it, loss 0.452218
Finished training it 39936/76743 of epoch 0, 35.05 ms/it, loss 0.453502
Finished training it 39936/76743 of epoch 0, 34.98 ms/it, loss 0.454819
Finished training it 39936/76743 of epoch 0, 35.32 ms/it, loss 0.452081
Finished training it 40960/76743 of epoch 0, 35.15 ms/it, loss 0.452094
Finished training it 40960/76743 of epoch 0, 34.81 ms/it, loss 0.452066
Finished training it 40960/76743 of epoch 0, 35.18 ms/it, loss 0.452163
Finished training it 40960/76743 of epoch 0, 34.86 ms/it, loss 0.452238
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2569504.0
get out
0 has test check 2569504.0 and sample count 3274240
 accuracy 78.476 %, best 78.476 %, roc auc score 0.7961, best 0.7961
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2569504.0
get out
2 has test check 2569504.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 35.16 ms/it, loss 0.449549
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2569504.0
get out
1 has test check 2569504.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 35.45 ms/it, loss 0.451209
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2569504.0
get out
3 has test check 2569504.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 35.06 ms/it, loss 0.452674
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 35.45 ms/it, loss 0.451899
Finished training it 43008/76743 of epoch 0, 40.02 ms/it, loss 0.451144
Finished training it 43008/76743 of epoch 0, 39.98 ms/it, loss 0.454062
Finished training it 43008/76743 of epoch 0, 40.21 ms/it, loss 0.451740
Finished training it 43008/76743 of epoch 0, 40.25 ms/it, loss 0.450408
Finished training it 44032/76743 of epoch 0, 40.29 ms/it, loss 0.451223
Finished training it 44032/76743 of epoch 0, 40.32 ms/it, loss 0.451459
Finished training it 44032/76743 of epoch 0, 40.08 ms/it, loss 0.452605
Finished training it 44032/76743 of epoch 0, 40.16 ms/it, loss 0.449993
Finished training it 45056/76743 of epoch 0, 39.90 ms/it, loss 0.449667
Finished training it 45056/76743 of epoch 0, 40.11 ms/it, loss 0.448805
Finished training it 45056/76743 of epoch 0, 39.90 ms/it, loss 0.449776
Finished training it 45056/76743 of epoch 0, 40.05 ms/it, loss 0.450344
Finished training it 46080/76743 of epoch 0, 40.55 ms/it, loss 0.449656
Finished training it 46080/76743 of epoch 0, 40.54 ms/it, loss 0.451514
Finished training it 46080/76743 of epoch 0, 40.76 ms/it, loss 0.452708
Finished training it 46080/76743 of epoch 0, 40.77 ms/it, loss 0.451990
Finished training it 47104/76743 of epoch 0, 40.23 ms/it, loss 0.451926
Finished training it 47104/76743 of epoch 0, 39.96 ms/it, loss 0.451476
Finished training it 47104/76743 of epoch 0, 40.25 ms/it, loss 0.451155
Finished training it 47104/76743 of epoch 0, 40.08 ms/it, loss 0.451900
Finished training it 48128/76743 of epoch 0, 40.02 ms/it, loss 0.451625
Finished training it 48128/76743 of epoch 0, 40.16 ms/it, loss 0.448856
Finished training it 48128/76743 of epoch 0, 40.20 ms/it, loss 0.450919
Finished training it 48128/76743 of epoch 0, 39.91 ms/it, loss 0.451617
Finished training it 49152/76743 of epoch 0, 40.28 ms/it, loss 0.447975
Finished training it 49152/76743 of epoch 0, 40.35 ms/it, loss 0.449834
Finished training it 49152/76743 of epoch 0, 40.50 ms/it, loss 0.454529
Finished training it 49152/76743 of epoch 0, 40.49 ms/it, loss 0.453175
Finished training it 50176/76743 of epoch 0, 40.67 ms/it, loss 0.450405
Finished training it 50176/76743 of epoch 0, 40.68 ms/it, loss 0.450922
Finished training it 50176/76743 of epoch 0, 40.49 ms/it, loss 0.450910
Finished training it 50176/76743 of epoch 0, 40.49 ms/it, loss 0.449147
Finished training it 51200/76743 of epoch 0, 40.15 ms/it, loss 0.451109
Finished training it 51200/76743 of epoch 0, 40.23 ms/it, loss 0.450893
Finished training it 51200/76743 of epoch 0, 39.99 ms/it, loss 0.451245
Finished training it 51200/76743 of epoch 0, 40.06 ms/it, loss 0.449754
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2572046.0
get out
0 has test check 2572046.0 and sample count 3274240
 accuracy 78.554 %, best 78.554 %, roc auc score 0.7966, best 0.7966
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2572046.0
get out
1 has test check 2572046.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 35.22 ms/it, loss 0.447271
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2572046.0
get out
3 has test check 2572046.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 34.97 ms/it, loss 0.453012
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2572046.0
get out
2 has test check 2572046.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 34.98 ms/it, loss 0.449266
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 35.18 ms/it, loss 0.449443
Finished training it 53248/76743 of epoch 0, 35.19 ms/it, loss 0.450632
Finished training it 53248/76743 of epoch 0, 35.14 ms/it, loss 0.448754
Finished training it 53248/76743 of epoch 0, 34.88 ms/it, loss 0.453261
Finished training it 53248/76743 of epoch 0, 34.91 ms/it, loss 0.452307
Finished training it 54272/76743 of epoch 0, 41.30 ms/it, loss 0.450665
Finished training it 54272/76743 of epoch 0, 40.19 ms/it, loss 0.450091
Finished training it 54272/76743 of epoch 0, 40.70 ms/it, loss 0.448935
Finished training it 54272/76743 of epoch 0, 41.06 ms/it, loss 0.452943
Finished training it 55296/76743 of epoch 0, 35.00 ms/it, loss 0.449017
Finished training it 55296/76743 of epoch 0, 35.36 ms/it, loss 0.449215
Finished training it 55296/76743 of epoch 0, 35.79 ms/it, loss 0.447910
Finished training it 55296/76743 of epoch 0, 35.13 ms/it, loss 0.451197
Finished training it 56320/76743 of epoch 0, 34.91 ms/it, loss 0.451514
Finished training it 56320/76743 of epoch 0, 35.12 ms/it, loss 0.451911
Finished training it 56320/76743 of epoch 0, 35.10 ms/it, loss 0.448665
Finished training it 56320/76743 of epoch 0, 34.74 ms/it, loss 0.452462
Finished training it 57344/76743 of epoch 0, 35.23 ms/it, loss 0.448589
Finished training it 57344/76743 of epoch 0, 35.08 ms/it, loss 0.449156
Finished training it 57344/76743 of epoch 0, 35.42 ms/it, loss 0.452246
Finished training it 57344/76743 of epoch 0, 35.47 ms/it, loss 0.449585
Finished training it 58368/76743 of epoch 0, 35.18 ms/it, loss 0.448756
Finished training it 58368/76743 of epoch 0, 35.19 ms/it, loss 0.450227
Finished training it 58368/76743 of epoch 0, 34.93 ms/it, loss 0.448875
Finished training it 58368/76743 of epoch 0, 34.77 ms/it, loss 0.450462
Finished training it 59392/76743 of epoch 0, 35.38 ms/it, loss 0.452563
Finished training it 59392/76743 of epoch 0, 35.43 ms/it, loss 0.446434
Finished training it 59392/76743 of epoch 0, 35.07 ms/it, loss 0.449449
Finished training it 59392/76743 of epoch 0, 34.98 ms/it, loss 0.448452
Finished training it 60416/76743 of epoch 0, 40.63 ms/it, loss 0.449590
Finished training it 60416/76743 of epoch 0, 40.70 ms/it, loss 0.447440
Finished training it 60416/76743 of epoch 0, 40.34 ms/it, loss 0.449282
Finished training it 60416/76743 of epoch 0, 40.51 ms/it, loss 0.448188
Finished training it 61440/76743 of epoch 0, 40.51 ms/it, loss 0.447810
Finished training it 61440/76743 of epoch 0, 40.66 ms/it, loss 0.448784
Finished training it 61440/76743 of epoch 0, 40.69 ms/it, loss 0.446985
Finished training it 61440/76743 of epoch 0, 40.50 ms/it, loss 0.448460
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2573790.0
get out
0 has test check 2573790.0 and sample count 3274240
 accuracy 78.607 %, best 78.607 %, roc auc score 0.7983, best 0.7983
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2573790.0
get out
1 has test check 2573790.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 40.42 ms/it, loss 0.449279
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 40.39 ms/it, loss 0.448273
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2573790.0
get out
2 has test check 2573790.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 40.28 ms/it, loss 0.448998
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2573790.0
get out
3 has test check 2573790.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 40.23 ms/it, loss 0.450172
Finished training it 63488/76743 of epoch 0, 40.35 ms/it, loss 0.447709
Finished training it 63488/76743 of epoch 0, 40.32 ms/it, loss 0.448360
Finished training it 63488/76743 of epoch 0, 40.50 ms/it, loss 0.450942
Finished training it 63488/76743 of epoch 0, 40.58 ms/it, loss 0.446484
Finished training it 64512/76743 of epoch 0, 40.62 ms/it, loss 0.444453
Finished training it 64512/76743 of epoch 0, 40.63 ms/it, loss 0.446320
Finished training it 64512/76743 of epoch 0, 40.40 ms/it, loss 0.446844
Finished training it 64512/76743 of epoch 0, 40.47 ms/it, loss 0.446760
Finished training it 65536/76743 of epoch 0, 40.67 ms/it, loss 0.447171
Finished training it 65536/76743 of epoch 0, 40.59 ms/it, loss 0.450437
Finished training it 65536/76743 of epoch 0, 40.35 ms/it, loss 0.448691
Finished training it 65536/76743 of epoch 0, 40.39 ms/it, loss 0.444863
Finished training it 66560/76743 of epoch 0, 36.56 ms/it, loss 0.447671
Finished training it 66560/76743 of epoch 0, 36.84 ms/it, loss 0.447916
Finished training it 66560/76743 of epoch 0, 36.84 ms/it, loss 0.448322
Finished training it 66560/76743 of epoch 0, 36.63 ms/it, loss 0.449950
Finished training it 67584/76743 of epoch 0, 35.12 ms/it, loss 0.449828
Finished training it 67584/76743 of epoch 0, 35.38 ms/it, loss 0.446616
Finished training it 67584/76743 of epoch 0, 35.31 ms/it, loss 0.446672
Finished training it 67584/76743 of epoch 0, 35.12 ms/it, loss 0.449321
Finished training it 68608/76743 of epoch 0, 34.84 ms/it, loss 0.448276
Finished training it 68608/76743 of epoch 0, 35.10 ms/it, loss 0.448978
Finished training it 68608/76743 of epoch 0, 34.87 ms/it, loss 0.450533
Finished training it 68608/76743 of epoch 0, 35.10 ms/it, loss 0.447108
Finished training it 69632/76743 of epoch 0, 35.16 ms/it, loss 0.446485
Finished training it 69632/76743 of epoch 0, 34.97 ms/it, loss 0.448332
Finished training it 69632/76743 of epoch 0, 35.28 ms/it, loss 0.450649
Finished training it 69632/76743 of epoch 0, 34.99 ms/it, loss 0.448393
Finished training it 70656/76743 of epoch 0, 34.83 ms/it, loss 0.449089
Finished training it 70656/76743 of epoch 0, 35.08 ms/it, loss 0.450837
Finished training it 70656/76743 of epoch 0, 35.14 ms/it, loss 0.449425
Finished training it 70656/76743 of epoch 0, 34.81 ms/it, loss 0.448015
Finished training it 71680/76743 of epoch 0, 34.70 ms/it, loss 0.448805
Finished training it 71680/76743 of epoch 0, 35.09 ms/it, loss 0.449474
Finished training it 71680/76743 of epoch 0, 34.81 ms/it, loss 0.445682
Finished training it 71680/76743 of epoch 0, 35.09 ms/it, loss 0.446973
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575134.0
get out
0 has test check 2575134.0 and sample count 3274240
 accuracy 78.648 %, best 78.648 %, roc auc score 0.7994, best 0.7994
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 40.42 ms/it, loss 0.447416
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575134.0
get out
2 has test check 2575134.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 40.34 ms/it, loss 0.447261
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575134.0
get out
3 has test check 2575134.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 40.19 ms/it, loss 0.447768
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575134.0
get out
1 has test check 2575134.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 40.42 ms/it, loss 0.446524
Finished training it 73728/76743 of epoch 0, 40.16 ms/it, loss 0.448822
Finished training it 73728/76743 of epoch 0, 40.35 ms/it, loss 0.450128
Finished training it 73728/76743 of epoch 0, 40.17 ms/it, loss 0.446838
Finished training it 73728/76743 of epoch 0, 40.33 ms/it, loss 0.446251
Finished training it 74752/76743 of epoch 0, 46.46 ms/it, loss 0.445201
Finished training it 74752/76743 of epoch 0, 45.91 ms/it, loss 0.448942
Finished training it 74752/76743 of epoch 0, 46.21 ms/it, loss 0.447988
Finished training it 74752/76743 of epoch 0, 45.97 ms/it, loss 0.448364
Finished training it 75776/76743 of epoch 0, 40.84 ms/it, loss 0.448923
Finished training it 75776/76743 of epoch 0, 40.59 ms/it, loss 0.446959
Finished training it 75776/76743 of epoch 0, 40.84 ms/it, loss 0.446825
Finished training it 75776/76743 of epoch 0, 40.71 ms/it, loss 0.447184
Warning: Skipping the batch 76742 with size 14
Using 4-bit precision
Finished training it 1024/76743 of epoch 1, 63.28 ms/it, loss 0.448353
Warning: Skipping the batch 76742 with size 14
Using 4-bit precision
Finished training it 1024/76743 of epoch 1, 63.06 ms/it, loss 0.451716
Warning: Skipping the batch 76742 with size 14
Using 4-bit precision
Finished training it 1024/76743 of epoch 1, 63.44 ms/it, loss 0.451484
Warning: Skipping the batch 76742 with size 14
Using 4-bit precision
Finished training it 1024/76743 of epoch 1, 63.00 ms/it, loss 0.450349
Finished training it 2048/76743 of epoch 1, 83.85 ms/it, loss 0.448710
Finished training it 2048/76743 of epoch 1, 83.85 ms/it, loss 0.450187
Finished training it 2048/76743 of epoch 1, 83.74 ms/it, loss 0.451800
Finished training it 2048/76743 of epoch 1, 83.65 ms/it, loss 0.453677
Finished training it 3072/76743 of epoch 1, 82.49 ms/it, loss 0.450811
Finished training it 3072/76743 of epoch 1, 82.65 ms/it, loss 0.452864
Finished training it 3072/76743 of epoch 1, 82.64 ms/it, loss 0.453543
Finished training it 3072/76743 of epoch 1, 82.40 ms/it, loss 0.453204
Finished training it 4096/76743 of epoch 1, 70.88 ms/it, loss 0.453570
Finished training it 4096/76743 of epoch 1, 70.62 ms/it, loss 0.453800
Finished training it 4096/76743 of epoch 1, 70.88 ms/it, loss 0.454012
Finished training it 4096/76743 of epoch 1, 70.61 ms/it, loss 0.451823
Finished training it 5120/76743 of epoch 1, 71.55 ms/it, loss 0.449391
Finished training it 5120/76743 of epoch 1, 71.57 ms/it, loss 0.451753
Finished training it 5120/76743 of epoch 1, 71.25 ms/it, loss 0.453300
Finished training it 5120/76743 of epoch 1, 71.35 ms/it, loss 0.454003
Finished training it 6144/76743 of epoch 1, 71.07 ms/it, loss 0.451185
Finished training it 6144/76743 of epoch 1, 71.01 ms/it, loss 0.449519
Finished training it 6144/76743 of epoch 1, 71.29 ms/it, loss 0.451675
Finished training it 6144/76743 of epoch 1, 71.29 ms/it, loss 0.450451
Finished training it 7168/76743 of epoch 1, 71.49 ms/it, loss 0.451407
Finished training it 7168/76743 of epoch 1, 71.52 ms/it, loss 0.451853
Finished training it 7168/76743 of epoch 1, 71.32 ms/it, loss 0.451553
Finished training it 7168/76743 of epoch 1, 71.27 ms/it, loss 0.448549
Finished training it 8192/76743 of epoch 1, 71.50 ms/it, loss 0.452133
Finished training it 8192/76743 of epoch 1, 71.42 ms/it, loss 0.449162
Finished training it 8192/76743 of epoch 1, 71.13 ms/it, loss 0.447481
Finished training it 8192/76743 of epoch 1, 71.19 ms/it, loss 0.451070
Finished training it 9216/76743 of epoch 1, 76.41 ms/it, loss 0.453516
Finished training it 9216/76743 of epoch 1, 76.43 ms/it, loss 0.451505
Finished training it 9216/76743 of epoch 1, 76.25 ms/it, loss 0.451246
Finished training it 9216/76743 of epoch 1, 76.19 ms/it, loss 0.450710
Finished training it 10240/76743 of epoch 1, 84.02 ms/it, loss 0.451725
Finished training it 10240/76743 of epoch 1, 83.94 ms/it, loss 0.449565
Finished training it 10240/76743 of epoch 1, 83.77 ms/it, loss 0.449677
Finished training it 10240/76743 of epoch 1, 83.88 ms/it, loss 0.449631
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2574039.0
get out
0 has test check 2574039.0 and sample count 3274240
 accuracy 78.615 %, best 78.648 %, roc auc score 0.7977, best 0.7994
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 71.71 ms/it, loss 0.449585
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2574039.0
get out
1 has test check 2574039.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 71.80 ms/it, loss 0.448684
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2574039.0
get out
3 has test check 2574039.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 71.51 ms/it, loss 0.451954
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2574039.0
get out
2 has test check 2574039.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 71.43 ms/it, loss 0.452901
Finished training it 12288/76743 of epoch 1, 70.93 ms/it, loss 0.447700
Finished training it 12288/76743 of epoch 1, 71.44 ms/it, loss 0.449378
Finished training it 12288/76743 of epoch 1, 71.16 ms/it, loss 0.452474
Finished training it 12288/76743 of epoch 1, 71.43 ms/it, loss 0.449776
Finished training it 13312/76743 of epoch 1, 71.45 ms/it, loss 0.449512
Finished training it 13312/76743 of epoch 1, 70.95 ms/it, loss 0.451672
Finished training it 13312/76743 of epoch 1, 71.23 ms/it, loss 0.448742
Finished training it 13312/76743 of epoch 1, 71.43 ms/it, loss 0.450599
Finished training it 14336/76743 of epoch 1, 71.27 ms/it, loss 0.451081
Finished training it 14336/76743 of epoch 1, 71.49 ms/it, loss 0.451885
Finished training it 14336/76743 of epoch 1, 71.12 ms/it, loss 0.449953
Finished training it 14336/76743 of epoch 1, 71.50 ms/it, loss 0.449303
Finished training it 15360/76743 of epoch 1, 72.90 ms/it, loss 0.452455
Finished training it 15360/76743 of epoch 1, 72.61 ms/it, loss 0.449353
Finished training it 15360/76743 of epoch 1, 72.44 ms/it, loss 0.452388
Finished training it 15360/76743 of epoch 1, 72.93 ms/it, loss 0.450612
Finished training it 16384/76743 of epoch 1, 83.51 ms/it, loss 0.450602
Finished training it 16384/76743 of epoch 1, 83.28 ms/it, loss 0.446241
Finished training it 16384/76743 of epoch 1, 83.36 ms/it, loss 0.452688
Finished training it 16384/76743 of epoch 1, 83.50 ms/it, loss 0.446977
Finished training it 17408/76743 of epoch 1, 89.42 ms/it, loss 0.449365
Finished training it 17408/76743 of epoch 1, 89.61 ms/it, loss 0.450091
Finished training it 17408/76743 of epoch 1, 84.44 ms/it, loss 0.451042
Finished training it 17408/76743 of epoch 1, 89.47 ms/it, loss 0.448831
Finished training it 18432/76743 of epoch 1, 83.56 ms/it, loss 0.449457
Finished training it 18432/76743 of epoch 1, 83.67 ms/it, loss 0.448277
Finished training it 18432/76743 of epoch 1, 83.54 ms/it, loss 0.449738
Finished training it 18432/76743 of epoch 1, 83.70 ms/it, loss 0.449927
Finished training it 19456/76743 of epoch 1, 83.77 ms/it, loss 0.450310
Finished training it 19456/76743 of epoch 1, 83.76 ms/it, loss 0.450771
Finished training it 19456/76743 of epoch 1, 83.89 ms/it, loss 0.449502
Finished training it 19456/76743 of epoch 1, 83.92 ms/it, loss 0.447703
Finished training it 20480/76743 of epoch 1, 82.49 ms/it, loss 0.447939
Finished training it 20480/76743 of epoch 1, 82.63 ms/it, loss 0.447780
Finished training it 20480/76743 of epoch 1, 82.67 ms/it, loss 0.447926
Finished training it 20480/76743 of epoch 1, 82.51 ms/it, loss 0.449218
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575039.0
get out
0 has test check 2575039.0 and sample count 3274240
 accuracy 78.645 %, best 78.648 %, roc auc score 0.7978, best 0.7994
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575039.0
get out
2 has test check 2575039.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 74.62 ms/it, loss 0.446310
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575039.0
get out
3 has test check 2575039.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 74.60 ms/it, loss 0.448860
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 74.86 ms/it, loss 0.449832
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575039.0
get out
1 has test check 2575039.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 74.84 ms/it, loss 0.448564
Finished training it 22528/76743 of epoch 1, 83.63 ms/it, loss 0.446955
Finished training it 22528/76743 of epoch 1, 83.62 ms/it, loss 0.450706
Finished training it 22528/76743 of epoch 1, 83.50 ms/it, loss 0.447029
Finished training it 22528/76743 of epoch 1, 83.48 ms/it, loss 0.448788
Finished training it 23552/76743 of epoch 1, 82.77 ms/it, loss 0.450464
Finished training it 23552/76743 of epoch 1, 87.82 ms/it, loss 0.444486
Finished training it 23552/76743 of epoch 1, 82.71 ms/it, loss 0.445761
Finished training it 23552/76743 of epoch 1, 82.94 ms/it, loss 0.446823
Finished training it 24576/76743 of epoch 1, 82.59 ms/it, loss 0.447146
Finished training it 24576/76743 of epoch 1, 82.41 ms/it, loss 0.448191
Finished training it 24576/76743 of epoch 1, 82.66 ms/it, loss 0.451116
Finished training it 24576/76743 of epoch 1, 82.44 ms/it, loss 0.449196
Finished training it 25600/76743 of epoch 1, 84.10 ms/it, loss 0.450278
Finished training it 25600/76743 of epoch 1, 84.11 ms/it, loss 0.445590
Finished training it 25600/76743 of epoch 1, 83.91 ms/it, loss 0.448936
Finished training it 25600/76743 of epoch 1, 83.97 ms/it, loss 0.449013
Finished training it 26624/76743 of epoch 1, 84.02 ms/it, loss 0.447180
Finished training it 26624/76743 of epoch 1, 83.84 ms/it, loss 0.448167
Finished training it 26624/76743 of epoch 1, 84.06 ms/it, loss 0.450399
Finished training it 26624/76743 of epoch 1, 83.79 ms/it, loss 0.447421
Finished training it 27648/76743 of epoch 1, 83.75 ms/it, loss 0.447155
Finished training it 27648/76743 of epoch 1, 83.81 ms/it, loss 0.447725
Finished training it 27648/76743 of epoch 1, 83.89 ms/it, loss 0.447719
Finished training it 27648/76743 of epoch 1, 83.92 ms/it, loss 0.449338
Finished training it 28672/76743 of epoch 1, 84.12 ms/it, loss 0.449082
Finished training it 28672/76743 of epoch 1, 83.89 ms/it, loss 0.447795
Finished training it 28672/76743 of epoch 1, 84.09 ms/it, loss 0.448746
Finished training it 28672/76743 of epoch 1, 83.92 ms/it, loss 0.448103
Finished training it 29696/76743 of epoch 1, 83.39 ms/it, loss 0.447743
Finished training it 29696/76743 of epoch 1, 83.38 ms/it, loss 0.448491
Finished training it 29696/76743 of epoch 1, 83.29 ms/it, loss 0.449067
Finished training it 29696/76743 of epoch 1, 83.21 ms/it, loss 0.448689
Finished training it 30720/76743 of epoch 1, 77.40 ms/it, loss 0.445738
Finished training it 30720/76743 of epoch 1, 77.24 ms/it, loss 0.448876
Finished training it 30720/76743 of epoch 1, 77.25 ms/it, loss 0.447733
Finished training it 30720/76743 of epoch 1, 77.43 ms/it, loss 0.447473
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575055.0
get out
0 has test check 2575055.0 and sample count 3274240
 accuracy 78.646 %, best 78.648 %, roc auc score 0.7983, best 0.7994
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 83.97 ms/it, loss 0.448786
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575055.0
get out
3 has test check 2575055.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 83.86 ms/it, loss 0.450215
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575055.0
get out
1 has test check 2575055.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 84.03 ms/it, loss 0.445734
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575055.0
get out
2 has test check 2575055.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 83.91 ms/it, loss 0.448586
Finished training it 32768/76743 of epoch 1, 83.69 ms/it, loss 0.444360
Finished training it 32768/76743 of epoch 1, 83.72 ms/it, loss 0.450001
Finished training it 32768/76743 of epoch 1, 83.53 ms/it, loss 0.447772
Finished training it 32768/76743 of epoch 1, 83.58 ms/it, loss 0.447035
Finished training it 33792/76743 of epoch 1, 84.04 ms/it, loss 0.447949
Finished training it 33792/76743 of epoch 1, 84.06 ms/it, loss 0.446354
Finished training it 33792/76743 of epoch 1, 83.90 ms/it, loss 0.448107
Finished training it 33792/76743 of epoch 1, 83.82 ms/it, loss 0.450844
Finished training it 34816/76743 of epoch 1, 83.15 ms/it, loss 0.449381
Finished training it 34816/76743 of epoch 1, 83.03 ms/it, loss 0.447734
Finished training it 34816/76743 of epoch 1, 83.20 ms/it, loss 0.447670
Finished training it 34816/76743 of epoch 1, 82.95 ms/it, loss 0.449244
Finished training it 35840/76743 of epoch 1, 83.74 ms/it, loss 0.448548
Finished training it 35840/76743 of epoch 1, 83.78 ms/it, loss 0.447178
Finished training it 35840/76743 of epoch 1, 83.51 ms/it, loss 0.450545
Finished training it 35840/76743 of epoch 1, 83.61 ms/it, loss 0.446807
Finished training it 36864/76743 of epoch 1, 77.17 ms/it, loss 0.447687
Finished training it 36864/76743 of epoch 1, 82.15 ms/it, loss 0.447909
Finished training it 36864/76743 of epoch 1, 81.87 ms/it, loss 0.446130
Finished training it 36864/76743 of epoch 1, 81.91 ms/it, loss 0.448661
Finished training it 37888/76743 of epoch 1, 71.07 ms/it, loss 0.446196
Finished training it 37888/76743 of epoch 1, 71.18 ms/it, loss 0.447958
Finished training it 37888/76743 of epoch 1, 70.79 ms/it, loss 0.447252
Finished training it 37888/76743 of epoch 1, 70.83 ms/it, loss 0.447302
Finished training it 38912/76743 of epoch 1, 71.12 ms/it, loss 0.448453
Finished training it 38912/76743 of epoch 1, 71.07 ms/it, loss 0.446799
Finished training it 38912/76743 of epoch 1, 70.76 ms/it, loss 0.447412
Finished training it 38912/76743 of epoch 1, 70.80 ms/it, loss 0.445857
Finished training it 39936/76743 of epoch 1, 71.16 ms/it, loss 0.448908
Finished training it 39936/76743 of epoch 1, 71.44 ms/it, loss 0.447701
Finished training it 39936/76743 of epoch 1, 71.41 ms/it, loss 0.447655
Finished training it 39936/76743 of epoch 1, 71.14 ms/it, loss 0.450167
Finished training it 40960/76743 of epoch 1, 71.57 ms/it, loss 0.448053
Finished training it 40960/76743 of epoch 1, 71.25 ms/it, loss 0.447469
Finished training it 40960/76743 of epoch 1, 71.82 ms/it, loss 0.447570
Finished training it 40960/76743 of epoch 1, 71.87 ms/it, loss 0.447461
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575734.0
get out
0 has test check 2575734.0 and sample count 3274240
 accuracy 78.667 %, best 78.667 %, roc auc score 0.7995, best 0.7995
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575734.0
get out
1 has test check 2575734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 83.97 ms/it, loss 0.446985
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 83.98 ms/it, loss 0.447562
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575734.0
get out
2 has test check 2575734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 83.80 ms/it, loss 0.445243
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575734.0
get out
3 has test check 2575734.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 83.65 ms/it, loss 0.448564
Finished training it 43008/76743 of epoch 1, 80.27 ms/it, loss 0.446302
Finished training it 43008/76743 of epoch 1, 80.28 ms/it, loss 0.447925
Finished training it 43008/76743 of epoch 1, 80.06 ms/it, loss 0.446826
Finished training it 43008/76743 of epoch 1, 79.92 ms/it, loss 0.450021
Finished training it 44032/76743 of epoch 1, 74.58 ms/it, loss 0.447095
Finished training it 44032/76743 of epoch 1, 79.22 ms/it, loss 0.447790
Finished training it 44032/76743 of epoch 1, 74.02 ms/it, loss 0.446196
Finished training it 44032/76743 of epoch 1, 71.14 ms/it, loss 0.448557
Finished training it 45056/76743 of epoch 1, 71.71 ms/it, loss 0.446189
Finished training it 45056/76743 of epoch 1, 71.69 ms/it, loss 0.445185
Finished training it 45056/76743 of epoch 1, 71.21 ms/it, loss 0.446053
Finished training it 45056/76743 of epoch 1, 71.46 ms/it, loss 0.445934
Finished training it 46080/76743 of epoch 1, 70.65 ms/it, loss 0.447625
Finished training it 46080/76743 of epoch 1, 71.19 ms/it, loss 0.448171
Finished training it 46080/76743 of epoch 1, 70.94 ms/it, loss 0.446163
Finished training it 46080/76743 of epoch 1, 71.15 ms/it, loss 0.448883
Finished training it 47104/76743 of epoch 1, 70.78 ms/it, loss 0.447882
Finished training it 47104/76743 of epoch 1, 71.27 ms/it, loss 0.448520
Finished training it 47104/76743 of epoch 1, 71.26 ms/it, loss 0.447153
Finished training it 47104/76743 of epoch 1, 71.06 ms/it, loss 0.447871
Finished training it 48128/76743 of epoch 1, 70.91 ms/it, loss 0.447796
Finished training it 48128/76743 of epoch 1, 71.15 ms/it, loss 0.448033
Finished training it 48128/76743 of epoch 1, 71.52 ms/it, loss 0.447347
Finished training it 48128/76743 of epoch 1, 71.48 ms/it, loss 0.445294
Finished training it 49152/76743 of epoch 1, 80.08 ms/it, loss 0.451075
Finished training it 49152/76743 of epoch 1, 79.75 ms/it, loss 0.444587
Finished training it 49152/76743 of epoch 1, 79.94 ms/it, loss 0.446182
Finished training it 49152/76743 of epoch 1, 80.15 ms/it, loss 0.449579
Finished training it 50176/76743 of epoch 1, 83.90 ms/it, loss 0.447245
Finished training it 50176/76743 of epoch 1, 83.86 ms/it, loss 0.447403
Finished training it 50176/76743 of epoch 1, 83.71 ms/it, loss 0.445728
Finished training it 50176/76743 of epoch 1, 83.99 ms/it, loss 0.446590
Finished training it 51200/76743 of epoch 1, 84.15 ms/it, loss 0.447746
Finished training it 51200/76743 of epoch 1, 84.18 ms/it, loss 0.447441
Finished training it 51200/76743 of epoch 1, 83.86 ms/it, loss 0.447957
Finished training it 51200/76743 of epoch 1, 83.97 ms/it, loss 0.446326
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2576076.0
get out
0 has test check 2576076.0 and sample count 3274240
 accuracy 78.677 %, best 78.677 %, roc auc score 0.7993, best 0.7995
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2576076.0
get out
3 has test check 2576076.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 71.00 ms/it, loss 0.449674
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576076.0
get out
1 has test check 2576076.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 71.34 ms/it, loss 0.444225
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 71.36 ms/it, loss 0.445837
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2576076.0
get out
2 has test check 2576076.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 71.05 ms/it, loss 0.445767
Finished training it 53248/76743 of epoch 1, 71.25 ms/it, loss 0.448923
Finished training it 53248/76743 of epoch 1, 71.60 ms/it, loss 0.445230
Finished training it 53248/76743 of epoch 1, 71.65 ms/it, loss 0.447370
Finished training it 53248/76743 of epoch 1, 71.33 ms/it, loss 0.450088
Finished training it 54272/76743 of epoch 1, 71.33 ms/it, loss 0.447795
Finished training it 54272/76743 of epoch 1, 71.04 ms/it, loss 0.445761
Finished training it 54272/76743 of epoch 1, 71.32 ms/it, loss 0.449935
Finished training it 54272/76743 of epoch 1, 71.00 ms/it, loss 0.447045
Finished training it 55296/76743 of epoch 1, 86.17 ms/it, loss 0.448349
Finished training it 55296/76743 of epoch 1, 86.37 ms/it, loss 0.444437
Finished training it 55296/76743 of epoch 1, 81.77 ms/it, loss 0.445994
Finished training it 55296/76743 of epoch 1, 86.13 ms/it, loss 0.446183
Finished training it 56320/76743 of epoch 1, 82.78 ms/it, loss 0.445896
Finished training it 56320/76743 of epoch 1, 82.57 ms/it, loss 0.449547
Finished training it 56320/76743 of epoch 1, 82.66 ms/it, loss 0.448496
Finished training it 56320/76743 of epoch 1, 82.79 ms/it, loss 0.448868
Finished training it 57344/76743 of epoch 1, 83.26 ms/it, loss 0.446390
Finished training it 57344/76743 of epoch 1, 83.23 ms/it, loss 0.445601
Finished training it 57344/76743 of epoch 1, 83.43 ms/it, loss 0.446987
Finished training it 57344/76743 of epoch 1, 83.37 ms/it, loss 0.449328
Finished training it 58368/76743 of epoch 1, 82.87 ms/it, loss 0.445630
Finished training it 58368/76743 of epoch 1, 82.80 ms/it, loss 0.447768
Finished training it 58368/76743 of epoch 1, 82.61 ms/it, loss 0.446536
Finished training it 58368/76743 of epoch 1, 82.63 ms/it, loss 0.447927
Finished training it 59392/76743 of epoch 1, 83.66 ms/it, loss 0.449976
Finished training it 59392/76743 of epoch 1, 83.65 ms/it, loss 0.444006
Finished training it 59392/76743 of epoch 1, 83.51 ms/it, loss 0.445847
Finished training it 59392/76743 of epoch 1, 83.43 ms/it, loss 0.446499
Finished training it 60416/76743 of epoch 1, 84.17 ms/it, loss 0.445457
Finished training it 60416/76743 of epoch 1, 84.11 ms/it, loss 0.446446
Finished training it 60416/76743 of epoch 1, 84.28 ms/it, loss 0.447055
Finished training it 60416/76743 of epoch 1, 84.30 ms/it, loss 0.444978
Finished training it 61440/76743 of epoch 1, 73.37 ms/it, loss 0.445988
Finished training it 61440/76743 of epoch 1, 73.38 ms/it, loss 0.444646
Finished training it 61440/76743 of epoch 1, 73.11 ms/it, loss 0.445924
Finished training it 61440/76743 of epoch 1, 73.17 ms/it, loss 0.445408
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577832.0
get out
0 has test check 2577832.0 and sample count 3274240
 accuracy 78.731 %, best 78.731 %, roc auc score 0.8003, best 0.8003
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577832.0
get out
3 has test check 2577832.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 84.17 ms/it, loss 0.447737
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577832.0
get out
1 has test check 2577832.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 84.31 ms/it, loss 0.447077
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 84.30 ms/it, loss 0.446009
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577832.0
get out
2 has test check 2577832.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 84.20 ms/it, loss 0.446397
Finished training it 63488/76743 of epoch 1, 83.98 ms/it, loss 0.445179
Finished training it 63488/76743 of epoch 1, 84.12 ms/it, loss 0.448734
Finished training it 63488/76743 of epoch 1, 84.15 ms/it, loss 0.443873
Finished training it 63488/76743 of epoch 1, 83.98 ms/it, loss 0.446276
Finished training it 64512/76743 of epoch 1, 83.92 ms/it, loss 0.444717
Finished training it 64512/76743 of epoch 1, 84.47 ms/it, loss 0.444384
Finished training it 64512/76743 of epoch 1, 90.45 ms/it, loss 0.442284
Finished training it 64512/76743 of epoch 1, 85.37 ms/it, loss 0.444159
Finished training it 65536/76743 of epoch 1, 83.81 ms/it, loss 0.448036
Finished training it 65536/76743 of epoch 1, 83.68 ms/it, loss 0.442800
Finished training it 65536/76743 of epoch 1, 83.83 ms/it, loss 0.445151
Finished training it 65536/76743 of epoch 1, 83.68 ms/it, loss 0.446287
Finished training it 66560/76743 of epoch 1, 83.77 ms/it, loss 0.447614
Finished training it 66560/76743 of epoch 1, 83.89 ms/it, loss 0.446473
Finished training it 66560/76743 of epoch 1, 83.89 ms/it, loss 0.445443
Finished training it 66560/76743 of epoch 1, 83.72 ms/it, loss 0.445520
Finished training it 67584/76743 of epoch 1, 71.86 ms/it, loss 0.444078
Finished training it 67584/76743 of epoch 1, 71.59 ms/it, loss 0.447188
Finished training it 67584/76743 of epoch 1, 71.89 ms/it, loss 0.444290
Finished training it 67584/76743 of epoch 1, 71.58 ms/it, loss 0.447894
Finished training it 68608/76743 of epoch 1, 70.96 ms/it, loss 0.446177
Finished training it 68608/76743 of epoch 1, 70.98 ms/it, loss 0.448528
Finished training it 68608/76743 of epoch 1, 71.33 ms/it, loss 0.445286
Finished training it 68608/76743 of epoch 1, 71.32 ms/it, loss 0.446429
Finished training it 69632/76743 of epoch 1, 71.61 ms/it, loss 0.448589
Finished training it 69632/76743 of epoch 1, 71.34 ms/it, loss 0.446365
Finished training it 69632/76743 of epoch 1, 71.53 ms/it, loss 0.444888
Finished training it 69632/76743 of epoch 1, 71.27 ms/it, loss 0.446454
Finished training it 70656/76743 of epoch 1, 70.95 ms/it, loss 0.446205
Finished training it 70656/76743 of epoch 1, 71.26 ms/it, loss 0.449122
Finished training it 70656/76743 of epoch 1, 70.98 ms/it, loss 0.446982
Finished training it 70656/76743 of epoch 1, 71.24 ms/it, loss 0.447306
Finished training it 71680/76743 of epoch 1, 71.25 ms/it, loss 0.443854
Finished training it 71680/76743 of epoch 1, 71.26 ms/it, loss 0.447202
Finished training it 71680/76743 of epoch 1, 71.42 ms/it, loss 0.445281
Finished training it 71680/76743 of epoch 1, 71.51 ms/it, loss 0.447937
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577418.0
get out
0 has test check 2577418.0 and sample count 3274240
 accuracy 78.718 %, best 78.731 %, roc auc score 0.8009, best 0.8009
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577418.0
get out
2 has test check 2577418.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 83.58 ms/it, loss 0.445870
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577418.0
get out
3 has test check 2577418.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 83.54 ms/it, loss 0.445796
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 83.66 ms/it, loss 0.445846
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577418.0
get out
1 has test check 2577418.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 83.66 ms/it, loss 0.444851
Finished training it 73728/76743 of epoch 1, 76.74 ms/it, loss 0.448251
Finished training it 73728/76743 of epoch 1, 76.72 ms/it, loss 0.444804
Finished training it 73728/76743 of epoch 1, 76.51 ms/it, loss 0.447608
Finished training it 73728/76743 of epoch 1, 76.57 ms/it, loss 0.445036
Finished training it 74752/76743 of epoch 1, 71.47 ms/it, loss 0.447193
Finished training it 74752/76743 of epoch 1, 76.01 ms/it, loss 0.446649
Finished training it 74752/76743 of epoch 1, 76.32 ms/it, loss 0.443562
Finished training it 74752/76743 of epoch 1, 76.04 ms/it, loss 0.446401
Finished training it 75776/76743 of epoch 1, 71.25 ms/it, loss 0.447902
Finished training it 75776/76743 of epoch 1, 71.03 ms/it, loss 0.445487
Finished training it 75776/76743 of epoch 1, 70.74 ms/it, loss 0.445825
Finished training it 75776/76743 of epoch 1, 70.79 ms/it, loss 0.445623
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 72.22 ms/it, loss 0.444434
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 71.77 ms/it, loss 0.447496
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 72.04 ms/it, loss 0.447372
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 71.76 ms/it, loss 0.446609
Finished training it 2048/76743 of epoch 2, 71.30 ms/it, loss 0.444771
Finished training it 2048/76743 of epoch 2, 71.30 ms/it, loss 0.443031
Finished training it 2048/76743 of epoch 2, 71.03 ms/it, loss 0.447794
Finished training it 2048/76743 of epoch 2, 71.02 ms/it, loss 0.445879
Finished training it 3072/76743 of epoch 2, 82.21 ms/it, loss 0.447026
Finished training it 3072/76743 of epoch 2, 82.21 ms/it, loss 0.448082
Finished training it 3072/76743 of epoch 2, 82.06 ms/it, loss 0.444704
Finished training it 3072/76743 of epoch 2, 82.03 ms/it, loss 0.447390
Finished training it 4096/76743 of epoch 2, 84.03 ms/it, loss 0.448375
Finished training it 4096/76743 of epoch 2, 84.05 ms/it, loss 0.447847
Finished training it 4096/76743 of epoch 2, 83.96 ms/it, loss 0.447879
Finished training it 4096/76743 of epoch 2, 83.86 ms/it, loss 0.446104
Finished training it 5120/76743 of epoch 2, 83.21 ms/it, loss 0.443472
Finished training it 5120/76743 of epoch 2, 83.26 ms/it, loss 0.446273
Finished training it 5120/76743 of epoch 2, 83.09 ms/it, loss 0.447831
Finished training it 5120/76743 of epoch 2, 83.05 ms/it, loss 0.448193
Finished training it 6144/76743 of epoch 2, 82.14 ms/it, loss 0.443914
Finished training it 6144/76743 of epoch 2, 82.15 ms/it, loss 0.446201
Finished training it 6144/76743 of epoch 2, 82.32 ms/it, loss 0.444892
Finished training it 6144/76743 of epoch 2, 82.34 ms/it, loss 0.446223
Finished training it 7168/76743 of epoch 2, 82.71 ms/it, loss 0.445688
Finished training it 7168/76743 of epoch 2, 82.59 ms/it, loss 0.445784
Finished training it 7168/76743 of epoch 2, 82.72 ms/it, loss 0.446359
Finished training it 7168/76743 of epoch 2, 82.60 ms/it, loss 0.443659
Finished training it 8192/76743 of epoch 2, 83.88 ms/it, loss 0.446816
Finished training it 8192/76743 of epoch 2, 83.87 ms/it, loss 0.443503
Finished training it 8192/76743 of epoch 2, 83.79 ms/it, loss 0.446022
Finished training it 8192/76743 of epoch 2, 83.76 ms/it, loss 0.442549
Finished training it 9216/76743 of epoch 2, 83.85 ms/it, loss 0.448294
Finished training it 9216/76743 of epoch 2, 83.69 ms/it, loss 0.445553
Finished training it 9216/76743 of epoch 2, 83.85 ms/it, loss 0.446331
Finished training it 9216/76743 of epoch 2, 83.72 ms/it, loss 0.446459
Finished training it 10240/76743 of epoch 2, 83.44 ms/it, loss 0.443895
Finished training it 10240/76743 of epoch 2, 83.42 ms/it, loss 0.446214
Finished training it 10240/76743 of epoch 2, 83.27 ms/it, loss 0.444270
Finished training it 10240/76743 of epoch 2, 83.30 ms/it, loss 0.444443
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580128.0
get out
0 has test check 2580128.0 and sample count 3274240
 accuracy 78.801 %, best 78.801 %, roc auc score 0.8016, best 0.8016
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580128.0
get out
1 has test check 2580128.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 71.41 ms/it, loss 0.443732
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 71.33 ms/it, loss 0.444586
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580128.0
get out
3 has test check 2580128.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 71.18 ms/it, loss 0.446671
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580128.0
get out
2 has test check 2580128.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 71.21 ms/it, loss 0.447499
Finished training it 12288/76743 of epoch 2, 80.03 ms/it, loss 0.445117
Finished training it 12288/76743 of epoch 2, 79.84 ms/it, loss 0.442583
Finished training it 12288/76743 of epoch 2, 79.86 ms/it, loss 0.447624
Finished training it 12288/76743 of epoch 2, 80.04 ms/it, loss 0.444424
Finished training it 13312/76743 of epoch 2, 82.50 ms/it, loss 0.444854
Finished training it 13312/76743 of epoch 2, 82.40 ms/it, loss 0.446723
Finished training it 13312/76743 of epoch 2, 87.62 ms/it, loss 0.445743
Finished training it 13312/76743 of epoch 2, 82.43 ms/it, loss 0.443906
Finished training it 14336/76743 of epoch 2, 83.70 ms/it, loss 0.444641
Finished training it 14336/76743 of epoch 2, 83.85 ms/it, loss 0.444168
Finished training it 14336/76743 of epoch 2, 83.86 ms/it, loss 0.446903
Finished training it 14336/76743 of epoch 2, 83.66 ms/it, loss 0.445750
Finished training it 15360/76743 of epoch 2, 83.68 ms/it, loss 0.444363
Finished training it 15360/76743 of epoch 2, 83.76 ms/it, loss 0.447551
Finished training it 15360/76743 of epoch 2, 83.86 ms/it, loss 0.445815
Finished training it 15360/76743 of epoch 2, 83.85 ms/it, loss 0.447518
Finished training it 16384/76743 of epoch 2, 82.53 ms/it, loss 0.441372
Finished training it 16384/76743 of epoch 2, 82.73 ms/it, loss 0.441806
Finished training it 16384/76743 of epoch 2, 82.65 ms/it, loss 0.445575
Finished training it 16384/76743 of epoch 2, 82.59 ms/it, loss 0.447525
Finished training it 17408/76743 of epoch 2, 83.75 ms/it, loss 0.443751
Finished training it 17408/76743 of epoch 2, 83.92 ms/it, loss 0.445638
Finished training it 17408/76743 of epoch 2, 83.92 ms/it, loss 0.445517
Finished training it 17408/76743 of epoch 2, 83.74 ms/it, loss 0.444813
Finished training it 18432/76743 of epoch 2, 74.04 ms/it, loss 0.443012
Finished training it 18432/76743 of epoch 2, 74.07 ms/it, loss 0.444712
Finished training it 18432/76743 of epoch 2, 73.78 ms/it, loss 0.444593
Finished training it 18432/76743 of epoch 2, 73.84 ms/it, loss 0.444530
Finished training it 19456/76743 of epoch 2, 70.76 ms/it, loss 0.445270
Finished training it 19456/76743 of epoch 2, 71.01 ms/it, loss 0.442638
Finished training it 19456/76743 of epoch 2, 71.00 ms/it, loss 0.444437
Finished training it 19456/76743 of epoch 2, 70.80 ms/it, loss 0.446032
Finished training it 20480/76743 of epoch 2, 71.50 ms/it, loss 0.442910
Finished training it 20480/76743 of epoch 2, 71.49 ms/it, loss 0.442998
Finished training it 20480/76743 of epoch 2, 71.18 ms/it, loss 0.444034
Finished training it 20480/76743 of epoch 2, 71.22 ms/it, loss 0.442907
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581024.0
get out
0 has test check 2581024.0 and sample count 3274240
 accuracy 78.828 %, best 78.828 %, roc auc score 0.8014, best 0.8016
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 84.18 ms/it, loss 0.444597
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581024.0
get out
1 has test check 2581024.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 84.20 ms/it, loss 0.443579
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581024.0
get out
2 has test check 2581024.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 84.06 ms/it, loss 0.441565
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581024.0
get out
3 has test check 2581024.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 84.02 ms/it, loss 0.444505
Finished training it 22528/76743 of epoch 2, 84.07 ms/it, loss 0.445274
Finished training it 22528/76743 of epoch 2, 83.83 ms/it, loss 0.444442
Finished training it 22528/76743 of epoch 2, 83.85 ms/it, loss 0.442072
Finished training it 22528/76743 of epoch 2, 84.04 ms/it, loss 0.442123
Finished training it 23552/76743 of epoch 2, 88.77 ms/it, loss 0.445757
Finished training it 23552/76743 of epoch 2, 88.84 ms/it, loss 0.440796
Finished training it 23552/76743 of epoch 2, 83.68 ms/it, loss 0.439719
Finished training it 23552/76743 of epoch 2, 88.91 ms/it, loss 0.442315
Finished training it 24576/76743 of epoch 2, 74.79 ms/it, loss 0.443430
Finished training it 24576/76743 of epoch 2, 75.02 ms/it, loss 0.446514
Finished training it 24576/76743 of epoch 2, 74.83 ms/it, loss 0.444715
Finished training it 24576/76743 of epoch 2, 75.01 ms/it, loss 0.442637
Finished training it 25600/76743 of epoch 2, 70.91 ms/it, loss 0.444607
Finished training it 25600/76743 of epoch 2, 71.21 ms/it, loss 0.445360
Finished training it 25600/76743 of epoch 2, 71.21 ms/it, loss 0.440953
Finished training it 25600/76743 of epoch 2, 70.82 ms/it, loss 0.444683
Finished training it 26624/76743 of epoch 2, 71.02 ms/it, loss 0.442789
Finished training it 26624/76743 of epoch 2, 71.22 ms/it, loss 0.445603
Finished training it 26624/76743 of epoch 2, 71.16 ms/it, loss 0.442629
Finished training it 26624/76743 of epoch 2, 70.99 ms/it, loss 0.443465
Finished training it 27648/76743 of epoch 2, 71.25 ms/it, loss 0.442967
Finished training it 27648/76743 of epoch 2, 71.28 ms/it, loss 0.444483
Finished training it 27648/76743 of epoch 2, 70.95 ms/it, loss 0.442610
Finished training it 27648/76743 of epoch 2, 70.98 ms/it, loss 0.443503
Finished training it 28672/76743 of epoch 2, 71.36 ms/it, loss 0.444513
Finished training it 28672/76743 of epoch 2, 71.08 ms/it, loss 0.443047
Finished training it 28672/76743 of epoch 2, 71.36 ms/it, loss 0.444520
Finished training it 28672/76743 of epoch 2, 71.07 ms/it, loss 0.443165
Finished training it 29696/76743 of epoch 2, 71.69 ms/it, loss 0.443719
Finished training it 29696/76743 of epoch 2, 71.44 ms/it, loss 0.444435
Finished training it 29696/76743 of epoch 2, 71.45 ms/it, loss 0.444332
Finished training it 29696/76743 of epoch 2, 71.75 ms/it, loss 0.443300
Finished training it 30720/76743 of epoch 2, 83.65 ms/it, loss 0.443180
Finished training it 30720/76743 of epoch 2, 83.52 ms/it, loss 0.443582
Finished training it 30720/76743 of epoch 2, 83.49 ms/it, loss 0.444287
Finished training it 30720/76743 of epoch 2, 83.63 ms/it, loss 0.441269
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580589.0
get out
0 has test check 2580589.0 and sample count 3274240
 accuracy 78.815 %, best 78.828 %, roc auc score 0.8017, best 0.8017
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580589.0
get out
1 has test check 2580589.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 71.44 ms/it, loss 0.441322
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580589.0
get out
2 has test check 2580589.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 71.23 ms/it, loss 0.444108
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 71.40 ms/it, loss 0.444331
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580589.0
get out
3 has test check 2580589.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 71.15 ms/it, loss 0.445500
Finished training it 32768/76743 of epoch 2, 70.97 ms/it, loss 0.442723
Finished training it 32768/76743 of epoch 2, 71.17 ms/it, loss 0.445268
Finished training it 32768/76743 of epoch 2, 71.20 ms/it, loss 0.439630
Finished training it 32768/76743 of epoch 2, 70.90 ms/it, loss 0.443234
Finished training it 33792/76743 of epoch 2, 70.92 ms/it, loss 0.446542
Finished training it 33792/76743 of epoch 2, 71.28 ms/it, loss 0.441846
Finished training it 33792/76743 of epoch 2, 71.19 ms/it, loss 0.443420
Finished training it 33792/76743 of epoch 2, 71.08 ms/it, loss 0.443876
Finished training it 34816/76743 of epoch 2, 77.13 ms/it, loss 0.443074
Finished training it 34816/76743 of epoch 2, 71.73 ms/it, loss 0.444879
Finished training it 34816/76743 of epoch 2, 72.47 ms/it, loss 0.445462
Finished training it 34816/76743 of epoch 2, 71.18 ms/it, loss 0.443371
Finished training it 35840/76743 of epoch 2, 71.21 ms/it, loss 0.443071
Finished training it 35840/76743 of epoch 2, 71.02 ms/it, loss 0.442718
Finished training it 35840/76743 of epoch 2, 70.93 ms/it, loss 0.445956
Finished training it 35840/76743 of epoch 2, 71.23 ms/it, loss 0.443945
Finished training it 36864/76743 of epoch 2, 79.97 ms/it, loss 0.443357
Finished training it 36864/76743 of epoch 2, 79.72 ms/it, loss 0.444413
Finished training it 36864/76743 of epoch 2, 79.94 ms/it, loss 0.443628
Finished training it 36864/76743 of epoch 2, 79.75 ms/it, loss 0.441694
Finished training it 37888/76743 of epoch 2, 83.54 ms/it, loss 0.443395
Finished training it 37888/76743 of epoch 2, 83.43 ms/it, loss 0.442622
Finished training it 37888/76743 of epoch 2, 83.55 ms/it, loss 0.441410
Finished training it 37888/76743 of epoch 2, 83.39 ms/it, loss 0.442709
Finished training it 38912/76743 of epoch 2, 83.67 ms/it, loss 0.442681
Finished training it 38912/76743 of epoch 2, 83.69 ms/it, loss 0.443844
Finished training it 38912/76743 of epoch 2, 83.53 ms/it, loss 0.442979
Finished training it 38912/76743 of epoch 2, 83.50 ms/it, loss 0.441283
Finished training it 39936/76743 of epoch 2, 82.63 ms/it, loss 0.443793
Finished training it 39936/76743 of epoch 2, 82.48 ms/it, loss 0.445997
Finished training it 39936/76743 of epoch 2, 82.43 ms/it, loss 0.444648
Finished training it 39936/76743 of epoch 2, 82.61 ms/it, loss 0.443425
Finished training it 40960/76743 of epoch 2, 83.45 ms/it, loss 0.443077
Finished training it 40960/76743 of epoch 2, 83.25 ms/it, loss 0.443371
Finished training it 40960/76743 of epoch 2, 83.44 ms/it, loss 0.443101
Finished training it 40960/76743 of epoch 2, 83.27 ms/it, loss 0.443784
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578742.0
get out
0 has test check 2578742.0 and sample count 3274240
 accuracy 78.758 %, best 78.828 %, roc auc score 0.8023, best 0.8023
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578742.0
get out
2 has test check 2578742.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 71.08 ms/it, loss 0.440939
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578742.0
get out
3 has test check 2578742.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 71.11 ms/it, loss 0.444115
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578742.0
get out
1 has test check 2578742.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 71.37 ms/it, loss 0.442931
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 71.38 ms/it, loss 0.443643
Finished training it 43008/76743 of epoch 2, 78.66 ms/it, loss 0.442019
Finished training it 43008/76743 of epoch 2, 78.64 ms/it, loss 0.443691
Finished training it 43008/76743 of epoch 2, 78.37 ms/it, loss 0.445603
Finished training it 43008/76743 of epoch 2, 78.46 ms/it, loss 0.442661
Finished training it 44032/76743 of epoch 2, 89.52 ms/it, loss 0.444301
Finished training it 44032/76743 of epoch 2, 89.56 ms/it, loss 0.441934
Finished training it 44032/76743 of epoch 2, 84.04 ms/it, loss 0.443422
Finished training it 44032/76743 of epoch 2, 89.74 ms/it, loss 0.442860
Finished training it 45056/76743 of epoch 2, 83.20 ms/it, loss 0.441835
Finished training it 45056/76743 of epoch 2, 83.21 ms/it, loss 0.441702
Finished training it 45056/76743 of epoch 2, 83.35 ms/it, loss 0.440915
Finished training it 45056/76743 of epoch 2, 83.37 ms/it, loss 0.441749
Finished training it 46080/76743 of epoch 2, 84.26 ms/it, loss 0.444435
Finished training it 46080/76743 of epoch 2, 84.28 ms/it, loss 0.443999
Finished training it 46080/76743 of epoch 2, 84.12 ms/it, loss 0.443259
Finished training it 46080/76743 of epoch 2, 84.12 ms/it, loss 0.442245
Finished training it 47104/76743 of epoch 2, 83.58 ms/it, loss 0.443926
Finished training it 47104/76743 of epoch 2, 83.60 ms/it, loss 0.443730
Finished training it 47104/76743 of epoch 2, 83.80 ms/it, loss 0.444360
Finished training it 47104/76743 of epoch 2, 83.77 ms/it, loss 0.442983
Finished training it 48128/76743 of epoch 2, 84.11 ms/it, loss 0.444002
Finished training it 48128/76743 of epoch 2, 84.25 ms/it, loss 0.441018
Finished training it 48128/76743 of epoch 2, 84.06 ms/it, loss 0.444093
Finished training it 48128/76743 of epoch 2, 84.25 ms/it, loss 0.443237
Finished training it 49152/76743 of epoch 2, 73.83 ms/it, loss 0.440309
Finished training it 49152/76743 of epoch 2, 74.12 ms/it, loss 0.445186
Finished training it 49152/76743 of epoch 2, 74.12 ms/it, loss 0.446432
Finished training it 49152/76743 of epoch 2, 73.84 ms/it, loss 0.441604
Finished training it 50176/76743 of epoch 2, 70.99 ms/it, loss 0.443381
Finished training it 50176/76743 of epoch 2, 71.01 ms/it, loss 0.441557
Finished training it 50176/76743 of epoch 2, 71.32 ms/it, loss 0.442500
Finished training it 50176/76743 of epoch 2, 71.28 ms/it, loss 0.442627
Finished training it 51200/76743 of epoch 2, 71.32 ms/it, loss 0.443315
Finished training it 51200/76743 of epoch 2, 71.30 ms/it, loss 0.443979
Finished training it 51200/76743 of epoch 2, 71.10 ms/it, loss 0.442085
Finished training it 51200/76743 of epoch 2, 71.04 ms/it, loss 0.443443
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580577.0
get out
0 has test check 2580577.0 and sample count 3274240
 accuracy 78.815 %, best 78.828 %, roc auc score 0.8020, best 0.8023
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580577.0
get out
2 has test check 2580577.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 83.16 ms/it, loss 0.441553
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580577.0
get out
1 has test check 2580577.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 83.23 ms/it, loss 0.439329
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580577.0
get out
3 has test check 2580577.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 83.08 ms/it, loss 0.445744
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 83.26 ms/it, loss 0.441699
Finished training it 53248/76743 of epoch 2, 83.59 ms/it, loss 0.445109
Finished training it 53248/76743 of epoch 2, 83.67 ms/it, loss 0.446171
Finished training it 53248/76743 of epoch 2, 83.77 ms/it, loss 0.443606
Finished training it 53248/76743 of epoch 2, 83.78 ms/it, loss 0.441171
Finished training it 54272/76743 of epoch 2, 83.62 ms/it, loss 0.443608
Finished training it 54272/76743 of epoch 2, 83.68 ms/it, loss 0.445475
Finished training it 54272/76743 of epoch 2, 83.41 ms/it, loss 0.441707
Finished training it 54272/76743 of epoch 2, 83.48 ms/it, loss 0.443053
Finished training it 55296/76743 of epoch 2, 75.80 ms/it, loss 0.441771
Finished training it 55296/76743 of epoch 2, 76.09 ms/it, loss 0.444187
Finished training it 55296/76743 of epoch 2, 76.27 ms/it, loss 0.440377
Finished training it 55296/76743 of epoch 2, 81.14 ms/it, loss 0.441908
Finished training it 56320/76743 of epoch 2, 71.29 ms/it, loss 0.441612
Finished training it 56320/76743 of epoch 2, 70.89 ms/it, loss 0.445271
Finished training it 56320/76743 of epoch 2, 70.97 ms/it, loss 0.444332
Finished training it 56320/76743 of epoch 2, 71.24 ms/it, loss 0.444801
Finished training it 57344/76743 of epoch 2, 70.97 ms/it, loss 0.443154
Finished training it 57344/76743 of epoch 2, 70.74 ms/it, loss 0.441650
Finished training it 57344/76743 of epoch 2, 71.00 ms/it, loss 0.445606
Finished training it 57344/76743 of epoch 2, 70.70 ms/it, loss 0.442486
Finished training it 58368/76743 of epoch 2, 71.02 ms/it, loss 0.441837
Finished training it 58368/76743 of epoch 2, 71.03 ms/it, loss 0.443814
Finished training it 58368/76743 of epoch 2, 70.76 ms/it, loss 0.442165
Finished training it 58368/76743 of epoch 2, 70.76 ms/it, loss 0.444064
Finished training it 59392/76743 of epoch 2, 71.51 ms/it, loss 0.439710
Finished training it 59392/76743 of epoch 2, 71.29 ms/it, loss 0.442692
Finished training it 59392/76743 of epoch 2, 71.52 ms/it, loss 0.445686
Finished training it 59392/76743 of epoch 2, 71.27 ms/it, loss 0.441653
Finished training it 60416/76743 of epoch 2, 71.31 ms/it, loss 0.441205
Finished training it 60416/76743 of epoch 2, 71.10 ms/it, loss 0.441623
Finished training it 60416/76743 of epoch 2, 71.00 ms/it, loss 0.442520
Finished training it 60416/76743 of epoch 2, 71.30 ms/it, loss 0.443231
Finished training it 61440/76743 of epoch 2, 80.68 ms/it, loss 0.441460
Finished training it 61440/76743 of epoch 2, 80.64 ms/it, loss 0.442305
Finished training it 61440/76743 of epoch 2, 80.82 ms/it, loss 0.440535
Finished training it 61440/76743 of epoch 2, 80.82 ms/it, loss 0.441895
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581803.0
get out
0 has test check 2581803.0 and sample count 3274240
 accuracy 78.852 %, best 78.852 %, roc auc score 0.8027, best 0.8027
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581803.0
get out
1 has test check 2581803.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 84.10 ms/it, loss 0.443229
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 84.06 ms/it, loss 0.442057
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581803.0
get out
2 has test check 2581803.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 83.95 ms/it, loss 0.442171
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581803.0
get out
3 has test check 2581803.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 83.91 ms/it, loss 0.443120
Finished training it 63488/76743 of epoch 2, 84.08 ms/it, loss 0.439746
Finished training it 63488/76743 of epoch 2, 84.08 ms/it, loss 0.444937
Finished training it 63488/76743 of epoch 2, 83.88 ms/it, loss 0.442077
Finished training it 63488/76743 of epoch 2, 83.90 ms/it, loss 0.441138
Finished training it 64512/76743 of epoch 2, 83.19 ms/it, loss 0.438453
Finished training it 64512/76743 of epoch 2, 88.52 ms/it, loss 0.440314
Finished training it 64512/76743 of epoch 2, 88.35 ms/it, loss 0.440659
Finished training it 64512/76743 of epoch 2, 88.39 ms/it, loss 0.441043
Finished training it 65536/76743 of epoch 2, 71.47 ms/it, loss 0.441114
Finished training it 65536/76743 of epoch 2, 71.45 ms/it, loss 0.444377
Finished training it 65536/76743 of epoch 2, 71.14 ms/it, loss 0.442592
Finished training it 65536/76743 of epoch 2, 71.25 ms/it, loss 0.438847
Finished training it 66560/76743 of epoch 2, 71.17 ms/it, loss 0.443719
Finished training it 66560/76743 of epoch 2, 71.12 ms/it, loss 0.441461
Finished training it 66560/76743 of epoch 2, 71.34 ms/it, loss 0.442291
Finished training it 66560/76743 of epoch 2, 71.40 ms/it, loss 0.441390
Finished training it 67584/76743 of epoch 2, 71.16 ms/it, loss 0.440581
Finished training it 67584/76743 of epoch 2, 70.95 ms/it, loss 0.443126
Finished training it 67584/76743 of epoch 2, 71.18 ms/it, loss 0.440025
Finished training it 67584/76743 of epoch 2, 70.91 ms/it, loss 0.444063
Finished training it 68608/76743 of epoch 2, 71.05 ms/it, loss 0.442136
Finished training it 68608/76743 of epoch 2, 71.10 ms/it, loss 0.444669
Finished training it 68608/76743 of epoch 2, 71.33 ms/it, loss 0.442651
Finished training it 68608/76743 of epoch 2, 71.31 ms/it, loss 0.441280
Finished training it 69632/76743 of epoch 2, 71.47 ms/it, loss 0.442250
Finished training it 69632/76743 of epoch 2, 71.61 ms/it, loss 0.441152
Finished training it 69632/76743 of epoch 2, 71.40 ms/it, loss 0.442369
Finished training it 69632/76743 of epoch 2, 71.59 ms/it, loss 0.444755
Finished training it 70656/76743 of epoch 2, 76.91 ms/it, loss 0.443379
Finished training it 70656/76743 of epoch 2, 76.75 ms/it, loss 0.442623
Finished training it 70656/76743 of epoch 2, 76.91 ms/it, loss 0.445046
Finished training it 70656/76743 of epoch 2, 76.70 ms/it, loss 0.443182
Finished training it 71680/76743 of epoch 2, 83.05 ms/it, loss 0.443309
Finished training it 71680/76743 of epoch 2, 83.27 ms/it, loss 0.444204
Finished training it 71680/76743 of epoch 2, 83.21 ms/it, loss 0.441330
Finished training it 71680/76743 of epoch 2, 83.09 ms/it, loss 0.440207
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581260.0
get out
0 has test check 2581260.0 and sample count 3274240
 accuracy 78.835 %, best 78.852 %, roc auc score 0.8033, best 0.8033
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581260.0
get out
3 has test check 2581260.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 70.81 ms/it, loss 0.442087
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581260.0
get out
2 has test check 2581260.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 70.85 ms/it, loss 0.441798
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581260.0
get out
1 has test check 2581260.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 71.09 ms/it, loss 0.440951
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 71.08 ms/it, loss 0.441476
Finished training it 73728/76743 of epoch 2, 71.35 ms/it, loss 0.440866
Finished training it 73728/76743 of epoch 2, 71.30 ms/it, loss 0.444362
Finished training it 73728/76743 of epoch 2, 70.96 ms/it, loss 0.443569
Finished training it 73728/76743 of epoch 2, 71.07 ms/it, loss 0.440996
Finished training it 74752/76743 of epoch 2, 71.06 ms/it, loss 0.442516
Finished training it 74752/76743 of epoch 2, 71.34 ms/it, loss 0.439792
Finished training it 74752/76743 of epoch 2, 71.09 ms/it, loss 0.442512
Finished training it 74752/76743 of epoch 2, 71.31 ms/it, loss 0.443608
Finished training it 75776/76743 of epoch 2, 73.39 ms/it, loss 0.441863
Finished training it 75776/76743 of epoch 2, 78.61 ms/it, loss 0.444495
Finished training it 75776/76743 of epoch 2, 72.90 ms/it, loss 0.441872
Finished training it 75776/76743 of epoch 2, 71.44 ms/it, loss 0.442122
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 84.58 ms/it, loss 0.443621
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 84.65 ms/it, loss 0.440810
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 84.30 ms/it, loss 0.442872
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 84.36 ms/it, loss 0.443943
Finished training it 2048/76743 of epoch 3, 82.50 ms/it, loss 0.441773
Finished training it 2048/76743 of epoch 3, 82.51 ms/it, loss 0.444090
Finished training it 2048/76743 of epoch 3, 82.65 ms/it, loss 0.439259
Finished training it 2048/76743 of epoch 3, 82.64 ms/it, loss 0.440964
Finished training it 3072/76743 of epoch 3, 83.48 ms/it, loss 0.443621
Finished training it 3072/76743 of epoch 3, 83.33 ms/it, loss 0.443613
Finished training it 3072/76743 of epoch 3, 83.49 ms/it, loss 0.444273
Finished training it 3072/76743 of epoch 3, 83.39 ms/it, loss 0.440687
Finished training it 4096/76743 of epoch 3, 83.06 ms/it, loss 0.443933
Finished training it 4096/76743 of epoch 3, 83.21 ms/it, loss 0.444121
Finished training it 4096/76743 of epoch 3, 83.00 ms/it, loss 0.442533
Finished training it 4096/76743 of epoch 3, 83.23 ms/it, loss 0.444864
Finished training it 5120/76743 of epoch 3, 84.04 ms/it, loss 0.442446
Finished training it 5120/76743 of epoch 3, 83.86 ms/it, loss 0.444384
Finished training it 5120/76743 of epoch 3, 83.82 ms/it, loss 0.444053
Finished training it 5120/76743 of epoch 3, 83.99 ms/it, loss 0.440059
Finished training it 6144/76743 of epoch 3, 81.23 ms/it, loss 0.441415
Finished training it 6144/76743 of epoch 3, 81.18 ms/it, loss 0.442700
Finished training it 6144/76743 of epoch 3, 81.07 ms/it, loss 0.440061
Finished training it 6144/76743 of epoch 3, 81.04 ms/it, loss 0.442258
Finished training it 7168/76743 of epoch 3, 71.18 ms/it, loss 0.439806
Finished training it 7168/76743 of epoch 3, 71.42 ms/it, loss 0.442697
Finished training it 7168/76743 of epoch 3, 71.40 ms/it, loss 0.442478
Finished training it 7168/76743 of epoch 3, 71.20 ms/it, loss 0.442254
Finished training it 8192/76743 of epoch 3, 70.85 ms/it, loss 0.439081
Finished training it 8192/76743 of epoch 3, 71.10 ms/it, loss 0.439743
Finished training it 8192/76743 of epoch 3, 70.86 ms/it, loss 0.442291
Finished training it 8192/76743 of epoch 3, 71.11 ms/it, loss 0.442973
Finished training it 9216/76743 of epoch 3, 70.99 ms/it, loss 0.442950
Finished training it 9216/76743 of epoch 3, 71.07 ms/it, loss 0.444686
Finished training it 9216/76743 of epoch 3, 70.82 ms/it, loss 0.442652
Finished training it 9216/76743 of epoch 3, 70.83 ms/it, loss 0.441934
Finished training it 10240/76743 of epoch 3, 70.84 ms/it, loss 0.440448
Finished training it 10240/76743 of epoch 3, 71.09 ms/it, loss 0.440140
Finished training it 10240/76743 of epoch 3, 70.85 ms/it, loss 0.440923
Finished training it 10240/76743 of epoch 3, 71.12 ms/it, loss 0.442738
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583560.0
get out
0 has test check 2583560.0 and sample count 3274240
 accuracy 78.906 %, best 78.906 %, roc auc score 0.8032, best 0.8033
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 82.75 ms/it, loss 0.440963
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583560.0
get out
2 has test check 2583560.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 82.56 ms/it, loss 0.444114
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583560.0
get out
3 has test check 2583560.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 82.58 ms/it, loss 0.443043
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583560.0
get out
1 has test check 2583560.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 82.79 ms/it, loss 0.440217
Finished training it 12288/76743 of epoch 3, 83.82 ms/it, loss 0.438695
Finished training it 12288/76743 of epoch 3, 84.01 ms/it, loss 0.440624
Finished training it 12288/76743 of epoch 3, 84.01 ms/it, loss 0.441770
Finished training it 12288/76743 of epoch 3, 83.81 ms/it, loss 0.443935
Finished training it 13312/76743 of epoch 3, 76.97 ms/it, loss 0.441082
Finished training it 13312/76743 of epoch 3, 72.16 ms/it, loss 0.441830
Finished training it 13312/76743 of epoch 3, 76.74 ms/it, loss 0.440406
Finished training it 13312/76743 of epoch 3, 76.67 ms/it, loss 0.443249
Finished training it 14336/76743 of epoch 3, 71.00 ms/it, loss 0.442189
Finished training it 14336/76743 of epoch 3, 71.25 ms/it, loss 0.440905
Finished training it 14336/76743 of epoch 3, 71.25 ms/it, loss 0.443156
Finished training it 14336/76743 of epoch 3, 71.07 ms/it, loss 0.441122
Finished training it 15360/76743 of epoch 3, 71.15 ms/it, loss 0.443823
Finished training it 15360/76743 of epoch 3, 71.19 ms/it, loss 0.442613
Finished training it 15360/76743 of epoch 3, 70.92 ms/it, loss 0.440933
Finished training it 15360/76743 of epoch 3, 71.01 ms/it, loss 0.443709
Finished training it 16384/76743 of epoch 3, 71.27 ms/it, loss 0.437577
Finished training it 16384/76743 of epoch 3, 71.51 ms/it, loss 0.438571
Finished training it 16384/76743 of epoch 3, 71.47 ms/it, loss 0.442299
Finished training it 16384/76743 of epoch 3, 71.24 ms/it, loss 0.444003
Finished training it 17408/76743 of epoch 3, 71.00 ms/it, loss 0.440076
Finished training it 17408/76743 of epoch 3, 71.23 ms/it, loss 0.442236
Finished training it 17408/76743 of epoch 3, 70.95 ms/it, loss 0.441251
Finished training it 17408/76743 of epoch 3, 71.21 ms/it, loss 0.441853
Finished training it 18432/76743 of epoch 3, 75.37 ms/it, loss 0.441244
Finished training it 18432/76743 of epoch 3, 75.56 ms/it, loss 0.441334
Finished training it 18432/76743 of epoch 3, 75.57 ms/it, loss 0.439684
Finished training it 18432/76743 of epoch 3, 75.31 ms/it, loss 0.441007
Finished training it 19456/76743 of epoch 3, 83.43 ms/it, loss 0.440781
Finished training it 19456/76743 of epoch 3, 83.34 ms/it, loss 0.441886
Finished training it 19456/76743 of epoch 3, 83.40 ms/it, loss 0.439071
Finished training it 19456/76743 of epoch 3, 83.29 ms/it, loss 0.442603
Finished training it 20480/76743 of epoch 3, 83.48 ms/it, loss 0.439217
Finished training it 20480/76743 of epoch 3, 83.47 ms/it, loss 0.440577
Finished training it 20480/76743 of epoch 3, 83.61 ms/it, loss 0.439736
Finished training it 20480/76743 of epoch 3, 83.59 ms/it, loss 0.439667
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582677.0
get out
0 has test check 2582677.0 and sample count 3274240
 accuracy 78.879 %, best 78.906 %, roc auc score 0.8029, best 0.8033
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 71.44 ms/it, loss 0.440989
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582677.0
get out
3 has test check 2582677.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.21 ms/it, loss 0.440897
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582677.0
get out
1 has test check 2582677.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.50 ms/it, loss 0.440259
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582677.0
get out
2 has test check 2582677.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 71.26 ms/it, loss 0.437898
Finished training it 22528/76743 of epoch 3, 70.82 ms/it, loss 0.440916
Finished training it 22528/76743 of epoch 3, 71.14 ms/it, loss 0.438596
Finished training it 22528/76743 of epoch 3, 71.13 ms/it, loss 0.441626
Finished training it 22528/76743 of epoch 3, 70.92 ms/it, loss 0.438399
Finished training it 23552/76743 of epoch 3, 72.63 ms/it, loss 0.439030
Finished training it 23552/76743 of epoch 3, 77.44 ms/it, loss 0.435865
Finished training it 23552/76743 of epoch 3, 72.24 ms/it, loss 0.441951
Finished training it 23552/76743 of epoch 3, 71.44 ms/it, loss 0.437190
Finished training it 24576/76743 of epoch 3, 73.00 ms/it, loss 0.439404
Finished training it 24576/76743 of epoch 3, 72.68 ms/it, loss 0.441381
Finished training it 24576/76743 of epoch 3, 73.03 ms/it, loss 0.443417
Finished training it 24576/76743 of epoch 3, 72.73 ms/it, loss 0.439915
Finished training it 25600/76743 of epoch 3, 83.88 ms/it, loss 0.441570
Finished training it 25600/76743 of epoch 3, 83.91 ms/it, loss 0.440927
Finished training it 25600/76743 of epoch 3, 84.08 ms/it, loss 0.442077
Finished training it 25600/76743 of epoch 3, 84.05 ms/it, loss 0.437325
Finished training it 26624/76743 of epoch 3, 83.71 ms/it, loss 0.442137
Finished training it 26624/76743 of epoch 3, 83.69 ms/it, loss 0.438798
Finished training it 26624/76743 of epoch 3, 83.61 ms/it, loss 0.439340
Finished training it 26624/76743 of epoch 3, 83.60 ms/it, loss 0.439828
Finished training it 27648/76743 of epoch 3, 83.61 ms/it, loss 0.439512
Finished training it 27648/76743 of epoch 3, 83.49 ms/it, loss 0.438947
Finished training it 27648/76743 of epoch 3, 83.45 ms/it, loss 0.440169
Finished training it 27648/76743 of epoch 3, 83.58 ms/it, loss 0.440836
Finished training it 28672/76743 of epoch 3, 83.30 ms/it, loss 0.440952
Finished training it 28672/76743 of epoch 3, 83.16 ms/it, loss 0.439669
Finished training it 28672/76743 of epoch 3, 83.09 ms/it, loss 0.439693
Finished training it 28672/76743 of epoch 3, 83.32 ms/it, loss 0.440692
Finished training it 29696/76743 of epoch 3, 83.79 ms/it, loss 0.440126
Finished training it 29696/76743 of epoch 3, 83.60 ms/it, loss 0.440402
Finished training it 29696/76743 of epoch 3, 83.67 ms/it, loss 0.440503
Finished training it 29696/76743 of epoch 3, 83.84 ms/it, loss 0.439829
Finished training it 30720/76743 of epoch 3, 79.40 ms/it, loss 0.440459
Finished training it 30720/76743 of epoch 3, 79.58 ms/it, loss 0.439645
Finished training it 30720/76743 of epoch 3, 79.61 ms/it, loss 0.437639
Finished training it 30720/76743 of epoch 3, 79.35 ms/it, loss 0.440297
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583876.0
get out
0 has test check 2583876.0 and sample count 3274240
 accuracy 78.915 %, best 78.915 %, roc auc score 0.8034, best 0.8034
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 83.89 ms/it, loss 0.440847
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583876.0
get out
1 has test check 2583876.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 83.89 ms/it, loss 0.437863
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583876.0
get out
3 has test check 2583876.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 83.80 ms/it, loss 0.442171
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583876.0
get out
2 has test check 2583876.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 83.79 ms/it, loss 0.440710
Finished training it 32768/76743 of epoch 3, 81.72 ms/it, loss 0.436292
Finished training it 32768/76743 of epoch 3, 81.64 ms/it, loss 0.439099
Finished training it 32768/76743 of epoch 3, 81.69 ms/it, loss 0.439596
Finished training it 32768/76743 of epoch 3, 81.75 ms/it, loss 0.441277
Finished training it 33792/76743 of epoch 3, 88.35 ms/it, loss 0.443088
Finished training it 33792/76743 of epoch 3, 88.37 ms/it, loss 0.440570
Finished training it 33792/76743 of epoch 3, 83.56 ms/it, loss 0.439889
Finished training it 33792/76743 of epoch 3, 88.52 ms/it, loss 0.438053
Finished training it 34816/76743 of epoch 3, 83.42 ms/it, loss 0.439440
Finished training it 34816/76743 of epoch 3, 83.25 ms/it, loss 0.441351
Finished training it 34816/76743 of epoch 3, 83.21 ms/it, loss 0.439800
Finished training it 34816/76743 of epoch 3, 83.35 ms/it, loss 0.441660
Finished training it 35840/76743 of epoch 3, 83.65 ms/it, loss 0.439534
Finished training it 35840/76743 of epoch 3, 83.76 ms/it, loss 0.439216
Finished training it 35840/76743 of epoch 3, 83.65 ms/it, loss 0.442558
Finished training it 35840/76743 of epoch 3, 83.79 ms/it, loss 0.440255
Finished training it 36864/76743 of epoch 3, 80.10 ms/it, loss 0.440021
Finished training it 36864/76743 of epoch 3, 80.09 ms/it, loss 0.439980
Finished training it 36864/76743 of epoch 3, 79.87 ms/it, loss 0.437544
Finished training it 36864/76743 of epoch 3, 79.91 ms/it, loss 0.440990
Finished training it 37888/76743 of epoch 3, 71.05 ms/it, loss 0.439216
Finished training it 37888/76743 of epoch 3, 71.36 ms/it, loss 0.439563
Finished training it 37888/76743 of epoch 3, 71.36 ms/it, loss 0.437914
Finished training it 37888/76743 of epoch 3, 71.06 ms/it, loss 0.438971
Finished training it 38912/76743 of epoch 3, 71.15 ms/it, loss 0.439932
Finished training it 38912/76743 of epoch 3, 70.91 ms/it, loss 0.437786
Finished training it 38912/76743 of epoch 3, 70.93 ms/it, loss 0.439047
Finished training it 38912/76743 of epoch 3, 71.15 ms/it, loss 0.439387
Finished training it 39936/76743 of epoch 3, 71.03 ms/it, loss 0.439529
Finished training it 39936/76743 of epoch 3, 71.03 ms/it, loss 0.439995
Finished training it 39936/76743 of epoch 3, 70.81 ms/it, loss 0.440812
Finished training it 39936/76743 of epoch 3, 70.71 ms/it, loss 0.442134
Finished training it 40960/76743 of epoch 3, 71.26 ms/it, loss 0.440201
Finished training it 40960/76743 of epoch 3, 71.53 ms/it, loss 0.439489
Finished training it 40960/76743 of epoch 3, 71.52 ms/it, loss 0.439449
Finished training it 40960/76743 of epoch 3, 71.29 ms/it, loss 0.439373
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580806.0
get out
0 has test check 2580806.0 and sample count 3274240
 accuracy 78.822 %, best 78.915 %, roc auc score 0.8037, best 0.8037
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580806.0
get out
2 has test check 2580806.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 83.17 ms/it, loss 0.437615
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580806.0
get out
1 has test check 2580806.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 83.32 ms/it, loss 0.439603
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 83.35 ms/it, loss 0.440052
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580806.0
get out
3 has test check 2580806.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 83.21 ms/it, loss 0.440070
Finished training it 43008/76743 of epoch 3, 83.81 ms/it, loss 0.441750
Finished training it 43008/76743 of epoch 3, 83.97 ms/it, loss 0.440189
Finished training it 43008/76743 of epoch 3, 83.79 ms/it, loss 0.439196
Finished training it 43008/76743 of epoch 3, 83.99 ms/it, loss 0.438662
Finished training it 44032/76743 of epoch 3, 84.39 ms/it, loss 0.439243
Finished training it 44032/76743 of epoch 3, 89.82 ms/it, loss 0.439584
Finished training it 44032/76743 of epoch 3, 84.61 ms/it, loss 0.440704
Finished training it 44032/76743 of epoch 3, 84.86 ms/it, loss 0.438604
Finished training it 45056/76743 of epoch 3, 83.20 ms/it, loss 0.438330
Finished training it 45056/76743 of epoch 3, 83.55 ms/it, loss 0.438349
Finished training it 45056/76743 of epoch 3, 83.32 ms/it, loss 0.437423
Finished training it 45056/76743 of epoch 3, 83.19 ms/it, loss 0.438082
Finished training it 46080/76743 of epoch 3, 84.18 ms/it, loss 0.439606
Finished training it 46080/76743 of epoch 3, 84.29 ms/it, loss 0.440291
Finished training it 46080/76743 of epoch 3, 84.33 ms/it, loss 0.440409
Finished training it 46080/76743 of epoch 3, 84.19 ms/it, loss 0.438452
Finished training it 47104/76743 of epoch 3, 71.56 ms/it, loss 0.439611
Finished training it 47104/76743 of epoch 3, 71.30 ms/it, loss 0.440129
Finished training it 47104/76743 of epoch 3, 71.55 ms/it, loss 0.441121
Finished training it 47104/76743 of epoch 3, 71.32 ms/it, loss 0.439971
Finished training it 48128/76743 of epoch 3, 71.29 ms/it, loss 0.437411
Finished training it 48128/76743 of epoch 3, 71.31 ms/it, loss 0.439590
Finished training it 48128/76743 of epoch 3, 71.07 ms/it, loss 0.440336
Finished training it 48128/76743 of epoch 3, 71.03 ms/it, loss 0.440872
Finished training it 49152/76743 of epoch 3, 71.00 ms/it, loss 0.443009
Finished training it 49152/76743 of epoch 3, 70.77 ms/it, loss 0.436613
Finished training it 49152/76743 of epoch 3, 70.77 ms/it, loss 0.438259
Finished training it 49152/76743 of epoch 3, 71.04 ms/it, loss 0.441735
Finished training it 50176/76743 of epoch 3, 71.25 ms/it, loss 0.439184
Finished training it 50176/76743 of epoch 3, 71.22 ms/it, loss 0.438890
Finished training it 50176/76743 of epoch 3, 70.93 ms/it, loss 0.437807
Finished training it 50176/76743 of epoch 3, 70.98 ms/it, loss 0.439794
Finished training it 51200/76743 of epoch 3, 71.02 ms/it, loss 0.438714
Finished training it 51200/76743 of epoch 3, 71.04 ms/it, loss 0.439978
Finished training it 51200/76743 of epoch 3, 71.35 ms/it, loss 0.439545
Finished training it 51200/76743 of epoch 3, 71.34 ms/it, loss 0.440522
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582533.0
get out
0 has test check 2582533.0 and sample count 3274240
 accuracy 78.874 %, best 78.915 %, roc auc score 0.8035, best 0.8037
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582533.0
get out
3 has test check 2582533.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 83.25 ms/it, loss 0.442004
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582533.0
get out
2 has test check 2582533.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 83.24 ms/it, loss 0.438090
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 83.36 ms/it, loss 0.438258
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582533.0
get out
1 has test check 2582533.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 83.36 ms/it, loss 0.435673
Finished training it 53248/76743 of epoch 3, 77.97 ms/it, loss 0.441297
Finished training it 53248/76743 of epoch 3, 78.21 ms/it, loss 0.437716
Finished training it 53248/76743 of epoch 3, 78.24 ms/it, loss 0.440081
Finished training it 53248/76743 of epoch 3, 78.09 ms/it, loss 0.442329
Finished training it 54272/76743 of epoch 3, 76.48 ms/it, loss 0.437933
Finished training it 54272/76743 of epoch 3, 71.55 ms/it, loss 0.441881
Finished training it 54272/76743 of epoch 3, 76.74 ms/it, loss 0.439970
Finished training it 54272/76743 of epoch 3, 76.51 ms/it, loss 0.439326
Finished training it 55296/76743 of epoch 3, 71.33 ms/it, loss 0.436615
Finished training it 55296/76743 of epoch 3, 71.46 ms/it, loss 0.438060
Finished training it 55296/76743 of epoch 3, 71.15 ms/it, loss 0.440353
Finished training it 55296/76743 of epoch 3, 71.10 ms/it, loss 0.438235
Finished training it 56320/76743 of epoch 3, 70.73 ms/it, loss 0.441715
Finished training it 56320/76743 of epoch 3, 70.83 ms/it, loss 0.440482
Finished training it 56320/76743 of epoch 3, 71.11 ms/it, loss 0.438327
Finished training it 56320/76743 of epoch 3, 71.11 ms/it, loss 0.441270
Finished training it 57344/76743 of epoch 3, 71.41 ms/it, loss 0.441863
Finished training it 57344/76743 of epoch 3, 71.10 ms/it, loss 0.438819
Finished training it 57344/76743 of epoch 3, 71.15 ms/it, loss 0.438314
Finished training it 57344/76743 of epoch 3, 71.41 ms/it, loss 0.439339
Finished training it 58368/76743 of epoch 3, 71.30 ms/it, loss 0.438064
Finished training it 58368/76743 of epoch 3, 71.29 ms/it, loss 0.440304
Finished training it 58368/76743 of epoch 3, 71.08 ms/it, loss 0.438474
Finished training it 58368/76743 of epoch 3, 71.07 ms/it, loss 0.440309
Finished training it 59392/76743 of epoch 3, 81.91 ms/it, loss 0.438009
Finished training it 59392/76743 of epoch 3, 82.10 ms/it, loss 0.436159
Finished training it 59392/76743 of epoch 3, 81.96 ms/it, loss 0.438976
Finished training it 59392/76743 of epoch 3, 82.16 ms/it, loss 0.442367
Finished training it 60416/76743 of epoch 3, 83.84 ms/it, loss 0.437711
Finished training it 60416/76743 of epoch 3, 83.68 ms/it, loss 0.438896
Finished training it 60416/76743 of epoch 3, 83.82 ms/it, loss 0.439365
Finished training it 60416/76743 of epoch 3, 83.66 ms/it, loss 0.437816
Finished training it 61440/76743 of epoch 3, 84.06 ms/it, loss 0.438375
Finished training it 61440/76743 of epoch 3, 83.89 ms/it, loss 0.438868
Finished training it 61440/76743 of epoch 3, 84.15 ms/it, loss 0.437137
Finished training it 61440/76743 of epoch 3, 83.96 ms/it, loss 0.437808
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584239.0
get out
0 has test check 2584239.0 and sample count 3274240
 accuracy 78.926 %, best 78.926 %, roc auc score 0.8037, best 0.8037
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584239.0
get out
1 has test check 2584239.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 71.27 ms/it, loss 0.439617
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584239.0
get out
2 has test check 2584239.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 71.02 ms/it, loss 0.438809
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 71.28 ms/it, loss 0.438669
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584239.0
get out
3 has test check 2584239.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 71.01 ms/it, loss 0.439389
Finished training it 63488/76743 of epoch 3, 71.16 ms/it, loss 0.438558
Finished training it 63488/76743 of epoch 3, 71.41 ms/it, loss 0.441143
Finished training it 63488/76743 of epoch 3, 71.11 ms/it, loss 0.437569
Finished training it 63488/76743 of epoch 3, 71.40 ms/it, loss 0.435946
Finished training it 64512/76743 of epoch 3, 77.09 ms/it, loss 0.435241
Finished training it 64512/76743 of epoch 3, 71.44 ms/it, loss 0.437125
Finished training it 64512/76743 of epoch 3, 71.52 ms/it, loss 0.437114
Finished training it 64512/76743 of epoch 3, 72.00 ms/it, loss 0.437008
Finished training it 65536/76743 of epoch 3, 81.25 ms/it, loss 0.440703
Finished training it 65536/76743 of epoch 3, 81.00 ms/it, loss 0.438887
Finished training it 65536/76743 of epoch 3, 81.45 ms/it, loss 0.437622
Finished training it 65536/76743 of epoch 3, 81.13 ms/it, loss 0.435345
Finished training it 66560/76743 of epoch 3, 82.50 ms/it, loss 0.440376
Finished training it 66560/76743 of epoch 3, 82.50 ms/it, loss 0.437993
Finished training it 66560/76743 of epoch 3, 82.68 ms/it, loss 0.438971
Finished training it 66560/76743 of epoch 3, 82.67 ms/it, loss 0.437640
Finished training it 67584/76743 of epoch 3, 83.36 ms/it, loss 0.436680
Finished training it 67584/76743 of epoch 3, 83.37 ms/it, loss 0.436445
Finished training it 67584/76743 of epoch 3, 83.21 ms/it, loss 0.440580
Finished training it 67584/76743 of epoch 3, 83.26 ms/it, loss 0.439575
Finished training it 68608/76743 of epoch 3, 84.43 ms/it, loss 0.438650
Finished training it 68608/76743 of epoch 3, 84.43 ms/it, loss 0.437190
Finished training it 68608/76743 of epoch 3, 84.25 ms/it, loss 0.438527
Finished training it 68608/76743 of epoch 3, 84.27 ms/it, loss 0.440715
Finished training it 69632/76743 of epoch 3, 84.08 ms/it, loss 0.440917
Finished training it 69632/76743 of epoch 3, 83.91 ms/it, loss 0.438563
Finished training it 69632/76743 of epoch 3, 84.03 ms/it, loss 0.437497
Finished training it 69632/76743 of epoch 3, 83.91 ms/it, loss 0.438755
Finished training it 70656/76743 of epoch 3, 83.24 ms/it, loss 0.441433
Finished training it 70656/76743 of epoch 3, 83.12 ms/it, loss 0.439270
Finished training it 70656/76743 of epoch 3, 83.27 ms/it, loss 0.439813
Finished training it 70656/76743 of epoch 3, 83.10 ms/it, loss 0.439531
Finished training it 71680/76743 of epoch 3, 72.24 ms/it, loss 0.439849
Finished training it 71680/76743 of epoch 3, 72.42 ms/it, loss 0.440675
Finished training it 71680/76743 of epoch 3, 72.44 ms/it, loss 0.437368
Finished training it 71680/76743 of epoch 3, 72.23 ms/it, loss 0.437023
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581094.0
get out
0 has test check 2581094.0 and sample count 3274240
 accuracy 78.830 %, best 78.926 %, roc auc score 0.8039, best 0.8039
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581094.0
get out
1 has test check 2581094.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 83.85 ms/it, loss 0.437530
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 83.77 ms/it, loss 0.438098
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581094.0
get out
3 has test check 2581094.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 83.76 ms/it, loss 0.438335
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581094.0
get out
2 has test check 2581094.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 83.73 ms/it, loss 0.438343
Finished training it 73728/76743 of epoch 3, 83.62 ms/it, loss 0.440853
Finished training it 73728/76743 of epoch 3, 83.68 ms/it, loss 0.437284
Finished training it 73728/76743 of epoch 3, 83.47 ms/it, loss 0.437346
Finished training it 73728/76743 of epoch 3, 83.47 ms/it, loss 0.440075
Finished training it 74752/76743 of epoch 3, 87.92 ms/it, loss 0.436242
Finished training it 74752/76743 of epoch 3, 83.01 ms/it, loss 0.439952
Finished training it 74752/76743 of epoch 3, 87.72 ms/it, loss 0.438938
Finished training it 74752/76743 of epoch 3, 87.75 ms/it, loss 0.439039
Finished training it 75776/76743 of epoch 3, 83.66 ms/it, loss 0.440853
Finished training it 75776/76743 of epoch 3, 83.48 ms/it, loss 0.438427
Finished training it 75776/76743 of epoch 3, 83.50 ms/it, loss 0.438602
Finished training it 75776/76743 of epoch 3, 83.61 ms/it, loss 0.438170
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 78.19 ms/it, loss 0.437399
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 78.05 ms/it, loss 0.439954
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 77.93 ms/it, loss 0.439211
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 78.00 ms/it, loss 0.440263
Finished training it 2048/76743 of epoch 4, 71.28 ms/it, loss 0.435794
Finished training it 2048/76743 of epoch 4, 70.92 ms/it, loss 0.438309
Finished training it 2048/76743 of epoch 4, 71.30 ms/it, loss 0.437201
Finished training it 2048/76743 of epoch 4, 71.03 ms/it, loss 0.440518
Finished training it 3072/76743 of epoch 4, 71.11 ms/it, loss 0.437085
Finished training it 3072/76743 of epoch 4, 71.34 ms/it, loss 0.439887
Finished training it 3072/76743 of epoch 4, 71.04 ms/it, loss 0.440254
Finished training it 3072/76743 of epoch 4, 71.29 ms/it, loss 0.440515
Finished training it 4096/76743 of epoch 4, 71.66 ms/it, loss 0.440222
Finished training it 4096/76743 of epoch 4, 71.63 ms/it, loss 0.441114
Finished training it 4096/76743 of epoch 4, 71.35 ms/it, loss 0.440439
Finished training it 4096/76743 of epoch 4, 71.39 ms/it, loss 0.438824
Finished training it 5120/76743 of epoch 4, 71.19 ms/it, loss 0.436488
Finished training it 5120/76743 of epoch 4, 71.23 ms/it, loss 0.438665
Finished training it 5120/76743 of epoch 4, 70.94 ms/it, loss 0.440460
Finished training it 5120/76743 of epoch 4, 70.94 ms/it, loss 0.440844
Finished training it 6144/76743 of epoch 4, 74.10 ms/it, loss 0.439171
Finished training it 6144/76743 of epoch 4, 74.07 ms/it, loss 0.437483
Finished training it 6144/76743 of epoch 4, 73.85 ms/it, loss 0.436281
Finished training it 6144/76743 of epoch 4, 73.83 ms/it, loss 0.438467
Finished training it 7168/76743 of epoch 4, 84.42 ms/it, loss 0.438983
Finished training it 7168/76743 of epoch 4, 84.30 ms/it, loss 0.438889
Finished training it 7168/76743 of epoch 4, 84.48 ms/it, loss 0.439172
Finished training it 7168/76743 of epoch 4, 84.28 ms/it, loss 0.436344
Finished training it 8192/76743 of epoch 4, 83.10 ms/it, loss 0.438921
Finished training it 8192/76743 of epoch 4, 83.05 ms/it, loss 0.436298
Finished training it 8192/76743 of epoch 4, 82.91 ms/it, loss 0.438419
Finished training it 8192/76743 of epoch 4, 82.98 ms/it, loss 0.435480
Finished training it 9216/76743 of epoch 4, 82.97 ms/it, loss 0.438944
Finished training it 9216/76743 of epoch 4, 83.12 ms/it, loss 0.440865
Finished training it 9216/76743 of epoch 4, 83.10 ms/it, loss 0.438800
Finished training it 9216/76743 of epoch 4, 83.01 ms/it, loss 0.438514
Finished training it 10240/76743 of epoch 4, 84.22 ms/it, loss 0.436363
Finished training it 10240/76743 of epoch 4, 84.25 ms/it, loss 0.439430
Finished training it 10240/76743 of epoch 4, 84.14 ms/it, loss 0.436446
Finished training it 10240/76743 of epoch 4, 84.09 ms/it, loss 0.437345
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584169.0
get out
0 has test check 2584169.0 and sample count 3274240
 accuracy 78.924 %, best 78.926 %, roc auc score 0.8038, best 0.8039
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584169.0
get out
1 has test check 2584169.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 71.24 ms/it, loss 0.436963
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584169.0
get out
2 has test check 2584169.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 71.02 ms/it, loss 0.440698
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 71.24 ms/it, loss 0.436924
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584169.0
get out
3 has test check 2584169.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 70.94 ms/it, loss 0.439677
Finished training it 12288/76743 of epoch 4, 73.31 ms/it, loss 0.437065
Finished training it 12288/76743 of epoch 4, 73.01 ms/it, loss 0.440138
Finished training it 12288/76743 of epoch 4, 73.06 ms/it, loss 0.434858
Finished training it 12288/76743 of epoch 4, 73.27 ms/it, loss 0.438455
Finished training it 13312/76743 of epoch 4, 89.50 ms/it, loss 0.438069
Finished training it 13312/76743 of epoch 4, 84.14 ms/it, loss 0.439384
Finished training it 13312/76743 of epoch 4, 84.34 ms/it, loss 0.436800
Finished training it 13312/76743 of epoch 4, 84.18 ms/it, loss 0.437290
Finished training it 14336/76743 of epoch 4, 82.79 ms/it, loss 0.438533
Finished training it 14336/76743 of epoch 4, 82.82 ms/it, loss 0.437416
Finished training it 14336/76743 of epoch 4, 82.94 ms/it, loss 0.439834
Finished training it 14336/76743 of epoch 4, 82.95 ms/it, loss 0.437315
Finished training it 15360/76743 of epoch 4, 84.03 ms/it, loss 0.440147
Finished training it 15360/76743 of epoch 4, 84.01 ms/it, loss 0.439145
Finished training it 15360/76743 of epoch 4, 83.85 ms/it, loss 0.436904
Finished training it 15360/76743 of epoch 4, 83.90 ms/it, loss 0.439815
Finished training it 16384/76743 of epoch 4, 83.97 ms/it, loss 0.434977
Finished training it 16384/76743 of epoch 4, 83.84 ms/it, loss 0.440494
Finished training it 16384/76743 of epoch 4, 84.00 ms/it, loss 0.438558
Finished training it 16384/76743 of epoch 4, 83.87 ms/it, loss 0.433602
Finished training it 17408/76743 of epoch 4, 83.56 ms/it, loss 0.437986
Finished training it 17408/76743 of epoch 4, 83.46 ms/it, loss 0.437543
Finished training it 17408/76743 of epoch 4, 83.62 ms/it, loss 0.438478
Finished training it 17408/76743 of epoch 4, 83.43 ms/it, loss 0.436417
Finished training it 18432/76743 of epoch 4, 83.34 ms/it, loss 0.435335
Finished training it 18432/76743 of epoch 4, 78.64 ms/it, loss 0.437565
Finished training it 18432/76743 of epoch 4, 83.16 ms/it, loss 0.436895
Finished training it 18432/76743 of epoch 4, 83.16 ms/it, loss 0.436838
Finished training it 19456/76743 of epoch 4, 71.09 ms/it, loss 0.436998
Finished training it 19456/76743 of epoch 4, 70.77 ms/it, loss 0.437837
Finished training it 19456/76743 of epoch 4, 71.06 ms/it, loss 0.434939
Finished training it 19456/76743 of epoch 4, 70.83 ms/it, loss 0.438443
Finished training it 20480/76743 of epoch 4, 71.24 ms/it, loss 0.435273
Finished training it 20480/76743 of epoch 4, 71.25 ms/it, loss 0.435445
Finished training it 20480/76743 of epoch 4, 70.98 ms/it, loss 0.436519
Finished training it 20480/76743 of epoch 4, 71.05 ms/it, loss 0.435053
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582724.0
get out
0 has test check 2582724.0 and sample count 3274240
 accuracy 78.880 %, best 78.926 %, roc auc score 0.8033, best 0.8039
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582724.0
get out
1 has test check 2582724.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 81.78 ms/it, loss 0.436181
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582724.0
get out
2 has test check 2582724.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 81.63 ms/it, loss 0.433878
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582724.0
get out
3 has test check 2582724.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 81.63 ms/it, loss 0.436755
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 81.75 ms/it, loss 0.436810
Finished training it 22528/76743 of epoch 4, 71.02 ms/it, loss 0.437533
Finished training it 22528/76743 of epoch 4, 70.78 ms/it, loss 0.434089
Finished training it 22528/76743 of epoch 4, 71.00 ms/it, loss 0.434682
Finished training it 22528/76743 of epoch 4, 70.77 ms/it, loss 0.436880
Finished training it 23552/76743 of epoch 4, 71.12 ms/it, loss 0.432961
Finished training it 23552/76743 of epoch 4, 71.33 ms/it, loss 0.434869
Finished training it 23552/76743 of epoch 4, 71.32 ms/it, loss 0.431628
Finished training it 23552/76743 of epoch 4, 71.02 ms/it, loss 0.438032
Finished training it 24576/76743 of epoch 4, 71.31 ms/it, loss 0.439301
Finished training it 24576/76743 of epoch 4, 71.25 ms/it, loss 0.435232
Finished training it 24576/76743 of epoch 4, 71.04 ms/it, loss 0.437116
Finished training it 24576/76743 of epoch 4, 71.04 ms/it, loss 0.435801
Finished training it 25600/76743 of epoch 4, 71.28 ms/it, loss 0.437381
Finished training it 25600/76743 of epoch 4, 71.56 ms/it, loss 0.437768
Finished training it 25600/76743 of epoch 4, 71.29 ms/it, loss 0.436968
Finished training it 25600/76743 of epoch 4, 71.59 ms/it, loss 0.433220
Finished training it 26624/76743 of epoch 4, 71.44 ms/it, loss 0.438043
Finished training it 26624/76743 of epoch 4, 71.43 ms/it, loss 0.434682
Finished training it 26624/76743 of epoch 4, 71.09 ms/it, loss 0.436062
Finished training it 26624/76743 of epoch 4, 71.17 ms/it, loss 0.434941
Finished training it 27648/76743 of epoch 4, 69.34 ms/it, loss 0.436267
Finished training it 27648/76743 of epoch 4, 69.25 ms/it, loss 0.435181
Finished training it 27648/76743 of epoch 4, 69.45 ms/it, loss 0.436474
Finished training it 27648/76743 of epoch 4, 69.54 ms/it, loss 0.435464
Finished training it 28672/76743 of epoch 4, 66.71 ms/it, loss 0.437101
Finished training it 28672/76743 of epoch 4, 66.61 ms/it, loss 0.435394
Finished training it 28672/76743 of epoch 4, 66.70 ms/it, loss 0.435842
Finished training it 28672/76743 of epoch 4, 66.67 ms/it, loss 0.436370
Finished training it 29696/76743 of epoch 4, 68.07 ms/it, loss 0.436295
Finished training it 29696/76743 of epoch 4, 72.81 ms/it, loss 0.435623
Finished training it 29696/76743 of epoch 4, 67.94 ms/it, loss 0.436226
Finished training it 29696/76743 of epoch 4, 67.74 ms/it, loss 0.435613
Finished training it 30720/76743 of epoch 4, 66.86 ms/it, loss 0.434976
Finished training it 30720/76743 of epoch 4, 66.79 ms/it, loss 0.436099
Finished training it 30720/76743 of epoch 4, 66.71 ms/it, loss 0.435765
Finished training it 30720/76743 of epoch 4, 66.84 ms/it, loss 0.433257
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583835.0
get out
0 has test check 2583835.0 and sample count 3274240
 accuracy 78.914 %, best 78.926 %, roc auc score 0.8032, best 0.8039
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 72.08 ms/it, loss 0.436319
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583835.0
get out
2 has test check 2583835.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 67.31 ms/it, loss 0.436135
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583835.0
get out
3 has test check 2583835.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 67.43 ms/it, loss 0.438047
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583835.0
get out
1 has test check 2583835.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 67.84 ms/it, loss 0.433922
Finished training it 32768/76743 of epoch 4, 66.95 ms/it, loss 0.431797
Finished training it 32768/76743 of epoch 4, 66.92 ms/it, loss 0.437074
Finished training it 32768/76743 of epoch 4, 66.92 ms/it, loss 0.435303
Finished training it 32768/76743 of epoch 4, 66.86 ms/it, loss 0.434795
Finished training it 33792/76743 of epoch 4, 66.88 ms/it, loss 0.435794
Finished training it 33792/76743 of epoch 4, 66.91 ms/it, loss 0.438757
Finished training it 33792/76743 of epoch 4, 66.97 ms/it, loss 0.433596
Finished training it 33792/76743 of epoch 4, 66.92 ms/it, loss 0.436446
Finished training it 34816/76743 of epoch 4, 67.05 ms/it, loss 0.434544
Finished training it 34816/76743 of epoch 4, 67.10 ms/it, loss 0.435265
Finished training it 34816/76743 of epoch 4, 67.15 ms/it, loss 0.437173
Finished training it 34816/76743 of epoch 4, 67.06 ms/it, loss 0.437129
Finished training it 35840/76743 of epoch 4, 67.06 ms/it, loss 0.435062
Finished training it 35840/76743 of epoch 4, 67.02 ms/it, loss 0.435619
Finished training it 35840/76743 of epoch 4, 66.99 ms/it, loss 0.437960
Finished training it 35840/76743 of epoch 4, 66.89 ms/it, loss 0.434846
Finished training it 36864/76743 of epoch 4, 67.27 ms/it, loss 0.435136
Finished training it 36864/76743 of epoch 4, 67.37 ms/it, loss 0.436287
Finished training it 36864/76743 of epoch 4, 67.26 ms/it, loss 0.435662
Finished training it 36864/76743 of epoch 4, 67.04 ms/it, loss 0.432788
Finished training it 37888/76743 of epoch 4, 67.40 ms/it, loss 0.434533
Finished training it 37888/76743 of epoch 4, 67.48 ms/it, loss 0.434848
Finished training it 37888/76743 of epoch 4, 67.59 ms/it, loss 0.434559
Finished training it 37888/76743 of epoch 4, 67.52 ms/it, loss 0.433223
Finished training it 38912/76743 of epoch 4, 67.52 ms/it, loss 0.435178
Finished training it 38912/76743 of epoch 4, 67.59 ms/it, loss 0.434612
Finished training it 38912/76743 of epoch 4, 67.38 ms/it, loss 0.433368
Finished training it 38912/76743 of epoch 4, 67.52 ms/it, loss 0.435185
Finished training it 39936/76743 of epoch 4, 66.74 ms/it, loss 0.437529
Finished training it 39936/76743 of epoch 4, 66.64 ms/it, loss 0.434731
Finished training it 39936/76743 of epoch 4, 66.62 ms/it, loss 0.436231
Finished training it 39936/76743 of epoch 4, 66.68 ms/it, loss 0.435438
Finished training it 40960/76743 of epoch 4, 68.03 ms/it, loss 0.435180
Finished training it 40960/76743 of epoch 4, 67.92 ms/it, loss 0.435547
Finished training it 40960/76743 of epoch 4, 67.97 ms/it, loss 0.434649
Finished training it 40960/76743 of epoch 4, 68.01 ms/it, loss 0.434628
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578766.0
get out
0 has test check 2578766.0 and sample count 3274240
 accuracy 78.759 %, best 78.926 %, roc auc score 0.8031, best 0.8039
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578766.0
get out
2 has test check 2578766.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 67.01 ms/it, loss 0.432611
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 67.14 ms/it, loss 0.435765
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578766.0
get out
3 has test check 2578766.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 67.07 ms/it, loss 0.435256
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578766.0
get out
1 has test check 2578766.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 67.11 ms/it, loss 0.435000
Finished training it 43008/76743 of epoch 4, 67.20 ms/it, loss 0.436988
Finished training it 43008/76743 of epoch 4, 67.11 ms/it, loss 0.435377
Finished training it 43008/76743 of epoch 4, 67.16 ms/it, loss 0.434177
Finished training it 43008/76743 of epoch 4, 67.14 ms/it, loss 0.434243
Finished training it 44032/76743 of epoch 4, 67.13 ms/it, loss 0.434874
Finished training it 44032/76743 of epoch 4, 67.20 ms/it, loss 0.435796
Finished training it 44032/76743 of epoch 4, 67.19 ms/it, loss 0.434582
Finished training it 44032/76743 of epoch 4, 66.97 ms/it, loss 0.433813
Finished training it 45056/76743 of epoch 4, 67.35 ms/it, loss 0.432607
Finished training it 45056/76743 of epoch 4, 67.26 ms/it, loss 0.433341
Finished training it 45056/76743 of epoch 4, 67.41 ms/it, loss 0.432856
Finished training it 45056/76743 of epoch 4, 67.36 ms/it, loss 0.432119
Finished training it 46080/76743 of epoch 4, 66.88 ms/it, loss 0.435343
Finished training it 46080/76743 of epoch 4, 71.52 ms/it, loss 0.435352
Finished training it 46080/76743 of epoch 4, 71.48 ms/it, loss 0.433651
Finished training it 46080/76743 of epoch 4, 71.39 ms/it, loss 0.434866
Finished training it 47104/76743 of epoch 4, 67.35 ms/it, loss 0.436047
Finished training it 47104/76743 of epoch 4, 66.81 ms/it, loss 0.435473
Finished training it 47104/76743 of epoch 4, 66.96 ms/it, loss 0.434903
Finished training it 47104/76743 of epoch 4, 66.94 ms/it, loss 0.435126
Finished training it 48128/76743 of epoch 4, 67.41 ms/it, loss 0.435760
Finished training it 48128/76743 of epoch 4, 67.43 ms/it, loss 0.434923
Finished training it 48128/76743 of epoch 4, 67.35 ms/it, loss 0.432234
Finished training it 48128/76743 of epoch 4, 67.41 ms/it, loss 0.435241
Finished training it 49152/76743 of epoch 4, 66.78 ms/it, loss 0.436306
Finished training it 49152/76743 of epoch 4, 66.69 ms/it, loss 0.437909
Finished training it 49152/76743 of epoch 4, 66.66 ms/it, loss 0.432933
Finished training it 49152/76743 of epoch 4, 66.57 ms/it, loss 0.431145
Finished training it 50176/76743 of epoch 4, 67.17 ms/it, loss 0.435010
Finished training it 50176/76743 of epoch 4, 67.22 ms/it, loss 0.434018
Finished training it 50176/76743 of epoch 4, 67.15 ms/it, loss 0.432465
Finished training it 50176/76743 of epoch 4, 67.17 ms/it, loss 0.433798
Finished training it 51200/76743 of epoch 4, 66.95 ms/it, loss 0.434288
Finished training it 51200/76743 of epoch 4, 67.00 ms/it, loss 0.434894
Finished training it 51200/76743 of epoch 4, 67.07 ms/it, loss 0.433955
Finished training it 51200/76743 of epoch 4, 67.11 ms/it, loss 0.433596
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581147.0
get out
0 has test check 2581147.0 and sample count 3274240
 accuracy 78.832 %, best 78.926 %, roc auc score 0.8030, best 0.8039
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581147.0
get out
3 has test check 2581147.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 66.72 ms/it, loss 0.436433
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581147.0
get out
2 has test check 2581147.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 66.48 ms/it, loss 0.432494
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581147.0
get out
1 has test check 2581147.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 66.62 ms/it, loss 0.429967
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 66.66 ms/it, loss 0.432151
Finished training it 53248/76743 of epoch 4, 67.28 ms/it, loss 0.431968
Finished training it 53248/76743 of epoch 4, 67.32 ms/it, loss 0.436780
Finished training it 53248/76743 of epoch 4, 67.14 ms/it, loss 0.435669
Finished training it 53248/76743 of epoch 4, 67.28 ms/it, loss 0.434380
Finished training it 54272/76743 of epoch 4, 67.11 ms/it, loss 0.432233
Finished training it 54272/76743 of epoch 4, 72.06 ms/it, loss 0.436904
Finished training it 54272/76743 of epoch 4, 67.47 ms/it, loss 0.433933
Finished training it 54272/76743 of epoch 4, 67.56 ms/it, loss 0.434634
Finished training it 55296/76743 of epoch 4, 67.37 ms/it, loss 0.432918
Finished training it 55296/76743 of epoch 4, 67.30 ms/it, loss 0.432548
Finished training it 55296/76743 of epoch 4, 67.40 ms/it, loss 0.430548
Finished training it 55296/76743 of epoch 4, 67.57 ms/it, loss 0.434084
Finished training it 56320/76743 of epoch 4, 66.85 ms/it, loss 0.435881
Finished training it 56320/76743 of epoch 4, 67.00 ms/it, loss 0.434629
Finished training it 56320/76743 of epoch 4, 67.03 ms/it, loss 0.432734
Finished training it 56320/76743 of epoch 4, 67.01 ms/it, loss 0.434971
Finished training it 57344/76743 of epoch 4, 67.31 ms/it, loss 0.433186
Finished training it 57344/76743 of epoch 4, 67.31 ms/it, loss 0.435924
Finished training it 57344/76743 of epoch 4, 67.26 ms/it, loss 0.432777
Finished training it 57344/76743 of epoch 4, 67.12 ms/it, loss 0.433300
Finished training it 58368/76743 of epoch 4, 67.23 ms/it, loss 0.432285
Finished training it 58368/76743 of epoch 4, 67.25 ms/it, loss 0.434002
Finished training it 58368/76743 of epoch 4, 67.25 ms/it, loss 0.434364
Finished training it 58368/76743 of epoch 4, 67.23 ms/it, loss 0.432306
Finished training it 59392/76743 of epoch 4, 67.18 ms/it, loss 0.435921
Finished training it 59392/76743 of epoch 4, 67.13 ms/it, loss 0.433240
Finished training it 59392/76743 of epoch 4, 67.22 ms/it, loss 0.431768
Finished training it 59392/76743 of epoch 4, 67.19 ms/it, loss 0.430240
Finished training it 60416/76743 of epoch 4, 67.20 ms/it, loss 0.432907
Finished training it 60416/76743 of epoch 4, 67.22 ms/it, loss 0.432925
Finished training it 60416/76743 of epoch 4, 67.20 ms/it, loss 0.431380
Finished training it 60416/76743 of epoch 4, 67.24 ms/it, loss 0.431609
Finished training it 61440/76743 of epoch 4, 67.11 ms/it, loss 0.431042
Finished training it 61440/76743 of epoch 4, 67.08 ms/it, loss 0.431535
Finished training it 61440/76743 of epoch 4, 67.01 ms/it, loss 0.432771
Finished training it 61440/76743 of epoch 4, 67.09 ms/it, loss 0.432164
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582202.0
get out
0 has test check 2582202.0 and sample count 3274240
 accuracy 78.864 %, best 78.926 %, roc auc score 0.8023, best 0.8039
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582202.0
get out
1 has test check 2582202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 67.53 ms/it, loss 0.433246
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 67.51 ms/it, loss 0.432732
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582202.0
get out
2 has test check 2582202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 67.45 ms/it, loss 0.432340
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582202.0
get out
3 has test check 2582202.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 67.37 ms/it, loss 0.432280
Finished training it 63488/76743 of epoch 4, 66.71 ms/it, loss 0.435238
Finished training it 63488/76743 of epoch 4, 66.68 ms/it, loss 0.429701
Finished training it 63488/76743 of epoch 4, 66.70 ms/it, loss 0.432450
Finished training it 63488/76743 of epoch 4, 66.64 ms/it, loss 0.431460
Finished training it 64512/76743 of epoch 4, 71.42 ms/it, loss 0.431092
Finished training it 64512/76743 of epoch 4, 71.44 ms/it, loss 0.430713
Finished training it 64512/76743 of epoch 4, 66.89 ms/it, loss 0.428861
Finished training it 64512/76743 of epoch 4, 71.46 ms/it, loss 0.430814
Finished training it 65536/76743 of epoch 4, 67.26 ms/it, loss 0.433759
Finished training it 65536/76743 of epoch 4, 67.07 ms/it, loss 0.432435
Finished training it 65536/76743 of epoch 4, 66.80 ms/it, loss 0.428474
Finished training it 65536/76743 of epoch 4, 67.02 ms/it, loss 0.430801
Finished training it 66560/76743 of epoch 4, 66.59 ms/it, loss 0.432939
Finished training it 66560/76743 of epoch 4, 66.69 ms/it, loss 0.430938
Finished training it 66560/76743 of epoch 4, 66.77 ms/it, loss 0.431815
Finished training it 66560/76743 of epoch 4, 66.75 ms/it, loss 0.430713
Finished training it 67584/76743 of epoch 4, 66.99 ms/it, loss 0.432359
Finished training it 67584/76743 of epoch 4, 66.88 ms/it, loss 0.433028
Finished training it 67584/76743 of epoch 4, 66.97 ms/it, loss 0.429480
Finished training it 67584/76743 of epoch 4, 66.99 ms/it, loss 0.428855
Finished training it 68608/76743 of epoch 4, 66.56 ms/it, loss 0.431293
Finished training it 68608/76743 of epoch 4, 66.60 ms/it, loss 0.429839
Finished training it 68608/76743 of epoch 4, 66.57 ms/it, loss 0.431469
Finished training it 68608/76743 of epoch 4, 66.53 ms/it, loss 0.433783
Finished training it 69632/76743 of epoch 4, 66.98 ms/it, loss 0.431813
Finished training it 69632/76743 of epoch 4, 67.03 ms/it, loss 0.430360
Finished training it 69632/76743 of epoch 4, 67.05 ms/it, loss 0.431394
Finished training it 69632/76743 of epoch 4, 67.03 ms/it, loss 0.434000
Finished training it 70656/76743 of epoch 4, 67.10 ms/it, loss 0.432584
Finished training it 70656/76743 of epoch 4, 67.09 ms/it, loss 0.432478
Finished training it 70656/76743 of epoch 4, 67.12 ms/it, loss 0.432962
Finished training it 70656/76743 of epoch 4, 67.09 ms/it, loss 0.434396
Finished training it 71680/76743 of epoch 4, 67.31 ms/it, loss 0.429813
Finished training it 71680/76743 of epoch 4, 67.23 ms/it, loss 0.429532
Finished training it 71680/76743 of epoch 4, 67.32 ms/it, loss 0.432711
Finished training it 71680/76743 of epoch 4, 67.39 ms/it, loss 0.431761
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575950.0
get out
0 has test check 2575950.0 and sample count 3274240
 accuracy 78.673 %, best 78.926 %, roc auc score 0.8020, best 0.8039
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575950.0
get out
3 has test check 2575950.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 67.39 ms/it, loss 0.430469
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 67.36 ms/it, loss 0.430572
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575950.0
get out
1 has test check 2575950.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 67.50 ms/it, loss 0.429676
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575950.0
get out
2 has test check 2575950.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 67.36 ms/it, loss 0.430322
Finished training it 73728/76743 of epoch 4, 66.87 ms/it, loss 0.429746
Finished training it 73728/76743 of epoch 4, 66.98 ms/it, loss 0.431971
Finished training it 73728/76743 of epoch 4, 66.86 ms/it, loss 0.429725
Finished training it 73728/76743 of epoch 4, 66.93 ms/it, loss 0.433239
Finished training it 74752/76743 of epoch 4, 67.10 ms/it, loss 0.428748
Finished training it 74752/76743 of epoch 4, 71.56 ms/it, loss 0.432455
Finished training it 74752/76743 of epoch 4, 67.01 ms/it, loss 0.431041
Finished training it 74752/76743 of epoch 4, 66.92 ms/it, loss 0.430285
Finished training it 75776/76743 of epoch 4, 66.91 ms/it, loss 0.432625
Finished training it 75776/76743 of epoch 4, 67.09 ms/it, loss 0.430249
Finished training it 75776/76743 of epoch 4, 66.83 ms/it, loss 0.430072
Finished training it 75776/76743 of epoch 4, 66.78 ms/it, loss 0.430557
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577284.0
get out
0 has test check 2577284.0 and sample count 3274240
 accuracy 78.714 %, best 78.926 %, roc auc score 0.8019, best 0.8039
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577284.0
get out
1 has test check 2577284.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577284.0
get out
3 has test check 2577284.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577284.0
get out
2 has test check 2577284.0 and sample count 3274240
