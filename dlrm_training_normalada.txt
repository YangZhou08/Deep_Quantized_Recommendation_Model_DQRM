Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.13 ms/it, loss 0.518288
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 36.66 ms/it, loss 0.518654
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 34.41 ms/it, loss 0.515513
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 36.19 ms/it, loss 0.518758
Finished training it 2048/76743 of epoch 0, 36.43 ms/it, loss 0.502180
Finished training it 2048/76743 of epoch 0, 36.85 ms/it, loss 0.498713
Finished training it 2048/76743 of epoch 0, 37.06 ms/it, loss 0.502956
Finished training it 2048/76743 of epoch 0, 36.43 ms/it, loss 0.499644
Finished training it 3072/76743 of epoch 0, 40.22 ms/it, loss 0.498813
Finished training it 3072/76743 of epoch 0, 40.16 ms/it, loss 0.499460
Finished training it 3072/76743 of epoch 0, 39.87 ms/it, loss 0.497647
Finished training it 3072/76743 of epoch 0, 39.61 ms/it, loss 0.494745
Finished training it 4096/76743 of epoch 0, 40.09 ms/it, loss 0.495995
Finished training it 4096/76743 of epoch 0, 39.62 ms/it, loss 0.494306
Finished training it 4096/76743 of epoch 0, 40.10 ms/it, loss 0.494632
Finished training it 4096/76743 of epoch 0, 39.80 ms/it, loss 0.495944
Finished training it 5120/76743 of epoch 0, 39.24 ms/it, loss 0.491956
Finished training it 5120/76743 of epoch 0, 39.12 ms/it, loss 0.492140
Finished training it 5120/76743 of epoch 0, 39.58 ms/it, loss 0.495914
Finished training it 5120/76743 of epoch 0, 39.61 ms/it, loss 0.493955
Finished training it 6144/76743 of epoch 0, 39.54 ms/it, loss 0.492790
Finished training it 6144/76743 of epoch 0, 39.84 ms/it, loss 0.491210
Finished training it 6144/76743 of epoch 0, 39.42 ms/it, loss 0.490559
Finished training it 6144/76743 of epoch 0, 39.75 ms/it, loss 0.492127
Finished training it 7168/76743 of epoch 0, 38.44 ms/it, loss 0.490161
Finished training it 7168/76743 of epoch 0, 38.14 ms/it, loss 0.492567
Finished training it 7168/76743 of epoch 0, 37.93 ms/it, loss 0.492033
Finished training it 7168/76743 of epoch 0, 38.45 ms/it, loss 0.489065
Finished training it 8192/76743 of epoch 0, 37.92 ms/it, loss 0.488487
Finished training it 8192/76743 of epoch 0, 38.07 ms/it, loss 0.488412
Finished training it 8192/76743 of epoch 0, 37.98 ms/it, loss 0.489795
Finished training it 8192/76743 of epoch 0, 37.83 ms/it, loss 0.490541
Finished training it 9216/76743 of epoch 0, 39.72 ms/it, loss 0.488588
Finished training it 9216/76743 of epoch 0, 39.89 ms/it, loss 0.487834
Finished training it 9216/76743 of epoch 0, 39.89 ms/it, loss 0.486226
Finished training it 9216/76743 of epoch 0, 39.65 ms/it, loss 0.487138
Finished training it 10240/76743 of epoch 0, 39.64 ms/it, loss 0.486577
Finished training it 10240/76743 of epoch 0, 39.32 ms/it, loss 0.485466
Finished training it 10240/76743 of epoch 0, 39.67 ms/it, loss 0.486707
Finished training it 10240/76743 of epoch 0, 39.47 ms/it, loss 0.486251
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2514481.0
get out
0 has test check 2514481.0 and sample count 3274240
 accuracy 76.796 %, best 76.796 %, roc auc score 0.7524, best 0.7524
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 31.55 ms/it, loss 0.485376
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2514481.0
get out
2 has test check 2514481.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 31.49 ms/it, loss 0.485146
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2514481.0
get out
3 has test check 2514481.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 31.93 ms/it, loss 0.485276
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2514481.0
get out
1 has test check 2514481.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 31.86 ms/it, loss 0.484555
Finished training it 12288/76743 of epoch 0, 32.06 ms/it, loss 0.485031
Finished training it 12288/76743 of epoch 0, 31.85 ms/it, loss 0.482799
Finished training it 12288/76743 of epoch 0, 32.12 ms/it, loss 0.486246
Finished training it 12288/76743 of epoch 0, 31.63 ms/it, loss 0.484233
Finished training it 13312/76743 of epoch 0, 40.24 ms/it, loss 0.483264
Finished training it 13312/76743 of epoch 0, 40.24 ms/it, loss 0.485546
Finished training it 13312/76743 of epoch 0, 40.36 ms/it, loss 0.484786
Finished training it 13312/76743 of epoch 0, 40.03 ms/it, loss 0.485868
Finished training it 14336/76743 of epoch 0, 39.76 ms/it, loss 0.484430
Finished training it 14336/76743 of epoch 0, 39.73 ms/it, loss 0.484045
Finished training it 14336/76743 of epoch 0, 39.50 ms/it, loss 0.483919
Finished training it 14336/76743 of epoch 0, 39.58 ms/it, loss 0.483023
Finished training it 15360/76743 of epoch 0, 40.01 ms/it, loss 0.481844
Finished training it 15360/76743 of epoch 0, 39.91 ms/it, loss 0.480797
Finished training it 15360/76743 of epoch 0, 40.06 ms/it, loss 0.482758
Finished training it 15360/76743 of epoch 0, 39.82 ms/it, loss 0.482794
Finished training it 16384/76743 of epoch 0, 47.05 ms/it, loss 0.483071
Finished training it 16384/76743 of epoch 0, 47.42 ms/it, loss 0.480991
Finished training it 16384/76743 of epoch 0, 45.54 ms/it, loss 0.482213
Finished training it 16384/76743 of epoch 0, 46.56 ms/it, loss 0.481587
Finished training it 17408/76743 of epoch 0, 40.02 ms/it, loss 0.480252
Finished training it 17408/76743 of epoch 0, 39.75 ms/it, loss 0.484336
Finished training it 17408/76743 of epoch 0, 39.83 ms/it, loss 0.479764
Finished training it 17408/76743 of epoch 0, 39.90 ms/it, loss 0.482304
Finished training it 18432/76743 of epoch 0, 39.56 ms/it, loss 0.478584
Finished training it 18432/76743 of epoch 0, 39.75 ms/it, loss 0.481579
Finished training it 18432/76743 of epoch 0, 39.40 ms/it, loss 0.479964
Finished training it 18432/76743 of epoch 0, 39.56 ms/it, loss 0.479714
Finished training it 19456/76743 of epoch 0, 40.15 ms/it, loss 0.479138
Finished training it 19456/76743 of epoch 0, 39.93 ms/it, loss 0.477439
Finished training it 19456/76743 of epoch 0, 39.82 ms/it, loss 0.482379
Finished training it 19456/76743 of epoch 0, 40.04 ms/it, loss 0.479435
Finished training it 20480/76743 of epoch 0, 39.97 ms/it, loss 0.479463
Finished training it 20480/76743 of epoch 0, 39.68 ms/it, loss 0.481959
Finished training it 20480/76743 of epoch 0, 39.95 ms/it, loss 0.479542
Finished training it 20480/76743 of epoch 0, 39.81 ms/it, loss 0.481077
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2524976.0
get out
0 has test check 2524976.0 and sample count 3274240
 accuracy 77.116 %, best 77.116 %, roc auc score 0.7609, best 0.7609
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2524976.0
get out
3 has test check 2524976.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 31.76 ms/it, loss 0.480936
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 31.64 ms/it, loss 0.479294
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2524976.0
get out
1 has test check 2524976.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 31.82 ms/it, loss 0.480152
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2524976.0
get out
2 has test check 2524976.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 31.57 ms/it, loss 0.481226
Finished training it 22528/76743 of epoch 0, 35.54 ms/it, loss 0.478641
Finished training it 22528/76743 of epoch 0, 35.69 ms/it, loss 0.477871
Finished training it 22528/76743 of epoch 0, 35.38 ms/it, loss 0.478559
Finished training it 22528/76743 of epoch 0, 35.67 ms/it, loss 0.478019
Finished training it 23552/76743 of epoch 0, 39.69 ms/it, loss 0.479359
Finished training it 23552/76743 of epoch 0, 39.39 ms/it, loss 0.476593
Finished training it 23552/76743 of epoch 0, 39.30 ms/it, loss 0.478426
Finished training it 23552/76743 of epoch 0, 39.54 ms/it, loss 0.478735
Finished training it 24576/76743 of epoch 0, 39.96 ms/it, loss 0.477779
Finished training it 24576/76743 of epoch 0, 39.60 ms/it, loss 0.477255
Finished training it 24576/76743 of epoch 0, 39.86 ms/it, loss 0.477686
Finished training it 24576/76743 of epoch 0, 39.82 ms/it, loss 0.478493
Finished training it 25600/76743 of epoch 0, 40.36 ms/it, loss 0.479540
Finished training it 25600/76743 of epoch 0, 40.34 ms/it, loss 0.477827
Finished training it 25600/76743 of epoch 0, 40.13 ms/it, loss 0.478390
Finished training it 25600/76743 of epoch 0, 40.07 ms/it, loss 0.476110
Finished training it 26624/76743 of epoch 0, 40.06 ms/it, loss 0.479501
Finished training it 26624/76743 of epoch 0, 40.03 ms/it, loss 0.477390
Finished training it 26624/76743 of epoch 0, 39.87 ms/it, loss 0.476185
Finished training it 26624/76743 of epoch 0, 39.76 ms/it, loss 0.481474
Finished training it 27648/76743 of epoch 0, 39.97 ms/it, loss 0.478347
Finished training it 27648/76743 of epoch 0, 39.87 ms/it, loss 0.478958
Finished training it 27648/76743 of epoch 0, 39.76 ms/it, loss 0.477352
Finished training it 27648/76743 of epoch 0, 40.01 ms/it, loss 0.473814
Finished training it 28672/76743 of epoch 0, 39.57 ms/it, loss 0.477585
Finished training it 28672/76743 of epoch 0, 39.78 ms/it, loss 0.474297
Finished training it 28672/76743 of epoch 0, 39.65 ms/it, loss 0.476877
Finished training it 28672/76743 of epoch 0, 39.81 ms/it, loss 0.478428
Finished training it 29696/76743 of epoch 0, 39.96 ms/it, loss 0.476408
Finished training it 29696/76743 of epoch 0, 39.80 ms/it, loss 0.477934
Finished training it 29696/76743 of epoch 0, 40.02 ms/it, loss 0.473650
Finished training it 29696/76743 of epoch 0, 40.07 ms/it, loss 0.477369
Finished training it 30720/76743 of epoch 0, 39.69 ms/it, loss 0.474326
Finished training it 30720/76743 of epoch 0, 39.72 ms/it, loss 0.477450
Finished training it 30720/76743 of epoch 0, 39.60 ms/it, loss 0.474292
Finished training it 30720/76743 of epoch 0, 39.55 ms/it, loss 0.472848
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2530925.0
get out
0 has test check 2530925.0 and sample count 3274240
 accuracy 77.298 %, best 77.298 %, roc auc score 0.7655, best 0.7655
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2530925.0
get out
2 has test check 2530925.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 31.66 ms/it, loss 0.475928
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 31.74 ms/it, loss 0.476816
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2530925.0
get out
1 has test check 2530925.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 31.82 ms/it, loss 0.477303
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2530925.0
get out
3 has test check 2530925.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 31.86 ms/it, loss 0.476351
Finished training it 32768/76743 of epoch 0, 35.19 ms/it, loss 0.474080
Finished training it 32768/76743 of epoch 0, 34.86 ms/it, loss 0.476934
Finished training it 32768/76743 of epoch 0, 35.18 ms/it, loss 0.474280
Finished training it 32768/76743 of epoch 0, 34.93 ms/it, loss 0.476233
Finished training it 33792/76743 of epoch 0, 39.81 ms/it, loss 0.474005
Finished training it 33792/76743 of epoch 0, 40.09 ms/it, loss 0.474018
Finished training it 33792/76743 of epoch 0, 39.99 ms/it, loss 0.473349
Finished training it 33792/76743 of epoch 0, 39.89 ms/it, loss 0.476102
Finished training it 34816/76743 of epoch 0, 39.75 ms/it, loss 0.475918
Finished training it 34816/76743 of epoch 0, 39.81 ms/it, loss 0.475112
Finished training it 34816/76743 of epoch 0, 39.69 ms/it, loss 0.476068
Finished training it 34816/76743 of epoch 0, 39.98 ms/it, loss 0.475679
Finished training it 35840/76743 of epoch 0, 39.68 ms/it, loss 0.474945
Finished training it 35840/76743 of epoch 0, 39.59 ms/it, loss 0.473158
Finished training it 35840/76743 of epoch 0, 39.75 ms/it, loss 0.475569
Finished training it 35840/76743 of epoch 0, 39.53 ms/it, loss 0.475500
Finished training it 36864/76743 of epoch 0, 40.37 ms/it, loss 0.477978
Finished training it 36864/76743 of epoch 0, 40.29 ms/it, loss 0.472415
Finished training it 36864/76743 of epoch 0, 40.20 ms/it, loss 0.477315
Finished training it 36864/76743 of epoch 0, 40.09 ms/it, loss 0.472968
Finished training it 37888/76743 of epoch 0, 40.08 ms/it, loss 0.472685
Finished training it 37888/76743 of epoch 0, 40.18 ms/it, loss 0.476099
Finished training it 37888/76743 of epoch 0, 40.02 ms/it, loss 0.472563
Finished training it 37888/76743 of epoch 0, 40.33 ms/it, loss 0.474314
Finished training it 38912/76743 of epoch 0, 37.07 ms/it, loss 0.471582
Finished training it 38912/76743 of epoch 0, 37.10 ms/it, loss 0.473241
Finished training it 38912/76743 of epoch 0, 37.33 ms/it, loss 0.474066
Finished training it 38912/76743 of epoch 0, 37.35 ms/it, loss 0.472244
Finished training it 39936/76743 of epoch 0, 39.91 ms/it, loss 0.475908
Finished training it 39936/76743 of epoch 0, 40.05 ms/it, loss 0.474909
Finished training it 39936/76743 of epoch 0, 39.77 ms/it, loss 0.472800
Finished training it 39936/76743 of epoch 0, 39.79 ms/it, loss 0.475113
Finished training it 40960/76743 of epoch 0, 40.34 ms/it, loss 0.471621
Finished training it 40960/76743 of epoch 0, 40.23 ms/it, loss 0.472907
Finished training it 40960/76743 of epoch 0, 40.02 ms/it, loss 0.475355
Finished training it 40960/76743 of epoch 0, 40.25 ms/it, loss 0.474355
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2535482.0
get out
0 has test check 2535482.0 and sample count 3274240
 accuracy 77.437 %, best 77.437 %, roc auc score 0.7684, best 0.7684
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2535482.0
get out
1 has test check 2535482.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 31.48 ms/it, loss 0.472404
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2535482.0
get out
3 has test check 2535482.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 31.45 ms/it, loss 0.470529
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 31.34 ms/it, loss 0.471999
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2535482.0
get out
2 has test check 2535482.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 31.20 ms/it, loss 0.472046
Finished training it 43008/76743 of epoch 0, 33.46 ms/it, loss 0.470615
Finished training it 43008/76743 of epoch 0, 33.35 ms/it, loss 0.470531
Finished training it 43008/76743 of epoch 0, 33.72 ms/it, loss 0.474059
Finished training it 43008/76743 of epoch 0, 33.58 ms/it, loss 0.474134
Finished training it 44032/76743 of epoch 0, 39.61 ms/it, loss 0.472463
Finished training it 44032/76743 of epoch 0, 39.86 ms/it, loss 0.470930
Finished training it 44032/76743 of epoch 0, 39.60 ms/it, loss 0.473058
Finished training it 44032/76743 of epoch 0, 39.81 ms/it, loss 0.469500
Finished training it 45056/76743 of epoch 0, 39.78 ms/it, loss 0.471316
Finished training it 45056/76743 of epoch 0, 40.00 ms/it, loss 0.471986
Finished training it 45056/76743 of epoch 0, 40.00 ms/it, loss 0.472013
Finished training it 45056/76743 of epoch 0, 39.86 ms/it, loss 0.474747
Finished training it 46080/76743 of epoch 0, 39.80 ms/it, loss 0.469300
Finished training it 46080/76743 of epoch 0, 39.72 ms/it, loss 0.470486
Finished training it 46080/76743 of epoch 0, 39.91 ms/it, loss 0.472163
Finished training it 46080/76743 of epoch 0, 39.96 ms/it, loss 0.475572
Finished training it 47104/76743 of epoch 0, 45.22 ms/it, loss 0.471107
Finished training it 47104/76743 of epoch 0, 46.38 ms/it, loss 0.474073
Finished training it 47104/76743 of epoch 0, 46.19 ms/it, loss 0.472339
Finished training it 47104/76743 of epoch 0, 45.72 ms/it, loss 0.469015
Finished training it 48128/76743 of epoch 0, 39.55 ms/it, loss 0.471203
Finished training it 48128/76743 of epoch 0, 39.92 ms/it, loss 0.472189
Finished training it 48128/76743 of epoch 0, 39.66 ms/it, loss 0.473593
Finished training it 48128/76743 of epoch 0, 39.67 ms/it, loss 0.471032
Finished training it 49152/76743 of epoch 0, 39.59 ms/it, loss 0.471002
Finished training it 49152/76743 of epoch 0, 39.66 ms/it, loss 0.472821
Finished training it 49152/76743 of epoch 0, 39.87 ms/it, loss 0.470047
Finished training it 49152/76743 of epoch 0, 39.70 ms/it, loss 0.471654
Finished training it 50176/76743 of epoch 0, 39.54 ms/it, loss 0.471120
Finished training it 50176/76743 of epoch 0, 39.39 ms/it, loss 0.470366
Finished training it 50176/76743 of epoch 0, 39.57 ms/it, loss 0.473102
Finished training it 50176/76743 of epoch 0, 39.68 ms/it, loss 0.468346
Finished training it 51200/76743 of epoch 0, 40.45 ms/it, loss 0.472593
Finished training it 51200/76743 of epoch 0, 40.58 ms/it, loss 0.469911
Finished training it 51200/76743 of epoch 0, 40.36 ms/it, loss 0.470170
Finished training it 51200/76743 of epoch 0, 40.64 ms/it, loss 0.470472
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2533238.0
get out
0 has test check 2533238.0 and sample count 3274240
 accuracy 77.369 %, best 77.437 %, roc auc score 0.7713, best 0.7713
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2533238.0
get out
1 has test check 2533238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 31.75 ms/it, loss 0.472923
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2533238.0
get out
3 has test check 2533238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 31.70 ms/it, loss 0.468950
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 31.63 ms/it, loss 0.472346
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2533238.0
get out
2 has test check 2533238.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 31.59 ms/it, loss 0.468662
Finished training it 53248/76743 of epoch 0, 36.07 ms/it, loss 0.471041
Finished training it 53248/76743 of epoch 0, 36.23 ms/it, loss 0.472553
Finished training it 53248/76743 of epoch 0, 36.01 ms/it, loss 0.469125
Finished training it 53248/76743 of epoch 0, 36.13 ms/it, loss 0.472012
Finished training it 54272/76743 of epoch 0, 39.73 ms/it, loss 0.468413
Finished training it 54272/76743 of epoch 0, 39.67 ms/it, loss 0.472132
Finished training it 54272/76743 of epoch 0, 39.53 ms/it, loss 0.471777
Finished training it 54272/76743 of epoch 0, 39.82 ms/it, loss 0.472991
Finished training it 55296/76743 of epoch 0, 39.89 ms/it, loss 0.468212
Finished training it 55296/76743 of epoch 0, 39.74 ms/it, loss 0.471530
Finished training it 55296/76743 of epoch 0, 39.59 ms/it, loss 0.470314
Finished training it 55296/76743 of epoch 0, 39.73 ms/it, loss 0.471062
Finished training it 56320/76743 of epoch 0, 39.86 ms/it, loss 0.471179
Finished training it 56320/76743 of epoch 0, 39.74 ms/it, loss 0.470062
Finished training it 56320/76743 of epoch 0, 39.84 ms/it, loss 0.468342
Finished training it 56320/76743 of epoch 0, 39.68 ms/it, loss 0.471116
Finished training it 57344/76743 of epoch 0, 39.55 ms/it, loss 0.468629
Finished training it 57344/76743 of epoch 0, 39.74 ms/it, loss 0.468623
Finished training it 57344/76743 of epoch 0, 39.62 ms/it, loss 0.467801
Finished training it 57344/76743 of epoch 0, 39.49 ms/it, loss 0.468755
Finished training it 58368/76743 of epoch 0, 39.38 ms/it, loss 0.469600
Finished training it 58368/76743 of epoch 0, 39.56 ms/it, loss 0.472508
Finished training it 58368/76743 of epoch 0, 39.26 ms/it, loss 0.468692
Finished training it 58368/76743 of epoch 0, 39.48 ms/it, loss 0.471337
Finished training it 59392/76743 of epoch 0, 39.07 ms/it, loss 0.468408
Finished training it 59392/76743 of epoch 0, 39.17 ms/it, loss 0.468400
Finished training it 59392/76743 of epoch 0, 39.27 ms/it, loss 0.471036
Finished training it 59392/76743 of epoch 0, 39.36 ms/it, loss 0.470017
Finished training it 60416/76743 of epoch 0, 39.78 ms/it, loss 0.469523
Finished training it 60416/76743 of epoch 0, 39.93 ms/it, loss 0.472520
Finished training it 60416/76743 of epoch 0, 39.94 ms/it, loss 0.469832
Finished training it 60416/76743 of epoch 0, 39.83 ms/it, loss 0.469644
Finished training it 61440/76743 of epoch 0, 39.74 ms/it, loss 0.468014
Finished training it 61440/76743 of epoch 0, 39.58 ms/it, loss 0.469623
Finished training it 61440/76743 of epoch 0, 39.64 ms/it, loss 0.468936
Finished training it 61440/76743 of epoch 0, 39.46 ms/it, loss 0.469795
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2538965.0
get out
0 has test check 2538965.0 and sample count 3274240
 accuracy 77.544 %, best 77.544 %, roc auc score 0.7731, best 0.7731
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 31.70 ms/it, loss 0.469544
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2538965.0
get out
3 has test check 2538965.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 31.84 ms/it, loss 0.467143
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2538965.0
get out
2 has test check 2538965.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 31.59 ms/it, loss 0.469925
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2538965.0
get out
1 has test check 2538965.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 31.81 ms/it, loss 0.468498
Finished training it 63488/76743 of epoch 0, 35.29 ms/it, loss 0.468139
Finished training it 63488/76743 of epoch 0, 35.65 ms/it, loss 0.468001
Finished training it 63488/76743 of epoch 0, 35.33 ms/it, loss 0.473242
Finished training it 63488/76743 of epoch 0, 35.53 ms/it, loss 0.469542
Finished training it 64512/76743 of epoch 0, 39.86 ms/it, loss 0.466816
Finished training it 64512/76743 of epoch 0, 39.97 ms/it, loss 0.469988
Finished training it 64512/76743 of epoch 0, 39.77 ms/it, loss 0.466485
Finished training it 64512/76743 of epoch 0, 39.69 ms/it, loss 0.468782
Finished training it 65536/76743 of epoch 0, 39.91 ms/it, loss 0.467599
Finished training it 65536/76743 of epoch 0, 40.01 ms/it, loss 0.468054
Finished training it 65536/76743 of epoch 0, 39.81 ms/it, loss 0.466880
Finished training it 65536/76743 of epoch 0, 40.06 ms/it, loss 0.469423
Finished training it 66560/76743 of epoch 0, 40.16 ms/it, loss 0.468115
Finished training it 66560/76743 of epoch 0, 39.94 ms/it, loss 0.469742
Finished training it 66560/76743 of epoch 0, 40.00 ms/it, loss 0.469257
Finished training it 66560/76743 of epoch 0, 40.11 ms/it, loss 0.470469
Finished training it 67584/76743 of epoch 0, 39.18 ms/it, loss 0.467684
Finished training it 67584/76743 of epoch 0, 39.29 ms/it, loss 0.469140
Finished training it 67584/76743 of epoch 0, 39.40 ms/it, loss 0.465638
Finished training it 67584/76743 of epoch 0, 39.39 ms/it, loss 0.467486
Finished training it 68608/76743 of epoch 0, 37.85 ms/it, loss 0.469352
Finished training it 68608/76743 of epoch 0, 38.10 ms/it, loss 0.467150
Finished training it 68608/76743 of epoch 0, 38.09 ms/it, loss 0.468454
Finished training it 68608/76743 of epoch 0, 37.96 ms/it, loss 0.470337
Finished training it 69632/76743 of epoch 0, 37.24 ms/it, loss 0.469990
Finished training it 69632/76743 of epoch 0, 37.09 ms/it, loss 0.467092
Finished training it 69632/76743 of epoch 0, 36.96 ms/it, loss 0.468489
Finished training it 69632/76743 of epoch 0, 37.22 ms/it, loss 0.467644
Finished training it 70656/76743 of epoch 0, 40.00 ms/it, loss 0.470284
Finished training it 70656/76743 of epoch 0, 39.95 ms/it, loss 0.471033
Finished training it 70656/76743 of epoch 0, 39.95 ms/it, loss 0.466759
Finished training it 70656/76743 of epoch 0, 39.78 ms/it, loss 0.471058
Finished training it 71680/76743 of epoch 0, 39.75 ms/it, loss 0.465178
Finished training it 71680/76743 of epoch 0, 39.89 ms/it, loss 0.465867
Finished training it 71680/76743 of epoch 0, 39.94 ms/it, loss 0.468240
Finished training it 71680/76743 of epoch 0, 39.92 ms/it, loss 0.465979
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2543487.0
get out
0 has test check 2543487.0 and sample count 3274240
 accuracy 77.682 %, best 77.682 %, roc auc score 0.7749, best 0.7749
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 39.86 ms/it, loss 0.468691
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2543487.0
get out
1 has test check 2543487.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 39.94 ms/it, loss 0.469386
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2543487.0
get out
2 has test check 2543487.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 39.71 ms/it, loss 0.469263
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2543487.0
get out
3 has test check 2543487.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 39.89 ms/it, loss 0.468585
Finished training it 73728/76743 of epoch 0, 38.97 ms/it, loss 0.468136
Finished training it 73728/76743 of epoch 0, 39.11 ms/it, loss 0.469057
Finished training it 73728/76743 of epoch 0, 39.27 ms/it, loss 0.467365
Finished training it 73728/76743 of epoch 0, 39.08 ms/it, loss 0.465950
Finished training it 74752/76743 of epoch 0, 31.60 ms/it, loss 0.467301
Finished training it 74752/76743 of epoch 0, 31.51 ms/it, loss 0.466171
Finished training it 74752/76743 of epoch 0, 31.86 ms/it, loss 0.464563
Finished training it 74752/76743 of epoch 0, 31.71 ms/it, loss 0.465447
Finished training it 75776/76743 of epoch 0, 31.78 ms/it, loss 0.465687
Finished training it 75776/76743 of epoch 0, 31.95 ms/it, loss 0.470048
Finished training it 75776/76743 of epoch 0, 31.73 ms/it, loss 0.467751
Finished training it 75776/76743 of epoch 0, 31.60 ms/it, loss 0.467418
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 34.94 ms/it, loss 0.469411
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 35.04 ms/it, loss 0.466537
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 35.01 ms/it, loss 0.466382
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 34.78 ms/it, loss 0.466437
Finished training it 2048/76743 of epoch 1, 33.12 ms/it, loss 0.463986
Finished training it 2048/76743 of epoch 1, 33.14 ms/it, loss 0.467277
Finished training it 2048/76743 of epoch 1, 32.96 ms/it, loss 0.464301
Finished training it 2048/76743 of epoch 1, 32.99 ms/it, loss 0.467581
Finished training it 3072/76743 of epoch 1, 39.06 ms/it, loss 0.463567
Finished training it 3072/76743 of epoch 1, 39.25 ms/it, loss 0.468022
Finished training it 3072/76743 of epoch 1, 39.21 ms/it, loss 0.468536
Finished training it 3072/76743 of epoch 1, 39.13 ms/it, loss 0.467661
Finished training it 4096/76743 of epoch 1, 39.59 ms/it, loss 0.467225
Finished training it 4096/76743 of epoch 1, 39.49 ms/it, loss 0.464486
Finished training it 4096/76743 of epoch 1, 39.68 ms/it, loss 0.466074
Finished training it 4096/76743 of epoch 1, 39.76 ms/it, loss 0.467511
Finished training it 5120/76743 of epoch 1, 39.57 ms/it, loss 0.465637
Finished training it 5120/76743 of epoch 1, 39.47 ms/it, loss 0.465053
Finished training it 5120/76743 of epoch 1, 39.62 ms/it, loss 0.467450
Finished training it 5120/76743 of epoch 1, 39.66 ms/it, loss 0.469576
Finished training it 6144/76743 of epoch 1, 40.34 ms/it, loss 0.465540
Finished training it 6144/76743 of epoch 1, 40.43 ms/it, loss 0.467108
Finished training it 6144/76743 of epoch 1, 40.22 ms/it, loss 0.467788
Finished training it 6144/76743 of epoch 1, 40.18 ms/it, loss 0.465558
Finished training it 7168/76743 of epoch 1, 39.78 ms/it, loss 0.464744
Finished training it 7168/76743 of epoch 1, 39.63 ms/it, loss 0.468136
Finished training it 7168/76743 of epoch 1, 39.81 ms/it, loss 0.466291
Finished training it 7168/76743 of epoch 1, 39.57 ms/it, loss 0.467878
Finished training it 8192/76743 of epoch 1, 39.78 ms/it, loss 0.465783
Finished training it 8192/76743 of epoch 1, 39.68 ms/it, loss 0.466867
Finished training it 8192/76743 of epoch 1, 39.63 ms/it, loss 0.465483
Finished training it 8192/76743 of epoch 1, 39.51 ms/it, loss 0.467594
Finished training it 9216/76743 of epoch 1, 39.48 ms/it, loss 0.466702
Finished training it 9216/76743 of epoch 1, 39.55 ms/it, loss 0.466685
Finished training it 9216/76743 of epoch 1, 39.65 ms/it, loss 0.464863
Finished training it 9216/76743 of epoch 1, 39.40 ms/it, loss 0.465205
Finished training it 10240/76743 of epoch 1, 40.07 ms/it, loss 0.465272
Finished training it 10240/76743 of epoch 1, 40.31 ms/it, loss 0.465578
Finished training it 10240/76743 of epoch 1, 40.20 ms/it, loss 0.465707
Finished training it 10240/76743 of epoch 1, 40.04 ms/it, loss 0.463944
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546903.0
get out
0 has test check 2546903.0 and sample count 3274240
 accuracy 77.786 %, best 77.786 %, roc auc score 0.7767, best 0.7767
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546903.0
get out
1 has test check 2546903.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 32.04 ms/it, loss 0.463960
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546903.0
get out
3 has test check 2546903.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 31.98 ms/it, loss 0.464805
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 31.87 ms/it, loss 0.466147
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546903.0
get out
2 has test check 2546903.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 31.84 ms/it, loss 0.464653
Finished training it 12288/76743 of epoch 1, 34.37 ms/it, loss 0.467426
Finished training it 12288/76743 of epoch 1, 34.17 ms/it, loss 0.464911
Finished training it 12288/76743 of epoch 1, 33.98 ms/it, loss 0.465677
Finished training it 12288/76743 of epoch 1, 34.12 ms/it, loss 0.464293
Finished training it 13312/76743 of epoch 1, 39.92 ms/it, loss 0.465003
Finished training it 13312/76743 of epoch 1, 40.14 ms/it, loss 0.466762
Finished training it 13312/76743 of epoch 1, 40.02 ms/it, loss 0.467208
Finished training it 13312/76743 of epoch 1, 39.83 ms/it, loss 0.466954
Finished training it 14336/76743 of epoch 1, 39.69 ms/it, loss 0.466245
Finished training it 14336/76743 of epoch 1, 39.40 ms/it, loss 0.465491
Finished training it 14336/76743 of epoch 1, 39.29 ms/it, loss 0.466333
Finished training it 14336/76743 of epoch 1, 39.50 ms/it, loss 0.465943
Finished training it 15360/76743 of epoch 1, 39.61 ms/it, loss 0.464789
Finished training it 15360/76743 of epoch 1, 39.79 ms/it, loss 0.463397
Finished training it 15360/76743 of epoch 1, 39.84 ms/it, loss 0.464073
Finished training it 15360/76743 of epoch 1, 39.95 ms/it, loss 0.465569
Finished training it 16384/76743 of epoch 1, 39.86 ms/it, loss 0.465715
Finished training it 16384/76743 of epoch 1, 39.79 ms/it, loss 0.463773
Finished training it 16384/76743 of epoch 1, 39.69 ms/it, loss 0.465118
Finished training it 16384/76743 of epoch 1, 39.65 ms/it, loss 0.465068
Finished training it 17408/76743 of epoch 1, 39.60 ms/it, loss 0.463473
Finished training it 17408/76743 of epoch 1, 39.64 ms/it, loss 0.466094
Finished training it 17408/76743 of epoch 1, 39.74 ms/it, loss 0.463579
Finished training it 17408/76743 of epoch 1, 39.49 ms/it, loss 0.467453
Finished training it 18432/76743 of epoch 1, 39.93 ms/it, loss 0.462966
Finished training it 18432/76743 of epoch 1, 40.00 ms/it, loss 0.463882
Finished training it 18432/76743 of epoch 1, 39.82 ms/it, loss 0.464131
Finished training it 18432/76743 of epoch 1, 40.08 ms/it, loss 0.465657
Finished training it 19456/76743 of epoch 1, 39.62 ms/it, loss 0.466502
Finished training it 19456/76743 of epoch 1, 39.70 ms/it, loss 0.461982
Finished training it 19456/76743 of epoch 1, 39.81 ms/it, loss 0.464573
Finished training it 19456/76743 of epoch 1, 39.81 ms/it, loss 0.464059
Finished training it 20480/76743 of epoch 1, 39.62 ms/it, loss 0.464405
Finished training it 20480/76743 of epoch 1, 39.41 ms/it, loss 0.466531
Finished training it 20480/76743 of epoch 1, 39.47 ms/it, loss 0.466534
Finished training it 20480/76743 of epoch 1, 39.60 ms/it, loss 0.464768
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547922.0
get out
0 has test check 2547922.0 and sample count 3274240
 accuracy 77.817 %, best 77.817 %, roc auc score 0.7780, best 0.7780
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547922.0
get out
2 has test check 2547922.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 31.59 ms/it, loss 0.466369
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547922.0
get out
3 has test check 2547922.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 31.79 ms/it, loss 0.465486
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547922.0
get out
1 has test check 2547922.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 31.85 ms/it, loss 0.464887
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 31.66 ms/it, loss 0.463891
Finished training it 22528/76743 of epoch 1, 33.11 ms/it, loss 0.463685
Finished training it 22528/76743 of epoch 1, 33.07 ms/it, loss 0.463377
Finished training it 22528/76743 of epoch 1, 32.84 ms/it, loss 0.463946
Finished training it 22528/76743 of epoch 1, 32.86 ms/it, loss 0.464762
Finished training it 23552/76743 of epoch 1, 39.70 ms/it, loss 0.464249
Finished training it 23552/76743 of epoch 1, 39.89 ms/it, loss 0.465024
Finished training it 23552/76743 of epoch 1, 39.63 ms/it, loss 0.462270
Finished training it 23552/76743 of epoch 1, 39.52 ms/it, loss 0.464289
Finished training it 24576/76743 of epoch 1, 39.74 ms/it, loss 0.463486
Finished training it 24576/76743 of epoch 1, 39.93 ms/it, loss 0.463527
Finished training it 24576/76743 of epoch 1, 39.77 ms/it, loss 0.464798
Finished training it 24576/76743 of epoch 1, 40.05 ms/it, loss 0.464176
Finished training it 25600/76743 of epoch 1, 39.22 ms/it, loss 0.463765
Finished training it 25600/76743 of epoch 1, 39.28 ms/it, loss 0.464075
Finished training it 25600/76743 of epoch 1, 39.30 ms/it, loss 0.465982
Finished training it 25600/76743 of epoch 1, 39.12 ms/it, loss 0.462727
Finished training it 26624/76743 of epoch 1, 39.83 ms/it, loss 0.463107
Finished training it 26624/76743 of epoch 1, 39.88 ms/it, loss 0.466521
Finished training it 26624/76743 of epoch 1, 39.82 ms/it, loss 0.464610
Finished training it 26624/76743 of epoch 1, 39.71 ms/it, loss 0.467524
Finished training it 27648/76743 of epoch 1, 37.53 ms/it, loss 0.464896
Finished training it 27648/76743 of epoch 1, 37.11 ms/it, loss 0.464123
Finished training it 27648/76743 of epoch 1, 37.29 ms/it, loss 0.466213
Finished training it 27648/76743 of epoch 1, 37.35 ms/it, loss 0.460895
Finished training it 28672/76743 of epoch 1, 39.90 ms/it, loss 0.461592
Finished training it 28672/76743 of epoch 1, 39.76 ms/it, loss 0.465277
Finished training it 28672/76743 of epoch 1, 39.96 ms/it, loss 0.464936
Finished training it 28672/76743 of epoch 1, 39.84 ms/it, loss 0.463724
Finished training it 29696/76743 of epoch 1, 39.92 ms/it, loss 0.461152
Finished training it 29696/76743 of epoch 1, 40.11 ms/it, loss 0.464948
Finished training it 29696/76743 of epoch 1, 39.92 ms/it, loss 0.464034
Finished training it 29696/76743 of epoch 1, 39.84 ms/it, loss 0.464925
Finished training it 30720/76743 of epoch 1, 39.76 ms/it, loss 0.462295
Finished training it 30720/76743 of epoch 1, 39.68 ms/it, loss 0.462137
Finished training it 30720/76743 of epoch 1, 39.88 ms/it, loss 0.465040
Finished training it 30720/76743 of epoch 1, 39.62 ms/it, loss 0.460377
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2549008.0
get out
0 has test check 2549008.0 and sample count 3274240
 accuracy 77.850 %, best 77.850 %, roc auc score 0.7789, best 0.7789
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 31.61 ms/it, loss 0.465451
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2549008.0
get out
2 has test check 2549008.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 31.53 ms/it, loss 0.464119
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2549008.0
get out
1 has test check 2549008.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 31.73 ms/it, loss 0.464633
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2549008.0
get out
3 has test check 2549008.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 31.71 ms/it, loss 0.464292
Finished training it 32768/76743 of epoch 1, 31.72 ms/it, loss 0.464400
Finished training it 32768/76743 of epoch 1, 31.81 ms/it, loss 0.461898
Finished training it 32768/76743 of epoch 1, 31.80 ms/it, loss 0.462181
Finished training it 32768/76743 of epoch 1, 31.63 ms/it, loss 0.464369
Finished training it 33792/76743 of epoch 1, 37.24 ms/it, loss 0.462711
Finished training it 33792/76743 of epoch 1, 37.28 ms/it, loss 0.464105
Finished training it 33792/76743 of epoch 1, 37.45 ms/it, loss 0.461257
Finished training it 33792/76743 of epoch 1, 37.56 ms/it, loss 0.462551
Finished training it 34816/76743 of epoch 1, 39.53 ms/it, loss 0.463455
Finished training it 34816/76743 of epoch 1, 39.58 ms/it, loss 0.464910
Finished training it 34816/76743 of epoch 1, 39.76 ms/it, loss 0.463623
Finished training it 34816/76743 of epoch 1, 39.71 ms/it, loss 0.464049
Finished training it 35840/76743 of epoch 1, 47.18 ms/it, loss 0.463931
Finished training it 35840/76743 of epoch 1, 48.19 ms/it, loss 0.463605
Finished training it 35840/76743 of epoch 1, 47.95 ms/it, loss 0.464260
Finished training it 35840/76743 of epoch 1, 46.31 ms/it, loss 0.461909
Finished training it 36864/76743 of epoch 1, 40.33 ms/it, loss 0.466490
Finished training it 36864/76743 of epoch 1, 40.30 ms/it, loss 0.461222
Finished training it 36864/76743 of epoch 1, 40.13 ms/it, loss 0.461645
Finished training it 36864/76743 of epoch 1, 40.21 ms/it, loss 0.465637
Finished training it 37888/76743 of epoch 1, 40.16 ms/it, loss 0.461415
Finished training it 37888/76743 of epoch 1, 40.25 ms/it, loss 0.463873
Finished training it 37888/76743 of epoch 1, 40.03 ms/it, loss 0.461277
Finished training it 37888/76743 of epoch 1, 40.24 ms/it, loss 0.465015
Finished training it 38912/76743 of epoch 1, 39.98 ms/it, loss 0.462228
Finished training it 38912/76743 of epoch 1, 40.04 ms/it, loss 0.461345
Finished training it 38912/76743 of epoch 1, 39.87 ms/it, loss 0.460726
Finished training it 38912/76743 of epoch 1, 40.17 ms/it, loss 0.462933
Finished training it 39936/76743 of epoch 1, 39.93 ms/it, loss 0.462012
Finished training it 39936/76743 of epoch 1, 40.12 ms/it, loss 0.464079
Finished training it 39936/76743 of epoch 1, 39.81 ms/it, loss 0.464466
Finished training it 39936/76743 of epoch 1, 40.03 ms/it, loss 0.464698
Finished training it 40960/76743 of epoch 1, 39.69 ms/it, loss 0.462301
Finished training it 40960/76743 of epoch 1, 39.80 ms/it, loss 0.463629
Finished training it 40960/76743 of epoch 1, 39.58 ms/it, loss 0.464911
Finished training it 40960/76743 of epoch 1, 39.88 ms/it, loss 0.460859
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550926.0
get out
0 has test check 2550926.0 and sample count 3274240
 accuracy 77.909 %, best 77.909 %, roc auc score 0.7799, best 0.7799
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550926.0
get out
2 has test check 2550926.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 31.69 ms/it, loss 0.462192
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550926.0
get out
1 has test check 2550926.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 31.74 ms/it, loss 0.462002
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550926.0
get out
3 has test check 2550926.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 31.82 ms/it, loss 0.459858
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 31.72 ms/it, loss 0.461467
Finished training it 43008/76743 of epoch 1, 32.85 ms/it, loss 0.464022
Finished training it 43008/76743 of epoch 1, 32.83 ms/it, loss 0.463442
Finished training it 43008/76743 of epoch 1, 32.54 ms/it, loss 0.460456
Finished training it 43008/76743 of epoch 1, 32.64 ms/it, loss 0.459749
Finished training it 44032/76743 of epoch 1, 39.07 ms/it, loss 0.463382
Finished training it 44032/76743 of epoch 1, 39.31 ms/it, loss 0.459626
Finished training it 44032/76743 of epoch 1, 39.37 ms/it, loss 0.461053
Finished training it 44032/76743 of epoch 1, 39.15 ms/it, loss 0.462285
Finished training it 45056/76743 of epoch 1, 39.60 ms/it, loss 0.464505
Finished training it 45056/76743 of epoch 1, 39.71 ms/it, loss 0.461734
Finished training it 45056/76743 of epoch 1, 39.80 ms/it, loss 0.461733
Finished training it 45056/76743 of epoch 1, 39.53 ms/it, loss 0.461266
Finished training it 46080/76743 of epoch 1, 39.42 ms/it, loss 0.459504
Finished training it 46080/76743 of epoch 1, 39.60 ms/it, loss 0.465372
Finished training it 46080/76743 of epoch 1, 39.37 ms/it, loss 0.460568
Finished training it 46080/76743 of epoch 1, 39.57 ms/it, loss 0.462427
Finished training it 47104/76743 of epoch 1, 39.32 ms/it, loss 0.461257
Finished training it 47104/76743 of epoch 1, 39.33 ms/it, loss 0.459748
Finished training it 47104/76743 of epoch 1, 39.44 ms/it, loss 0.462573
Finished training it 47104/76743 of epoch 1, 39.49 ms/it, loss 0.464533
Finished training it 48128/76743 of epoch 1, 39.59 ms/it, loss 0.461553
Finished training it 48128/76743 of epoch 1, 39.50 ms/it, loss 0.461643
Finished training it 48128/76743 of epoch 1, 39.64 ms/it, loss 0.464501
Finished training it 48128/76743 of epoch 1, 39.74 ms/it, loss 0.463105
Finished training it 49152/76743 of epoch 1, 39.54 ms/it, loss 0.463658
Finished training it 49152/76743 of epoch 1, 39.61 ms/it, loss 0.462381
Finished training it 49152/76743 of epoch 1, 39.71 ms/it, loss 0.460556
Finished training it 49152/76743 of epoch 1, 39.44 ms/it, loss 0.461506
Finished training it 50176/76743 of epoch 1, 39.96 ms/it, loss 0.459426
Finished training it 50176/76743 of epoch 1, 39.89 ms/it, loss 0.463722
Finished training it 50176/76743 of epoch 1, 39.74 ms/it, loss 0.460567
Finished training it 50176/76743 of epoch 1, 39.83 ms/it, loss 0.462210
Finished training it 51200/76743 of epoch 1, 40.25 ms/it, loss 0.462826
Finished training it 51200/76743 of epoch 1, 40.47 ms/it, loss 0.461769
Finished training it 51200/76743 of epoch 1, 40.40 ms/it, loss 0.460574
Finished training it 51200/76743 of epoch 1, 40.18 ms/it, loss 0.461453
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2546595.0
get out
0 has test check 2546595.0 and sample count 3274240
 accuracy 77.777 %, best 77.909 %, roc auc score 0.7804, best 0.7804
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2546595.0
get out
1 has test check 2546595.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 31.94 ms/it, loss 0.464075
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2546595.0
get out
2 has test check 2546595.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 31.71 ms/it, loss 0.459638
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2546595.0
get out
3 has test check 2546595.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 31.88 ms/it, loss 0.460083
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 31.76 ms/it, loss 0.462967
Finished training it 53248/76743 of epoch 1, 31.59 ms/it, loss 0.460209
Finished training it 53248/76743 of epoch 1, 31.85 ms/it, loss 0.463504
Finished training it 53248/76743 of epoch 1, 31.67 ms/it, loss 0.462400
Finished training it 53248/76743 of epoch 1, 31.81 ms/it, loss 0.462678
Finished training it 54272/76743 of epoch 1, 39.66 ms/it, loss 0.463377
Finished training it 54272/76743 of epoch 1, 39.85 ms/it, loss 0.459555
Finished training it 54272/76743 of epoch 1, 39.67 ms/it, loss 0.462792
Finished training it 54272/76743 of epoch 1, 39.97 ms/it, loss 0.464224
Finished training it 55296/76743 of epoch 1, 39.64 ms/it, loss 0.461667
Finished training it 55296/76743 of epoch 1, 39.73 ms/it, loss 0.462884
Finished training it 55296/76743 of epoch 1, 39.94 ms/it, loss 0.460014
Finished training it 55296/76743 of epoch 1, 39.89 ms/it, loss 0.462522
Finished training it 56320/76743 of epoch 1, 39.58 ms/it, loss 0.462458
Finished training it 56320/76743 of epoch 1, 39.80 ms/it, loss 0.462706
Finished training it 56320/76743 of epoch 1, 39.75 ms/it, loss 0.459646
Finished training it 56320/76743 of epoch 1, 39.68 ms/it, loss 0.461391
Finished training it 57344/76743 of epoch 1, 39.34 ms/it, loss 0.460029
Finished training it 57344/76743 of epoch 1, 39.26 ms/it, loss 0.459663
Finished training it 57344/76743 of epoch 1, 39.19 ms/it, loss 0.460261
Finished training it 57344/76743 of epoch 1, 39.11 ms/it, loss 0.460658
Finished training it 58368/76743 of epoch 1, 36.93 ms/it, loss 0.460438
Finished training it 58368/76743 of epoch 1, 37.05 ms/it, loss 0.461386
Finished training it 58368/76743 of epoch 1, 37.26 ms/it, loss 0.463575
Finished training it 58368/76743 of epoch 1, 37.21 ms/it, loss 0.463103
Finished training it 59392/76743 of epoch 1, 39.81 ms/it, loss 0.459932
Finished training it 59392/76743 of epoch 1, 40.05 ms/it, loss 0.462047
Finished training it 59392/76743 of epoch 1, 39.75 ms/it, loss 0.460232
Finished training it 59392/76743 of epoch 1, 39.94 ms/it, loss 0.462986
Finished training it 60416/76743 of epoch 1, 40.34 ms/it, loss 0.461589
Finished training it 60416/76743 of epoch 1, 40.04 ms/it, loss 0.461082
Finished training it 60416/76743 of epoch 1, 40.25 ms/it, loss 0.463895
Finished training it 60416/76743 of epoch 1, 40.16 ms/it, loss 0.461551
Finished training it 61440/76743 of epoch 1, 39.81 ms/it, loss 0.461381
Finished training it 61440/76743 of epoch 1, 39.92 ms/it, loss 0.459656
Finished training it 61440/76743 of epoch 1, 39.93 ms/it, loss 0.460860
Finished training it 61440/76743 of epoch 1, 39.73 ms/it, loss 0.461406
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2550230.0
get out
0 has test check 2550230.0 and sample count 3274240
 accuracy 77.888 %, best 77.909 %, roc auc score 0.7812, best 0.7812
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2550230.0
get out
3 has test check 2550230.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 31.54 ms/it, loss 0.458610
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2550230.0
get out
2 has test check 2550230.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 31.39 ms/it, loss 0.462185
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2550230.0
get out
1 has test check 2550230.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 31.58 ms/it, loss 0.460317
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 31.46 ms/it, loss 0.461246
Finished training it 63488/76743 of epoch 1, 32.02 ms/it, loss 0.461005
Finished training it 63488/76743 of epoch 1, 31.78 ms/it, loss 0.459750
Finished training it 63488/76743 of epoch 1, 31.74 ms/it, loss 0.465581
Finished training it 63488/76743 of epoch 1, 31.95 ms/it, loss 0.460002
Finished training it 64512/76743 of epoch 1, 37.03 ms/it, loss 0.459061
Finished training it 64512/76743 of epoch 1, 37.13 ms/it, loss 0.462471
Finished training it 64512/76743 of epoch 1, 36.89 ms/it, loss 0.460720
Finished training it 64512/76743 of epoch 1, 36.91 ms/it, loss 0.458292
Finished training it 65536/76743 of epoch 1, 40.20 ms/it, loss 0.459205
Finished training it 65536/76743 of epoch 1, 40.37 ms/it, loss 0.460112
Finished training it 65536/76743 of epoch 1, 40.42 ms/it, loss 0.461582
Finished training it 65536/76743 of epoch 1, 40.45 ms/it, loss 0.460310
Finished training it 66560/76743 of epoch 1, 44.96 ms/it, loss 0.462912
Finished training it 66560/76743 of epoch 1, 46.95 ms/it, loss 0.461060
Finished training it 66560/76743 of epoch 1, 46.87 ms/it, loss 0.460182
Finished training it 66560/76743 of epoch 1, 46.73 ms/it, loss 0.462340
Finished training it 67584/76743 of epoch 1, 39.75 ms/it, loss 0.461169
Finished training it 67584/76743 of epoch 1, 39.67 ms/it, loss 0.460334
Finished training it 67584/76743 of epoch 1, 39.96 ms/it, loss 0.457679
Finished training it 67584/76743 of epoch 1, 39.87 ms/it, loss 0.459968
Finished training it 68608/76743 of epoch 1, 39.00 ms/it, loss 0.460481
Finished training it 68608/76743 of epoch 1, 39.14 ms/it, loss 0.459590
Finished training it 68608/76743 of epoch 1, 38.90 ms/it, loss 0.462687
Finished training it 68608/76743 of epoch 1, 38.80 ms/it, loss 0.461998
Finished training it 69632/76743 of epoch 1, 38.96 ms/it, loss 0.462450
Finished training it 69632/76743 of epoch 1, 38.75 ms/it, loss 0.460856
Finished training it 69632/76743 of epoch 1, 39.06 ms/it, loss 0.460322
Finished training it 69632/76743 of epoch 1, 38.89 ms/it, loss 0.460200
Finished training it 70656/76743 of epoch 1, 39.94 ms/it, loss 0.463756
Finished training it 70656/76743 of epoch 1, 40.20 ms/it, loss 0.462575
Finished training it 70656/76743 of epoch 1, 40.13 ms/it, loss 0.459410
Finished training it 70656/76743 of epoch 1, 40.02 ms/it, loss 0.463872
Finished training it 71680/76743 of epoch 1, 40.43 ms/it, loss 0.461068
Finished training it 71680/76743 of epoch 1, 40.19 ms/it, loss 0.458550
Finished training it 71680/76743 of epoch 1, 40.15 ms/it, loss 0.458039
Finished training it 71680/76743 of epoch 1, 40.29 ms/it, loss 0.458276
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2552543.0
get out
0 has test check 2552543.0 and sample count 3274240
 accuracy 77.958 %, best 77.958 %, roc auc score 0.7817, best 0.7817
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2552543.0
get out
2 has test check 2552543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 40.45 ms/it, loss 0.461666
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 40.58 ms/it, loss 0.460913
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2552543.0
get out
1 has test check 2552543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 40.67 ms/it, loss 0.462036
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2552543.0
get out
3 has test check 2552543.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 40.66 ms/it, loss 0.461124
Finished training it 73728/76743 of epoch 1, 40.12 ms/it, loss 0.461781
Finished training it 73728/76743 of epoch 1, 40.01 ms/it, loss 0.458527
Finished training it 73728/76743 of epoch 1, 39.96 ms/it, loss 0.460775
Finished training it 73728/76743 of epoch 1, 40.18 ms/it, loss 0.460088
Finished training it 74752/76743 of epoch 1, 32.65 ms/it, loss 0.458248
Finished training it 74752/76743 of epoch 1, 32.68 ms/it, loss 0.457217
Finished training it 74752/76743 of epoch 1, 32.47 ms/it, loss 0.459292
Finished training it 74752/76743 of epoch 1, 32.56 ms/it, loss 0.460033
Finished training it 75776/76743 of epoch 1, 31.75 ms/it, loss 0.459196
Finished training it 75776/76743 of epoch 1, 31.45 ms/it, loss 0.459948
Finished training it 75776/76743 of epoch 1, 31.64 ms/it, loss 0.463373
Finished training it 75776/76743 of epoch 1, 31.50 ms/it, loss 0.460897
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 32.60 ms/it, loss 0.459798
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 32.40 ms/it, loss 0.459234
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 32.92 ms/it, loss 0.462366
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 32.56 ms/it, loss 0.459710
Finished training it 2048/76743 of epoch 2, 31.61 ms/it, loss 0.460676
Finished training it 2048/76743 of epoch 2, 31.83 ms/it, loss 0.457187
Finished training it 2048/76743 of epoch 2, 31.82 ms/it, loss 0.460446
Finished training it 2048/76743 of epoch 2, 31.53 ms/it, loss 0.457587
Finished training it 3072/76743 of epoch 2, 39.70 ms/it, loss 0.456906
Finished training it 3072/76743 of epoch 2, 39.82 ms/it, loss 0.460919
Finished training it 3072/76743 of epoch 2, 39.94 ms/it, loss 0.461569
Finished training it 3072/76743 of epoch 2, 39.97 ms/it, loss 0.461553
Finished training it 4096/76743 of epoch 2, 39.79 ms/it, loss 0.460476
Finished training it 4096/76743 of epoch 2, 39.71 ms/it, loss 0.457564
Finished training it 4096/76743 of epoch 2, 39.96 ms/it, loss 0.460862
Finished training it 4096/76743 of epoch 2, 39.88 ms/it, loss 0.459445
Finished training it 5120/76743 of epoch 2, 39.75 ms/it, loss 0.458300
Finished training it 5120/76743 of epoch 2, 39.81 ms/it, loss 0.459253
Finished training it 5120/76743 of epoch 2, 39.92 ms/it, loss 0.460607
Finished training it 5120/76743 of epoch 2, 39.95 ms/it, loss 0.463127
Finished training it 6144/76743 of epoch 2, 40.08 ms/it, loss 0.461144
Finished training it 6144/76743 of epoch 2, 39.94 ms/it, loss 0.459108
Finished training it 6144/76743 of epoch 2, 40.23 ms/it, loss 0.459315
Finished training it 6144/76743 of epoch 2, 40.29 ms/it, loss 0.460422
Finished training it 7168/76743 of epoch 2, 39.73 ms/it, loss 0.461723
Finished training it 7168/76743 of epoch 2, 39.63 ms/it, loss 0.460811
Finished training it 7168/76743 of epoch 2, 39.82 ms/it, loss 0.458177
Finished training it 7168/76743 of epoch 2, 39.96 ms/it, loss 0.460120
Finished training it 8192/76743 of epoch 2, 36.71 ms/it, loss 0.461604
Finished training it 8192/76743 of epoch 2, 37.11 ms/it, loss 0.459570
Finished training it 8192/76743 of epoch 2, 36.83 ms/it, loss 0.459127
Finished training it 8192/76743 of epoch 2, 36.99 ms/it, loss 0.460276
Finished training it 9216/76743 of epoch 2, 39.45 ms/it, loss 0.458379
Finished training it 9216/76743 of epoch 2, 39.51 ms/it, loss 0.460103
Finished training it 9216/76743 of epoch 2, 39.60 ms/it, loss 0.460519
Finished training it 9216/76743 of epoch 2, 39.70 ms/it, loss 0.458657
Finished training it 10240/76743 of epoch 2, 40.34 ms/it, loss 0.459355
Finished training it 10240/76743 of epoch 2, 40.53 ms/it, loss 0.459401
Finished training it 10240/76743 of epoch 2, 40.12 ms/it, loss 0.457392
Finished training it 10240/76743 of epoch 2, 40.25 ms/it, loss 0.458976
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554826.0
get out
0 has test check 2554826.0 and sample count 3274240
 accuracy 78.028 %, best 78.028 %, roc auc score 0.7825, best 0.7825
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554826.0
get out
2 has test check 2554826.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 31.37 ms/it, loss 0.458040
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554826.0
get out
3 has test check 2554826.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 31.62 ms/it, loss 0.458580
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 31.43 ms/it, loss 0.460025
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554826.0
get out
1 has test check 2554826.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 31.62 ms/it, loss 0.457772
Finished training it 12288/76743 of epoch 2, 32.11 ms/it, loss 0.458693
Finished training it 12288/76743 of epoch 2, 32.18 ms/it, loss 0.461417
Finished training it 12288/76743 of epoch 2, 32.00 ms/it, loss 0.457870
Finished training it 12288/76743 of epoch 2, 31.91 ms/it, loss 0.459507
Finished training it 13312/76743 of epoch 2, 36.55 ms/it, loss 0.460733
Finished training it 13312/76743 of epoch 2, 36.72 ms/it, loss 0.458856
Finished training it 13312/76743 of epoch 2, 36.97 ms/it, loss 0.460662
Finished training it 13312/76743 of epoch 2, 36.80 ms/it, loss 0.461448
Finished training it 14336/76743 of epoch 2, 39.43 ms/it, loss 0.460262
Finished training it 14336/76743 of epoch 2, 39.52 ms/it, loss 0.459600
Finished training it 14336/76743 of epoch 2, 39.67 ms/it, loss 0.459802
Finished training it 14336/76743 of epoch 2, 39.59 ms/it, loss 0.459845
Finished training it 15360/76743 of epoch 2, 40.01 ms/it, loss 0.457854
Finished training it 15360/76743 of epoch 2, 39.94 ms/it, loss 0.457542
Finished training it 15360/76743 of epoch 2, 40.08 ms/it, loss 0.459473
Finished training it 15360/76743 of epoch 2, 39.84 ms/it, loss 0.458804
Finished training it 16384/76743 of epoch 2, 47.29 ms/it, loss 0.458039
Finished training it 16384/76743 of epoch 2, 47.25 ms/it, loss 0.458999
Finished training it 16384/76743 of epoch 2, 45.54 ms/it, loss 0.458795
Finished training it 16384/76743 of epoch 2, 48.02 ms/it, loss 0.459541
Finished training it 17408/76743 of epoch 2, 40.12 ms/it, loss 0.457728
Finished training it 17408/76743 of epoch 2, 39.81 ms/it, loss 0.461221
Finished training it 17408/76743 of epoch 2, 40.01 ms/it, loss 0.459846
Finished training it 17408/76743 of epoch 2, 39.94 ms/it, loss 0.457492
Finished training it 18432/76743 of epoch 2, 39.98 ms/it, loss 0.457038
Finished training it 18432/76743 of epoch 2, 40.06 ms/it, loss 0.457913
Finished training it 18432/76743 of epoch 2, 39.93 ms/it, loss 0.458158
Finished training it 18432/76743 of epoch 2, 40.10 ms/it, loss 0.459916
Finished training it 19456/76743 of epoch 2, 39.52 ms/it, loss 0.455936
Finished training it 19456/76743 of epoch 2, 39.55 ms/it, loss 0.458898
Finished training it 19456/76743 of epoch 2, 39.63 ms/it, loss 0.458496
Finished training it 19456/76743 of epoch 2, 39.34 ms/it, loss 0.460525
Finished training it 20480/76743 of epoch 2, 39.81 ms/it, loss 0.460550
Finished training it 20480/76743 of epoch 2, 39.82 ms/it, loss 0.459144
Finished training it 20480/76743 of epoch 2, 39.93 ms/it, loss 0.458735
Finished training it 20480/76743 of epoch 2, 39.74 ms/it, loss 0.460699
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555378.0
get out
0 has test check 2555378.0 and sample count 3274240
 accuracy 78.045 %, best 78.045 %, roc auc score 0.7833, best 0.7833
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555378.0
get out
3 has test check 2555378.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 31.95 ms/it, loss 0.459594
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555378.0
get out
2 has test check 2555378.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 31.75 ms/it, loss 0.460593
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 31.85 ms/it, loss 0.457938
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555378.0
get out
1 has test check 2555378.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 31.98 ms/it, loss 0.459015
Finished training it 22528/76743 of epoch 2, 31.90 ms/it, loss 0.457511
Finished training it 22528/76743 of epoch 2, 31.83 ms/it, loss 0.457828
Finished training it 22528/76743 of epoch 2, 31.68 ms/it, loss 0.458973
Finished training it 22528/76743 of epoch 2, 31.63 ms/it, loss 0.458373
Finished training it 23552/76743 of epoch 2, 39.13 ms/it, loss 0.458971
Finished training it 23552/76743 of epoch 2, 38.94 ms/it, loss 0.458705
Finished training it 23552/76743 of epoch 2, 38.99 ms/it, loss 0.456637
Finished training it 23552/76743 of epoch 2, 39.09 ms/it, loss 0.458531
Finished training it 24576/76743 of epoch 2, 40.14 ms/it, loss 0.458624
Finished training it 24576/76743 of epoch 2, 40.11 ms/it, loss 0.459039
Finished training it 24576/76743 of epoch 2, 40.03 ms/it, loss 0.457701
Finished training it 24576/76743 of epoch 2, 40.22 ms/it, loss 0.457724
Finished training it 25600/76743 of epoch 2, 39.83 ms/it, loss 0.460245
Finished training it 25600/76743 of epoch 2, 39.81 ms/it, loss 0.458568
Finished training it 25600/76743 of epoch 2, 39.67 ms/it, loss 0.457144
Finished training it 25600/76743 of epoch 2, 39.83 ms/it, loss 0.458105
Finished training it 26624/76743 of epoch 2, 39.45 ms/it, loss 0.457702
Finished training it 26624/76743 of epoch 2, 39.50 ms/it, loss 0.461252
Finished training it 26624/76743 of epoch 2, 39.42 ms/it, loss 0.461951
Finished training it 26624/76743 of epoch 2, 39.53 ms/it, loss 0.459259
Finished training it 27648/76743 of epoch 2, 39.67 ms/it, loss 0.455356
Finished training it 27648/76743 of epoch 2, 39.56 ms/it, loss 0.458534
Finished training it 27648/76743 of epoch 2, 39.66 ms/it, loss 0.459149
Finished training it 27648/76743 of epoch 2, 39.70 ms/it, loss 0.460639
Finished training it 28672/76743 of epoch 2, 39.69 ms/it, loss 0.459800
Finished training it 28672/76743 of epoch 2, 39.86 ms/it, loss 0.455989
Finished training it 28672/76743 of epoch 2, 39.82 ms/it, loss 0.459333
Finished training it 28672/76743 of epoch 2, 39.80 ms/it, loss 0.458514
Finished training it 29696/76743 of epoch 2, 39.74 ms/it, loss 0.455826
Finished training it 29696/76743 of epoch 2, 39.72 ms/it, loss 0.459784
Finished training it 29696/76743 of epoch 2, 39.53 ms/it, loss 0.459266
Finished training it 29696/76743 of epoch 2, 39.62 ms/it, loss 0.458741
Finished training it 30720/76743 of epoch 2, 40.00 ms/it, loss 0.454985
Finished training it 30720/76743 of epoch 2, 40.08 ms/it, loss 0.456683
Finished training it 30720/76743 of epoch 2, 40.15 ms/it, loss 0.456715
Finished training it 30720/76743 of epoch 2, 40.08 ms/it, loss 0.459302
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555253.0
get out
0 has test check 2555253.0 and sample count 3274240
 accuracy 78.041 %, best 78.045 %, roc auc score 0.7836, best 0.7836
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555253.0
get out
3 has test check 2555253.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 31.92 ms/it, loss 0.459016
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555253.0
get out
1 has test check 2555253.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 31.87 ms/it, loss 0.459300
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 31.83 ms/it, loss 0.460211
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555253.0
get out
2 has test check 2555253.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 31.77 ms/it, loss 0.458698
Finished training it 32768/76743 of epoch 2, 31.95 ms/it, loss 0.456941
Finished training it 32768/76743 of epoch 2, 31.95 ms/it, loss 0.456295
Finished training it 32768/76743 of epoch 2, 31.96 ms/it, loss 0.459228
Finished training it 32768/76743 of epoch 2, 31.69 ms/it, loss 0.458776
Finished training it 33792/76743 of epoch 2, 40.10 ms/it, loss 0.457515
Finished training it 33792/76743 of epoch 2, 40.30 ms/it, loss 0.457397
Finished training it 33792/76743 of epoch 2, 40.32 ms/it, loss 0.455711
Finished training it 33792/76743 of epoch 2, 40.18 ms/it, loss 0.458718
Finished training it 34816/76743 of epoch 2, 39.80 ms/it, loss 0.458387
Finished training it 34816/76743 of epoch 2, 39.63 ms/it, loss 0.457843
Finished training it 34816/76743 of epoch 2, 39.81 ms/it, loss 0.458083
Finished training it 34816/76743 of epoch 2, 39.75 ms/it, loss 0.459730
Finished training it 35840/76743 of epoch 2, 40.01 ms/it, loss 0.459422
Finished training it 35840/76743 of epoch 2, 40.13 ms/it, loss 0.457073
Finished training it 35840/76743 of epoch 2, 40.18 ms/it, loss 0.459150
Finished training it 35840/76743 of epoch 2, 40.18 ms/it, loss 0.458184
Finished training it 36864/76743 of epoch 2, 39.35 ms/it, loss 0.461512
Finished training it 36864/76743 of epoch 2, 39.20 ms/it, loss 0.456307
Finished training it 36864/76743 of epoch 2, 39.31 ms/it, loss 0.455951
Finished training it 36864/76743 of epoch 2, 39.27 ms/it, loss 0.460110
Finished training it 37888/76743 of epoch 2, 39.80 ms/it, loss 0.459782
Finished training it 37888/76743 of epoch 2, 39.66 ms/it, loss 0.456085
Finished training it 37888/76743 of epoch 2, 39.61 ms/it, loss 0.456133
Finished training it 37888/76743 of epoch 2, 39.78 ms/it, loss 0.458781
Finished training it 38912/76743 of epoch 2, 37.37 ms/it, loss 0.457513
Finished training it 38912/76743 of epoch 2, 37.12 ms/it, loss 0.455273
Finished training it 38912/76743 of epoch 2, 37.20 ms/it, loss 0.456999
Finished training it 38912/76743 of epoch 2, 37.37 ms/it, loss 0.456438
Finished training it 39936/76743 of epoch 2, 39.54 ms/it, loss 0.459173
Finished training it 39936/76743 of epoch 2, 39.48 ms/it, loss 0.459087
Finished training it 39936/76743 of epoch 2, 39.41 ms/it, loss 0.459382
Finished training it 39936/76743 of epoch 2, 39.53 ms/it, loss 0.456990
Finished training it 40960/76743 of epoch 2, 39.18 ms/it, loss 0.459733
Finished training it 40960/76743 of epoch 2, 39.22 ms/it, loss 0.457224
Finished training it 40960/76743 of epoch 2, 39.34 ms/it, loss 0.458762
Finished training it 40960/76743 of epoch 2, 39.31 ms/it, loss 0.455570
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2557144.0
get out
0 has test check 2557144.0 and sample count 3274240
 accuracy 78.099 %, best 78.099 %, roc auc score 0.7842, best 0.7842
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2557144.0
get out
3 has test check 2557144.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 31.75 ms/it, loss 0.454575
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2557144.0
get out
1 has test check 2557144.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 31.61 ms/it, loss 0.457036
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2557144.0
get out
2 has test check 2557144.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 31.55 ms/it, loss 0.456917
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 31.62 ms/it, loss 0.456585
Finished training it 43008/76743 of epoch 2, 32.25 ms/it, loss 0.459019
Finished training it 43008/76743 of epoch 2, 32.30 ms/it, loss 0.458531
Finished training it 43008/76743 of epoch 2, 32.17 ms/it, loss 0.454860
Finished training it 43008/76743 of epoch 2, 32.06 ms/it, loss 0.455549
Finished training it 44032/76743 of epoch 2, 37.51 ms/it, loss 0.458526
Finished training it 44032/76743 of epoch 2, 37.65 ms/it, loss 0.456269
Finished training it 44032/76743 of epoch 2, 37.59 ms/it, loss 0.457084
Finished training it 44032/76743 of epoch 2, 37.66 ms/it, loss 0.454903
Finished training it 45056/76743 of epoch 2, 40.06 ms/it, loss 0.459539
Finished training it 45056/76743 of epoch 2, 40.11 ms/it, loss 0.456821
Finished training it 45056/76743 of epoch 2, 39.97 ms/it, loss 0.456771
Finished training it 45056/76743 of epoch 2, 40.17 ms/it, loss 0.456750
Finished training it 46080/76743 of epoch 2, 40.05 ms/it, loss 0.457278
Finished training it 46080/76743 of epoch 2, 39.89 ms/it, loss 0.455488
Finished training it 46080/76743 of epoch 2, 40.00 ms/it, loss 0.460281
Finished training it 46080/76743 of epoch 2, 39.91 ms/it, loss 0.454412
Finished training it 47104/76743 of epoch 2, 45.33 ms/it, loss 0.455406
Finished training it 47104/76743 of epoch 2, 44.76 ms/it, loss 0.456222
Finished training it 47104/76743 of epoch 2, 45.35 ms/it, loss 0.459942
Finished training it 47104/76743 of epoch 2, 45.21 ms/it, loss 0.457778
Finished training it 48128/76743 of epoch 2, 39.81 ms/it, loss 0.456874
Finished training it 48128/76743 of epoch 2, 39.90 ms/it, loss 0.459727
Finished training it 48128/76743 of epoch 2, 39.95 ms/it, loss 0.456488
Finished training it 48128/76743 of epoch 2, 39.92 ms/it, loss 0.458324
Finished training it 49152/76743 of epoch 2, 39.89 ms/it, loss 0.456410
Finished training it 49152/76743 of epoch 2, 40.03 ms/it, loss 0.455416
Finished training it 49152/76743 of epoch 2, 39.97 ms/it, loss 0.458887
Finished training it 49152/76743 of epoch 2, 40.01 ms/it, loss 0.457417
Finished training it 50176/76743 of epoch 2, 39.59 ms/it, loss 0.458733
Finished training it 50176/76743 of epoch 2, 39.61 ms/it, loss 0.454812
Finished training it 50176/76743 of epoch 2, 39.54 ms/it, loss 0.457478
Finished training it 50176/76743 of epoch 2, 39.45 ms/it, loss 0.455760
Finished training it 51200/76743 of epoch 2, 39.95 ms/it, loss 0.455734
Finished training it 51200/76743 of epoch 2, 39.94 ms/it, loss 0.457121
Finished training it 51200/76743 of epoch 2, 39.73 ms/it, loss 0.456809
Finished training it 51200/76743 of epoch 2, 39.83 ms/it, loss 0.457915
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2551544.0
get out
0 has test check 2551544.0 and sample count 3274240
 accuracy 77.928 %, best 78.099 %, roc auc score 0.7844, best 0.7844
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 32.06 ms/it, loss 0.458411
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2551544.0
get out
2 has test check 2551544.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 31.90 ms/it, loss 0.455047
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2551544.0
get out
1 has test check 2551544.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 32.16 ms/it, loss 0.459232
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2551544.0
get out
3 has test check 2551544.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 32.23 ms/it, loss 0.455752
Finished training it 53248/76743 of epoch 2, 32.49 ms/it, loss 0.455731
Finished training it 53248/76743 of epoch 2, 32.75 ms/it, loss 0.457963
Finished training it 53248/76743 of epoch 2, 32.64 ms/it, loss 0.457783
Finished training it 53248/76743 of epoch 2, 32.62 ms/it, loss 0.458815
Finished training it 54272/76743 of epoch 2, 39.66 ms/it, loss 0.454804
Finished training it 54272/76743 of epoch 2, 39.62 ms/it, loss 0.459495
Finished training it 54272/76743 of epoch 2, 39.61 ms/it, loss 0.458511
Finished training it 54272/76743 of epoch 2, 39.46 ms/it, loss 0.458107
Finished training it 55296/76743 of epoch 2, 40.05 ms/it, loss 0.456829
Finished training it 55296/76743 of epoch 2, 40.12 ms/it, loss 0.458253
Finished training it 55296/76743 of epoch 2, 40.22 ms/it, loss 0.455355
Finished training it 55296/76743 of epoch 2, 40.22 ms/it, loss 0.457752
Finished training it 56320/76743 of epoch 2, 39.91 ms/it, loss 0.456803
Finished training it 56320/76743 of epoch 2, 39.79 ms/it, loss 0.458042
Finished training it 56320/76743 of epoch 2, 39.97 ms/it, loss 0.455042
Finished training it 56320/76743 of epoch 2, 39.92 ms/it, loss 0.457980
Finished training it 57344/76743 of epoch 2, 39.70 ms/it, loss 0.455723
Finished training it 57344/76743 of epoch 2, 39.77 ms/it, loss 0.455521
Finished training it 57344/76743 of epoch 2, 39.81 ms/it, loss 0.455497
Finished training it 57344/76743 of epoch 2, 39.63 ms/it, loss 0.455927
Finished training it 58368/76743 of epoch 2, 39.52 ms/it, loss 0.459017
Finished training it 58368/76743 of epoch 2, 39.49 ms/it, loss 0.458561
Finished training it 58368/76743 of epoch 2, 39.36 ms/it, loss 0.456737
Finished training it 58368/76743 of epoch 2, 39.41 ms/it, loss 0.455737
Finished training it 59392/76743 of epoch 2, 39.40 ms/it, loss 0.455833
Finished training it 59392/76743 of epoch 2, 39.58 ms/it, loss 0.457530
Finished training it 59392/76743 of epoch 2, 39.45 ms/it, loss 0.455546
Finished training it 59392/76743 of epoch 2, 39.52 ms/it, loss 0.458731
Finished training it 60416/76743 of epoch 2, 39.62 ms/it, loss 0.456586
Finished training it 60416/76743 of epoch 2, 39.77 ms/it, loss 0.457204
Finished training it 60416/76743 of epoch 2, 39.80 ms/it, loss 0.456988
Finished training it 60416/76743 of epoch 2, 39.76 ms/it, loss 0.459296
Finished training it 61440/76743 of epoch 2, 40.00 ms/it, loss 0.456678
Finished training it 61440/76743 of epoch 2, 39.90 ms/it, loss 0.456932
Finished training it 61440/76743 of epoch 2, 40.01 ms/it, loss 0.456245
Finished training it 61440/76743 of epoch 2, 39.99 ms/it, loss 0.455211
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555418.0
get out
0 has test check 2555418.0 and sample count 3274240
 accuracy 78.046 %, best 78.099 %, roc auc score 0.7850, best 0.7850
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 31.77 ms/it, loss 0.456599
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555418.0
get out
1 has test check 2555418.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 31.78 ms/it, loss 0.455673
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555418.0
get out
2 has test check 2555418.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 31.68 ms/it, loss 0.457818
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555418.0
get out
3 has test check 2555418.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 31.83 ms/it, loss 0.453880
Finished training it 63488/76743 of epoch 2, 32.12 ms/it, loss 0.461045
Finished training it 63488/76743 of epoch 2, 32.06 ms/it, loss 0.454973
Finished training it 63488/76743 of epoch 2, 32.27 ms/it, loss 0.455359
Finished training it 63488/76743 of epoch 2, 32.27 ms/it, loss 0.456459
Finished training it 64512/76743 of epoch 2, 39.73 ms/it, loss 0.453762
Finished training it 64512/76743 of epoch 2, 39.84 ms/it, loss 0.454676
Finished training it 64512/76743 of epoch 2, 39.63 ms/it, loss 0.456173
Finished training it 64512/76743 of epoch 2, 39.79 ms/it, loss 0.458349
Finished training it 65536/76743 of epoch 2, 39.36 ms/it, loss 0.454868
Finished training it 65536/76743 of epoch 2, 39.51 ms/it, loss 0.457163
Finished training it 65536/76743 of epoch 2, 39.56 ms/it, loss 0.456104
Finished training it 65536/76743 of epoch 2, 39.47 ms/it, loss 0.455680
Finished training it 66560/76743 of epoch 2, 39.98 ms/it, loss 0.458057
Finished training it 66560/76743 of epoch 2, 40.18 ms/it, loss 0.458320
Finished training it 66560/76743 of epoch 2, 40.16 ms/it, loss 0.455799
Finished training it 66560/76743 of epoch 2, 40.08 ms/it, loss 0.456673
Finished training it 67584/76743 of epoch 2, 39.59 ms/it, loss 0.452853
Finished training it 67584/76743 of epoch 2, 39.56 ms/it, loss 0.455799
Finished training it 67584/76743 of epoch 2, 39.41 ms/it, loss 0.456151
Finished training it 67584/76743 of epoch 2, 39.55 ms/it, loss 0.456505
Finished training it 68608/76743 of epoch 2, 39.82 ms/it, loss 0.456145
Finished training it 68608/76743 of epoch 2, 39.68 ms/it, loss 0.457680
Finished training it 68608/76743 of epoch 2, 39.82 ms/it, loss 0.454917
Finished training it 68608/76743 of epoch 2, 39.70 ms/it, loss 0.458289
Finished training it 69632/76743 of epoch 2, 35.82 ms/it, loss 0.456917
Finished training it 69632/76743 of epoch 2, 35.99 ms/it, loss 0.456043
Finished training it 69632/76743 of epoch 2, 36.06 ms/it, loss 0.456119
Finished training it 69632/76743 of epoch 2, 36.10 ms/it, loss 0.458359
Finished training it 70656/76743 of epoch 2, 40.15 ms/it, loss 0.455212
Finished training it 70656/76743 of epoch 2, 40.18 ms/it, loss 0.458141
Finished training it 70656/76743 of epoch 2, 40.14 ms/it, loss 0.459485
Finished training it 70656/76743 of epoch 2, 40.00 ms/it, loss 0.459263
Finished training it 71680/76743 of epoch 2, 40.01 ms/it, loss 0.454147
Finished training it 71680/76743 of epoch 2, 40.04 ms/it, loss 0.453814
Finished training it 71680/76743 of epoch 2, 40.06 ms/it, loss 0.456812
Finished training it 71680/76743 of epoch 2, 39.87 ms/it, loss 0.453944
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2557772.0
get out
0 has test check 2557772.0 and sample count 3274240
 accuracy 78.118 %, best 78.118 %, roc auc score 0.7849, best 0.7850
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2557772.0
get out
1 has test check 2557772.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 39.74 ms/it, loss 0.457629
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2557772.0
get out
2 has test check 2557772.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 39.63 ms/it, loss 0.457297
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 39.72 ms/it, loss 0.456203
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2557772.0
get out
3 has test check 2557772.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 39.81 ms/it, loss 0.456963
Finished training it 73728/76743 of epoch 2, 39.46 ms/it, loss 0.455588
Finished training it 73728/76743 of epoch 2, 39.36 ms/it, loss 0.456582
Finished training it 73728/76743 of epoch 2, 39.44 ms/it, loss 0.454354
Finished training it 73728/76743 of epoch 2, 39.51 ms/it, loss 0.457389
Finished training it 74752/76743 of epoch 2, 35.97 ms/it, loss 0.456068
Finished training it 74752/76743 of epoch 2, 35.91 ms/it, loss 0.455336
Finished training it 74752/76743 of epoch 2, 36.02 ms/it, loss 0.453153
Finished training it 74752/76743 of epoch 2, 36.04 ms/it, loss 0.453902
Finished training it 75776/76743 of epoch 2, 31.54 ms/it, loss 0.455617
Finished training it 75776/76743 of epoch 2, 31.61 ms/it, loss 0.456571
Finished training it 75776/76743 of epoch 2, 31.68 ms/it, loss 0.459274
Finished training it 75776/76743 of epoch 2, 31.76 ms/it, loss 0.455449
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 34.89 ms/it, loss 0.455008
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 35.00 ms/it, loss 0.455498
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 35.02 ms/it, loss 0.455679
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 35.05 ms/it, loss 0.458071
Finished training it 2048/76743 of epoch 3, 31.80 ms/it, loss 0.456232
Finished training it 2048/76743 of epoch 3, 31.88 ms/it, loss 0.453026
Finished training it 2048/76743 of epoch 3, 31.63 ms/it, loss 0.453478
Finished training it 2048/76743 of epoch 3, 31.68 ms/it, loss 0.456464
Finished training it 3072/76743 of epoch 3, 39.32 ms/it, loss 0.453042
Finished training it 3072/76743 of epoch 3, 39.43 ms/it, loss 0.456737
Finished training it 3072/76743 of epoch 3, 39.49 ms/it, loss 0.457561
Finished training it 3072/76743 of epoch 3, 39.48 ms/it, loss 0.457746
Finished training it 4096/76743 of epoch 3, 39.87 ms/it, loss 0.456515
Finished training it 4096/76743 of epoch 3, 39.96 ms/it, loss 0.455517
Finished training it 4096/76743 of epoch 3, 39.80 ms/it, loss 0.453626
Finished training it 4096/76743 of epoch 3, 39.91 ms/it, loss 0.456749
Finished training it 5120/76743 of epoch 3, 40.32 ms/it, loss 0.459300
Finished training it 5120/76743 of epoch 3, 40.16 ms/it, loss 0.454110
Finished training it 5120/76743 of epoch 3, 40.29 ms/it, loss 0.456303
Finished training it 5120/76743 of epoch 3, 40.20 ms/it, loss 0.455134
Finished training it 6144/76743 of epoch 3, 41.47 ms/it, loss 0.455464
Finished training it 6144/76743 of epoch 3, 41.42 ms/it, loss 0.456383
Finished training it 6144/76743 of epoch 3, 41.32 ms/it, loss 0.456903
Finished training it 6144/76743 of epoch 3, 41.18 ms/it, loss 0.454981
Finished training it 7168/76743 of epoch 3, 39.82 ms/it, loss 0.456319
Finished training it 7168/76743 of epoch 3, 39.67 ms/it, loss 0.457691
Finished training it 7168/76743 of epoch 3, 39.61 ms/it, loss 0.456601
Finished training it 7168/76743 of epoch 3, 39.81 ms/it, loss 0.454176
Finished training it 8192/76743 of epoch 3, 39.61 ms/it, loss 0.455293
Finished training it 8192/76743 of epoch 3, 39.59 ms/it, loss 0.457710
Finished training it 8192/76743 of epoch 3, 39.69 ms/it, loss 0.456191
Finished training it 8192/76743 of epoch 3, 39.65 ms/it, loss 0.455599
Finished training it 9216/76743 of epoch 3, 39.62 ms/it, loss 0.456004
Finished training it 9216/76743 of epoch 3, 39.63 ms/it, loss 0.454575
Finished training it 9216/76743 of epoch 3, 39.51 ms/it, loss 0.454100
Finished training it 9216/76743 of epoch 3, 39.60 ms/it, loss 0.456505
Finished training it 10240/76743 of epoch 3, 40.23 ms/it, loss 0.455567
Finished training it 10240/76743 of epoch 3, 40.16 ms/it, loss 0.455373
Finished training it 10240/76743 of epoch 3, 40.09 ms/it, loss 0.455060
Finished training it 10240/76743 of epoch 3, 40.11 ms/it, loss 0.453480
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2558716.0
get out
0 has test check 2558716.0 and sample count 3274240
 accuracy 78.147 %, best 78.147 %, roc auc score 0.7856, best 0.7856
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2558716.0
get out
2 has test check 2558716.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 31.87 ms/it, loss 0.453870
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2558716.0
get out
1 has test check 2558716.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 31.95 ms/it, loss 0.454017
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2558716.0
get out
3 has test check 2558716.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 31.97 ms/it, loss 0.454436
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 31.87 ms/it, loss 0.456221
Finished training it 12288/76743 of epoch 3, 34.57 ms/it, loss 0.455433
Finished training it 12288/76743 of epoch 3, 34.74 ms/it, loss 0.453849
Finished training it 12288/76743 of epoch 3, 34.88 ms/it, loss 0.454981
Finished training it 12288/76743 of epoch 3, 34.86 ms/it, loss 0.457888
Finished training it 13312/76743 of epoch 3, 39.73 ms/it, loss 0.457067
Finished training it 13312/76743 of epoch 3, 39.84 ms/it, loss 0.457066
Finished training it 13312/76743 of epoch 3, 39.91 ms/it, loss 0.457794
Finished training it 13312/76743 of epoch 3, 39.82 ms/it, loss 0.455174
Finished training it 14336/76743 of epoch 3, 39.43 ms/it, loss 0.456010
Finished training it 14336/76743 of epoch 3, 39.46 ms/it, loss 0.456015
Finished training it 14336/76743 of epoch 3, 39.31 ms/it, loss 0.456517
Finished training it 14336/76743 of epoch 3, 39.49 ms/it, loss 0.455870
Finished training it 15360/76743 of epoch 3, 40.20 ms/it, loss 0.454842
Finished training it 15360/76743 of epoch 3, 40.35 ms/it, loss 0.453869
Finished training it 15360/76743 of epoch 3, 40.48 ms/it, loss 0.454092
Finished training it 15360/76743 of epoch 3, 40.42 ms/it, loss 0.455461
Finished training it 16384/76743 of epoch 3, 39.37 ms/it, loss 0.454377
Finished training it 16384/76743 of epoch 3, 39.32 ms/it, loss 0.455850
Finished training it 16384/76743 of epoch 3, 39.18 ms/it, loss 0.455054
Finished training it 16384/76743 of epoch 3, 39.34 ms/it, loss 0.454893
Finished training it 17408/76743 of epoch 3, 39.49 ms/it, loss 0.457454
Finished training it 17408/76743 of epoch 3, 39.68 ms/it, loss 0.455957
Finished training it 17408/76743 of epoch 3, 39.67 ms/it, loss 0.453877
Finished training it 17408/76743 of epoch 3, 39.59 ms/it, loss 0.453668
Finished training it 18432/76743 of epoch 3, 37.26 ms/it, loss 0.454324
Finished training it 18432/76743 of epoch 3, 37.41 ms/it, loss 0.453227
Finished training it 18432/76743 of epoch 3, 37.41 ms/it, loss 0.456073
Finished training it 18432/76743 of epoch 3, 37.50 ms/it, loss 0.454154
Finished training it 19456/76743 of epoch 3, 40.24 ms/it, loss 0.454891
Finished training it 19456/76743 of epoch 3, 40.29 ms/it, loss 0.455162
Finished training it 19456/76743 of epoch 3, 40.21 ms/it, loss 0.452336
Finished training it 19456/76743 of epoch 3, 40.05 ms/it, loss 0.456854
Finished training it 20480/76743 of epoch 3, 40.18 ms/it, loss 0.455653
Finished training it 20480/76743 of epoch 3, 40.12 ms/it, loss 0.455030
Finished training it 20480/76743 of epoch 3, 39.97 ms/it, loss 0.457014
Finished training it 20480/76743 of epoch 3, 40.05 ms/it, loss 0.456904
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2559614.0
get out
0 has test check 2559614.0 and sample count 3274240
 accuracy 78.174 %, best 78.174 %, roc auc score 0.7861, best 0.7861
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2559614.0
get out
3 has test check 2559614.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 31.64 ms/it, loss 0.455814
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2559614.0
get out
1 has test check 2559614.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 31.66 ms/it, loss 0.455293
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 31.40 ms/it, loss 0.454129
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2559614.0
get out
2 has test check 2559614.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 31.29 ms/it, loss 0.456927
Finished training it 22528/76743 of epoch 3, 31.75 ms/it, loss 0.454746
Finished training it 22528/76743 of epoch 3, 31.89 ms/it, loss 0.455136
Finished training it 22528/76743 of epoch 3, 32.02 ms/it, loss 0.454131
Finished training it 22528/76743 of epoch 3, 31.92 ms/it, loss 0.453583
Finished training it 23552/76743 of epoch 3, 40.19 ms/it, loss 0.455315
Finished training it 23552/76743 of epoch 3, 39.95 ms/it, loss 0.454891
Finished training it 23552/76743 of epoch 3, 40.10 ms/it, loss 0.452837
Finished training it 23552/76743 of epoch 3, 40.21 ms/it, loss 0.454817
Finished training it 24576/76743 of epoch 3, 39.37 ms/it, loss 0.454175
Finished training it 24576/76743 of epoch 3, 39.48 ms/it, loss 0.454612
Finished training it 24576/76743 of epoch 3, 39.54 ms/it, loss 0.454024
Finished training it 24576/76743 of epoch 3, 39.42 ms/it, loss 0.455152
Finished training it 25600/76743 of epoch 3, 40.05 ms/it, loss 0.456318
Finished training it 25600/76743 of epoch 3, 39.92 ms/it, loss 0.453510
Finished training it 25600/76743 of epoch 3, 40.07 ms/it, loss 0.454729
Finished training it 25600/76743 of epoch 3, 39.98 ms/it, loss 0.454520
Finished training it 26624/76743 of epoch 3, 45.38 ms/it, loss 0.458209
Finished training it 26624/76743 of epoch 3, 46.73 ms/it, loss 0.455755
Finished training it 26624/76743 of epoch 3, 46.10 ms/it, loss 0.454011
Finished training it 26624/76743 of epoch 3, 46.22 ms/it, loss 0.457687
Finished training it 27648/76743 of epoch 3, 39.59 ms/it, loss 0.454941
Finished training it 27648/76743 of epoch 3, 39.58 ms/it, loss 0.456954
Finished training it 27648/76743 of epoch 3, 39.67 ms/it, loss 0.455664
Finished training it 27648/76743 of epoch 3, 39.75 ms/it, loss 0.451807
Finished training it 28672/76743 of epoch 3, 39.02 ms/it, loss 0.455890
Finished training it 28672/76743 of epoch 3, 39.06 ms/it, loss 0.452080
Finished training it 28672/76743 of epoch 3, 38.93 ms/it, loss 0.456071
Finished training it 28672/76743 of epoch 3, 38.93 ms/it, loss 0.454694
Finished training it 29696/76743 of epoch 3, 40.19 ms/it, loss 0.455188
Finished training it 29696/76743 of epoch 3, 40.07 ms/it, loss 0.455841
Finished training it 29696/76743 of epoch 3, 40.23 ms/it, loss 0.452172
Finished training it 29696/76743 of epoch 3, 40.22 ms/it, loss 0.456517
Finished training it 30720/76743 of epoch 3, 40.27 ms/it, loss 0.453273
Finished training it 30720/76743 of epoch 3, 40.21 ms/it, loss 0.455541
Finished training it 30720/76743 of epoch 3, 40.17 ms/it, loss 0.453121
Finished training it 30720/76743 of epoch 3, 40.04 ms/it, loss 0.451399
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2559226.0
get out
0 has test check 2559226.0 and sample count 3274240
 accuracy 78.162 %, best 78.174 %, roc auc score 0.7863, best 0.7863
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2559226.0
get out
2 has test check 2559226.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 31.84 ms/it, loss 0.455135
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2559226.0
get out
1 has test check 2559226.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 31.97 ms/it, loss 0.455733
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2559226.0
get out
3 has test check 2559226.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 31.99 ms/it, loss 0.455341
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 31.89 ms/it, loss 0.456848
Finished training it 32768/76743 of epoch 3, 34.83 ms/it, loss 0.453391
Finished training it 32768/76743 of epoch 3, 34.56 ms/it, loss 0.455010
Finished training it 32768/76743 of epoch 3, 34.79 ms/it, loss 0.452545
Finished training it 32768/76743 of epoch 3, 34.74 ms/it, loss 0.455631
Finished training it 33792/76743 of epoch 3, 39.30 ms/it, loss 0.454071
Finished training it 33792/76743 of epoch 3, 39.47 ms/it, loss 0.455284
Finished training it 33792/76743 of epoch 3, 39.51 ms/it, loss 0.452266
Finished training it 33792/76743 of epoch 3, 39.48 ms/it, loss 0.453864
Finished training it 34816/76743 of epoch 3, 40.40 ms/it, loss 0.454453
Finished training it 34816/76743 of epoch 3, 40.23 ms/it, loss 0.454231
Finished training it 34816/76743 of epoch 3, 40.38 ms/it, loss 0.454927
Finished training it 34816/76743 of epoch 3, 40.35 ms/it, loss 0.456328
Finished training it 35840/76743 of epoch 3, 39.86 ms/it, loss 0.455870
Finished training it 35840/76743 of epoch 3, 39.84 ms/it, loss 0.454521
Finished training it 35840/76743 of epoch 3, 39.73 ms/it, loss 0.455926
Finished training it 35840/76743 of epoch 3, 39.77 ms/it, loss 0.453767
Finished training it 36864/76743 of epoch 3, 39.34 ms/it, loss 0.457691
Finished training it 36864/76743 of epoch 3, 39.16 ms/it, loss 0.452646
Finished training it 36864/76743 of epoch 3, 39.31 ms/it, loss 0.456738
Finished training it 36864/76743 of epoch 3, 39.37 ms/it, loss 0.452324
Finished training it 37888/76743 of epoch 3, 39.32 ms/it, loss 0.452534
Finished training it 37888/76743 of epoch 3, 39.44 ms/it, loss 0.456285
Finished training it 37888/76743 of epoch 3, 39.43 ms/it, loss 0.455485
Finished training it 37888/76743 of epoch 3, 39.39 ms/it, loss 0.452801
Finished training it 38912/76743 of epoch 3, 39.62 ms/it, loss 0.452774
Finished training it 38912/76743 of epoch 3, 39.53 ms/it, loss 0.453385
Finished training it 38912/76743 of epoch 3, 39.44 ms/it, loss 0.451565
Finished training it 38912/76743 of epoch 3, 39.54 ms/it, loss 0.453966
Finished training it 39936/76743 of epoch 3, 40.00 ms/it, loss 0.453324
Finished training it 39936/76743 of epoch 3, 39.96 ms/it, loss 0.456278
Finished training it 39936/76743 of epoch 3, 40.04 ms/it, loss 0.455664
Finished training it 39936/76743 of epoch 3, 40.11 ms/it, loss 0.455430
Finished training it 40960/76743 of epoch 3, 39.41 ms/it, loss 0.456054
Finished training it 40960/76743 of epoch 3, 39.57 ms/it, loss 0.452013
Finished training it 40960/76743 of epoch 3, 39.59 ms/it, loss 0.455215
Finished training it 40960/76743 of epoch 3, 39.51 ms/it, loss 0.453723
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2560367.0
get out
0 has test check 2560367.0 and sample count 3274240
 accuracy 78.197 %, best 78.197 %, roc auc score 0.7866, best 0.7866
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2560367.0
get out
1 has test check 2560367.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 31.91 ms/it, loss 0.453554
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2560367.0
get out
3 has test check 2560367.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 31.99 ms/it, loss 0.451277
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2560367.0
get out
2 has test check 2560367.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 31.80 ms/it, loss 0.453345
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 31.90 ms/it, loss 0.453092
Finished training it 43008/76743 of epoch 3, 33.47 ms/it, loss 0.451559
Finished training it 43008/76743 of epoch 3, 33.58 ms/it, loss 0.454996
Finished training it 43008/76743 of epoch 3, 33.57 ms/it, loss 0.455472
Finished training it 43008/76743 of epoch 3, 33.39 ms/it, loss 0.452184
Finished training it 44032/76743 of epoch 3, 39.80 ms/it, loss 0.452923
Finished training it 44032/76743 of epoch 3, 39.82 ms/it, loss 0.451655
Finished training it 44032/76743 of epoch 3, 39.74 ms/it, loss 0.453893
Finished training it 44032/76743 of epoch 3, 39.66 ms/it, loss 0.455356
Finished training it 45056/76743 of epoch 3, 39.82 ms/it, loss 0.453673
Finished training it 45056/76743 of epoch 3, 39.97 ms/it, loss 0.453327
Finished training it 45056/76743 of epoch 3, 39.93 ms/it, loss 0.456038
Finished training it 45056/76743 of epoch 3, 39.95 ms/it, loss 0.453517
Finished training it 46080/76743 of epoch 3, 40.07 ms/it, loss 0.451037
Finished training it 46080/76743 of epoch 3, 39.95 ms/it, loss 0.452232
Finished training it 46080/76743 of epoch 3, 40.09 ms/it, loss 0.453888
Finished training it 46080/76743 of epoch 3, 40.10 ms/it, loss 0.456786
Finished training it 47104/76743 of epoch 3, 39.81 ms/it, loss 0.452495
Finished training it 47104/76743 of epoch 3, 40.03 ms/it, loss 0.454363
Finished training it 47104/76743 of epoch 3, 39.91 ms/it, loss 0.456650
Finished training it 47104/76743 of epoch 3, 39.81 ms/it, loss 0.452849
Finished training it 48128/76743 of epoch 3, 39.04 ms/it, loss 0.456508
Finished training it 48128/76743 of epoch 3, 38.97 ms/it, loss 0.453123
Finished training it 48128/76743 of epoch 3, 38.94 ms/it, loss 0.454982
Finished training it 48128/76743 of epoch 3, 38.86 ms/it, loss 0.453698
Finished training it 49152/76743 of epoch 3, 36.89 ms/it, loss 0.454075
Finished training it 49152/76743 of epoch 3, 36.73 ms/it, loss 0.455333
Finished training it 49152/76743 of epoch 3, 36.62 ms/it, loss 0.452847
Finished training it 49152/76743 of epoch 3, 36.79 ms/it, loss 0.451860
Finished training it 50176/76743 of epoch 3, 39.80 ms/it, loss 0.452604
Finished training it 50176/76743 of epoch 3, 40.00 ms/it, loss 0.455550
Finished training it 50176/76743 of epoch 3, 39.93 ms/it, loss 0.451443
Finished training it 50176/76743 of epoch 3, 39.87 ms/it, loss 0.454154
Finished training it 51200/76743 of epoch 3, 39.26 ms/it, loss 0.453668
Finished training it 51200/76743 of epoch 3, 39.40 ms/it, loss 0.454563
Finished training it 51200/76743 of epoch 3, 39.47 ms/it, loss 0.453963
Finished training it 51200/76743 of epoch 3, 39.53 ms/it, loss 0.452315
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2554215.0
get out
0 has test check 2554215.0 and sample count 3274240
 accuracy 78.009 %, best 78.197 %, roc auc score 0.7867, best 0.7867
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 31.48 ms/it, loss 0.454916
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2554215.0
get out
2 has test check 2554215.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 31.34 ms/it, loss 0.451878
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2554215.0
get out
1 has test check 2554215.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 31.58 ms/it, loss 0.456010
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2554215.0
get out
3 has test check 2554215.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 31.63 ms/it, loss 0.452656
Finished training it 53248/76743 of epoch 3, 31.95 ms/it, loss 0.454708
Finished training it 53248/76743 of epoch 3, 31.88 ms/it, loss 0.452569
Finished training it 53248/76743 of epoch 3, 31.92 ms/it, loss 0.454642
Finished training it 53248/76743 of epoch 3, 31.97 ms/it, loss 0.455497
Finished training it 54272/76743 of epoch 3, 37.60 ms/it, loss 0.454890
Finished training it 54272/76743 of epoch 3, 37.88 ms/it, loss 0.451599
Finished training it 54272/76743 of epoch 3, 37.81 ms/it, loss 0.456152
Finished training it 54272/76743 of epoch 3, 37.72 ms/it, loss 0.455062
Finished training it 55296/76743 of epoch 3, 39.49 ms/it, loss 0.453413
Finished training it 55296/76743 of epoch 3, 39.57 ms/it, loss 0.455164
Finished training it 55296/76743 of epoch 3, 39.60 ms/it, loss 0.452020
Finished training it 55296/76743 of epoch 3, 39.69 ms/it, loss 0.454497
Finished training it 56320/76743 of epoch 3, 39.93 ms/it, loss 0.451764
Finished training it 56320/76743 of epoch 3, 39.70 ms/it, loss 0.454728
Finished training it 56320/76743 of epoch 3, 39.81 ms/it, loss 0.453452
Finished training it 56320/76743 of epoch 3, 39.86 ms/it, loss 0.454713
Finished training it 57344/76743 of epoch 3, 46.11 ms/it, loss 0.452298
Finished training it 57344/76743 of epoch 3, 46.16 ms/it, loss 0.452683
Finished training it 57344/76743 of epoch 3, 46.42 ms/it, loss 0.452395
Finished training it 57344/76743 of epoch 3, 44.83 ms/it, loss 0.452796
Finished training it 58368/76743 of epoch 3, 39.77 ms/it, loss 0.453430
Finished training it 58368/76743 of epoch 3, 39.73 ms/it, loss 0.452413
Finished training it 58368/76743 of epoch 3, 39.83 ms/it, loss 0.456012
Finished training it 58368/76743 of epoch 3, 39.85 ms/it, loss 0.455321
Finished training it 59392/76743 of epoch 3, 39.66 ms/it, loss 0.452474
Finished training it 59392/76743 of epoch 3, 39.56 ms/it, loss 0.452553
Finished training it 59392/76743 of epoch 3, 39.73 ms/it, loss 0.454371
Finished training it 59392/76743 of epoch 3, 39.79 ms/it, loss 0.455958
Finished training it 60416/76743 of epoch 3, 39.40 ms/it, loss 0.456045
Finished training it 60416/76743 of epoch 3, 39.29 ms/it, loss 0.453949
Finished training it 60416/76743 of epoch 3, 39.31 ms/it, loss 0.453955
Finished training it 60416/76743 of epoch 3, 39.20 ms/it, loss 0.453547
Finished training it 61440/76743 of epoch 3, 39.92 ms/it, loss 0.451949
Finished training it 61440/76743 of epoch 3, 39.78 ms/it, loss 0.453612
Finished training it 61440/76743 of epoch 3, 39.89 ms/it, loss 0.453432
Finished training it 61440/76743 of epoch 3, 39.98 ms/it, loss 0.453195
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2558287.0
get out
0 has test check 2558287.0 and sample count 3274240
 accuracy 78.134 %, best 78.197 %, roc auc score 0.7871, best 0.7871
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2558287.0
get out
1 has test check 2558287.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 31.91 ms/it, loss 0.452385
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2558287.0
get out
2 has test check 2558287.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 31.75 ms/it, loss 0.454730
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 31.81 ms/it, loss 0.453286
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2558287.0
get out
3 has test check 2558287.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 31.92 ms/it, loss 0.450442
Finished training it 63488/76743 of epoch 3, 31.68 ms/it, loss 0.451699
Finished training it 63488/76743 of epoch 3, 31.76 ms/it, loss 0.457930
Finished training it 63488/76743 of epoch 3, 31.86 ms/it, loss 0.453216
Finished training it 63488/76743 of epoch 3, 31.95 ms/it, loss 0.452266
Finished training it 64512/76743 of epoch 3, 40.00 ms/it, loss 0.452804
Finished training it 64512/76743 of epoch 3, 40.12 ms/it, loss 0.450629
Finished training it 64512/76743 of epoch 3, 40.18 ms/it, loss 0.455491
Finished training it 64512/76743 of epoch 3, 40.21 ms/it, loss 0.451453
Finished training it 65536/76743 of epoch 3, 39.42 ms/it, loss 0.454043
Finished training it 65536/76743 of epoch 3, 39.41 ms/it, loss 0.453040
Finished training it 65536/76743 of epoch 3, 39.37 ms/it, loss 0.452751
Finished training it 65536/76743 of epoch 3, 39.26 ms/it, loss 0.451882
Finished training it 66560/76743 of epoch 3, 39.64 ms/it, loss 0.453797
Finished training it 66560/76743 of epoch 3, 39.68 ms/it, loss 0.455307
Finished training it 66560/76743 of epoch 3, 39.48 ms/it, loss 0.454738
Finished training it 66560/76743 of epoch 3, 39.69 ms/it, loss 0.452594
Finished training it 67584/76743 of epoch 3, 39.37 ms/it, loss 0.453053
Finished training it 67584/76743 of epoch 3, 39.62 ms/it, loss 0.452804
Finished training it 67584/76743 of epoch 3, 39.58 ms/it, loss 0.449653
Finished training it 67584/76743 of epoch 3, 39.47 ms/it, loss 0.453378
Finished training it 68608/76743 of epoch 3, 39.79 ms/it, loss 0.452987
Finished training it 68608/76743 of epoch 3, 39.77 ms/it, loss 0.451699
Finished training it 68608/76743 of epoch 3, 39.68 ms/it, loss 0.455256
Finished training it 68608/76743 of epoch 3, 39.58 ms/it, loss 0.454440
Finished training it 69632/76743 of epoch 3, 38.31 ms/it, loss 0.453102
Finished training it 69632/76743 of epoch 3, 38.31 ms/it, loss 0.454338
Finished training it 69632/76743 of epoch 3, 38.47 ms/it, loss 0.455380
Finished training it 69632/76743 of epoch 3, 38.47 ms/it, loss 0.453106
Finished training it 70656/76743 of epoch 3, 40.09 ms/it, loss 0.456021
Finished training it 70656/76743 of epoch 3, 40.36 ms/it, loss 0.455066
Finished training it 70656/76743 of epoch 3, 40.31 ms/it, loss 0.452119
Finished training it 70656/76743 of epoch 3, 40.19 ms/it, loss 0.456329
Finished training it 71680/76743 of epoch 3, 40.04 ms/it, loss 0.450890
Finished training it 71680/76743 of epoch 3, 39.89 ms/it, loss 0.451365
Finished training it 71680/76743 of epoch 3, 40.06 ms/it, loss 0.453780
Finished training it 71680/76743 of epoch 3, 39.84 ms/it, loss 0.450956
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2560173.0
get out
0 has test check 2560173.0 and sample count 3274240
 accuracy 78.191 %, best 78.197 %, roc auc score 0.7868, best 0.7871
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2560173.0
get out
1 has test check 2560173.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 40.11 ms/it, loss 0.454387
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2560173.0
get out
2 has test check 2560173.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 39.98 ms/it, loss 0.454112
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 40.06 ms/it, loss 0.453043
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2560173.0
get out
3 has test check 2560173.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 40.17 ms/it, loss 0.453878
Finished training it 73728/76743 of epoch 3, 40.03 ms/it, loss 0.452368
Finished training it 73728/76743 of epoch 3, 39.98 ms/it, loss 0.451516
Finished training it 73728/76743 of epoch 3, 40.08 ms/it, loss 0.454391
Finished training it 73728/76743 of epoch 3, 39.87 ms/it, loss 0.453655
Finished training it 74752/76743 of epoch 3, 32.73 ms/it, loss 0.452204
Finished training it 74752/76743 of epoch 3, 32.88 ms/it, loss 0.452947
Finished training it 74752/76743 of epoch 3, 32.82 ms/it, loss 0.450323
Finished training it 74752/76743 of epoch 3, 32.95 ms/it, loss 0.450836
Finished training it 75776/76743 of epoch 3, 31.75 ms/it, loss 0.452662
Finished training it 75776/76743 of epoch 3, 31.66 ms/it, loss 0.453413
Finished training it 75776/76743 of epoch 3, 31.74 ms/it, loss 0.456395
Finished training it 75776/76743 of epoch 3, 31.53 ms/it, loss 0.452656
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 32.70 ms/it, loss 0.455011
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 32.56 ms/it, loss 0.451915
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 32.91 ms/it, loss 0.452652
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 32.64 ms/it, loss 0.452533
Finished training it 2048/76743 of epoch 4, 31.90 ms/it, loss 0.450107
Finished training it 2048/76743 of epoch 4, 31.89 ms/it, loss 0.453272
Finished training it 2048/76743 of epoch 4, 31.72 ms/it, loss 0.450587
Finished training it 2048/76743 of epoch 4, 31.86 ms/it, loss 0.453548
Finished training it 3072/76743 of epoch 4, 39.59 ms/it, loss 0.453527
Finished training it 3072/76743 of epoch 4, 39.72 ms/it, loss 0.454859
Finished training it 3072/76743 of epoch 4, 39.56 ms/it, loss 0.450185
Finished training it 3072/76743 of epoch 4, 39.64 ms/it, loss 0.454726
Finished training it 4096/76743 of epoch 4, 39.53 ms/it, loss 0.452617
Finished training it 4096/76743 of epoch 4, 39.55 ms/it, loss 0.453806
Finished training it 4096/76743 of epoch 4, 39.45 ms/it, loss 0.453470
Finished training it 4096/76743 of epoch 4, 39.38 ms/it, loss 0.450922
Finished training it 5120/76743 of epoch 4, 39.55 ms/it, loss 0.451165
Finished training it 5120/76743 of epoch 4, 39.61 ms/it, loss 0.452006
Finished training it 5120/76743 of epoch 4, 39.68 ms/it, loss 0.453200
Finished training it 5120/76743 of epoch 4, 39.73 ms/it, loss 0.456338
Finished training it 6144/76743 of epoch 4, 40.03 ms/it, loss 0.453469
Finished training it 6144/76743 of epoch 4, 40.08 ms/it, loss 0.452678
Finished training it 6144/76743 of epoch 4, 39.99 ms/it, loss 0.453950
Finished training it 6144/76743 of epoch 4, 39.92 ms/it, loss 0.451945
Finished training it 7168/76743 of epoch 4, 39.72 ms/it, loss 0.454802
Finished training it 7168/76743 of epoch 4, 39.77 ms/it, loss 0.451229
Finished training it 7168/76743 of epoch 4, 39.60 ms/it, loss 0.453794
Finished training it 7168/76743 of epoch 4, 39.76 ms/it, loss 0.453550
Finished training it 8192/76743 of epoch 4, 37.53 ms/it, loss 0.454909
Finished training it 8192/76743 of epoch 4, 37.63 ms/it, loss 0.453138
Finished training it 8192/76743 of epoch 4, 37.70 ms/it, loss 0.452730
Finished training it 8192/76743 of epoch 4, 37.60 ms/it, loss 0.452474
Finished training it 9216/76743 of epoch 4, 39.87 ms/it, loss 0.451878
Finished training it 9216/76743 of epoch 4, 39.69 ms/it, loss 0.451072
Finished training it 9216/76743 of epoch 4, 39.77 ms/it, loss 0.453153
Finished training it 9216/76743 of epoch 4, 39.91 ms/it, loss 0.453460
Finished training it 10240/76743 of epoch 4, 40.15 ms/it, loss 0.452096
Finished training it 10240/76743 of epoch 4, 40.23 ms/it, loss 0.452728
Finished training it 10240/76743 of epoch 4, 40.20 ms/it, loss 0.452282
Finished training it 10240/76743 of epoch 4, 40.00 ms/it, loss 0.450903
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2561520.0
get out
0 has test check 2561520.0 and sample count 3274240
 accuracy 78.233 %, best 78.233 %, roc auc score 0.7873, best 0.7873
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2561520.0
get out
3 has test check 2561520.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 31.98 ms/it, loss 0.451455
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2561520.0
get out
2 has test check 2561520.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 31.71 ms/it, loss 0.451195
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2561520.0
get out
1 has test check 2561520.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 31.89 ms/it, loss 0.450981
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 31.77 ms/it, loss 0.453290
Finished training it 12288/76743 of epoch 4, 32.08 ms/it, loss 0.455028
Finished training it 12288/76743 of epoch 4, 31.96 ms/it, loss 0.450958
Finished training it 12288/76743 of epoch 4, 31.92 ms/it, loss 0.452425
Finished training it 12288/76743 of epoch 4, 32.13 ms/it, loss 0.452094
Finished training it 13312/76743 of epoch 4, 37.54 ms/it, loss 0.452405
Finished training it 13312/76743 of epoch 4, 37.67 ms/it, loss 0.455112
Finished training it 13312/76743 of epoch 4, 37.76 ms/it, loss 0.454331
Finished training it 13312/76743 of epoch 4, 37.49 ms/it, loss 0.454256
Finished training it 14336/76743 of epoch 4, 39.76 ms/it, loss 0.453861
Finished training it 14336/76743 of epoch 4, 39.90 ms/it, loss 0.453056
Finished training it 14336/76743 of epoch 4, 39.97 ms/it, loss 0.453066
Finished training it 14336/76743 of epoch 4, 39.94 ms/it, loss 0.452866
Finished training it 15360/76743 of epoch 4, 40.16 ms/it, loss 0.451293
Finished training it 15360/76743 of epoch 4, 40.11 ms/it, loss 0.451296
Finished training it 15360/76743 of epoch 4, 40.12 ms/it, loss 0.452548
Finished training it 15360/76743 of epoch 4, 40.00 ms/it, loss 0.451973
Finished training it 16384/76743 of epoch 4, 45.97 ms/it, loss 0.452105
Finished training it 16384/76743 of epoch 4, 45.87 ms/it, loss 0.452080
Finished training it 16384/76743 of epoch 4, 45.20 ms/it, loss 0.451609
Finished training it 16384/76743 of epoch 4, 45.31 ms/it, loss 0.453108
Finished training it 17408/76743 of epoch 4, 39.87 ms/it, loss 0.452868
Finished training it 17408/76743 of epoch 4, 39.74 ms/it, loss 0.450843
Finished training it 17408/76743 of epoch 4, 39.68 ms/it, loss 0.454509
Finished training it 17408/76743 of epoch 4, 39.79 ms/it, loss 0.450911
Finished training it 18432/76743 of epoch 4, 39.45 ms/it, loss 0.450459
Finished training it 18432/76743 of epoch 4, 39.52 ms/it, loss 0.453463
Finished training it 18432/76743 of epoch 4, 39.53 ms/it, loss 0.451366
Finished training it 18432/76743 of epoch 4, 39.36 ms/it, loss 0.451555
Finished training it 19456/76743 of epoch 4, 40.55 ms/it, loss 0.449455
Finished training it 19456/76743 of epoch 4, 40.65 ms/it, loss 0.452042
Finished training it 19456/76743 of epoch 4, 40.51 ms/it, loss 0.454124
Finished training it 19456/76743 of epoch 4, 40.68 ms/it, loss 0.452533
Finished training it 20480/76743 of epoch 4, 39.54 ms/it, loss 0.454391
Finished training it 20480/76743 of epoch 4, 39.64 ms/it, loss 0.453052
Finished training it 20480/76743 of epoch 4, 39.59 ms/it, loss 0.452406
Finished training it 20480/76743 of epoch 4, 39.43 ms/it, loss 0.454180
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2561478.0
get out
0 has test check 2561478.0 and sample count 3274240
 accuracy 78.231 %, best 78.233 %, roc auc score 0.7876, best 0.7876
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2561478.0
get out
1 has test check 2561478.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 31.78 ms/it, loss 0.452407
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2561478.0
get out
2 has test check 2561478.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 31.70 ms/it, loss 0.454267
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 31.74 ms/it, loss 0.451359
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2561478.0
get out
3 has test check 2561478.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 31.85 ms/it, loss 0.452986
Finished training it 22528/76743 of epoch 4, 31.73 ms/it, loss 0.450841
Finished training it 22528/76743 of epoch 4, 31.82 ms/it, loss 0.451237
Finished training it 22528/76743 of epoch 4, 31.53 ms/it, loss 0.452328
Finished training it 22528/76743 of epoch 4, 31.53 ms/it, loss 0.452136
Finished training it 23552/76743 of epoch 4, 39.99 ms/it, loss 0.452105
Finished training it 23552/76743 of epoch 4, 40.20 ms/it, loss 0.452049
Finished training it 23552/76743 of epoch 4, 40.18 ms/it, loss 0.452593
Finished training it 23552/76743 of epoch 4, 40.16 ms/it, loss 0.450229
Finished training it 24576/76743 of epoch 4, 40.24 ms/it, loss 0.451418
Finished training it 24576/76743 of epoch 4, 40.47 ms/it, loss 0.451408
Finished training it 24576/76743 of epoch 4, 40.41 ms/it, loss 0.452422
Finished training it 24576/76743 of epoch 4, 40.48 ms/it, loss 0.451721
Finished training it 25600/76743 of epoch 4, 39.65 ms/it, loss 0.453470
Finished training it 25600/76743 of epoch 4, 39.47 ms/it, loss 0.450857
Finished training it 25600/76743 of epoch 4, 39.71 ms/it, loss 0.452107
Finished training it 25600/76743 of epoch 4, 39.59 ms/it, loss 0.451762
Finished training it 26624/76743 of epoch 4, 40.04 ms/it, loss 0.451447
Finished training it 26624/76743 of epoch 4, 39.95 ms/it, loss 0.455563
Finished training it 26624/76743 of epoch 4, 40.19 ms/it, loss 0.455052
Finished training it 26624/76743 of epoch 4, 40.18 ms/it, loss 0.452960
Finished training it 27648/76743 of epoch 4, 40.01 ms/it, loss 0.452912
Finished training it 27648/76743 of epoch 4, 39.83 ms/it, loss 0.452302
Finished training it 27648/76743 of epoch 4, 39.91 ms/it, loss 0.454086
Finished training it 27648/76743 of epoch 4, 40.01 ms/it, loss 0.449099
Finished training it 28672/76743 of epoch 4, 40.00 ms/it, loss 0.453032
Finished training it 28672/76743 of epoch 4, 39.87 ms/it, loss 0.451882
Finished training it 28672/76743 of epoch 4, 39.80 ms/it, loss 0.453591
Finished training it 28672/76743 of epoch 4, 39.99 ms/it, loss 0.449133
Finished training it 29696/76743 of epoch 4, 39.98 ms/it, loss 0.452602
Finished training it 29696/76743 of epoch 4, 40.07 ms/it, loss 0.449341
Finished training it 29696/76743 of epoch 4, 40.06 ms/it, loss 0.453982
Finished training it 29696/76743 of epoch 4, 39.87 ms/it, loss 0.453262
Finished training it 30720/76743 of epoch 4, 39.91 ms/it, loss 0.450555
Finished training it 30720/76743 of epoch 4, 39.88 ms/it, loss 0.452666
Finished training it 30720/76743 of epoch 4, 39.78 ms/it, loss 0.448804
Finished training it 30720/76743 of epoch 4, 39.85 ms/it, loss 0.450409
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2560916.0
get out
0 has test check 2560916.0 and sample count 3274240
 accuracy 78.214 %, best 78.233 %, roc auc score 0.7878, best 0.7878
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2560916.0
get out
3 has test check 2560916.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 31.78 ms/it, loss 0.452613
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2560916.0
get out
1 has test check 2560916.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 31.74 ms/it, loss 0.453029
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2560916.0
get out
2 has test check 2560916.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 31.59 ms/it, loss 0.452413
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 31.67 ms/it, loss 0.454103
Finished training it 32768/76743 of epoch 4, 31.88 ms/it, loss 0.450644
Finished training it 32768/76743 of epoch 4, 31.73 ms/it, loss 0.452909
Finished training it 32768/76743 of epoch 4, 31.66 ms/it, loss 0.452244
Finished training it 32768/76743 of epoch 4, 31.85 ms/it, loss 0.449923
Finished training it 33792/76743 of epoch 4, 39.99 ms/it, loss 0.449576
Finished training it 33792/76743 of epoch 4, 40.02 ms/it, loss 0.451390
Finished training it 33792/76743 of epoch 4, 39.74 ms/it, loss 0.451311
Finished training it 33792/76743 of epoch 4, 39.91 ms/it, loss 0.452565
Finished training it 34816/76743 of epoch 4, 39.83 ms/it, loss 0.453563
Finished training it 34816/76743 of epoch 4, 39.67 ms/it, loss 0.451652
Finished training it 34816/76743 of epoch 4, 39.78 ms/it, loss 0.452469
Finished training it 34816/76743 of epoch 4, 39.87 ms/it, loss 0.451833
Finished training it 35840/76743 of epoch 4, 39.56 ms/it, loss 0.451981
Finished training it 35840/76743 of epoch 4, 39.41 ms/it, loss 0.453077
Finished training it 35840/76743 of epoch 4, 39.61 ms/it, loss 0.453331
Finished training it 35840/76743 of epoch 4, 39.46 ms/it, loss 0.451090
Finished training it 36864/76743 of epoch 4, 40.04 ms/it, loss 0.454912
Finished training it 36864/76743 of epoch 4, 39.99 ms/it, loss 0.454047
Finished training it 36864/76743 of epoch 4, 40.05 ms/it, loss 0.449629
Finished training it 36864/76743 of epoch 4, 39.92 ms/it, loss 0.450062
Finished training it 37888/76743 of epoch 4, 39.63 ms/it, loss 0.453606
Finished training it 37888/76743 of epoch 4, 39.63 ms/it, loss 0.452971
Finished training it 37888/76743 of epoch 4, 39.41 ms/it, loss 0.449898
Finished training it 37888/76743 of epoch 4, 39.54 ms/it, loss 0.450258
Finished training it 38912/76743 of epoch 4, 36.84 ms/it, loss 0.451276
Finished training it 38912/76743 of epoch 4, 36.77 ms/it, loss 0.450663
Finished training it 38912/76743 of epoch 4, 36.93 ms/it, loss 0.450072
Finished training it 38912/76743 of epoch 4, 36.71 ms/it, loss 0.448795
Finished training it 39936/76743 of epoch 4, 39.87 ms/it, loss 0.453107
Finished training it 39936/76743 of epoch 4, 39.92 ms/it, loss 0.452574
Finished training it 39936/76743 of epoch 4, 39.69 ms/it, loss 0.453586
Finished training it 39936/76743 of epoch 4, 39.77 ms/it, loss 0.450847
Finished training it 40960/76743 of epoch 4, 40.28 ms/it, loss 0.449379
Finished training it 40960/76743 of epoch 4, 40.18 ms/it, loss 0.451214
Finished training it 40960/76743 of epoch 4, 40.27 ms/it, loss 0.452443
Finished training it 40960/76743 of epoch 4, 40.14 ms/it, loss 0.453471
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562369.0
get out
0 has test check 2562369.0 and sample count 3274240
 accuracy 78.258 %, best 78.258 %, roc auc score 0.7881, best 0.7881
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 31.35 ms/it, loss 0.450309
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562369.0
get out
2 has test check 2562369.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 31.31 ms/it, loss 0.450643
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562369.0
get out
1 has test check 2562369.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 31.55 ms/it, loss 0.451112
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562369.0
get out
3 has test check 2562369.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 31.52 ms/it, loss 0.448903
Finished training it 43008/76743 of epoch 4, 31.95 ms/it, loss 0.449669
Finished training it 43008/76743 of epoch 4, 32.03 ms/it, loss 0.449017
Finished training it 43008/76743 of epoch 4, 32.18 ms/it, loss 0.452895
Finished training it 43008/76743 of epoch 4, 32.14 ms/it, loss 0.452397
Finished training it 44032/76743 of epoch 4, 36.75 ms/it, loss 0.449332
Finished training it 44032/76743 of epoch 4, 36.49 ms/it, loss 0.452899
Finished training it 44032/76743 of epoch 4, 36.62 ms/it, loss 0.451427
Finished training it 44032/76743 of epoch 4, 36.72 ms/it, loss 0.450414
Finished training it 45056/76743 of epoch 4, 40.00 ms/it, loss 0.450901
Finished training it 45056/76743 of epoch 4, 39.96 ms/it, loss 0.453400
Finished training it 45056/76743 of epoch 4, 40.00 ms/it, loss 0.450988
Finished training it 45056/76743 of epoch 4, 39.85 ms/it, loss 0.451007
Finished training it 46080/76743 of epoch 4, 40.20 ms/it, loss 0.453920
Finished training it 46080/76743 of epoch 4, 40.19 ms/it, loss 0.451473
Finished training it 46080/76743 of epoch 4, 40.08 ms/it, loss 0.448367
Finished training it 46080/76743 of epoch 4, 40.05 ms/it, loss 0.449763
Finished training it 47104/76743 of epoch 4, 45.42 ms/it, loss 0.450152
Finished training it 47104/76743 of epoch 4, 46.67 ms/it, loss 0.453885
Finished training it 47104/76743 of epoch 4, 46.60 ms/it, loss 0.451833
Finished training it 47104/76743 of epoch 4, 46.13 ms/it, loss 0.450103
Finished training it 48128/76743 of epoch 4, 39.36 ms/it, loss 0.450584
Finished training it 48128/76743 of epoch 4, 39.50 ms/it, loss 0.453908
Finished training it 48128/76743 of epoch 4, 39.44 ms/it, loss 0.452362
Finished training it 48128/76743 of epoch 4, 39.33 ms/it, loss 0.451216
Finished training it 49152/76743 of epoch 4, 39.92 ms/it, loss 0.451601
Finished training it 49152/76743 of epoch 4, 39.84 ms/it, loss 0.449318
Finished training it 49152/76743 of epoch 4, 39.72 ms/it, loss 0.450048
Finished training it 49152/76743 of epoch 4, 39.79 ms/it, loss 0.452650
Finished training it 50176/76743 of epoch 4, 40.05 ms/it, loss 0.448911
Finished training it 50176/76743 of epoch 4, 39.86 ms/it, loss 0.450185
Finished training it 50176/76743 of epoch 4, 40.04 ms/it, loss 0.452957
Finished training it 50176/76743 of epoch 4, 39.92 ms/it, loss 0.451563
Finished training it 51200/76743 of epoch 4, 39.72 ms/it, loss 0.452034
Finished training it 51200/76743 of epoch 4, 39.94 ms/it, loss 0.451574
Finished training it 51200/76743 of epoch 4, 39.85 ms/it, loss 0.449890
Finished training it 51200/76743 of epoch 4, 39.69 ms/it, loss 0.451194
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2555834.0
get out
0 has test check 2555834.0 and sample count 3274240
 accuracy 78.059 %, best 78.258 %, roc auc score 0.7881, best 0.7881
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2555834.0
get out
3 has test check 2555834.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 31.87 ms/it, loss 0.450119
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2555834.0
get out
2 has test check 2555834.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 31.76 ms/it, loss 0.449478
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2555834.0
get out
1 has test check 2555834.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 31.90 ms/it, loss 0.453475
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 31.79 ms/it, loss 0.452292
Finished training it 53248/76743 of epoch 4, 32.06 ms/it, loss 0.452131
Finished training it 53248/76743 of epoch 4, 32.08 ms/it, loss 0.452999
Finished training it 53248/76743 of epoch 4, 31.90 ms/it, loss 0.452151
Finished training it 53248/76743 of epoch 4, 31.82 ms/it, loss 0.449953
Finished training it 54272/76743 of epoch 4, 40.19 ms/it, loss 0.452257
Finished training it 54272/76743 of epoch 4, 40.24 ms/it, loss 0.452591
Finished training it 54272/76743 of epoch 4, 40.26 ms/it, loss 0.453294
Finished training it 54272/76743 of epoch 4, 40.38 ms/it, loss 0.449079
Finished training it 55296/76743 of epoch 4, 39.82 ms/it, loss 0.452652
Finished training it 55296/76743 of epoch 4, 39.93 ms/it, loss 0.452088
Finished training it 55296/76743 of epoch 4, 39.97 ms/it, loss 0.449487
Finished training it 55296/76743 of epoch 4, 39.86 ms/it, loss 0.450838
Finished training it 56320/76743 of epoch 4, 39.95 ms/it, loss 0.452173
Finished training it 56320/76743 of epoch 4, 40.06 ms/it, loss 0.449366
Finished training it 56320/76743 of epoch 4, 40.00 ms/it, loss 0.451017
Finished training it 56320/76743 of epoch 4, 40.07 ms/it, loss 0.452075
Finished training it 57344/76743 of epoch 4, 39.60 ms/it, loss 0.449930
Finished training it 57344/76743 of epoch 4, 39.59 ms/it, loss 0.449849
Finished training it 57344/76743 of epoch 4, 39.54 ms/it, loss 0.450132
Finished training it 57344/76743 of epoch 4, 39.47 ms/it, loss 0.450194
Finished training it 58368/76743 of epoch 4, 39.95 ms/it, loss 0.453702
Finished training it 58368/76743 of epoch 4, 39.74 ms/it, loss 0.449880
Finished training it 58368/76743 of epoch 4, 39.95 ms/it, loss 0.452970
Finished training it 58368/76743 of epoch 4, 39.84 ms/it, loss 0.450868
Finished training it 59392/76743 of epoch 4, 40.01 ms/it, loss 0.451915
Finished training it 59392/76743 of epoch 4, 40.00 ms/it, loss 0.453718
Finished training it 59392/76743 of epoch 4, 39.92 ms/it, loss 0.450150
Finished training it 59392/76743 of epoch 4, 39.84 ms/it, loss 0.450191
Finished training it 60416/76743 of epoch 4, 40.22 ms/it, loss 0.451234
Finished training it 60416/76743 of epoch 4, 40.38 ms/it, loss 0.451395
Finished training it 60416/76743 of epoch 4, 40.43 ms/it, loss 0.453608
Finished training it 60416/76743 of epoch 4, 40.34 ms/it, loss 0.451505
Finished training it 61440/76743 of epoch 4, 40.26 ms/it, loss 0.450892
Finished training it 61440/76743 of epoch 4, 40.25 ms/it, loss 0.449592
Finished training it 61440/76743 of epoch 4, 40.15 ms/it, loss 0.451278
Finished training it 61440/76743 of epoch 4, 40.33 ms/it, loss 0.450814
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2559939.0
get out
0 has test check 2559939.0 and sample count 3274240
 accuracy 78.184 %, best 78.258 %, roc auc score 0.7885, best 0.7885
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 31.87 ms/it, loss 0.450556
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2559939.0
get out
3 has test check 2559939.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 31.99 ms/it, loss 0.447768
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2559939.0
get out
1 has test check 2559939.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 31.92 ms/it, loss 0.449831
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2559939.0
get out
2 has test check 2559939.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 31.79 ms/it, loss 0.452466
Finished training it 63488/76743 of epoch 4, 31.90 ms/it, loss 0.450946
Finished training it 63488/76743 of epoch 4, 31.99 ms/it, loss 0.449631
Finished training it 63488/76743 of epoch 4, 31.92 ms/it, loss 0.455344
Finished training it 63488/76743 of epoch 4, 31.69 ms/it, loss 0.449202
Finished training it 64512/76743 of epoch 4, 39.82 ms/it, loss 0.448126
Finished training it 64512/76743 of epoch 4, 39.72 ms/it, loss 0.450259
Finished training it 64512/76743 of epoch 4, 39.94 ms/it, loss 0.452995
Finished training it 64512/76743 of epoch 4, 39.93 ms/it, loss 0.449035
Finished training it 65536/76743 of epoch 4, 39.77 ms/it, loss 0.450304
Finished training it 65536/76743 of epoch 4, 39.86 ms/it, loss 0.450724
Finished training it 65536/76743 of epoch 4, 39.76 ms/it, loss 0.449607
Finished training it 65536/76743 of epoch 4, 39.93 ms/it, loss 0.451540
Finished training it 66560/76743 of epoch 4, 39.68 ms/it, loss 0.452614
Finished training it 66560/76743 of epoch 4, 39.60 ms/it, loss 0.451299
Finished training it 66560/76743 of epoch 4, 39.56 ms/it, loss 0.452191
Finished training it 66560/76743 of epoch 4, 39.72 ms/it, loss 0.449952
Finished training it 67584/76743 of epoch 4, 39.83 ms/it, loss 0.450758
Finished training it 67584/76743 of epoch 4, 40.12 ms/it, loss 0.450573
Finished training it 67584/76743 of epoch 4, 39.96 ms/it, loss 0.450771
Finished training it 67584/76743 of epoch 4, 40.03 ms/it, loss 0.447362
Finished training it 68608/76743 of epoch 4, 39.40 ms/it, loss 0.452764
Finished training it 68608/76743 of epoch 4, 39.57 ms/it, loss 0.450566
Finished training it 68608/76743 of epoch 4, 39.52 ms/it, loss 0.448961
Finished training it 68608/76743 of epoch 4, 39.37 ms/it, loss 0.452020
Finished training it 69632/76743 of epoch 4, 32.03 ms/it, loss 0.452765
Finished training it 69632/76743 of epoch 4, 31.98 ms/it, loss 0.450878
Finished training it 69632/76743 of epoch 4, 31.91 ms/it, loss 0.450835
Finished training it 69632/76743 of epoch 4, 31.84 ms/it, loss 0.452020
Finished training it 70656/76743 of epoch 4, 32.02 ms/it, loss 0.449557
Finished training it 70656/76743 of epoch 4, 31.84 ms/it, loss 0.453489
Finished training it 70656/76743 of epoch 4, 31.89 ms/it, loss 0.453949
Finished training it 70656/76743 of epoch 4, 31.99 ms/it, loss 0.452707
Finished training it 71680/76743 of epoch 4, 32.11 ms/it, loss 0.448921
Finished training it 71680/76743 of epoch 4, 32.19 ms/it, loss 0.448558
Finished training it 71680/76743 of epoch 4, 32.21 ms/it, loss 0.451692
Finished training it 71680/76743 of epoch 4, 31.98 ms/it, loss 0.448558
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2561550.0
get out
0 has test check 2561550.0 and sample count 3274240
 accuracy 78.233 %, best 78.258 %, roc auc score 0.7880, best 0.7885
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2561550.0
get out
1 has test check 2561550.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 30.62 ms/it, loss 0.451900
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2561550.0
get out
3 has test check 2561550.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 30.82 ms/it, loss 0.451557
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 30.00 ms/it, loss 0.450750
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2561550.0
get out
2 has test check 2561550.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 30.24 ms/it, loss 0.451614
Finished training it 73728/76743 of epoch 4, 30.06 ms/it, loss 0.449278
Finished training it 73728/76743 of epoch 4, 30.09 ms/it, loss 0.449854
Finished training it 73728/76743 of epoch 4, 29.78 ms/it, loss 0.451210
Finished training it 73728/76743 of epoch 4, 30.27 ms/it, loss 0.452098
Finished training it 74752/76743 of epoch 4, 30.27 ms/it, loss 0.448409
Finished training it 74752/76743 of epoch 4, 30.60 ms/it, loss 0.447973
Finished training it 74752/76743 of epoch 4, 30.30 ms/it, loss 0.449769
Finished training it 74752/76743 of epoch 4, 30.33 ms/it, loss 0.450608
Finished training it 75776/76743 of epoch 4, 30.16 ms/it, loss 0.454261
Finished training it 75776/76743 of epoch 4, 30.22 ms/it, loss 0.450387
Finished training it 75776/76743 of epoch 4, 30.29 ms/it, loss 0.450422
Finished training it 75776/76743 of epoch 4, 30.16 ms/it, loss 0.451023
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562004.0
get out
0 has test check 2562004.0 and sample count 3274240
 accuracy 78.247 %, best 78.258 %, roc auc score 0.7884, best 0.7885
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562004.0
get out
1 has test check 2562004.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562004.0
get out
3 has test check 2562004.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562004.0
get out
2 has test check 2562004.0 and sample count 3274240
