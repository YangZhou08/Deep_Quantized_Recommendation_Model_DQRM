Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 64, weight_bit 16
use quant linear, input 64, output 16, weight_bit 16
use quant linear, input 367, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 1, weight_bit 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
Finished training it 1024/76743 of epoch 0, 88.88 ms/it, loss 0.514841
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 64, weight_bit 16
use quant linear, input 64, output 16, weight_bit 16
use quant linear, input 367, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 1, weight_bit 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
Finished training it 1024/76743 of epoch 0, 89.62 ms/it, loss 0.517249
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 64, weight_bit 16
use quant linear, input 64, output 16, weight_bit 16
use quant linear, input 367, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 1, weight_bit 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
Finished training it 1024/76743 of epoch 0, 89.44 ms/it, loss 0.514916
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
use quant linear, input 13, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 64, weight_bit 16
use quant linear, input 64, output 16, weight_bit 16
use quant linear, input 367, output 512, weight_bit 16
use quant linear, input 512, output 256, weight_bit 16
use quant linear, input 256, output 1, weight_bit 16
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
bound increasing to 300
Finished training it 1024/76743 of epoch 0, 89.06 ms/it, loss 0.514877
Finished training it 2048/76743 of epoch 0, 81.82 ms/it, loss 0.499914
Finished training it 2048/76743 of epoch 0, 81.31 ms/it, loss 0.500853
Finished training it 2048/76743 of epoch 0, 81.59 ms/it, loss 0.501030
Finished training it 2048/76743 of epoch 0, 81.39 ms/it, loss 0.499096
Finished training it 3072/76743 of epoch 0, 80.75 ms/it, loss 0.492761
Finished training it 3072/76743 of epoch 0, 81.15 ms/it, loss 0.493546
Finished training it 3072/76743 of epoch 0, 80.95 ms/it, loss 0.492144
Finished training it 3072/76743 of epoch 0, 80.67 ms/it, loss 0.491513
Finished training it 4096/76743 of epoch 0, 80.30 ms/it, loss 0.484247
Finished training it 4096/76743 of epoch 0, 80.92 ms/it, loss 0.486209
Finished training it 4096/76743 of epoch 0, 80.74 ms/it, loss 0.483833
Finished training it 4096/76743 of epoch 0, 80.55 ms/it, loss 0.485262
Finished training it 5120/76743 of epoch 0, 80.05 ms/it, loss 0.479220
Finished training it 5120/76743 of epoch 0, 80.11 ms/it, loss 0.480284
Finished training it 5120/76743 of epoch 0, 80.81 ms/it, loss 0.480097
Finished training it 5120/76743 of epoch 0, 80.18 ms/it, loss 0.478952
Finished training it 6144/76743 of epoch 0, 79.82 ms/it, loss 0.475754
Finished training it 6144/76743 of epoch 0, 80.18 ms/it, loss 0.474470
Finished training it 6144/76743 of epoch 0, 80.11 ms/it, loss 0.474132
Finished training it 6144/76743 of epoch 0, 80.22 ms/it, loss 0.474463
Finished training it 7168/76743 of epoch 0, 79.98 ms/it, loss 0.473273
Finished training it 7168/76743 of epoch 0, 79.73 ms/it, loss 0.470158
Finished training it 7168/76743 of epoch 0, 79.74 ms/it, loss 0.473516
Finished training it 7168/76743 of epoch 0, 80.24 ms/it, loss 0.474048
Finished training it 8192/76743 of epoch 0, 80.40 ms/it, loss 0.471066
Finished training it 8192/76743 of epoch 0, 80.75 ms/it, loss 0.468552
Finished training it 8192/76743 of epoch 0, 80.59 ms/it, loss 0.470603
Finished training it 8192/76743 of epoch 0, 80.57 ms/it, loss 0.472044
Finished training it 9216/76743 of epoch 0, 79.56 ms/it, loss 0.467278
Finished training it 9216/76743 of epoch 0, 79.04 ms/it, loss 0.468258
Finished training it 9216/76743 of epoch 0, 78.92 ms/it, loss 0.468838
Finished training it 9216/76743 of epoch 0, 79.39 ms/it, loss 0.468674
Finished training it 10240/76743 of epoch 0, 79.65 ms/it, loss 0.468029
Finished training it 10240/76743 of epoch 0, 79.76 ms/it, loss 0.469102
Finished training it 10240/76743 of epoch 0, 80.17 ms/it, loss 0.467110
Finished training it 10240/76743 of epoch 0, 80.08 ms/it, loss 0.469284
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2547099.0
get out
0 has test check 2547099.0 and sample count 3274240
 accuracy 77.792 %, best 77.792 %, roc auc score 0.7788, best 0.7788
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2547079.0
get out
3 has test check 2547079.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 81.46 ms/it, loss 0.466134
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2547090.0
get out
1 has test check 2547090.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 81.63 ms/it, loss 0.464955
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 81.14 ms/it, loss 0.467243
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2547086.0
get out
2 has test check 2547086.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 81.20 ms/it, loss 0.465761
Finished training it 12288/76743 of epoch 0, 81.02 ms/it, loss 0.465931
Finished training it 12288/76743 of epoch 0, 80.79 ms/it, loss 0.467102
Finished training it 12288/76743 of epoch 0, 80.93 ms/it, loss 0.466952
Finished training it 12288/76743 of epoch 0, 80.95 ms/it, loss 0.464788
Finished training it 13312/76743 of epoch 0, 91.61 ms/it, loss 0.463661
Finished training it 13312/76743 of epoch 0, 91.29 ms/it, loss 0.467183
Finished training it 13312/76743 of epoch 0, 90.97 ms/it, loss 0.467093
Finished training it 13312/76743 of epoch 0, 91.69 ms/it, loss 0.462160
Finished training it 14336/76743 of epoch 0, 80.89 ms/it, loss 0.463541
Finished training it 14336/76743 of epoch 0, 81.09 ms/it, loss 0.464455
Finished training it 14336/76743 of epoch 0, 81.46 ms/it, loss 0.465531
Finished training it 14336/76743 of epoch 0, 80.88 ms/it, loss 0.462159
Finished training it 15360/76743 of epoch 0, 80.50 ms/it, loss 0.464289
Finished training it 15360/76743 of epoch 0, 80.93 ms/it, loss 0.464993
Finished training it 15360/76743 of epoch 0, 81.30 ms/it, loss 0.463473
Finished training it 15360/76743 of epoch 0, 80.77 ms/it, loss 0.465321
Finished training it 16384/76743 of epoch 0, 81.47 ms/it, loss 0.461074
Finished training it 16384/76743 of epoch 0, 81.61 ms/it, loss 0.462914
Finished training it 16384/76743 of epoch 0, 81.37 ms/it, loss 0.464430
Finished training it 16384/76743 of epoch 0, 81.34 ms/it, loss 0.462685
Finished training it 17408/76743 of epoch 0, 81.80 ms/it, loss 0.462858
Finished training it 17408/76743 of epoch 0, 80.78 ms/it, loss 0.464426
Finished training it 17408/76743 of epoch 0, 81.11 ms/it, loss 0.461888
Finished training it 17408/76743 of epoch 0, 81.08 ms/it, loss 0.463848
Finished training it 18432/76743 of epoch 0, 81.44 ms/it, loss 0.457927
Finished training it 18432/76743 of epoch 0, 81.31 ms/it, loss 0.460501
Finished training it 18432/76743 of epoch 0, 80.92 ms/it, loss 0.459369
Finished training it 18432/76743 of epoch 0, 81.10 ms/it, loss 0.465738
Finished training it 19456/76743 of epoch 0, 81.06 ms/it, loss 0.463098
Finished training it 19456/76743 of epoch 0, 81.13 ms/it, loss 0.463876
Finished training it 19456/76743 of epoch 0, 81.06 ms/it, loss 0.461551
Finished training it 19456/76743 of epoch 0, 81.20 ms/it, loss 0.462683
Finished training it 20480/76743 of epoch 0, 81.48 ms/it, loss 0.463600
Finished training it 20480/76743 of epoch 0, 81.71 ms/it, loss 0.461282
Finished training it 20480/76743 of epoch 0, 81.76 ms/it, loss 0.462644
Finished training it 20480/76743 of epoch 0, 81.72 ms/it, loss 0.461775
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2557685.0
get out
0 has test check 2557685.0 and sample count 3274240
 accuracy 78.115 %, best 78.115 %, roc auc score 0.7867, best 0.7867
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 81.02 ms/it, loss 0.462260
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2557679.0
get out
1 has test check 2557679.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 80.85 ms/it, loss 0.462893
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2557676.0
get out
2 has test check 2557676.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 80.63 ms/it, loss 0.458177
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2557679.0
get out
3 has test check 2557679.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 81.09 ms/it, loss 0.460809
Finished training it 22528/76743 of epoch 0, 82.60 ms/it, loss 0.459929
Finished training it 22528/76743 of epoch 0, 82.18 ms/it, loss 0.461354
Finished training it 22528/76743 of epoch 0, 82.30 ms/it, loss 0.460736
Finished training it 22528/76743 of epoch 0, 82.25 ms/it, loss 0.460708
Finished training it 23552/76743 of epoch 0, 81.96 ms/it, loss 0.459832
Finished training it 23552/76743 of epoch 0, 82.15 ms/it, loss 0.461428
Finished training it 23552/76743 of epoch 0, 82.20 ms/it, loss 0.460584
Finished training it 23552/76743 of epoch 0, 81.77 ms/it, loss 0.459932
Finished training it 24576/76743 of epoch 0, 86.90 ms/it, loss 0.459819
Finished training it 24576/76743 of epoch 0, 86.77 ms/it, loss 0.461326
Finished training it 24576/76743 of epoch 0, 86.46 ms/it, loss 0.458533
Finished training it 24576/76743 of epoch 0, 86.45 ms/it, loss 0.457245
Finished training it 25600/76743 of epoch 0, 91.11 ms/it, loss 0.456134
Finished training it 25600/76743 of epoch 0, 91.45 ms/it, loss 0.460327
Finished training it 25600/76743 of epoch 0, 91.17 ms/it, loss 0.459351
Finished training it 25600/76743 of epoch 0, 91.42 ms/it, loss 0.458498
Finished training it 26624/76743 of epoch 0, 81.55 ms/it, loss 0.458112
Finished training it 26624/76743 of epoch 0, 81.65 ms/it, loss 0.458194
Finished training it 26624/76743 of epoch 0, 81.70 ms/it, loss 0.455672
Finished training it 26624/76743 of epoch 0, 81.64 ms/it, loss 0.460901
Finished training it 27648/76743 of epoch 0, 81.32 ms/it, loss 0.456158
Finished training it 27648/76743 of epoch 0, 81.30 ms/it, loss 0.456375
Finished training it 27648/76743 of epoch 0, 81.22 ms/it, loss 0.456305
Finished training it 27648/76743 of epoch 0, 81.71 ms/it, loss 0.458669
Finished training it 28672/76743 of epoch 0, 81.65 ms/it, loss 0.459434
Finished training it 28672/76743 of epoch 0, 80.88 ms/it, loss 0.461624
Finished training it 28672/76743 of epoch 0, 81.59 ms/it, loss 0.459821
Finished training it 28672/76743 of epoch 0, 81.16 ms/it, loss 0.458973
Finished training it 29696/76743 of epoch 0, 81.33 ms/it, loss 0.456840
Finished training it 29696/76743 of epoch 0, 80.70 ms/it, loss 0.458528
Finished training it 29696/76743 of epoch 0, 81.15 ms/it, loss 0.459262
Finished training it 29696/76743 of epoch 0, 81.10 ms/it, loss 0.459508
Finished training it 30720/76743 of epoch 0, 80.56 ms/it, loss 0.456550
Finished training it 30720/76743 of epoch 0, 80.93 ms/it, loss 0.458344
Finished training it 30720/76743 of epoch 0, 80.44 ms/it, loss 0.456543
Finished training it 30720/76743 of epoch 0, 80.57 ms/it, loss 0.459140
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2562708.0
get out
0 has test check 2562708.0 and sample count 3274240
 accuracy 78.269 %, best 78.269 %, roc auc score 0.7908, best 0.7908
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2562707.0
get out
1 has test check 2562707.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 81.50 ms/it, loss 0.456646
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2562709.0
get out
2 has test check 2562709.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 81.59 ms/it, loss 0.458162
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2562707.0
get out
3 has test check 2562707.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 81.43 ms/it, loss 0.459235
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 81.48 ms/it, loss 0.456326
Finished training it 32768/76743 of epoch 0, 81.80 ms/it, loss 0.457230
Finished training it 32768/76743 of epoch 0, 81.40 ms/it, loss 0.457398
Finished training it 32768/76743 of epoch 0, 81.30 ms/it, loss 0.460051
Finished training it 32768/76743 of epoch 0, 81.48 ms/it, loss 0.457352
Finished training it 33792/76743 of epoch 0, 82.11 ms/it, loss 0.455692
Finished training it 33792/76743 of epoch 0, 82.52 ms/it, loss 0.454836
Finished training it 33792/76743 of epoch 0, 81.67 ms/it, loss 0.454920
Finished training it 33792/76743 of epoch 0, 82.04 ms/it, loss 0.456663
Finished training it 34816/76743 of epoch 0, 80.05 ms/it, loss 0.457087
Finished training it 34816/76743 of epoch 0, 79.98 ms/it, loss 0.458104
Finished training it 34816/76743 of epoch 0, 80.28 ms/it, loss 0.456378
Finished training it 34816/76743 of epoch 0, 80.32 ms/it, loss 0.457764
Finished training it 35840/76743 of epoch 0, 80.73 ms/it, loss 0.455264
Finished training it 35840/76743 of epoch 0, 80.84 ms/it, loss 0.457765
Finished training it 35840/76743 of epoch 0, 81.26 ms/it, loss 0.457574
Finished training it 35840/76743 of epoch 0, 81.23 ms/it, loss 0.454793
Finished training it 36864/76743 of epoch 0, 91.32 ms/it, loss 0.456714
Finished training it 36864/76743 of epoch 0, 91.94 ms/it, loss 0.454416
Finished training it 36864/76743 of epoch 0, 92.02 ms/it, loss 0.454193
Finished training it 36864/76743 of epoch 0, 91.01 ms/it, loss 0.455905
Finished training it 37888/76743 of epoch 0, 80.22 ms/it, loss 0.456927
Finished training it 37888/76743 of epoch 0, 80.69 ms/it, loss 0.456068
Finished training it 37888/76743 of epoch 0, 80.23 ms/it, loss 0.456253
Finished training it 37888/76743 of epoch 0, 80.46 ms/it, loss 0.457143
Finished training it 38912/76743 of epoch 0, 81.88 ms/it, loss 0.457530
Finished training it 38912/76743 of epoch 0, 81.45 ms/it, loss 0.456531
Finished training it 38912/76743 of epoch 0, 81.79 ms/it, loss 0.456558
Finished training it 38912/76743 of epoch 0, 81.16 ms/it, loss 0.455946
Finished training it 39936/76743 of epoch 0, 82.02 ms/it, loss 0.456121
Finished training it 39936/76743 of epoch 0, 81.81 ms/it, loss 0.454313
Finished training it 39936/76743 of epoch 0, 81.46 ms/it, loss 0.453324
Finished training it 39936/76743 of epoch 0, 82.11 ms/it, loss 0.456859
Finished training it 40960/76743 of epoch 0, 82.39 ms/it, loss 0.454819
Finished training it 40960/76743 of epoch 0, 82.03 ms/it, loss 0.452918
Finished training it 40960/76743 of epoch 0, 81.87 ms/it, loss 0.456069
Finished training it 40960/76743 of epoch 0, 82.08 ms/it, loss 0.457983
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2566502.0
get out
0 has test check 2566502.0 and sample count 3274240
 accuracy 78.385 %, best 78.385 %, roc auc score 0.7929, best 0.7929
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2566519.0
get out
1 has test check 2566519.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 81.66 ms/it, loss 0.455938
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2566519.0
get out
3 has test check 2566519.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 81.66 ms/it, loss 0.454778
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2566502.0
get out
2 has test check 2566502.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 81.41 ms/it, loss 0.453276
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 81.33 ms/it, loss 0.457296
Finished training it 43008/76743 of epoch 0, 81.96 ms/it, loss 0.456921
Finished training it 43008/76743 of epoch 0, 81.25 ms/it, loss 0.455490
Finished training it 43008/76743 of epoch 0, 81.42 ms/it, loss 0.454431
Finished training it 43008/76743 of epoch 0, 81.77 ms/it, loss 0.454375
Finished training it 44032/76743 of epoch 0, 86.38 ms/it, loss 0.458579
Finished training it 44032/76743 of epoch 0, 85.92 ms/it, loss 0.455414
Finished training it 44032/76743 of epoch 0, 85.74 ms/it, loss 0.457771
Finished training it 44032/76743 of epoch 0, 86.09 ms/it, loss 0.455094
Finished training it 45056/76743 of epoch 0, 81.23 ms/it, loss 0.454889
Finished training it 45056/76743 of epoch 0, 80.98 ms/it, loss 0.453610
Finished training it 45056/76743 of epoch 0, 80.90 ms/it, loss 0.453494
Finished training it 45056/76743 of epoch 0, 81.09 ms/it, loss 0.451162
Finished training it 46080/76743 of epoch 0, 81.28 ms/it, loss 0.454899
Finished training it 46080/76743 of epoch 0, 81.82 ms/it, loss 0.454831
Finished training it 46080/76743 of epoch 0, 81.99 ms/it, loss 0.450860
Finished training it 46080/76743 of epoch 0, 81.49 ms/it, loss 0.455844
Finished training it 47104/76743 of epoch 0, 81.62 ms/it, loss 0.454874
Finished training it 47104/76743 of epoch 0, 81.03 ms/it, loss 0.454309
Finished training it 47104/76743 of epoch 0, 81.65 ms/it, loss 0.454486
Finished training it 47104/76743 of epoch 0, 81.55 ms/it, loss 0.455449
Finished training it 48128/76743 of epoch 0, 81.40 ms/it, loss 0.453945
Finished training it 48128/76743 of epoch 0, 82.06 ms/it, loss 0.454018
Finished training it 48128/76743 of epoch 0, 81.49 ms/it, loss 0.452263
Finished training it 48128/76743 of epoch 0, 81.63 ms/it, loss 0.452774
Finished training it 49152/76743 of epoch 0, 81.49 ms/it, loss 0.454184
Finished training it 49152/76743 of epoch 0, 81.82 ms/it, loss 0.452975
Finished training it 49152/76743 of epoch 0, 81.65 ms/it, loss 0.452872
Finished training it 49152/76743 of epoch 0, 81.55 ms/it, loss 0.455088
Finished training it 50176/76743 of epoch 0, 80.96 ms/it, loss 0.452273
Finished training it 50176/76743 of epoch 0, 81.23 ms/it, loss 0.452509
Finished training it 50176/76743 of epoch 0, 81.18 ms/it, loss 0.454314
Finished training it 50176/76743 of epoch 0, 81.10 ms/it, loss 0.454416
Finished training it 51200/76743 of epoch 0, 81.19 ms/it, loss 0.453585
Finished training it 51200/76743 of epoch 0, 80.82 ms/it, loss 0.451608
Finished training it 51200/76743 of epoch 0, 81.09 ms/it, loss 0.454312
Finished training it 51200/76743 of epoch 0, 81.06 ms/it, loss 0.453005
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2568951.0
get out
0 has test check 2568951.0 and sample count 3274240
 accuracy 78.459 %, best 78.459 %, roc auc score 0.7944, best 0.7944
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2568947.0
get out
3 has test check 2568947.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 81.09 ms/it, loss 0.453151
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 80.70 ms/it, loss 0.450769
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2568950.0
get out
2 has test check 2568950.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 80.95 ms/it, loss 0.453359
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2568958.0
get out
1 has test check 2568958.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 80.75 ms/it, loss 0.455206
Finished training it 53248/76743 of epoch 0, 81.37 ms/it, loss 0.452695
Finished training it 53248/76743 of epoch 0, 80.85 ms/it, loss 0.454893
Finished training it 53248/76743 of epoch 0, 81.05 ms/it, loss 0.451874
Finished training it 53248/76743 of epoch 0, 81.07 ms/it, loss 0.454418
Finished training it 54272/76743 of epoch 0, 92.93 ms/it, loss 0.456642
Finished training it 54272/76743 of epoch 0, 92.46 ms/it, loss 0.452743
Finished training it 54272/76743 of epoch 0, 91.71 ms/it, loss 0.451196
Finished training it 54272/76743 of epoch 0, 92.61 ms/it, loss 0.455196
Finished training it 55296/76743 of epoch 0, 82.19 ms/it, loss 0.452509
Finished training it 55296/76743 of epoch 0, 82.09 ms/it, loss 0.455964
Finished training it 55296/76743 of epoch 0, 82.40 ms/it, loss 0.453863
Finished training it 55296/76743 of epoch 0, 82.02 ms/it, loss 0.452827
Finished training it 56320/76743 of epoch 0, 87.55 ms/it, loss 0.452666
Finished training it 56320/76743 of epoch 0, 86.95 ms/it, loss 0.451692
Finished training it 56320/76743 of epoch 0, 86.94 ms/it, loss 0.454531
Finished training it 56320/76743 of epoch 0, 86.50 ms/it, loss 0.451939
Finished training it 57344/76743 of epoch 0, 81.76 ms/it, loss 0.453007
Finished training it 57344/76743 of epoch 0, 82.41 ms/it, loss 0.451745
Finished training it 57344/76743 of epoch 0, 82.19 ms/it, loss 0.453972
Finished training it 57344/76743 of epoch 0, 82.30 ms/it, loss 0.452348
Finished training it 58368/76743 of epoch 0, 80.63 ms/it, loss 0.454879
Finished training it 58368/76743 of epoch 0, 81.03 ms/it, loss 0.452365
Finished training it 58368/76743 of epoch 0, 80.88 ms/it, loss 0.452834
Finished training it 58368/76743 of epoch 0, 81.28 ms/it, loss 0.451184
Finished training it 59392/76743 of epoch 0, 81.34 ms/it, loss 0.452750
Finished training it 59392/76743 of epoch 0, 81.40 ms/it, loss 0.452152
Finished training it 59392/76743 of epoch 0, 81.62 ms/it, loss 0.453653
Finished training it 59392/76743 of epoch 0, 81.13 ms/it, loss 0.449847
Finished training it 60416/76743 of epoch 0, 82.27 ms/it, loss 0.449454
Finished training it 60416/76743 of epoch 0, 82.60 ms/it, loss 0.451822
Finished training it 60416/76743 of epoch 0, 82.04 ms/it, loss 0.453060
Finished training it 60416/76743 of epoch 0, 81.85 ms/it, loss 0.451576
Finished training it 61440/76743 of epoch 0, 81.41 ms/it, loss 0.455309
Finished training it 61440/76743 of epoch 0, 81.13 ms/it, loss 0.451040
Finished training it 61440/76743 of epoch 0, 80.92 ms/it, loss 0.452883
Finished training it 61440/76743 of epoch 0, 81.12 ms/it, loss 0.453496
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2570140.0
get out
0 has test check 2570140.0 and sample count 3274240
 accuracy 78.496 %, best 78.496 %, roc auc score 0.7959, best 0.7959
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2570145.0
get out
2 has test check 2570145.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 81.73 ms/it, loss 0.450984
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2570141.0
get out
1 has test check 2570141.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 81.38 ms/it, loss 0.450709
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 81.61 ms/it, loss 0.454770
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2570147.0
get out
3 has test check 2570147.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 81.70 ms/it, loss 0.452020
Finished training it 63488/76743 of epoch 0, 81.61 ms/it, loss 0.450782
Finished training it 63488/76743 of epoch 0, 82.02 ms/it, loss 0.452755
Finished training it 63488/76743 of epoch 0, 81.55 ms/it, loss 0.454712
Finished training it 63488/76743 of epoch 0, 81.60 ms/it, loss 0.450796
Finished training it 64512/76743 of epoch 0, 81.07 ms/it, loss 0.452150
Finished training it 64512/76743 of epoch 0, 80.83 ms/it, loss 0.452682
Finished training it 64512/76743 of epoch 0, 81.68 ms/it, loss 0.453619
Finished training it 64512/76743 of epoch 0, 80.60 ms/it, loss 0.451844
Finished training it 65536/76743 of epoch 0, 92.12 ms/it, loss 0.453192
Finished training it 65536/76743 of epoch 0, 91.82 ms/it, loss 0.452037
Finished training it 65536/76743 of epoch 0, 91.19 ms/it, loss 0.454207
Finished training it 65536/76743 of epoch 0, 92.05 ms/it, loss 0.452159
Finished training it 66560/76743 of epoch 0, 86.23 ms/it, loss 0.450436
Finished training it 66560/76743 of epoch 0, 86.62 ms/it, loss 0.451289
Finished training it 66560/76743 of epoch 0, 86.50 ms/it, loss 0.453351
Finished training it 66560/76743 of epoch 0, 86.34 ms/it, loss 0.453074
Finished training it 67584/76743 of epoch 0, 81.18 ms/it, loss 0.451917
Finished training it 67584/76743 of epoch 0, 81.61 ms/it, loss 0.453679
Finished training it 67584/76743 of epoch 0, 81.26 ms/it, loss 0.452701
Finished training it 67584/76743 of epoch 0, 81.33 ms/it, loss 0.454002
Finished training it 68608/76743 of epoch 0, 81.83 ms/it, loss 0.450405
Finished training it 68608/76743 of epoch 0, 81.56 ms/it, loss 0.449345
Finished training it 68608/76743 of epoch 0, 81.89 ms/it, loss 0.452000
Finished training it 68608/76743 of epoch 0, 81.88 ms/it, loss 0.450943
Finished training it 69632/76743 of epoch 0, 81.62 ms/it, loss 0.452310
Finished training it 69632/76743 of epoch 0, 81.55 ms/it, loss 0.450262
Finished training it 69632/76743 of epoch 0, 81.85 ms/it, loss 0.451473
Finished training it 69632/76743 of epoch 0, 81.88 ms/it, loss 0.453517
Finished training it 70656/76743 of epoch 0, 80.79 ms/it, loss 0.454491
Finished training it 70656/76743 of epoch 0, 80.69 ms/it, loss 0.450778
Finished training it 70656/76743 of epoch 0, 80.54 ms/it, loss 0.450018
Finished training it 70656/76743 of epoch 0, 80.71 ms/it, loss 0.452720
Finished training it 71680/76743 of epoch 0, 81.11 ms/it, loss 0.450487
Finished training it 71680/76743 of epoch 0, 81.42 ms/it, loss 0.455015
Finished training it 71680/76743 of epoch 0, 81.19 ms/it, loss 0.447957
Finished training it 71680/76743 of epoch 0, 81.18 ms/it, loss 0.448165
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2571240.0
get out
0 has test check 2571240.0 and sample count 3274240
 accuracy 78.529 %, best 78.529 %, roc auc score 0.7969, best 0.7969
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2571231.0
get out
1 has test check 2571231.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 81.09 ms/it, loss 0.454760
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 81.11 ms/it, loss 0.450407
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2571229.0
get out
2 has test check 2571229.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 81.12 ms/it, loss 0.450957
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2571228.0
get out
3 has test check 2571228.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 81.38 ms/it, loss 0.451932
Finished training it 73728/76743 of epoch 0, 80.83 ms/it, loss 0.451985
Finished training it 73728/76743 of epoch 0, 81.02 ms/it, loss 0.450679
Finished training it 73728/76743 of epoch 0, 80.64 ms/it, loss 0.450420
Finished training it 73728/76743 of epoch 0, 80.81 ms/it, loss 0.452900
Finished training it 74752/76743 of epoch 0, 81.57 ms/it, loss 0.454904
Finished training it 74752/76743 of epoch 0, 81.40 ms/it, loss 0.451415
Finished training it 74752/76743 of epoch 0, 81.42 ms/it, loss 0.453035
Finished training it 74752/76743 of epoch 0, 81.30 ms/it, loss 0.451189
Finished training it 75776/76743 of epoch 0, 81.09 ms/it, loss 0.450183
Finished training it 75776/76743 of epoch 0, 80.79 ms/it, loss 0.451015
Finished training it 75776/76743 of epoch 0, 81.23 ms/it, loss 0.450507
Finished training it 75776/76743 of epoch 0, 80.88 ms/it, loss 0.450569
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 87.68 ms/it, loss 0.451619
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 87.75 ms/it, loss 0.453500
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 88.15 ms/it, loss 0.453109
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 87.74 ms/it, loss 0.452374
Finished training it 2048/76743 of epoch 1, 81.76 ms/it, loss 0.449598
Finished training it 2048/76743 of epoch 1, 81.63 ms/it, loss 0.450683
Finished training it 2048/76743 of epoch 1, 82.16 ms/it, loss 0.453299
Finished training it 2048/76743 of epoch 1, 81.52 ms/it, loss 0.451341
Finished training it 3072/76743 of epoch 1, 81.28 ms/it, loss 0.449308
Finished training it 3072/76743 of epoch 1, 81.24 ms/it, loss 0.451122
Finished training it 3072/76743 of epoch 1, 81.45 ms/it, loss 0.449757
Finished training it 3072/76743 of epoch 1, 81.51 ms/it, loss 0.451878
Finished training it 4096/76743 of epoch 1, 81.60 ms/it, loss 0.449986
Finished training it 4096/76743 of epoch 1, 81.66 ms/it, loss 0.451456
Finished training it 4096/76743 of epoch 1, 81.57 ms/it, loss 0.448901
Finished training it 4096/76743 of epoch 1, 82.16 ms/it, loss 0.451191
Finished training it 5120/76743 of epoch 1, 81.41 ms/it, loss 0.451434
Finished training it 5120/76743 of epoch 1, 81.16 ms/it, loss 0.449367
Finished training it 5120/76743 of epoch 1, 81.28 ms/it, loss 0.449444
Finished training it 5120/76743 of epoch 1, 81.02 ms/it, loss 0.450034
Finished training it 6144/76743 of epoch 1, 81.48 ms/it, loss 0.448902
Finished training it 6144/76743 of epoch 1, 81.12 ms/it, loss 0.449839
Finished training it 6144/76743 of epoch 1, 81.89 ms/it, loss 0.448715
Finished training it 6144/76743 of epoch 1, 81.19 ms/it, loss 0.452110
Finished training it 7168/76743 of epoch 1, 80.28 ms/it, loss 0.451224
Finished training it 7168/76743 of epoch 1, 80.48 ms/it, loss 0.448213
Finished training it 7168/76743 of epoch 1, 80.55 ms/it, loss 0.451544
Finished training it 7168/76743 of epoch 1, 79.93 ms/it, loss 0.450601
Finished training it 8192/76743 of epoch 1, 81.99 ms/it, loss 0.451054
Finished training it 8192/76743 of epoch 1, 82.01 ms/it, loss 0.448088
Finished training it 8192/76743 of epoch 1, 82.51 ms/it, loss 0.449453
Finished training it 8192/76743 of epoch 1, 81.66 ms/it, loss 0.450643
Finished training it 9216/76743 of epoch 1, 81.69 ms/it, loss 0.449064
Finished training it 9216/76743 of epoch 1, 81.49 ms/it, loss 0.449475
Finished training it 9216/76743 of epoch 1, 81.17 ms/it, loss 0.448695
Finished training it 9216/76743 of epoch 1, 81.29 ms/it, loss 0.447828
Finished training it 10240/76743 of epoch 1, 81.12 ms/it, loss 0.450293
Finished training it 10240/76743 of epoch 1, 80.45 ms/it, loss 0.451014
Finished training it 10240/76743 of epoch 1, 81.22 ms/it, loss 0.451888
Finished training it 10240/76743 of epoch 1, 81.40 ms/it, loss 0.449816
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575093.0
get out
0 has test check 2575093.0 and sample count 3274240
 accuracy 78.647 %, best 78.647 %, roc auc score 0.7975, best 0.7975
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575088.0
get out
1 has test check 2575088.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 80.73 ms/it, loss 0.447561
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575081.0
get out
2 has test check 2575081.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 80.58 ms/it, loss 0.448551
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575090.0
get out
3 has test check 2575090.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 80.93 ms/it, loss 0.449283
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 80.70 ms/it, loss 0.449765
Finished training it 12288/76743 of epoch 1, 81.77 ms/it, loss 0.451534
Finished training it 12288/76743 of epoch 1, 81.32 ms/it, loss 0.450628
Finished training it 12288/76743 of epoch 1, 81.49 ms/it, loss 0.449418
Finished training it 12288/76743 of epoch 1, 81.39 ms/it, loss 0.447676
Finished training it 13312/76743 of epoch 1, 81.78 ms/it, loss 0.448141
Finished training it 13312/76743 of epoch 1, 81.87 ms/it, loss 0.452483
Finished training it 13312/76743 of epoch 1, 81.92 ms/it, loss 0.451943
Finished training it 13312/76743 of epoch 1, 82.05 ms/it, loss 0.446033
Finished training it 14336/76743 of epoch 1, 93.19 ms/it, loss 0.449573
Finished training it 14336/76743 of epoch 1, 93.34 ms/it, loss 0.450651
Finished training it 14336/76743 of epoch 1, 92.98 ms/it, loss 0.448472
Finished training it 14336/76743 of epoch 1, 92.82 ms/it, loss 0.446601
Finished training it 15360/76743 of epoch 1, 81.94 ms/it, loss 0.450013
Finished training it 15360/76743 of epoch 1, 81.56 ms/it, loss 0.449153
Finished training it 15360/76743 of epoch 1, 81.87 ms/it, loss 0.450984
Finished training it 15360/76743 of epoch 1, 81.22 ms/it, loss 0.450343
Finished training it 16384/76743 of epoch 1, 82.22 ms/it, loss 0.447747
Finished training it 16384/76743 of epoch 1, 82.45 ms/it, loss 0.448983
Finished training it 16384/76743 of epoch 1, 82.06 ms/it, loss 0.449063
Finished training it 16384/76743 of epoch 1, 81.78 ms/it, loss 0.449789
Finished training it 17408/76743 of epoch 1, 80.90 ms/it, loss 0.451063
Finished training it 17408/76743 of epoch 1, 81.00 ms/it, loss 0.451003
Finished training it 17408/76743 of epoch 1, 80.90 ms/it, loss 0.448937
Finished training it 17408/76743 of epoch 1, 80.96 ms/it, loss 0.449622
Finished training it 18432/76743 of epoch 1, 81.67 ms/it, loss 0.445165
Finished training it 18432/76743 of epoch 1, 81.39 ms/it, loss 0.446770
Finished training it 18432/76743 of epoch 1, 81.26 ms/it, loss 0.448130
Finished training it 18432/76743 of epoch 1, 81.65 ms/it, loss 0.452600
Finished training it 19456/76743 of epoch 1, 81.05 ms/it, loss 0.450985
Finished training it 19456/76743 of epoch 1, 80.82 ms/it, loss 0.449474
Finished training it 19456/76743 of epoch 1, 80.67 ms/it, loss 0.450446
Finished training it 19456/76743 of epoch 1, 81.01 ms/it, loss 0.451166
Finished training it 20480/76743 of epoch 1, 82.60 ms/it, loss 0.449332
Finished training it 20480/76743 of epoch 1, 82.62 ms/it, loss 0.449512
Finished training it 20480/76743 of epoch 1, 82.61 ms/it, loss 0.451800
Finished training it 20480/76743 of epoch 1, 82.08 ms/it, loss 0.449938
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575993.0
get out
0 has test check 2575993.0 and sample count 3274240
 accuracy 78.675 %, best 78.675 %, roc auc score 0.7982, best 0.7982
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 80.99 ms/it, loss 0.450177
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575992.0
get out
2 has test check 2575992.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 81.41 ms/it, loss 0.446629
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575994.0
get out
3 has test check 2575994.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 81.62 ms/it, loss 0.449514
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2576000.0
get out
1 has test check 2576000.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 81.46 ms/it, loss 0.451900
Finished training it 22528/76743 of epoch 1, 81.25 ms/it, loss 0.450098
Finished training it 22528/76743 of epoch 1, 81.11 ms/it, loss 0.449483
Finished training it 22528/76743 of epoch 1, 80.81 ms/it, loss 0.449838
Finished training it 22528/76743 of epoch 1, 81.49 ms/it, loss 0.448505
Finished training it 23552/76743 of epoch 1, 81.74 ms/it, loss 0.450031
Finished training it 23552/76743 of epoch 1, 81.49 ms/it, loss 0.449978
Finished training it 23552/76743 of epoch 1, 81.62 ms/it, loss 0.450137
Finished training it 23552/76743 of epoch 1, 81.88 ms/it, loss 0.449100
Finished training it 24576/76743 of epoch 1, 82.04 ms/it, loss 0.451781
Finished training it 24576/76743 of epoch 1, 81.63 ms/it, loss 0.448380
Finished training it 24576/76743 of epoch 1, 81.11 ms/it, loss 0.449597
Finished training it 24576/76743 of epoch 1, 81.72 ms/it, loss 0.446297
Finished training it 25600/76743 of epoch 1, 80.73 ms/it, loss 0.449250
Finished training it 25600/76743 of epoch 1, 80.58 ms/it, loss 0.445408
Finished training it 25600/76743 of epoch 1, 80.28 ms/it, loss 0.447783
Finished training it 25600/76743 of epoch 1, 81.07 ms/it, loss 0.449361
Finished training it 26624/76743 of epoch 1, 81.52 ms/it, loss 0.447517
Finished training it 26624/76743 of epoch 1, 81.86 ms/it, loss 0.450637
Finished training it 26624/76743 of epoch 1, 81.80 ms/it, loss 0.448013
Finished training it 26624/76743 of epoch 1, 81.97 ms/it, loss 0.445529
Finished training it 27648/76743 of epoch 1, 82.49 ms/it, loss 0.448745
Finished training it 27648/76743 of epoch 1, 82.40 ms/it, loss 0.446777
Finished training it 27648/76743 of epoch 1, 82.50 ms/it, loss 0.445816
Finished training it 27648/76743 of epoch 1, 82.01 ms/it, loss 0.446609
Finished training it 28672/76743 of epoch 1, 81.21 ms/it, loss 0.452067
Finished training it 28672/76743 of epoch 1, 81.11 ms/it, loss 0.450410
Finished training it 28672/76743 of epoch 1, 81.39 ms/it, loss 0.449495
Finished training it 28672/76743 of epoch 1, 81.24 ms/it, loss 0.449777
Finished training it 29696/76743 of epoch 1, 82.13 ms/it, loss 0.447728
Finished training it 29696/76743 of epoch 1, 81.48 ms/it, loss 0.449350
Finished training it 29696/76743 of epoch 1, 81.59 ms/it, loss 0.449169
Finished training it 29696/76743 of epoch 1, 81.79 ms/it, loss 0.449830
Finished training it 30720/76743 of epoch 1, 81.53 ms/it, loss 0.447836
Finished training it 30720/76743 of epoch 1, 81.38 ms/it, loss 0.447897
Finished training it 30720/76743 of epoch 1, 81.40 ms/it, loss 0.450053
Finished training it 30720/76743 of epoch 1, 81.82 ms/it, loss 0.449504
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2575977.0
get out
0 has test check 2575977.0 and sample count 3274240
 accuracy 78.674 %, best 78.675 %, roc auc score 0.7987, best 0.7987
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2575986.0
get out
1 has test check 2575986.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 81.34 ms/it, loss 0.447764
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2575985.0
get out
2 has test check 2575985.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 81.66 ms/it, loss 0.449033
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2575988.0
get out
3 has test check 2575988.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 81.77 ms/it, loss 0.450265
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 80.89 ms/it, loss 0.446694
Finished training it 32768/76743 of epoch 1, 82.06 ms/it, loss 0.448544
Finished training it 32768/76743 of epoch 1, 82.16 ms/it, loss 0.451447
Finished training it 32768/76743 of epoch 1, 81.99 ms/it, loss 0.448338
Finished training it 32768/76743 of epoch 1, 81.56 ms/it, loss 0.448285
Finished training it 33792/76743 of epoch 1, 91.04 ms/it, loss 0.446489
Finished training it 33792/76743 of epoch 1, 91.24 ms/it, loss 0.447615
Finished training it 33792/76743 of epoch 1, 91.35 ms/it, loss 0.447200
Finished training it 33792/76743 of epoch 1, 91.20 ms/it, loss 0.446023
Finished training it 34816/76743 of epoch 1, 80.96 ms/it, loss 0.449495
Finished training it 34816/76743 of epoch 1, 80.78 ms/it, loss 0.448191
Finished training it 34816/76743 of epoch 1, 80.82 ms/it, loss 0.449949
Finished training it 34816/76743 of epoch 1, 81.15 ms/it, loss 0.447892
Finished training it 35840/76743 of epoch 1, 81.19 ms/it, loss 0.446039
Finished training it 35840/76743 of epoch 1, 81.23 ms/it, loss 0.449175
Finished training it 35840/76743 of epoch 1, 81.08 ms/it, loss 0.447042
Finished training it 35840/76743 of epoch 1, 80.48 ms/it, loss 0.449291
Finished training it 36864/76743 of epoch 1, 81.76 ms/it, loss 0.445559
Finished training it 36864/76743 of epoch 1, 81.77 ms/it, loss 0.445923
Finished training it 36864/76743 of epoch 1, 81.98 ms/it, loss 0.447985
Finished training it 36864/76743 of epoch 1, 81.41 ms/it, loss 0.447016
Finished training it 37888/76743 of epoch 1, 81.21 ms/it, loss 0.448622
Finished training it 37888/76743 of epoch 1, 80.61 ms/it, loss 0.448617
Finished training it 37888/76743 of epoch 1, 80.75 ms/it, loss 0.447442
Finished training it 37888/76743 of epoch 1, 81.11 ms/it, loss 0.448241
Finished training it 38912/76743 of epoch 1, 81.99 ms/it, loss 0.448021
Finished training it 38912/76743 of epoch 1, 81.62 ms/it, loss 0.449980
Finished training it 38912/76743 of epoch 1, 81.77 ms/it, loss 0.448919
Finished training it 38912/76743 of epoch 1, 81.79 ms/it, loss 0.448805
Finished training it 39936/76743 of epoch 1, 82.37 ms/it, loss 0.448803
Finished training it 39936/76743 of epoch 1, 82.38 ms/it, loss 0.448473
Finished training it 39936/76743 of epoch 1, 82.30 ms/it, loss 0.446859
Finished training it 39936/76743 of epoch 1, 82.77 ms/it, loss 0.445745
Finished training it 40960/76743 of epoch 1, 81.35 ms/it, loss 0.448077
Finished training it 40960/76743 of epoch 1, 81.28 ms/it, loss 0.445373
Finished training it 40960/76743 of epoch 1, 81.71 ms/it, loss 0.450534
Finished training it 40960/76743 of epoch 1, 81.85 ms/it, loss 0.447137
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577365.0
get out
0 has test check 2577365.0 and sample count 3274240
 accuracy 78.716 %, best 78.716 %, roc auc score 0.7993, best 0.7993
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577369.0
get out
2 has test check 2577369.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 81.82 ms/it, loss 0.445716
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577365.0
get out
3 has test check 2577365.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 82.07 ms/it, loss 0.447205
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 81.86 ms/it, loss 0.449545
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577361.0
get out
1 has test check 2577361.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 82.21 ms/it, loss 0.448405
Finished training it 43008/76743 of epoch 1, 81.38 ms/it, loss 0.448336
Finished training it 43008/76743 of epoch 1, 81.90 ms/it, loss 0.449434
Finished training it 43008/76743 of epoch 1, 81.38 ms/it, loss 0.447200
Finished training it 43008/76743 of epoch 1, 81.72 ms/it, loss 0.447229
Finished training it 44032/76743 of epoch 1, 81.50 ms/it, loss 0.447483
Finished training it 44032/76743 of epoch 1, 80.91 ms/it, loss 0.450228
Finished training it 44032/76743 of epoch 1, 80.83 ms/it, loss 0.448021
Finished training it 44032/76743 of epoch 1, 80.55 ms/it, loss 0.451790
Finished training it 45056/76743 of epoch 1, 86.65 ms/it, loss 0.447933
Finished training it 45056/76743 of epoch 1, 86.57 ms/it, loss 0.446167
Finished training it 45056/76743 of epoch 1, 86.91 ms/it, loss 0.443991
Finished training it 45056/76743 of epoch 1, 86.45 ms/it, loss 0.446070
Finished training it 46080/76743 of epoch 1, 82.62 ms/it, loss 0.447824
Finished training it 46080/76743 of epoch 1, 82.53 ms/it, loss 0.448619
Finished training it 46080/76743 of epoch 1, 82.51 ms/it, loss 0.443982
Finished training it 46080/76743 of epoch 1, 82.40 ms/it, loss 0.447852
Finished training it 47104/76743 of epoch 1, 81.30 ms/it, loss 0.447617
Finished training it 47104/76743 of epoch 1, 81.31 ms/it, loss 0.447984
Finished training it 47104/76743 of epoch 1, 81.61 ms/it, loss 0.447332
Finished training it 47104/76743 of epoch 1, 81.85 ms/it, loss 0.448225
Finished training it 48128/76743 of epoch 1, 81.87 ms/it, loss 0.446842
Finished training it 48128/76743 of epoch 1, 82.39 ms/it, loss 0.446052
Finished training it 48128/76743 of epoch 1, 81.77 ms/it, loss 0.447242
Finished training it 48128/76743 of epoch 1, 82.69 ms/it, loss 0.445524
Finished training it 49152/76743 of epoch 1, 80.85 ms/it, loss 0.447283
Finished training it 49152/76743 of epoch 1, 81.45 ms/it, loss 0.448126
Finished training it 49152/76743 of epoch 1, 81.62 ms/it, loss 0.445434
Finished training it 49152/76743 of epoch 1, 81.24 ms/it, loss 0.446306
Finished training it 50176/76743 of epoch 1, 80.80 ms/it, loss 0.445943
Finished training it 50176/76743 of epoch 1, 81.29 ms/it, loss 0.445517
Finished training it 50176/76743 of epoch 1, 81.39 ms/it, loss 0.447551
Finished training it 50176/76743 of epoch 1, 81.29 ms/it, loss 0.447514
Finished training it 51200/76743 of epoch 1, 81.94 ms/it, loss 0.447497
Finished training it 51200/76743 of epoch 1, 81.82 ms/it, loss 0.444878
Finished training it 51200/76743 of epoch 1, 81.77 ms/it, loss 0.446419
Finished training it 51200/76743 of epoch 1, 81.62 ms/it, loss 0.447257
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577543.0
get out
0 has test check 2577543.0 and sample count 3274240
 accuracy 78.722 %, best 78.722 %, roc auc score 0.7997, best 0.7997
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577539.0
get out
2 has test check 2577539.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 81.22 ms/it, loss 0.446796
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 81.39 ms/it, loss 0.444251
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577543.0
get out
1 has test check 2577543.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 81.02 ms/it, loss 0.448504
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577550.0
get out
3 has test check 2577550.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 81.42 ms/it, loss 0.446317
Finished training it 53248/76743 of epoch 1, 82.45 ms/it, loss 0.445448
Finished training it 53248/76743 of epoch 1, 82.70 ms/it, loss 0.446235
Finished training it 53248/76743 of epoch 1, 82.57 ms/it, loss 0.448178
Finished training it 53248/76743 of epoch 1, 82.13 ms/it, loss 0.448111
Finished training it 54272/76743 of epoch 1, 89.11 ms/it, loss 0.449237
Finished training it 54272/76743 of epoch 1, 88.66 ms/it, loss 0.450472
Finished training it 54272/76743 of epoch 1, 88.62 ms/it, loss 0.446439
Finished training it 54272/76743 of epoch 1, 87.73 ms/it, loss 0.445119
Finished training it 55296/76743 of epoch 1, 81.46 ms/it, loss 0.447389
Finished training it 55296/76743 of epoch 1, 81.17 ms/it, loss 0.446682
Finished training it 55296/76743 of epoch 1, 81.15 ms/it, loss 0.445954
Finished training it 55296/76743 of epoch 1, 81.48 ms/it, loss 0.449865
Finished training it 56320/76743 of epoch 1, 81.21 ms/it, loss 0.448389
Finished training it 56320/76743 of epoch 1, 80.96 ms/it, loss 0.445577
Finished training it 56320/76743 of epoch 1, 81.02 ms/it, loss 0.445874
Finished training it 56320/76743 of epoch 1, 81.39 ms/it, loss 0.446281
Finished training it 57344/76743 of epoch 1, 80.21 ms/it, loss 0.446847
Finished training it 57344/76743 of epoch 1, 80.72 ms/it, loss 0.445583
Finished training it 57344/76743 of epoch 1, 80.10 ms/it, loss 0.447313
Finished training it 57344/76743 of epoch 1, 80.73 ms/it, loss 0.446260
Finished training it 58368/76743 of epoch 1, 81.14 ms/it, loss 0.445330
Finished training it 58368/76743 of epoch 1, 80.53 ms/it, loss 0.446464
Finished training it 58368/76743 of epoch 1, 80.65 ms/it, loss 0.449003
Finished training it 58368/76743 of epoch 1, 80.70 ms/it, loss 0.447094
Finished training it 59392/76743 of epoch 1, 80.97 ms/it, loss 0.446457
Finished training it 59392/76743 of epoch 1, 81.26 ms/it, loss 0.443922
Finished training it 59392/76743 of epoch 1, 81.43 ms/it, loss 0.447739
Finished training it 59392/76743 of epoch 1, 81.07 ms/it, loss 0.445954
Finished training it 60416/76743 of epoch 1, 80.68 ms/it, loss 0.445592
Finished training it 60416/76743 of epoch 1, 80.67 ms/it, loss 0.446727
Finished training it 60416/76743 of epoch 1, 80.60 ms/it, loss 0.443227
Finished training it 60416/76743 of epoch 1, 81.24 ms/it, loss 0.445964
Finished training it 61440/76743 of epoch 1, 82.04 ms/it, loss 0.449449
Finished training it 61440/76743 of epoch 1, 82.40 ms/it, loss 0.447968
Finished training it 61440/76743 of epoch 1, 81.68 ms/it, loss 0.444920
Finished training it 61440/76743 of epoch 1, 81.86 ms/it, loss 0.447425
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2578575.0
get out
0 has test check 2578575.0 and sample count 3274240
 accuracy 78.753 %, best 78.753 %, roc auc score 0.8008, best 0.8008
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2578579.0
get out
2 has test check 2578579.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 81.09 ms/it, loss 0.445438
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2578578.0
get out
1 has test check 2578578.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 81.40 ms/it, loss 0.445077
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 80.74 ms/it, loss 0.449014
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2578587.0
get out
3 has test check 2578587.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 81.09 ms/it, loss 0.446366
Finished training it 63488/76743 of epoch 1, 81.26 ms/it, loss 0.449395
Finished training it 63488/76743 of epoch 1, 81.62 ms/it, loss 0.444563
Finished training it 63488/76743 of epoch 1, 81.63 ms/it, loss 0.446974
Finished training it 63488/76743 of epoch 1, 81.78 ms/it, loss 0.444701
Finished training it 64512/76743 of epoch 1, 86.41 ms/it, loss 0.446178
Finished training it 64512/76743 of epoch 1, 85.94 ms/it, loss 0.446961
Finished training it 64512/76743 of epoch 1, 85.97 ms/it, loss 0.446624
Finished training it 64512/76743 of epoch 1, 86.07 ms/it, loss 0.447853
Finished training it 65536/76743 of epoch 1, 80.79 ms/it, loss 0.447394
Finished training it 65536/76743 of epoch 1, 80.51 ms/it, loss 0.448500
Finished training it 65536/76743 of epoch 1, 80.88 ms/it, loss 0.446372
Finished training it 65536/76743 of epoch 1, 81.02 ms/it, loss 0.446168
Finished training it 66560/76743 of epoch 1, 80.81 ms/it, loss 0.447069
Finished training it 66560/76743 of epoch 1, 80.96 ms/it, loss 0.444514
Finished training it 66560/76743 of epoch 1, 81.02 ms/it, loss 0.445685
Finished training it 66560/76743 of epoch 1, 80.70 ms/it, loss 0.447684
Finished training it 67584/76743 of epoch 1, 81.79 ms/it, loss 0.446394
Finished training it 67584/76743 of epoch 1, 82.38 ms/it, loss 0.447201
Finished training it 67584/76743 of epoch 1, 81.74 ms/it, loss 0.448632
Finished training it 67584/76743 of epoch 1, 81.85 ms/it, loss 0.448494
Finished training it 68608/76743 of epoch 1, 80.64 ms/it, loss 0.443690
Finished training it 68608/76743 of epoch 1, 80.33 ms/it, loss 0.446604
Finished training it 68608/76743 of epoch 1, 81.00 ms/it, loss 0.445374
Finished training it 68608/76743 of epoch 1, 80.70 ms/it, loss 0.445520
Finished training it 69632/76743 of epoch 1, 81.23 ms/it, loss 0.446836
Finished training it 69632/76743 of epoch 1, 80.73 ms/it, loss 0.444967
Finished training it 69632/76743 of epoch 1, 81.06 ms/it, loss 0.447904
Finished training it 69632/76743 of epoch 1, 80.92 ms/it, loss 0.446331
Finished training it 70656/76743 of epoch 1, 81.11 ms/it, loss 0.445054
Finished training it 70656/76743 of epoch 1, 81.67 ms/it, loss 0.448957
Finished training it 70656/76743 of epoch 1, 81.45 ms/it, loss 0.444750
Finished training it 70656/76743 of epoch 1, 81.71 ms/it, loss 0.447296
Finished training it 71680/76743 of epoch 1, 80.23 ms/it, loss 0.442775
Finished training it 71680/76743 of epoch 1, 80.20 ms/it, loss 0.449935
Finished training it 71680/76743 of epoch 1, 80.35 ms/it, loss 0.445504
Finished training it 71680/76743 of epoch 1, 80.05 ms/it, loss 0.442608
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2577237.0
get out
0 has test check 2577237.0 and sample count 3274240
 accuracy 78.713 %, best 78.753 %, roc auc score 0.8010, best 0.8010
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2577248.0
get out
2 has test check 2577248.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 82.56 ms/it, loss 0.445309
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2577242.0
get out
3 has test check 2577242.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 82.56 ms/it, loss 0.446765
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 82.29 ms/it, loss 0.445124
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2577258.0
get out
1 has test check 2577258.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 82.86 ms/it, loss 0.449452
Finished training it 73728/76743 of epoch 1, 81.18 ms/it, loss 0.446962
Finished training it 73728/76743 of epoch 1, 81.29 ms/it, loss 0.445082
Finished training it 73728/76743 of epoch 1, 81.26 ms/it, loss 0.447593
Finished training it 73728/76743 of epoch 1, 81.38 ms/it, loss 0.445152
Finished training it 74752/76743 of epoch 1, 86.89 ms/it, loss 0.447735
Finished training it 74752/76743 of epoch 1, 86.99 ms/it, loss 0.449623
Finished training it 74752/76743 of epoch 1, 86.88 ms/it, loss 0.445854
Finished training it 74752/76743 of epoch 1, 87.07 ms/it, loss 0.446183
Finished training it 75776/76743 of epoch 1, 80.75 ms/it, loss 0.445514
Finished training it 75776/76743 of epoch 1, 80.82 ms/it, loss 0.446166
Finished training it 75776/76743 of epoch 1, 81.16 ms/it, loss 0.445301
Finished training it 75776/76743 of epoch 1, 80.76 ms/it, loss 0.445259
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 85.15 ms/it, loss 0.447051
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 85.13 ms/it, loss 0.446330
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 85.03 ms/it, loss 0.448077
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 85.46 ms/it, loss 0.447927
Finished training it 2048/76743 of epoch 2, 81.18 ms/it, loss 0.446493
Finished training it 2048/76743 of epoch 2, 81.04 ms/it, loss 0.445635
Finished training it 2048/76743 of epoch 2, 81.19 ms/it, loss 0.448718
Finished training it 2048/76743 of epoch 2, 81.16 ms/it, loss 0.444357
Finished training it 3072/76743 of epoch 2, 80.93 ms/it, loss 0.446733
Finished training it 3072/76743 of epoch 2, 81.07 ms/it, loss 0.444785
Finished training it 3072/76743 of epoch 2, 80.81 ms/it, loss 0.444493
Finished training it 3072/76743 of epoch 2, 80.87 ms/it, loss 0.446053
Finished training it 4096/76743 of epoch 2, 81.27 ms/it, loss 0.446210
Finished training it 4096/76743 of epoch 2, 81.22 ms/it, loss 0.443821
Finished training it 4096/76743 of epoch 2, 81.27 ms/it, loss 0.445069
Finished training it 4096/76743 of epoch 2, 81.48 ms/it, loss 0.446396
Finished training it 5120/76743 of epoch 2, 81.69 ms/it, loss 0.446854
Finished training it 5120/76743 of epoch 2, 81.71 ms/it, loss 0.444426
Finished training it 5120/76743 of epoch 2, 81.07 ms/it, loss 0.444434
Finished training it 5120/76743 of epoch 2, 81.48 ms/it, loss 0.445250
Finished training it 6144/76743 of epoch 2, 81.18 ms/it, loss 0.444991
Finished training it 6144/76743 of epoch 2, 81.77 ms/it, loss 0.444251
Finished training it 6144/76743 of epoch 2, 80.94 ms/it, loss 0.446756
Finished training it 6144/76743 of epoch 2, 81.86 ms/it, loss 0.444346
Finished training it 7168/76743 of epoch 2, 81.75 ms/it, loss 0.446752
Finished training it 7168/76743 of epoch 2, 81.87 ms/it, loss 0.443587
Finished training it 7168/76743 of epoch 2, 81.53 ms/it, loss 0.446189
Finished training it 7168/76743 of epoch 2, 81.37 ms/it, loss 0.445867
Finished training it 8192/76743 of epoch 2, 81.47 ms/it, loss 0.443436
Finished training it 8192/76743 of epoch 2, 81.81 ms/it, loss 0.446589
Finished training it 8192/76743 of epoch 2, 81.18 ms/it, loss 0.446118
Finished training it 8192/76743 of epoch 2, 81.82 ms/it, loss 0.444588
Finished training it 9216/76743 of epoch 2, 82.01 ms/it, loss 0.444685
Finished training it 9216/76743 of epoch 2, 81.89 ms/it, loss 0.443116
Finished training it 9216/76743 of epoch 2, 81.85 ms/it, loss 0.444451
Finished training it 9216/76743 of epoch 2, 81.33 ms/it, loss 0.443926
Finished training it 10240/76743 of epoch 2, 80.77 ms/it, loss 0.445208
Finished training it 10240/76743 of epoch 2, 80.78 ms/it, loss 0.447260
Finished training it 10240/76743 of epoch 2, 80.67 ms/it, loss 0.446318
Finished training it 10240/76743 of epoch 2, 80.37 ms/it, loss 0.445441
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579973.0
get out
0 has test check 2579973.0 and sample count 3274240
 accuracy 78.796 %, best 78.796 %, roc auc score 0.8009, best 0.8010
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579964.0
get out
1 has test check 2579964.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 81.13 ms/it, loss 0.442903
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579979.0
get out
3 has test check 2579979.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 80.83 ms/it, loss 0.444461
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579955.0
get out
2 has test check 2579955.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 80.82 ms/it, loss 0.444073
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 80.87 ms/it, loss 0.445340
Finished training it 12288/76743 of epoch 2, 81.11 ms/it, loss 0.442807
Finished training it 12288/76743 of epoch 2, 81.27 ms/it, loss 0.446600
Finished training it 12288/76743 of epoch 2, 80.78 ms/it, loss 0.445990
Finished training it 12288/76743 of epoch 2, 81.13 ms/it, loss 0.444988
Finished training it 13312/76743 of epoch 2, 80.80 ms/it, loss 0.443604
Finished training it 13312/76743 of epoch 2, 81.24 ms/it, loss 0.447244
Finished training it 13312/76743 of epoch 2, 81.14 ms/it, loss 0.441371
Finished training it 13312/76743 of epoch 2, 80.76 ms/it, loss 0.447871
Finished training it 14336/76743 of epoch 2, 95.74 ms/it, loss 0.445809
Finished training it 14336/76743 of epoch 2, 95.94 ms/it, loss 0.444402
Finished training it 14336/76743 of epoch 2, 96.34 ms/it, loss 0.441773
Finished training it 14336/76743 of epoch 2, 95.32 ms/it, loss 0.444179
Finished training it 15360/76743 of epoch 2, 80.90 ms/it, loss 0.444998
Finished training it 15360/76743 of epoch 2, 80.72 ms/it, loss 0.445507
Finished training it 15360/76743 of epoch 2, 80.76 ms/it, loss 0.446563
Finished training it 15360/76743 of epoch 2, 80.67 ms/it, loss 0.445555
Finished training it 16384/76743 of epoch 2, 80.53 ms/it, loss 0.443328
Finished training it 16384/76743 of epoch 2, 80.37 ms/it, loss 0.445326
Finished training it 16384/76743 of epoch 2, 80.72 ms/it, loss 0.444445
Finished training it 16384/76743 of epoch 2, 80.56 ms/it, loss 0.444614
Finished training it 17408/76743 of epoch 2, 80.57 ms/it, loss 0.446897
Finished training it 17408/76743 of epoch 2, 80.87 ms/it, loss 0.444967
Finished training it 17408/76743 of epoch 2, 80.69 ms/it, loss 0.444579
Finished training it 17408/76743 of epoch 2, 80.69 ms/it, loss 0.446629
Finished training it 18432/76743 of epoch 2, 81.91 ms/it, loss 0.442359
Finished training it 18432/76743 of epoch 2, 81.81 ms/it, loss 0.443409
Finished training it 18432/76743 of epoch 2, 81.78 ms/it, loss 0.440547
Finished training it 18432/76743 of epoch 2, 81.96 ms/it, loss 0.448133
Finished training it 19456/76743 of epoch 2, 81.48 ms/it, loss 0.445046
Finished training it 19456/76743 of epoch 2, 81.56 ms/it, loss 0.446484
Finished training it 19456/76743 of epoch 2, 80.87 ms/it, loss 0.446095
Finished training it 19456/76743 of epoch 2, 81.11 ms/it, loss 0.446376
Finished training it 20480/76743 of epoch 2, 81.51 ms/it, loss 0.445376
Finished training it 20480/76743 of epoch 2, 81.56 ms/it, loss 0.444421
Finished training it 20480/76743 of epoch 2, 81.67 ms/it, loss 0.447572
Finished training it 20480/76743 of epoch 2, 81.39 ms/it, loss 0.445101
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580878.0
get out
0 has test check 2580878.0 and sample count 3274240
 accuracy 78.824 %, best 78.824 %, roc auc score 0.8013, best 0.8013
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580884.0
get out
3 has test check 2580884.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 82.44 ms/it, loss 0.445010
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580876.0
get out
2 has test check 2580876.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 81.80 ms/it, loss 0.442399
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 81.80 ms/it, loss 0.445835
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580876.0
get out
1 has test check 2580876.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 82.17 ms/it, loss 0.447495
Finished training it 22528/76743 of epoch 2, 82.36 ms/it, loss 0.445739
Finished training it 22528/76743 of epoch 2, 82.24 ms/it, loss 0.445575
Finished training it 22528/76743 of epoch 2, 82.51 ms/it, loss 0.445027
Finished training it 22528/76743 of epoch 2, 82.29 ms/it, loss 0.444251
Finished training it 23552/76743 of epoch 2, 80.70 ms/it, loss 0.445721
Finished training it 23552/76743 of epoch 2, 81.36 ms/it, loss 0.444791
Finished training it 23552/76743 of epoch 2, 81.03 ms/it, loss 0.446024
Finished training it 23552/76743 of epoch 2, 81.10 ms/it, loss 0.445681
Finished training it 24576/76743 of epoch 2, 82.02 ms/it, loss 0.447569
Finished training it 24576/76743 of epoch 2, 82.03 ms/it, loss 0.445471
Finished training it 24576/76743 of epoch 2, 81.65 ms/it, loss 0.442199
Finished training it 24576/76743 of epoch 2, 82.13 ms/it, loss 0.444118
Finished training it 25600/76743 of epoch 2, 81.64 ms/it, loss 0.443453
Finished training it 25600/76743 of epoch 2, 81.55 ms/it, loss 0.441277
Finished training it 25600/76743 of epoch 2, 81.68 ms/it, loss 0.444811
Finished training it 25600/76743 of epoch 2, 81.68 ms/it, loss 0.445459
Finished training it 26624/76743 of epoch 2, 80.92 ms/it, loss 0.441192
Finished training it 26624/76743 of epoch 2, 80.88 ms/it, loss 0.443334
Finished training it 26624/76743 of epoch 2, 80.90 ms/it, loss 0.444140
Finished training it 26624/76743 of epoch 2, 81.13 ms/it, loss 0.446000
Finished training it 27648/76743 of epoch 2, 81.91 ms/it, loss 0.444879
Finished training it 27648/76743 of epoch 2, 82.02 ms/it, loss 0.442161
Finished training it 27648/76743 of epoch 2, 81.61 ms/it, loss 0.441716
Finished training it 27648/76743 of epoch 2, 82.00 ms/it, loss 0.443027
Finished training it 28672/76743 of epoch 2, 81.97 ms/it, loss 0.444991
Finished training it 28672/76743 of epoch 2, 81.85 ms/it, loss 0.445842
Finished training it 28672/76743 of epoch 2, 81.68 ms/it, loss 0.446455
Finished training it 28672/76743 of epoch 2, 81.89 ms/it, loss 0.447884
Finished training it 29696/76743 of epoch 2, 80.98 ms/it, loss 0.445203
Finished training it 29696/76743 of epoch 2, 81.21 ms/it, loss 0.443837
Finished training it 29696/76743 of epoch 2, 81.03 ms/it, loss 0.445702
Finished training it 29696/76743 of epoch 2, 80.81 ms/it, loss 0.445542
Finished training it 30720/76743 of epoch 2, 81.81 ms/it, loss 0.443875
Finished training it 30720/76743 of epoch 2, 81.56 ms/it, loss 0.444990
Finished training it 30720/76743 of epoch 2, 81.56 ms/it, loss 0.443766
Finished training it 30720/76743 of epoch 2, 81.93 ms/it, loss 0.445670
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580873.0
get out
0 has test check 2580873.0 and sample count 3274240
 accuracy 78.824 %, best 78.824 %, roc auc score 0.8015, best 0.8015
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580891.0
get out
2 has test check 2580891.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 82.27 ms/it, loss 0.444862
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 81.89 ms/it, loss 0.442576
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580892.0
get out
1 has test check 2580892.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 82.61 ms/it, loss 0.443647
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580883.0
get out
3 has test check 2580883.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 82.54 ms/it, loss 0.446694
Finished training it 32768/76743 of epoch 2, 81.77 ms/it, loss 0.444513
Finished training it 32768/76743 of epoch 2, 82.06 ms/it, loss 0.444015
Finished training it 32768/76743 of epoch 2, 82.14 ms/it, loss 0.444743
Finished training it 32768/76743 of epoch 2, 81.78 ms/it, loss 0.447580
Finished training it 33792/76743 of epoch 2, 90.78 ms/it, loss 0.443552
Finished training it 33792/76743 of epoch 2, 91.69 ms/it, loss 0.442317
Finished training it 33792/76743 of epoch 2, 91.39 ms/it, loss 0.443484
Finished training it 33792/76743 of epoch 2, 91.27 ms/it, loss 0.441903
Finished training it 34816/76743 of epoch 2, 81.33 ms/it, loss 0.444175
Finished training it 34816/76743 of epoch 2, 81.13 ms/it, loss 0.445954
Finished training it 34816/76743 of epoch 2, 81.22 ms/it, loss 0.445542
Finished training it 34816/76743 of epoch 2, 81.41 ms/it, loss 0.443905
Finished training it 35840/76743 of epoch 2, 81.77 ms/it, loss 0.445723
Finished training it 35840/76743 of epoch 2, 82.14 ms/it, loss 0.443143
Finished training it 35840/76743 of epoch 2, 81.53 ms/it, loss 0.445059
Finished training it 35840/76743 of epoch 2, 82.17 ms/it, loss 0.441943
Finished training it 36864/76743 of epoch 2, 82.20 ms/it, loss 0.441793
Finished training it 36864/76743 of epoch 2, 82.30 ms/it, loss 0.444194
Finished training it 36864/76743 of epoch 2, 82.41 ms/it, loss 0.441730
Finished training it 36864/76743 of epoch 2, 82.25 ms/it, loss 0.443308
Finished training it 37888/76743 of epoch 2, 81.47 ms/it, loss 0.444536
Finished training it 37888/76743 of epoch 2, 81.34 ms/it, loss 0.443280
Finished training it 37888/76743 of epoch 2, 81.91 ms/it, loss 0.444203
Finished training it 37888/76743 of epoch 2, 81.88 ms/it, loss 0.444390
Finished training it 38912/76743 of epoch 2, 82.11 ms/it, loss 0.446307
Finished training it 38912/76743 of epoch 2, 81.63 ms/it, loss 0.444880
Finished training it 38912/76743 of epoch 2, 81.80 ms/it, loss 0.444394
Finished training it 38912/76743 of epoch 2, 81.18 ms/it, loss 0.443886
Finished training it 39936/76743 of epoch 2, 80.02 ms/it, loss 0.444807
Finished training it 39936/76743 of epoch 2, 80.34 ms/it, loss 0.441687
Finished training it 39936/76743 of epoch 2, 80.23 ms/it, loss 0.444953
Finished training it 39936/76743 of epoch 2, 80.37 ms/it, loss 0.443031
Finished training it 40960/76743 of epoch 2, 80.61 ms/it, loss 0.444072
Finished training it 40960/76743 of epoch 2, 80.56 ms/it, loss 0.443231
Finished training it 40960/76743 of epoch 2, 80.76 ms/it, loss 0.446578
Finished training it 40960/76743 of epoch 2, 80.75 ms/it, loss 0.441139
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2581283.0
get out
0 has test check 2581283.0 and sample count 3274240
 accuracy 78.836 %, best 78.836 %, roc auc score 0.8021, best 0.8021
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2581279.0
get out
1 has test check 2581279.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 80.69 ms/it, loss 0.444656
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 80.84 ms/it, loss 0.445283
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2581274.0
get out
2 has test check 2581274.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 80.62 ms/it, loss 0.441692
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2581274.0
get out
3 has test check 2581274.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 80.63 ms/it, loss 0.443045
Finished training it 43008/76743 of epoch 2, 81.57 ms/it, loss 0.443110
Finished training it 43008/76743 of epoch 2, 81.52 ms/it, loss 0.443526
Finished training it 43008/76743 of epoch 2, 81.33 ms/it, loss 0.444532
Finished training it 43008/76743 of epoch 2, 81.77 ms/it, loss 0.445476
Finished training it 44032/76743 of epoch 2, 81.06 ms/it, loss 0.446255
Finished training it 44032/76743 of epoch 2, 80.88 ms/it, loss 0.447845
Finished training it 44032/76743 of epoch 2, 80.58 ms/it, loss 0.443622
Finished training it 44032/76743 of epoch 2, 80.91 ms/it, loss 0.444168
Finished training it 45056/76743 of epoch 2, 85.89 ms/it, loss 0.439805
Finished training it 45056/76743 of epoch 2, 85.74 ms/it, loss 0.441974
Finished training it 45056/76743 of epoch 2, 86.38 ms/it, loss 0.442507
Finished training it 45056/76743 of epoch 2, 86.55 ms/it, loss 0.444635
Finished training it 46080/76743 of epoch 2, 82.26 ms/it, loss 0.443947
Finished training it 46080/76743 of epoch 2, 82.56 ms/it, loss 0.444533
Finished training it 46080/76743 of epoch 2, 82.56 ms/it, loss 0.440241
Finished training it 46080/76743 of epoch 2, 82.75 ms/it, loss 0.444050
Finished training it 47104/76743 of epoch 2, 81.11 ms/it, loss 0.443837
Finished training it 47104/76743 of epoch 2, 81.09 ms/it, loss 0.444029
Finished training it 47104/76743 of epoch 2, 81.31 ms/it, loss 0.443736
Finished training it 47104/76743 of epoch 2, 81.07 ms/it, loss 0.443378
Finished training it 48128/76743 of epoch 2, 80.89 ms/it, loss 0.442831
Finished training it 48128/76743 of epoch 2, 81.07 ms/it, loss 0.443413
Finished training it 48128/76743 of epoch 2, 81.07 ms/it, loss 0.441583
Finished training it 48128/76743 of epoch 2, 81.19 ms/it, loss 0.442353
Finished training it 49152/76743 of epoch 2, 81.67 ms/it, loss 0.444494
Finished training it 49152/76743 of epoch 2, 81.85 ms/it, loss 0.441668
Finished training it 49152/76743 of epoch 2, 81.41 ms/it, loss 0.443797
Finished training it 49152/76743 of epoch 2, 81.55 ms/it, loss 0.442669
Finished training it 50176/76743 of epoch 2, 80.55 ms/it, loss 0.441985
Finished training it 50176/76743 of epoch 2, 80.74 ms/it, loss 0.441908
Finished training it 50176/76743 of epoch 2, 81.10 ms/it, loss 0.443694
Finished training it 50176/76743 of epoch 2, 80.68 ms/it, loss 0.444062
Finished training it 51200/76743 of epoch 2, 80.91 ms/it, loss 0.443887
Finished training it 51200/76743 of epoch 2, 80.90 ms/it, loss 0.442961
Finished training it 51200/76743 of epoch 2, 80.68 ms/it, loss 0.443179
Finished training it 51200/76743 of epoch 2, 80.51 ms/it, loss 0.440696
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580599.0
get out
0 has test check 2580599.0 and sample count 3274240
 accuracy 78.815 %, best 78.836 %, roc auc score 0.8018, best 0.8021
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580586.0
get out
1 has test check 2580586.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 80.99 ms/it, loss 0.445121
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580603.0
get out
3 has test check 2580603.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 80.94 ms/it, loss 0.442719
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580602.0
get out
2 has test check 2580602.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 80.71 ms/it, loss 0.442991
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 80.61 ms/it, loss 0.440796
Finished training it 53248/76743 of epoch 2, 82.41 ms/it, loss 0.442508
Finished training it 53248/76743 of epoch 2, 81.51 ms/it, loss 0.444563
Finished training it 53248/76743 of epoch 2, 81.72 ms/it, loss 0.441593
Finished training it 53248/76743 of epoch 2, 81.85 ms/it, loss 0.444501
Finished training it 54272/76743 of epoch 2, 91.55 ms/it, loss 0.441670
Finished training it 54272/76743 of epoch 2, 92.65 ms/it, loss 0.445790
Finished training it 54272/76743 of epoch 2, 92.63 ms/it, loss 0.446907
Finished training it 54272/76743 of epoch 2, 92.17 ms/it, loss 0.442993
Finished training it 55296/76743 of epoch 2, 81.43 ms/it, loss 0.444089
Finished training it 55296/76743 of epoch 2, 81.40 ms/it, loss 0.442825
Finished training it 55296/76743 of epoch 2, 81.41 ms/it, loss 0.446216
Finished training it 55296/76743 of epoch 2, 81.16 ms/it, loss 0.442321
Finished training it 56320/76743 of epoch 2, 80.50 ms/it, loss 0.441949
Finished training it 56320/76743 of epoch 2, 80.59 ms/it, loss 0.442520
Finished training it 56320/76743 of epoch 2, 80.86 ms/it, loss 0.444722
Finished training it 56320/76743 of epoch 2, 80.93 ms/it, loss 0.442168
Finished training it 57344/76743 of epoch 2, 81.68 ms/it, loss 0.442286
Finished training it 57344/76743 of epoch 2, 81.11 ms/it, loss 0.443068
Finished training it 57344/76743 of epoch 2, 81.41 ms/it, loss 0.442012
Finished training it 57344/76743 of epoch 2, 81.44 ms/it, loss 0.443845
Finished training it 58368/76743 of epoch 2, 82.42 ms/it, loss 0.445319
Finished training it 58368/76743 of epoch 2, 82.21 ms/it, loss 0.442917
Finished training it 58368/76743 of epoch 2, 82.30 ms/it, loss 0.441512
Finished training it 58368/76743 of epoch 2, 81.90 ms/it, loss 0.443645
Finished training it 59392/76743 of epoch 2, 81.53 ms/it, loss 0.444207
Finished training it 59392/76743 of epoch 2, 81.56 ms/it, loss 0.442287
Finished training it 59392/76743 of epoch 2, 81.41 ms/it, loss 0.442574
Finished training it 59392/76743 of epoch 2, 81.76 ms/it, loss 0.440287
Finished training it 60416/76743 of epoch 2, 80.40 ms/it, loss 0.442628
Finished training it 60416/76743 of epoch 2, 80.23 ms/it, loss 0.441909
Finished training it 60416/76743 of epoch 2, 80.45 ms/it, loss 0.443449
Finished training it 60416/76743 of epoch 2, 80.59 ms/it, loss 0.439400
Finished training it 61440/76743 of epoch 2, 81.60 ms/it, loss 0.445908
Finished training it 61440/76743 of epoch 2, 81.48 ms/it, loss 0.443405
Finished training it 61440/76743 of epoch 2, 81.71 ms/it, loss 0.444176
Finished training it 61440/76743 of epoch 2, 81.18 ms/it, loss 0.441320
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582225.0
get out
0 has test check 2582225.0 and sample count 3274240
 accuracy 78.865 %, best 78.865 %, roc auc score 0.8027, best 0.8027
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582219.0
get out
2 has test check 2582219.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 81.42 ms/it, loss 0.442128
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 81.23 ms/it, loss 0.445723
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582231.0
get out
3 has test check 2582231.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 81.41 ms/it, loss 0.442669
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582222.0
get out
1 has test check 2582222.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 81.40 ms/it, loss 0.441618
Finished training it 63488/76743 of epoch 2, 81.81 ms/it, loss 0.440987
Finished training it 63488/76743 of epoch 2, 81.89 ms/it, loss 0.446111
Finished training it 63488/76743 of epoch 2, 81.42 ms/it, loss 0.441463
Finished training it 63488/76743 of epoch 2, 81.87 ms/it, loss 0.443340
Finished training it 64512/76743 of epoch 2, 86.34 ms/it, loss 0.443281
Finished training it 64512/76743 of epoch 2, 86.76 ms/it, loss 0.442562
Finished training it 64512/76743 of epoch 2, 85.94 ms/it, loss 0.444883
Finished training it 64512/76743 of epoch 2, 86.06 ms/it, loss 0.443581
Finished training it 65536/76743 of epoch 2, 92.28 ms/it, loss 0.444033
Finished training it 65536/76743 of epoch 2, 91.93 ms/it, loss 0.442959
Finished training it 65536/76743 of epoch 2, 91.59 ms/it, loss 0.444918
Finished training it 65536/76743 of epoch 2, 91.85 ms/it, loss 0.442522
Finished training it 66560/76743 of epoch 2, 80.82 ms/it, loss 0.444272
Finished training it 66560/76743 of epoch 2, 81.51 ms/it, loss 0.442161
Finished training it 66560/76743 of epoch 2, 81.35 ms/it, loss 0.440968
Finished training it 66560/76743 of epoch 2, 81.28 ms/it, loss 0.443476
Finished training it 67584/76743 of epoch 2, 82.59 ms/it, loss 0.445589
Finished training it 67584/76743 of epoch 2, 82.73 ms/it, loss 0.444035
Finished training it 67584/76743 of epoch 2, 82.63 ms/it, loss 0.442998
Finished training it 67584/76743 of epoch 2, 82.55 ms/it, loss 0.445005
Finished training it 68608/76743 of epoch 2, 81.64 ms/it, loss 0.442152
Finished training it 68608/76743 of epoch 2, 81.61 ms/it, loss 0.440453
Finished training it 68608/76743 of epoch 2, 81.60 ms/it, loss 0.443214
Finished training it 68608/76743 of epoch 2, 81.22 ms/it, loss 0.441929
Finished training it 69632/76743 of epoch 2, 81.66 ms/it, loss 0.443115
Finished training it 69632/76743 of epoch 2, 81.07 ms/it, loss 0.441588
Finished training it 69632/76743 of epoch 2, 81.83 ms/it, loss 0.443436
Finished training it 69632/76743 of epoch 2, 82.04 ms/it, loss 0.444694
Finished training it 70656/76743 of epoch 2, 80.82 ms/it, loss 0.443779
Finished training it 70656/76743 of epoch 2, 80.78 ms/it, loss 0.445607
Finished training it 70656/76743 of epoch 2, 80.75 ms/it, loss 0.441303
Finished training it 70656/76743 of epoch 2, 80.47 ms/it, loss 0.441629
Finished training it 71680/76743 of epoch 2, 81.11 ms/it, loss 0.442190
Finished training it 71680/76743 of epoch 2, 81.27 ms/it, loss 0.446938
Finished training it 71680/76743 of epoch 2, 81.26 ms/it, loss 0.439096
Finished training it 71680/76743 of epoch 2, 81.28 ms/it, loss 0.439697
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580519.0
get out
0 has test check 2580519.0 and sample count 3274240
 accuracy 78.813 %, best 78.865 %, roc auc score 0.8028, best 0.8028
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580552.0
get out
2 has test check 2580552.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 81.62 ms/it, loss 0.441962
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 80.93 ms/it, loss 0.441832
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580546.0
get out
1 has test check 2580546.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 81.52 ms/it, loss 0.446544
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580528.0
get out
3 has test check 2580528.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 81.44 ms/it, loss 0.443238
Finished training it 73728/76743 of epoch 2, 81.65 ms/it, loss 0.441637
Finished training it 73728/76743 of epoch 2, 81.98 ms/it, loss 0.443453
Finished training it 73728/76743 of epoch 2, 82.10 ms/it, loss 0.442224
Finished training it 73728/76743 of epoch 2, 81.84 ms/it, loss 0.444566
Finished training it 74752/76743 of epoch 2, 82.80 ms/it, loss 0.443026
Finished training it 74752/76743 of epoch 2, 82.55 ms/it, loss 0.442826
Finished training it 74752/76743 of epoch 2, 82.96 ms/it, loss 0.446413
Finished training it 74752/76743 of epoch 2, 82.04 ms/it, loss 0.444219
Finished training it 75776/76743 of epoch 2, 81.26 ms/it, loss 0.443108
Finished training it 75776/76743 of epoch 2, 81.05 ms/it, loss 0.442464
Finished training it 75776/76743 of epoch 2, 81.49 ms/it, loss 0.441983
Finished training it 75776/76743 of epoch 2, 81.00 ms/it, loss 0.441905
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.43 ms/it, loss 0.443373
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.74 ms/it, loss 0.444964
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 87.79 ms/it, loss 0.444778
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 88.02 ms/it, loss 0.443774
Finished training it 2048/76743 of epoch 3, 81.43 ms/it, loss 0.445726
Finished training it 2048/76743 of epoch 3, 81.56 ms/it, loss 0.442504
Finished training it 2048/76743 of epoch 3, 81.23 ms/it, loss 0.443762
Finished training it 2048/76743 of epoch 3, 81.72 ms/it, loss 0.441276
Finished training it 3072/76743 of epoch 3, 80.61 ms/it, loss 0.442931
Finished training it 3072/76743 of epoch 3, 80.46 ms/it, loss 0.441455
Finished training it 3072/76743 of epoch 3, 80.33 ms/it, loss 0.443532
Finished training it 3072/76743 of epoch 3, 80.22 ms/it, loss 0.441393
Finished training it 4096/76743 of epoch 3, 81.19 ms/it, loss 0.440736
Finished training it 4096/76743 of epoch 3, 81.48 ms/it, loss 0.443386
Finished training it 4096/76743 of epoch 3, 81.04 ms/it, loss 0.443106
Finished training it 4096/76743 of epoch 3, 80.95 ms/it, loss 0.441902
Finished training it 5120/76743 of epoch 3, 83.12 ms/it, loss 0.443781
Finished training it 5120/76743 of epoch 3, 82.81 ms/it, loss 0.441092
Finished training it 5120/76743 of epoch 3, 82.93 ms/it, loss 0.441965
Finished training it 5120/76743 of epoch 3, 82.34 ms/it, loss 0.441559
Finished training it 6144/76743 of epoch 3, 81.79 ms/it, loss 0.443572
Finished training it 6144/76743 of epoch 3, 81.66 ms/it, loss 0.441161
Finished training it 6144/76743 of epoch 3, 81.43 ms/it, loss 0.441954
Finished training it 6144/76743 of epoch 3, 81.64 ms/it, loss 0.441248
Finished training it 7168/76743 of epoch 3, 81.30 ms/it, loss 0.440181
Finished training it 7168/76743 of epoch 3, 81.14 ms/it, loss 0.443100
Finished training it 7168/76743 of epoch 3, 81.55 ms/it, loss 0.443331
Finished training it 7168/76743 of epoch 3, 81.54 ms/it, loss 0.442978
Finished training it 8192/76743 of epoch 3, 81.26 ms/it, loss 0.443241
Finished training it 8192/76743 of epoch 3, 81.82 ms/it, loss 0.440487
Finished training it 8192/76743 of epoch 3, 81.35 ms/it, loss 0.443576
Finished training it 8192/76743 of epoch 3, 81.22 ms/it, loss 0.441782
Finished training it 9216/76743 of epoch 3, 81.98 ms/it, loss 0.441532
Finished training it 9216/76743 of epoch 3, 81.92 ms/it, loss 0.440029
Finished training it 9216/76743 of epoch 3, 82.00 ms/it, loss 0.441340
Finished training it 9216/76743 of epoch 3, 81.83 ms/it, loss 0.440959
Finished training it 10240/76743 of epoch 3, 82.18 ms/it, loss 0.442980
Finished training it 10240/76743 of epoch 3, 82.01 ms/it, loss 0.444204
Finished training it 10240/76743 of epoch 3, 82.00 ms/it, loss 0.442484
Finished training it 10240/76743 of epoch 3, 81.88 ms/it, loss 0.441949
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582588.0
get out
0 has test check 2582588.0 and sample count 3274240
 accuracy 78.876 %, best 78.876 %, roc auc score 0.8025, best 0.8028
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 81.30 ms/it, loss 0.442334
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582574.0
get out
1 has test check 2582574.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.05 ms/it, loss 0.439689
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582581.0
get out
3 has test check 2582581.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.27 ms/it, loss 0.441320
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582588.0
get out
2 has test check 2582588.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 81.28 ms/it, loss 0.440690
Finished training it 12288/76743 of epoch 3, 81.76 ms/it, loss 0.442815
Finished training it 12288/76743 of epoch 3, 81.65 ms/it, loss 0.443589
Finished training it 12288/76743 of epoch 3, 82.03 ms/it, loss 0.439577
Finished training it 12288/76743 of epoch 3, 82.06 ms/it, loss 0.441930
Finished training it 13312/76743 of epoch 3, 81.22 ms/it, loss 0.440349
Finished training it 13312/76743 of epoch 3, 81.07 ms/it, loss 0.444934
Finished training it 13312/76743 of epoch 3, 81.41 ms/it, loss 0.438156
Finished training it 13312/76743 of epoch 3, 81.57 ms/it, loss 0.443940
Finished training it 14336/76743 of epoch 3, 92.12 ms/it, loss 0.442825
Finished training it 14336/76743 of epoch 3, 92.10 ms/it, loss 0.441601
Finished training it 14336/76743 of epoch 3, 92.65 ms/it, loss 0.438291
Finished training it 14336/76743 of epoch 3, 92.50 ms/it, loss 0.441042
Finished training it 15360/76743 of epoch 3, 82.06 ms/it, loss 0.442387
Finished training it 15360/76743 of epoch 3, 82.01 ms/it, loss 0.443597
Finished training it 15360/76743 of epoch 3, 82.78 ms/it, loss 0.442415
Finished training it 15360/76743 of epoch 3, 82.06 ms/it, loss 0.442067
Finished training it 16384/76743 of epoch 3, 80.92 ms/it, loss 0.441453
Finished training it 16384/76743 of epoch 3, 80.90 ms/it, loss 0.442187
Finished training it 16384/76743 of epoch 3, 80.74 ms/it, loss 0.440488
Finished training it 16384/76743 of epoch 3, 80.76 ms/it, loss 0.441846
Finished training it 17408/76743 of epoch 3, 82.78 ms/it, loss 0.443942
Finished training it 17408/76743 of epoch 3, 82.97 ms/it, loss 0.441828
Finished training it 17408/76743 of epoch 3, 82.54 ms/it, loss 0.441619
Finished training it 17408/76743 of epoch 3, 82.51 ms/it, loss 0.443270
Finished training it 18432/76743 of epoch 3, 81.36 ms/it, loss 0.439377
Finished training it 18432/76743 of epoch 3, 81.37 ms/it, loss 0.440049
Finished training it 18432/76743 of epoch 3, 81.43 ms/it, loss 0.437403
Finished training it 18432/76743 of epoch 3, 81.31 ms/it, loss 0.445533
Finished training it 19456/76743 of epoch 3, 81.73 ms/it, loss 0.442436
Finished training it 19456/76743 of epoch 3, 82.39 ms/it, loss 0.443270
Finished training it 19456/76743 of epoch 3, 82.41 ms/it, loss 0.443150
Finished training it 19456/76743 of epoch 3, 81.77 ms/it, loss 0.442931
Finished training it 20480/76743 of epoch 3, 81.38 ms/it, loss 0.442342
Finished training it 20480/76743 of epoch 3, 81.08 ms/it, loss 0.442375
Finished training it 20480/76743 of epoch 3, 81.55 ms/it, loss 0.441762
Finished training it 20480/76743 of epoch 3, 81.37 ms/it, loss 0.444656
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583054.0
get out
0 has test check 2583054.0 and sample count 3274240
 accuracy 78.890 %, best 78.890 %, roc auc score 0.8030, best 0.8030
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583051.0
get out
3 has test check 2583051.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 81.94 ms/it, loss 0.442051
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583045.0
get out
2 has test check 2583045.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 81.64 ms/it, loss 0.439435
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583047.0
get out
1 has test check 2583047.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 81.35 ms/it, loss 0.444335
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 81.32 ms/it, loss 0.442756
Finished training it 22528/76743 of epoch 3, 81.54 ms/it, loss 0.441141
Finished training it 22528/76743 of epoch 3, 81.71 ms/it, loss 0.441956
Finished training it 22528/76743 of epoch 3, 81.73 ms/it, loss 0.443114
Finished training it 22528/76743 of epoch 3, 81.18 ms/it, loss 0.442699
Finished training it 23552/76743 of epoch 3, 82.13 ms/it, loss 0.441484
Finished training it 23552/76743 of epoch 3, 82.12 ms/it, loss 0.442652
Finished training it 23552/76743 of epoch 3, 81.80 ms/it, loss 0.442981
Finished training it 23552/76743 of epoch 3, 81.74 ms/it, loss 0.442670
Finished training it 24576/76743 of epoch 3, 81.44 ms/it, loss 0.442693
Finished training it 24576/76743 of epoch 3, 81.99 ms/it, loss 0.441121
Finished training it 24576/76743 of epoch 3, 82.22 ms/it, loss 0.444784
Finished training it 24576/76743 of epoch 3, 81.85 ms/it, loss 0.438993
Finished training it 25600/76743 of epoch 3, 80.81 ms/it, loss 0.441851
Finished training it 25600/76743 of epoch 3, 81.39 ms/it, loss 0.440243
Finished training it 25600/76743 of epoch 3, 81.14 ms/it, loss 0.438177
Finished training it 25600/76743 of epoch 3, 81.50 ms/it, loss 0.442414
Finished training it 26624/76743 of epoch 3, 81.17 ms/it, loss 0.442942
Finished training it 26624/76743 of epoch 3, 81.24 ms/it, loss 0.440276
Finished training it 26624/76743 of epoch 3, 81.52 ms/it, loss 0.441376
Finished training it 26624/76743 of epoch 3, 81.52 ms/it, loss 0.438286
Finished training it 27648/76743 of epoch 3, 81.15 ms/it, loss 0.439240
Finished training it 27648/76743 of epoch 3, 81.36 ms/it, loss 0.440284
Finished training it 27648/76743 of epoch 3, 81.68 ms/it, loss 0.441914
Finished training it 27648/76743 of epoch 3, 81.66 ms/it, loss 0.438823
Finished training it 28672/76743 of epoch 3, 81.54 ms/it, loss 0.443394
Finished training it 28672/76743 of epoch 3, 81.81 ms/it, loss 0.442068
Finished training it 28672/76743 of epoch 3, 81.69 ms/it, loss 0.445260
Finished training it 28672/76743 of epoch 3, 82.15 ms/it, loss 0.442604
Finished training it 29696/76743 of epoch 3, 81.23 ms/it, loss 0.443084
Finished training it 29696/76743 of epoch 3, 81.46 ms/it, loss 0.442738
Finished training it 29696/76743 of epoch 3, 81.63 ms/it, loss 0.441903
Finished training it 29696/76743 of epoch 3, 81.65 ms/it, loss 0.441121
Finished training it 30720/76743 of epoch 3, 81.71 ms/it, loss 0.442567
Finished training it 30720/76743 of epoch 3, 81.43 ms/it, loss 0.441396
Finished training it 30720/76743 of epoch 3, 81.87 ms/it, loss 0.442266
Finished training it 30720/76743 of epoch 3, 81.60 ms/it, loss 0.440842
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583164.0
get out
0 has test check 2583164.0 and sample count 3274240
 accuracy 78.894 %, best 78.894 %, roc auc score 0.8030, best 0.8030
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583167.0
get out
1 has test check 2583167.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 81.96 ms/it, loss 0.440997
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 81.77 ms/it, loss 0.439565
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583167.0
get out
3 has test check 2583167.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 82.48 ms/it, loss 0.443694
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583167.0
get out
2 has test check 2583167.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 82.09 ms/it, loss 0.442021
Finished training it 32768/76743 of epoch 3, 82.30 ms/it, loss 0.441290
Finished training it 32768/76743 of epoch 3, 81.48 ms/it, loss 0.441717
Finished training it 32768/76743 of epoch 3, 81.72 ms/it, loss 0.441589
Finished training it 32768/76743 of epoch 3, 81.15 ms/it, loss 0.444359
Finished training it 33792/76743 of epoch 3, 95.75 ms/it, loss 0.439653
Finished training it 33792/76743 of epoch 3, 95.86 ms/it, loss 0.439015
Finished training it 33792/76743 of epoch 3, 95.70 ms/it, loss 0.440766
Finished training it 33792/76743 of epoch 3, 95.64 ms/it, loss 0.441100
Finished training it 34816/76743 of epoch 3, 81.26 ms/it, loss 0.443323
Finished training it 34816/76743 of epoch 3, 81.38 ms/it, loss 0.441301
Finished training it 34816/76743 of epoch 3, 81.68 ms/it, loss 0.440853
Finished training it 34816/76743 of epoch 3, 81.46 ms/it, loss 0.442700
Finished training it 35840/76743 of epoch 3, 80.48 ms/it, loss 0.442962
Finished training it 35840/76743 of epoch 3, 80.71 ms/it, loss 0.439126
Finished training it 35840/76743 of epoch 3, 80.94 ms/it, loss 0.440696
Finished training it 35840/76743 of epoch 3, 81.18 ms/it, loss 0.442540
Finished training it 36864/76743 of epoch 3, 81.05 ms/it, loss 0.441227
Finished training it 36864/76743 of epoch 3, 80.29 ms/it, loss 0.439184
Finished training it 36864/76743 of epoch 3, 80.26 ms/it, loss 0.438494
Finished training it 36864/76743 of epoch 3, 80.27 ms/it, loss 0.440756
Finished training it 37888/76743 of epoch 3, 81.46 ms/it, loss 0.441700
Finished training it 37888/76743 of epoch 3, 81.38 ms/it, loss 0.441651
Finished training it 37888/76743 of epoch 3, 81.42 ms/it, loss 0.441555
Finished training it 37888/76743 of epoch 3, 81.02 ms/it, loss 0.440540
Finished training it 38912/76743 of epoch 3, 82.59 ms/it, loss 0.442115
Finished training it 38912/76743 of epoch 3, 82.57 ms/it, loss 0.441479
Finished training it 38912/76743 of epoch 3, 82.07 ms/it, loss 0.441114
Finished training it 38912/76743 of epoch 3, 82.50 ms/it, loss 0.443691
Finished training it 39936/76743 of epoch 3, 81.29 ms/it, loss 0.441968
Finished training it 39936/76743 of epoch 3, 81.91 ms/it, loss 0.442127
Finished training it 39936/76743 of epoch 3, 80.88 ms/it, loss 0.440520
Finished training it 39936/76743 of epoch 3, 80.92 ms/it, loss 0.439142
Finished training it 40960/76743 of epoch 3, 80.67 ms/it, loss 0.440465
Finished training it 40960/76743 of epoch 3, 80.61 ms/it, loss 0.443734
Finished training it 40960/76743 of epoch 3, 80.76 ms/it, loss 0.441267
Finished training it 40960/76743 of epoch 3, 80.67 ms/it, loss 0.438512
Testing at - 40960/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583251.0
get out
0 has test check 2583251.0 and sample count 3274240
 accuracy 78.896 %, best 78.896 %, roc auc score 0.8036, best 0.8036
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583257.0
get out
3 has test check 2583257.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 82.22 ms/it, loss 0.440321
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583268.0
get out
2 has test check 2583268.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 82.27 ms/it, loss 0.438935
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 3, 81.87 ms/it, loss 0.442384
Testing at - 40960/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583267.0
get out
1 has test check 2583267.0 and sample count 3274240
Finished training it 41984/76743 of epoch 3, 81.43 ms/it, loss 0.442118
Finished training it 43008/76743 of epoch 3, 80.78 ms/it, loss 0.440743
Finished training it 43008/76743 of epoch 3, 81.00 ms/it, loss 0.441461
Finished training it 43008/76743 of epoch 3, 80.74 ms/it, loss 0.440249
Finished training it 43008/76743 of epoch 3, 80.94 ms/it, loss 0.442821
Finished training it 44032/76743 of epoch 3, 80.63 ms/it, loss 0.444653
Finished training it 44032/76743 of epoch 3, 80.92 ms/it, loss 0.443213
Finished training it 44032/76743 of epoch 3, 80.91 ms/it, loss 0.441433
Finished training it 44032/76743 of epoch 3, 81.04 ms/it, loss 0.440698
Finished training it 45056/76743 of epoch 3, 88.95 ms/it, loss 0.436972
Finished training it 45056/76743 of epoch 3, 88.83 ms/it, loss 0.439659
Finished training it 45056/76743 of epoch 3, 89.02 ms/it, loss 0.441715
Finished training it 45056/76743 of epoch 3, 88.74 ms/it, loss 0.439258
Finished training it 46080/76743 of epoch 3, 80.65 ms/it, loss 0.441104
Finished training it 46080/76743 of epoch 3, 80.57 ms/it, loss 0.441620
Finished training it 46080/76743 of epoch 3, 80.59 ms/it, loss 0.441107
Finished training it 46080/76743 of epoch 3, 80.47 ms/it, loss 0.437574
Finished training it 47104/76743 of epoch 3, 81.65 ms/it, loss 0.441155
Finished training it 47104/76743 of epoch 3, 81.82 ms/it, loss 0.441323
Finished training it 47104/76743 of epoch 3, 80.94 ms/it, loss 0.441408
Finished training it 47104/76743 of epoch 3, 81.81 ms/it, loss 0.440522
Finished training it 48128/76743 of epoch 3, 81.38 ms/it, loss 0.440583
Finished training it 48128/76743 of epoch 3, 81.34 ms/it, loss 0.440147
Finished training it 48128/76743 of epoch 3, 80.74 ms/it, loss 0.440035
Finished training it 48128/76743 of epoch 3, 81.21 ms/it, loss 0.438880
Finished training it 49152/76743 of epoch 3, 81.97 ms/it, loss 0.441753
Finished training it 49152/76743 of epoch 3, 81.57 ms/it, loss 0.441104
Finished training it 49152/76743 of epoch 3, 81.59 ms/it, loss 0.439006
Finished training it 49152/76743 of epoch 3, 81.78 ms/it, loss 0.440118
Finished training it 50176/76743 of epoch 3, 81.27 ms/it, loss 0.441140
Finished training it 50176/76743 of epoch 3, 80.95 ms/it, loss 0.439233
Finished training it 50176/76743 of epoch 3, 81.02 ms/it, loss 0.441401
Finished training it 50176/76743 of epoch 3, 80.80 ms/it, loss 0.438944
Finished training it 51200/76743 of epoch 3, 81.05 ms/it, loss 0.438099
Finished training it 51200/76743 of epoch 3, 81.22 ms/it, loss 0.440103
Finished training it 51200/76743 of epoch 3, 81.06 ms/it, loss 0.440191
Finished training it 51200/76743 of epoch 3, 81.17 ms/it, loss 0.441192
Testing at - 51200/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582545.0
get out
0 has test check 2582545.0 and sample count 3274240
 accuracy 78.875 %, best 78.896 %, roc auc score 0.8027, best 0.8036
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 3, 82.51 ms/it, loss 0.437837
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582535.0
get out
2 has test check 2582535.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 82.73 ms/it, loss 0.439936
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582537.0
get out
3 has test check 2582537.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 82.84 ms/it, loss 0.439933
Testing at - 51200/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582533.0
get out
1 has test check 2582533.0 and sample count 3274240
Finished training it 52224/76743 of epoch 3, 82.78 ms/it, loss 0.442461
Finished training it 53248/76743 of epoch 3, 81.26 ms/it, loss 0.441796
Finished training it 53248/76743 of epoch 3, 81.71 ms/it, loss 0.439878
Finished training it 53248/76743 of epoch 3, 81.76 ms/it, loss 0.442026
Finished training it 53248/76743 of epoch 3, 81.72 ms/it, loss 0.438727
Finished training it 54272/76743 of epoch 3, 91.86 ms/it, loss 0.439102
Finished training it 54272/76743 of epoch 3, 92.33 ms/it, loss 0.440598
Finished training it 54272/76743 of epoch 3, 92.47 ms/it, loss 0.444145
Finished training it 54272/76743 of epoch 3, 92.44 ms/it, loss 0.442976
Finished training it 55296/76743 of epoch 3, 82.61 ms/it, loss 0.439741
Finished training it 55296/76743 of epoch 3, 82.22 ms/it, loss 0.443601
Finished training it 55296/76743 of epoch 3, 82.81 ms/it, loss 0.441411
Finished training it 55296/76743 of epoch 3, 82.34 ms/it, loss 0.440159
Finished training it 56320/76743 of epoch 3, 80.87 ms/it, loss 0.439365
Finished training it 56320/76743 of epoch 3, 80.88 ms/it, loss 0.439791
Finished training it 56320/76743 of epoch 3, 81.12 ms/it, loss 0.442223
Finished training it 56320/76743 of epoch 3, 81.29 ms/it, loss 0.439059
Finished training it 57344/76743 of epoch 3, 80.66 ms/it, loss 0.440298
Finished training it 57344/76743 of epoch 3, 80.80 ms/it, loss 0.439197
Finished training it 57344/76743 of epoch 3, 81.06 ms/it, loss 0.439489
Finished training it 57344/76743 of epoch 3, 80.55 ms/it, loss 0.440693
Finished training it 58368/76743 of epoch 3, 82.23 ms/it, loss 0.441007
Finished training it 58368/76743 of epoch 3, 81.54 ms/it, loss 0.442510
Finished training it 58368/76743 of epoch 3, 82.16 ms/it, loss 0.438735
Finished training it 58368/76743 of epoch 3, 82.12 ms/it, loss 0.440027
Finished training it 59392/76743 of epoch 3, 82.45 ms/it, loss 0.437734
Finished training it 59392/76743 of epoch 3, 82.41 ms/it, loss 0.439652
Finished training it 59392/76743 of epoch 3, 82.57 ms/it, loss 0.441095
Finished training it 59392/76743 of epoch 3, 82.31 ms/it, loss 0.439780
Finished training it 60416/76743 of epoch 3, 82.06 ms/it, loss 0.436796
Finished training it 60416/76743 of epoch 3, 82.13 ms/it, loss 0.439741
Finished training it 60416/76743 of epoch 3, 81.47 ms/it, loss 0.439000
Finished training it 60416/76743 of epoch 3, 82.00 ms/it, loss 0.440453
Finished training it 61440/76743 of epoch 3, 81.39 ms/it, loss 0.441707
Finished training it 61440/76743 of epoch 3, 81.04 ms/it, loss 0.443497
Finished training it 61440/76743 of epoch 3, 80.71 ms/it, loss 0.438840
Finished training it 61440/76743 of epoch 3, 81.11 ms/it, loss 0.440972
Testing at - 61440/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583511.0
get out
0 has test check 2583511.0 and sample count 3274240
 accuracy 78.904 %, best 78.904 %, roc auc score 0.8035, best 0.8036
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 3, 81.21 ms/it, loss 0.442676
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583515.0
get out
3 has test check 2583515.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 81.38 ms/it, loss 0.440174
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583501.0
get out
1 has test check 2583501.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 81.07 ms/it, loss 0.438700
Testing at - 61440/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583500.0
get out
2 has test check 2583500.0 and sample count 3274240
Finished training it 62464/76743 of epoch 3, 81.06 ms/it, loss 0.439248
Finished training it 63488/76743 of epoch 3, 80.33 ms/it, loss 0.440491
Finished training it 63488/76743 of epoch 3, 80.33 ms/it, loss 0.443513
Finished training it 63488/76743 of epoch 3, 80.64 ms/it, loss 0.439226
Finished training it 63488/76743 of epoch 3, 79.99 ms/it, loss 0.438342
Finished training it 64512/76743 of epoch 3, 84.32 ms/it, loss 0.440572
Finished training it 64512/76743 of epoch 3, 84.84 ms/it, loss 0.441987
Finished training it 64512/76743 of epoch 3, 85.33 ms/it, loss 0.439567
Finished training it 64512/76743 of epoch 3, 84.07 ms/it, loss 0.440773
Finished training it 65536/76743 of epoch 3, 80.74 ms/it, loss 0.440276
Finished training it 65536/76743 of epoch 3, 80.84 ms/it, loss 0.441420
Finished training it 65536/76743 of epoch 3, 80.94 ms/it, loss 0.439639
Finished training it 65536/76743 of epoch 3, 80.87 ms/it, loss 0.442345
Finished training it 66560/76743 of epoch 3, 81.07 ms/it, loss 0.441454
Finished training it 66560/76743 of epoch 3, 80.99 ms/it, loss 0.439539
Finished training it 66560/76743 of epoch 3, 80.94 ms/it, loss 0.438283
Finished training it 66560/76743 of epoch 3, 80.65 ms/it, loss 0.441080
Finished training it 67584/76743 of epoch 3, 81.10 ms/it, loss 0.442123
Finished training it 67584/76743 of epoch 3, 81.31 ms/it, loss 0.442955
Finished training it 67584/76743 of epoch 3, 80.95 ms/it, loss 0.441337
Finished training it 67584/76743 of epoch 3, 80.71 ms/it, loss 0.440138
Finished training it 68608/76743 of epoch 3, 81.05 ms/it, loss 0.439463
Finished training it 68608/76743 of epoch 3, 81.16 ms/it, loss 0.439886
Finished training it 68608/76743 of epoch 3, 81.04 ms/it, loss 0.437603
Finished training it 68608/76743 of epoch 3, 81.15 ms/it, loss 0.440764
Finished training it 69632/76743 of epoch 3, 81.21 ms/it, loss 0.440447
Finished training it 69632/76743 of epoch 3, 81.16 ms/it, loss 0.441955
Finished training it 69632/76743 of epoch 3, 81.31 ms/it, loss 0.438557
Finished training it 69632/76743 of epoch 3, 80.84 ms/it, loss 0.440804
Finished training it 70656/76743 of epoch 3, 81.64 ms/it, loss 0.438972
Finished training it 70656/76743 of epoch 3, 81.52 ms/it, loss 0.441392
Finished training it 70656/76743 of epoch 3, 81.34 ms/it, loss 0.442805
Finished training it 70656/76743 of epoch 3, 81.33 ms/it, loss 0.438849
Finished training it 71680/76743 of epoch 3, 81.32 ms/it, loss 0.439611
Finished training it 71680/76743 of epoch 3, 81.53 ms/it, loss 0.436870
Finished training it 71680/76743 of epoch 3, 81.27 ms/it, loss 0.436307
Finished training it 71680/76743 of epoch 3, 81.74 ms/it, loss 0.444379
Testing at - 71680/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2580598.0
get out
0 has test check 2580598.0 and sample count 3274240
 accuracy 78.815 %, best 78.904 %, roc auc score 0.8036, best 0.8036
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 3, 81.28 ms/it, loss 0.438925
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2580592.0
get out
2 has test check 2580592.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 81.49 ms/it, loss 0.439036
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2580576.0
get out
1 has test check 2580576.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 81.54 ms/it, loss 0.443774
Testing at - 71680/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2580595.0
get out
3 has test check 2580595.0 and sample count 3274240
Finished training it 72704/76743 of epoch 3, 81.58 ms/it, loss 0.440921
Finished training it 73728/76743 of epoch 3, 80.96 ms/it, loss 0.439237
Finished training it 73728/76743 of epoch 3, 80.92 ms/it, loss 0.441582
Finished training it 73728/76743 of epoch 3, 80.73 ms/it, loss 0.440917
Finished training it 73728/76743 of epoch 3, 80.55 ms/it, loss 0.439003
Finished training it 74752/76743 of epoch 3, 93.40 ms/it, loss 0.441447
Finished training it 74752/76743 of epoch 3, 93.63 ms/it, loss 0.443695
Finished training it 74752/76743 of epoch 3, 92.97 ms/it, loss 0.440134
Finished training it 74752/76743 of epoch 3, 93.46 ms/it, loss 0.440091
Finished training it 75776/76743 of epoch 3, 82.21 ms/it, loss 0.440075
Finished training it 75776/76743 of epoch 3, 81.85 ms/it, loss 0.439963
Finished training it 75776/76743 of epoch 3, 82.20 ms/it, loss 0.439036
Finished training it 75776/76743 of epoch 3, 81.92 ms/it, loss 0.439008
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 84.24 ms/it, loss 0.442184
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 84.67 ms/it, loss 0.442169
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 84.47 ms/it, loss 0.440412
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 4, 84.14 ms/it, loss 0.441114
Finished training it 2048/76743 of epoch 4, 80.85 ms/it, loss 0.440729
Finished training it 2048/76743 of epoch 4, 80.76 ms/it, loss 0.439957
Finished training it 2048/76743 of epoch 4, 80.99 ms/it, loss 0.438624
Finished training it 2048/76743 of epoch 4, 80.99 ms/it, loss 0.443092
Finished training it 3072/76743 of epoch 4, 81.39 ms/it, loss 0.438571
Finished training it 3072/76743 of epoch 4, 81.85 ms/it, loss 0.440859
Finished training it 3072/76743 of epoch 4, 81.97 ms/it, loss 0.438802
Finished training it 3072/76743 of epoch 4, 81.60 ms/it, loss 0.440424
Finished training it 4096/76743 of epoch 4, 82.76 ms/it, loss 0.438356
Finished training it 4096/76743 of epoch 4, 82.46 ms/it, loss 0.440555
Finished training it 4096/76743 of epoch 4, 82.57 ms/it, loss 0.440629
Finished training it 4096/76743 of epoch 4, 82.08 ms/it, loss 0.439113
Finished training it 5120/76743 of epoch 4, 80.36 ms/it, loss 0.438849
Finished training it 5120/76743 of epoch 4, 80.11 ms/it, loss 0.439019
Finished training it 5120/76743 of epoch 4, 80.20 ms/it, loss 0.438699
Finished training it 5120/76743 of epoch 4, 80.57 ms/it, loss 0.441493
Finished training it 6144/76743 of epoch 4, 80.95 ms/it, loss 0.439445
Finished training it 6144/76743 of epoch 4, 81.01 ms/it, loss 0.440927
Finished training it 6144/76743 of epoch 4, 81.40 ms/it, loss 0.438746
Finished training it 6144/76743 of epoch 4, 81.03 ms/it, loss 0.438538
Finished training it 7168/76743 of epoch 4, 81.70 ms/it, loss 0.437994
Finished training it 7168/76743 of epoch 4, 81.39 ms/it, loss 0.440904
Finished training it 7168/76743 of epoch 4, 81.46 ms/it, loss 0.440282
Finished training it 7168/76743 of epoch 4, 81.43 ms/it, loss 0.440692
Finished training it 8192/76743 of epoch 4, 81.02 ms/it, loss 0.440976
Finished training it 8192/76743 of epoch 4, 80.77 ms/it, loss 0.440703
Finished training it 8192/76743 of epoch 4, 81.43 ms/it, loss 0.437802
Finished training it 8192/76743 of epoch 4, 80.87 ms/it, loss 0.439039
Finished training it 9216/76743 of epoch 4, 82.02 ms/it, loss 0.438862
Finished training it 9216/76743 of epoch 4, 82.18 ms/it, loss 0.438825
Finished training it 9216/76743 of epoch 4, 81.79 ms/it, loss 0.437520
Finished training it 9216/76743 of epoch 4, 81.61 ms/it, loss 0.438504
Finished training it 10240/76743 of epoch 4, 82.35 ms/it, loss 0.439957
Finished training it 10240/76743 of epoch 4, 82.41 ms/it, loss 0.441809
Finished training it 10240/76743 of epoch 4, 82.64 ms/it, loss 0.440840
Finished training it 10240/76743 of epoch 4, 82.39 ms/it, loss 0.439484
Testing at - 10240/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583162.0
get out
0 has test check 2583162.0 and sample count 3274240
 accuracy 78.893 %, best 78.904 %, roc auc score 0.8032, best 0.8036
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 4, 81.90 ms/it, loss 0.439661
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583166.0
get out
1 has test check 2583166.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 81.74 ms/it, loss 0.437262
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583158.0
get out
3 has test check 2583158.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 81.84 ms/it, loss 0.438494
Testing at - 10240/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583168.0
get out
2 has test check 2583168.0 and sample count 3274240
Finished training it 11264/76743 of epoch 4, 82.08 ms/it, loss 0.437942
Finished training it 12288/76743 of epoch 4, 81.30 ms/it, loss 0.441154
Finished training it 12288/76743 of epoch 4, 81.13 ms/it, loss 0.440345
Finished training it 12288/76743 of epoch 4, 80.80 ms/it, loss 0.437146
Finished training it 12288/76743 of epoch 4, 80.74 ms/it, loss 0.439073
Finished training it 13312/76743 of epoch 4, 81.53 ms/it, loss 0.437923
Finished training it 13312/76743 of epoch 4, 81.06 ms/it, loss 0.441825
Finished training it 13312/76743 of epoch 4, 81.60 ms/it, loss 0.435602
Finished training it 13312/76743 of epoch 4, 81.08 ms/it, loss 0.441482
Finished training it 14336/76743 of epoch 4, 96.18 ms/it, loss 0.438264
Finished training it 14336/76743 of epoch 4, 95.90 ms/it, loss 0.440540
Finished training it 14336/76743 of epoch 4, 96.31 ms/it, loss 0.439179
Finished training it 14336/76743 of epoch 4, 96.19 ms/it, loss 0.435374
Finished training it 15360/76743 of epoch 4, 81.53 ms/it, loss 0.440960
Finished training it 15360/76743 of epoch 4, 81.72 ms/it, loss 0.439805
Finished training it 15360/76743 of epoch 4, 81.56 ms/it, loss 0.439366
Finished training it 15360/76743 of epoch 4, 81.26 ms/it, loss 0.439879
Finished training it 16384/76743 of epoch 4, 82.18 ms/it, loss 0.438173
Finished training it 16384/76743 of epoch 4, 82.45 ms/it, loss 0.439359
Finished training it 16384/76743 of epoch 4, 82.44 ms/it, loss 0.439003
Finished training it 16384/76743 of epoch 4, 81.60 ms/it, loss 0.439638
Finished training it 17408/76743 of epoch 4, 80.36 ms/it, loss 0.439374
Finished training it 17408/76743 of epoch 4, 80.34 ms/it, loss 0.440623
Finished training it 17408/76743 of epoch 4, 79.93 ms/it, loss 0.441626
Finished training it 17408/76743 of epoch 4, 80.10 ms/it, loss 0.438911
Finished training it 18432/76743 of epoch 4, 81.03 ms/it, loss 0.442898
Finished training it 18432/76743 of epoch 4, 81.22 ms/it, loss 0.434805
Finished training it 18432/76743 of epoch 4, 80.91 ms/it, loss 0.436950
Finished training it 18432/76743 of epoch 4, 80.50 ms/it, loss 0.437810
Finished training it 19456/76743 of epoch 4, 81.61 ms/it, loss 0.439677
Finished training it 19456/76743 of epoch 4, 81.75 ms/it, loss 0.440722
Finished training it 19456/76743 of epoch 4, 81.31 ms/it, loss 0.440762
Finished training it 19456/76743 of epoch 4, 81.35 ms/it, loss 0.440476
Finished training it 20480/76743 of epoch 4, 82.23 ms/it, loss 0.440090
Finished training it 20480/76743 of epoch 4, 82.19 ms/it, loss 0.439175
Finished training it 20480/76743 of epoch 4, 82.04 ms/it, loss 0.442486
Finished training it 20480/76743 of epoch 4, 81.82 ms/it, loss 0.439695
Testing at - 20480/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584264.0
get out
0 has test check 2584264.0 and sample count 3274240
 accuracy 78.927 %, best 78.927 %, roc auc score 0.8038, best 0.8038
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584270.0
get out
1 has test check 2584270.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 82.07 ms/it, loss 0.441968
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 4, 81.56 ms/it, loss 0.439951
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584270.0
get out
3 has test check 2584270.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 82.38 ms/it, loss 0.439676
Testing at - 20480/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584265.0
get out
2 has test check 2584265.0 and sample count 3274240
Finished training it 21504/76743 of epoch 4, 82.38 ms/it, loss 0.436758
Finished training it 22528/76743 of epoch 4, 80.74 ms/it, loss 0.439565
Finished training it 22528/76743 of epoch 4, 80.97 ms/it, loss 0.440676
Finished training it 22528/76743 of epoch 4, 80.44 ms/it, loss 0.438457
Finished training it 22528/76743 of epoch 4, 80.39 ms/it, loss 0.440223
Finished training it 23552/76743 of epoch 4, 81.56 ms/it, loss 0.438756
Finished training it 23552/76743 of epoch 4, 80.89 ms/it, loss 0.440348
Finished training it 23552/76743 of epoch 4, 81.28 ms/it, loss 0.440471
Finished training it 23552/76743 of epoch 4, 81.71 ms/it, loss 0.440240
Finished training it 24576/76743 of epoch 4, 81.42 ms/it, loss 0.438433
Finished training it 24576/76743 of epoch 4, 81.55 ms/it, loss 0.442236
Finished training it 24576/76743 of epoch 4, 81.29 ms/it, loss 0.436386
Finished training it 24576/76743 of epoch 4, 81.43 ms/it, loss 0.440312
Finished training it 25600/76743 of epoch 4, 82.27 ms/it, loss 0.439741
Finished training it 25600/76743 of epoch 4, 81.88 ms/it, loss 0.439298
Finished training it 25600/76743 of epoch 4, 81.84 ms/it, loss 0.435726
Finished training it 25600/76743 of epoch 4, 81.64 ms/it, loss 0.437879
Finished training it 26624/76743 of epoch 4, 82.06 ms/it, loss 0.437666
Finished training it 26624/76743 of epoch 4, 82.01 ms/it, loss 0.440481
Finished training it 26624/76743 of epoch 4, 81.92 ms/it, loss 0.438739
Finished training it 26624/76743 of epoch 4, 81.97 ms/it, loss 0.435624
Finished training it 27648/76743 of epoch 4, 81.40 ms/it, loss 0.437853
Finished training it 27648/76743 of epoch 4, 80.91 ms/it, loss 0.439640
Finished training it 27648/76743 of epoch 4, 80.84 ms/it, loss 0.436453
Finished training it 27648/76743 of epoch 4, 81.06 ms/it, loss 0.436865
Finished training it 28672/76743 of epoch 4, 80.65 ms/it, loss 0.441231
Finished training it 28672/76743 of epoch 4, 80.78 ms/it, loss 0.442478
Finished training it 28672/76743 of epoch 4, 81.04 ms/it, loss 0.439631
Finished training it 28672/76743 of epoch 4, 80.77 ms/it, loss 0.440002
Finished training it 29696/76743 of epoch 4, 81.10 ms/it, loss 0.440020
Finished training it 29696/76743 of epoch 4, 81.63 ms/it, loss 0.439521
Finished training it 29696/76743 of epoch 4, 81.29 ms/it, loss 0.438450
Finished training it 29696/76743 of epoch 4, 81.90 ms/it, loss 0.440089
Finished training it 30720/76743 of epoch 4, 81.30 ms/it, loss 0.440114
Finished training it 30720/76743 of epoch 4, 81.30 ms/it, loss 0.439604
Finished training it 30720/76743 of epoch 4, 81.55 ms/it, loss 0.438908
Finished training it 30720/76743 of epoch 4, 81.20 ms/it, loss 0.438397
Testing at - 30720/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583978.0
get out
0 has test check 2583978.0 and sample count 3274240
 accuracy 78.918 %, best 78.927 %, roc auc score 0.8038, best 0.8038
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 4, 80.42 ms/it, loss 0.437232
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583976.0
get out
3 has test check 2583976.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 80.72 ms/it, loss 0.441227
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583986.0
get out
2 has test check 2583986.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 80.67 ms/it, loss 0.439636
Testing at - 30720/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583995.0
get out
1 has test check 2583995.0 and sample count 3274240
Finished training it 31744/76743 of epoch 4, 80.48 ms/it, loss 0.438445
Finished training it 32768/76743 of epoch 4, 81.86 ms/it, loss 0.438896
Finished training it 32768/76743 of epoch 4, 81.81 ms/it, loss 0.438907
Finished training it 32768/76743 of epoch 4, 81.72 ms/it, loss 0.441504
Finished training it 32768/76743 of epoch 4, 81.34 ms/it, loss 0.439036
Finished training it 33792/76743 of epoch 4, 92.08 ms/it, loss 0.436471
Finished training it 33792/76743 of epoch 4, 92.12 ms/it, loss 0.438114
Finished training it 33792/76743 of epoch 4, 92.17 ms/it, loss 0.436970
Finished training it 33792/76743 of epoch 4, 91.69 ms/it, loss 0.438448
Finished training it 34816/76743 of epoch 4, 81.61 ms/it, loss 0.440645
Finished training it 34816/76743 of epoch 4, 81.98 ms/it, loss 0.440467
Finished training it 34816/76743 of epoch 4, 81.73 ms/it, loss 0.438392
Finished training it 34816/76743 of epoch 4, 81.93 ms/it, loss 0.438537
Finished training it 35840/76743 of epoch 4, 81.77 ms/it, loss 0.440014
Finished training it 35840/76743 of epoch 4, 81.93 ms/it, loss 0.439915
Finished training it 35840/76743 of epoch 4, 81.81 ms/it, loss 0.438309
Finished training it 35840/76743 of epoch 4, 81.58 ms/it, loss 0.436814
Finished training it 36864/76743 of epoch 4, 81.97 ms/it, loss 0.435810
Finished training it 36864/76743 of epoch 4, 81.80 ms/it, loss 0.438154
Finished training it 36864/76743 of epoch 4, 81.51 ms/it, loss 0.436425
Finished training it 36864/76743 of epoch 4, 82.01 ms/it, loss 0.438746
Finished training it 37888/76743 of epoch 4, 82.45 ms/it, loss 0.439097
Finished training it 37888/76743 of epoch 4, 82.10 ms/it, loss 0.438544
Finished training it 37888/76743 of epoch 4, 82.40 ms/it, loss 0.437855
Finished training it 37888/76743 of epoch 4, 82.45 ms/it, loss 0.438958
Finished training it 38912/76743 of epoch 4, 82.23 ms/it, loss 0.438869
Finished training it 38912/76743 of epoch 4, 81.93 ms/it, loss 0.438785
Finished training it 38912/76743 of epoch 4, 82.17 ms/it, loss 0.439580
Finished training it 38912/76743 of epoch 4, 82.16 ms/it, loss 0.441257
Finished training it 39936/76743 of epoch 4, 80.89 ms/it, loss 0.436697
Finished training it 39936/76743 of epoch 4, 80.81 ms/it, loss 0.439668
Finished training it 39936/76743 of epoch 4, 80.56 ms/it, loss 0.437772
Finished training it 39936/76743 of epoch 4, 80.95 ms/it, loss 0.439342
Finished training it 40960/76743 of epoch 4, 81.37 ms/it, loss 0.441297
Finished training it 40960/76743 of epoch 4, 81.11 ms/it, loss 0.437902
Finished training it 40960/76743 of epoch 4, 81.51 ms/it, loss 0.435931
Finished training it 40960/76743 of epoch 4, 81.61 ms/it, loss 0.438376
Testing at - 40960/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2583908.0
get out
0 has test check 2583908.0 and sample count 3274240
 accuracy 78.916 %, best 78.927 %, roc auc score 0.8036, best 0.8038
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2583921.0
get out
2 has test check 2583921.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 81.90 ms/it, loss 0.436426
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 4, 82.08 ms/it, loss 0.439738
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2583918.0
get out
1 has test check 2583918.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 82.00 ms/it, loss 0.439666
Testing at - 40960/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2583908.0
get out
3 has test check 2583908.0 and sample count 3274240
Finished training it 41984/76743 of epoch 4, 82.01 ms/it, loss 0.437286
Finished training it 43008/76743 of epoch 4, 82.07 ms/it, loss 0.438365
Finished training it 43008/76743 of epoch 4, 82.35 ms/it, loss 0.440352
Finished training it 43008/76743 of epoch 4, 81.61 ms/it, loss 0.438700
Finished training it 43008/76743 of epoch 4, 82.13 ms/it, loss 0.437677
Finished training it 44032/76743 of epoch 4, 80.51 ms/it, loss 0.441972
Finished training it 44032/76743 of epoch 4, 80.67 ms/it, loss 0.440471
Finished training it 44032/76743 of epoch 4, 80.12 ms/it, loss 0.438806
Finished training it 44032/76743 of epoch 4, 80.54 ms/it, loss 0.437849
Finished training it 45056/76743 of epoch 4, 85.83 ms/it, loss 0.437022
Finished training it 45056/76743 of epoch 4, 85.92 ms/it, loss 0.436657
Finished training it 45056/76743 of epoch 4, 85.82 ms/it, loss 0.434450
Finished training it 45056/76743 of epoch 4, 86.12 ms/it, loss 0.439218
Finished training it 46080/76743 of epoch 4, 81.56 ms/it, loss 0.438809
Finished training it 46080/76743 of epoch 4, 81.32 ms/it, loss 0.439010
Finished training it 46080/76743 of epoch 4, 81.60 ms/it, loss 0.438364
Finished training it 46080/76743 of epoch 4, 81.44 ms/it, loss 0.435091
Finished training it 47104/76743 of epoch 4, 81.16 ms/it, loss 0.438493
Finished training it 47104/76743 of epoch 4, 80.85 ms/it, loss 0.438684
Finished training it 47104/76743 of epoch 4, 81.06 ms/it, loss 0.437939
Finished training it 47104/76743 of epoch 4, 81.11 ms/it, loss 0.438728
Finished training it 48128/76743 of epoch 4, 80.79 ms/it, loss 0.437418
Finished training it 48128/76743 of epoch 4, 81.52 ms/it, loss 0.436152
Finished training it 48128/76743 of epoch 4, 81.52 ms/it, loss 0.437314
Finished training it 48128/76743 of epoch 4, 81.31 ms/it, loss 0.437844
Finished training it 49152/76743 of epoch 4, 80.69 ms/it, loss 0.438703
Finished training it 49152/76743 of epoch 4, 80.46 ms/it, loss 0.439406
Finished training it 49152/76743 of epoch 4, 80.67 ms/it, loss 0.437526
Finished training it 49152/76743 of epoch 4, 80.44 ms/it, loss 0.436834
Finished training it 50176/76743 of epoch 4, 82.33 ms/it, loss 0.438911
Finished training it 50176/76743 of epoch 4, 82.60 ms/it, loss 0.436671
Finished training it 50176/76743 of epoch 4, 82.30 ms/it, loss 0.438656
Finished training it 50176/76743 of epoch 4, 82.31 ms/it, loss 0.436357
Finished training it 51200/76743 of epoch 4, 80.64 ms/it, loss 0.438481
Finished training it 51200/76743 of epoch 4, 80.51 ms/it, loss 0.437591
Finished training it 51200/76743 of epoch 4, 80.59 ms/it, loss 0.435403
Finished training it 51200/76743 of epoch 4, 80.59 ms/it, loss 0.437839
Testing at - 51200/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582775.0
get out
0 has test check 2582775.0 and sample count 3274240
 accuracy 78.882 %, best 78.927 %, roc auc score 0.8032, best 0.8038
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 4, 81.47 ms/it, loss 0.435172
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582782.0
get out
3 has test check 2582782.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 81.72 ms/it, loss 0.437244
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582779.0
get out
1 has test check 2582779.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 81.50 ms/it, loss 0.439815
Testing at - 51200/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582791.0
get out
2 has test check 2582791.0 and sample count 3274240
Finished training it 52224/76743 of epoch 4, 81.42 ms/it, loss 0.437099
Finished training it 53248/76743 of epoch 4, 80.51 ms/it, loss 0.437388
Finished training it 53248/76743 of epoch 4, 80.20 ms/it, loss 0.436047
Finished training it 53248/76743 of epoch 4, 80.19 ms/it, loss 0.439256
Finished training it 53248/76743 of epoch 4, 80.33 ms/it, loss 0.439181
Finished training it 54272/76743 of epoch 4, 87.18 ms/it, loss 0.440282
Finished training it 54272/76743 of epoch 4, 86.64 ms/it, loss 0.436878
Finished training it 54272/76743 of epoch 4, 87.31 ms/it, loss 0.437862
Finished training it 54272/76743 of epoch 4, 86.80 ms/it, loss 0.442015
Finished training it 55296/76743 of epoch 4, 81.44 ms/it, loss 0.437684
Finished training it 55296/76743 of epoch 4, 81.53 ms/it, loss 0.437615
Finished training it 55296/76743 of epoch 4, 81.46 ms/it, loss 0.441196
Finished training it 55296/76743 of epoch 4, 81.34 ms/it, loss 0.438632
Finished training it 56320/76743 of epoch 4, 80.91 ms/it, loss 0.439603
Finished training it 56320/76743 of epoch 4, 80.78 ms/it, loss 0.436552
Finished training it 56320/76743 of epoch 4, 80.37 ms/it, loss 0.436782
Finished training it 56320/76743 of epoch 4, 80.68 ms/it, loss 0.437085
Finished training it 57344/76743 of epoch 4, 82.16 ms/it, loss 0.436221
Finished training it 57344/76743 of epoch 4, 82.12 ms/it, loss 0.437841
Finished training it 57344/76743 of epoch 4, 81.62 ms/it, loss 0.437587
Finished training it 57344/76743 of epoch 4, 81.74 ms/it, loss 0.436668
Finished training it 58368/76743 of epoch 4, 81.05 ms/it, loss 0.436045
Finished training it 58368/76743 of epoch 4, 80.71 ms/it, loss 0.437155
Finished training it 58368/76743 of epoch 4, 80.98 ms/it, loss 0.440099
Finished training it 58368/76743 of epoch 4, 80.70 ms/it, loss 0.438607
Finished training it 59392/76743 of epoch 4, 81.23 ms/it, loss 0.438793
Finished training it 59392/76743 of epoch 4, 81.24 ms/it, loss 0.437076
Finished training it 59392/76743 of epoch 4, 81.30 ms/it, loss 0.437188
Finished training it 59392/76743 of epoch 4, 81.20 ms/it, loss 0.435033
Finished training it 60416/76743 of epoch 4, 80.61 ms/it, loss 0.436120
Finished training it 60416/76743 of epoch 4, 80.62 ms/it, loss 0.437985
Finished training it 60416/76743 of epoch 4, 80.69 ms/it, loss 0.434098
Finished training it 60416/76743 of epoch 4, 80.88 ms/it, loss 0.436982
Finished training it 61440/76743 of epoch 4, 82.39 ms/it, loss 0.438982
Finished training it 61440/76743 of epoch 4, 82.05 ms/it, loss 0.436199
Finished training it 61440/76743 of epoch 4, 82.24 ms/it, loss 0.440878
Finished training it 61440/76743 of epoch 4, 82.03 ms/it, loss 0.437870
Testing at - 61440/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2584164.0
get out
0 has test check 2584164.0 and sample count 3274240
 accuracy 78.924 %, best 78.927 %, roc auc score 0.8040, best 0.8040
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2584155.0
get out
3 has test check 2584155.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 81.15 ms/it, loss 0.437280
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2584159.0
get out
2 has test check 2584159.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 80.86 ms/it, loss 0.436861
Testing at - 61440/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2584154.0
get out
1 has test check 2584154.0 and sample count 3274240
Finished training it 62464/76743 of epoch 4, 81.17 ms/it, loss 0.435948
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 4, 81.29 ms/it, loss 0.439883
Finished training it 63488/76743 of epoch 4, 80.60 ms/it, loss 0.441202
Finished training it 63488/76743 of epoch 4, 80.76 ms/it, loss 0.437322
Finished training it 63488/76743 of epoch 4, 80.47 ms/it, loss 0.436032
Finished training it 63488/76743 of epoch 4, 80.62 ms/it, loss 0.436484
Finished training it 64512/76743 of epoch 4, 85.21 ms/it, loss 0.437060
Finished training it 64512/76743 of epoch 4, 84.55 ms/it, loss 0.438106
Finished training it 64512/76743 of epoch 4, 84.84 ms/it, loss 0.439359
Finished training it 64512/76743 of epoch 4, 84.38 ms/it, loss 0.438097
Finished training it 65536/76743 of epoch 4, 86.63 ms/it, loss 0.436914
Finished training it 65536/76743 of epoch 4, 86.10 ms/it, loss 0.437608
Finished training it 65536/76743 of epoch 4, 86.03 ms/it, loss 0.440004
Finished training it 65536/76743 of epoch 4, 86.31 ms/it, loss 0.438875
Finished training it 66560/76743 of epoch 4, 81.53 ms/it, loss 0.435524
Finished training it 66560/76743 of epoch 4, 81.04 ms/it, loss 0.438648
Finished training it 66560/76743 of epoch 4, 81.23 ms/it, loss 0.438445
Finished training it 66560/76743 of epoch 4, 80.96 ms/it, loss 0.437151
Finished training it 67584/76743 of epoch 4, 81.58 ms/it, loss 0.437341
Finished training it 67584/76743 of epoch 4, 81.86 ms/it, loss 0.439812
Finished training it 67584/76743 of epoch 4, 82.26 ms/it, loss 0.440068
Finished training it 67584/76743 of epoch 4, 81.71 ms/it, loss 0.438554
Finished training it 68608/76743 of epoch 4, 80.73 ms/it, loss 0.436490
Finished training it 68608/76743 of epoch 4, 80.81 ms/it, loss 0.437003
Finished training it 68608/76743 of epoch 4, 80.55 ms/it, loss 0.435058
Finished training it 68608/76743 of epoch 4, 80.72 ms/it, loss 0.437886
Finished training it 69632/76743 of epoch 4, 81.11 ms/it, loss 0.439433
Finished training it 69632/76743 of epoch 4, 81.25 ms/it, loss 0.436015
Finished training it 69632/76743 of epoch 4, 80.84 ms/it, loss 0.437862
Finished training it 69632/76743 of epoch 4, 80.86 ms/it, loss 0.437598
Finished training it 70656/76743 of epoch 4, 82.06 ms/it, loss 0.438520
Finished training it 70656/76743 of epoch 4, 82.11 ms/it, loss 0.436147
Finished training it 70656/76743 of epoch 4, 81.89 ms/it, loss 0.436407
Finished training it 70656/76743 of epoch 4, 82.20 ms/it, loss 0.439968
Finished training it 71680/76743 of epoch 4, 81.30 ms/it, loss 0.434050
Finished training it 71680/76743 of epoch 4, 81.27 ms/it, loss 0.433798
Finished training it 71680/76743 of epoch 4, 81.04 ms/it, loss 0.436971
Finished training it 71680/76743 of epoch 4, 81.57 ms/it, loss 0.441611
Testing at - 71680/76743 of epoch 4,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2579482.0
get out
0 has test check 2579482.0 and sample count 3274240
 accuracy 78.781 %, best 78.927 %, roc auc score 0.8037, best 0.8040
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2579471.0
get out
3 has test check 2579471.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 81.77 ms/it, loss 0.438316
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2579507.0
get out
2 has test check 2579507.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 81.18 ms/it, loss 0.436246
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 4, 81.47 ms/it, loss 0.436529
Testing at - 71680/76743 of epoch 4,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2579484.0
get out
1 has test check 2579484.0 and sample count 3274240
Finished training it 72704/76743 of epoch 4, 81.55 ms/it, loss 0.441201
Finished training it 73728/76743 of epoch 4, 81.84 ms/it, loss 0.438305
Finished training it 73728/76743 of epoch 4, 81.65 ms/it, loss 0.436446
Finished training it 73728/76743 of epoch 4, 81.90 ms/it, loss 0.436108
Finished training it 73728/76743 of epoch 4, 82.06 ms/it, loss 0.438608
Finished training it 74752/76743 of epoch 4, 83.63 ms/it, loss 0.441056
Finished training it 74752/76743 of epoch 4, 82.85 ms/it, loss 0.437414
Finished training it 74752/76743 of epoch 4, 83.06 ms/it, loss 0.438881
Finished training it 74752/76743 of epoch 4, 83.00 ms/it, loss 0.437135
Finished training it 75776/76743 of epoch 4, 81.57 ms/it, loss 0.436227
Finished training it 75776/76743 of epoch 4, 81.26 ms/it, loss 0.437110
Finished training it 75776/76743 of epoch 4, 81.33 ms/it, loss 0.437012
Finished training it 75776/76743 of epoch 4, 81.34 ms/it, loss 0.436133
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2582953.0
get out
0 has test check 2582953.0 and sample count 3274240
 accuracy 78.887 %, best 78.927 %, roc auc score 0.8039, best 0.8040
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2582976.0
get out
2 has test check 2582976.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2582968.0
get out
1 has test check 2582968.0 and sample count 3274240
Warning: Skipping the batch 76742 with size 14
Testing at - 76743/76743 of epoch 5,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2582958.0
get out
3 has test check 2582958.0 and sample count 3274240
