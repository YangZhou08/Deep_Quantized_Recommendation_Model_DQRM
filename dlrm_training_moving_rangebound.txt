Unable to import mlperf_logging,  No module named 'mlperf_logging'
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 80.36 ms/it, loss 19.233753
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 80.57 ms/it, loss 19.302453
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 80.42 ms/it, loss 19.187383
Unable to import mlperf_logging,  No module named 'mlperf_logging'
Warning: local_rank gpu mismatch
4 out of -1 (GPU)
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/rscratch/data/dlrm_criteo/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
number of devices 1
world size found is -1
---------- Embedding Table 0, quantization used, n = 1460, m = 16, quantization bit set to 4
---------- Embedding Table 1, quantization used, n = 583, m = 16, quantization bit set to 4
---------- Embedding Table 2, quantization used, n = 10131227, m = 16, quantization bit set to 4
---------- Embedding Table 3, quantization used, n = 2202608, m = 16, quantization bit set to 4
---------- Embedding Table 4, quantization used, n = 305, m = 16, quantization bit set to 4
---------- Embedding Table 5, quantization used, n = 24, m = 16, quantization bit set to 4
---------- Embedding Table 6, quantization used, n = 12517, m = 16, quantization bit set to 4
---------- Embedding Table 7, quantization used, n = 633, m = 16, quantization bit set to 4
---------- Embedding Table 8, quantization used, n = 3, m = 16, quantization bit set to 4
---------- Embedding Table 9, quantization used, n = 93145, m = 16, quantization bit set to 4
---------- Embedding Table 10, quantization used, n = 5683, m = 16, quantization bit set to 4
---------- Embedding Table 11, quantization used, n = 8351593, m = 16, quantization bit set to 4
---------- Embedding Table 12, quantization used, n = 3194, m = 16, quantization bit set to 4
---------- Embedding Table 13, quantization used, n = 27, m = 16, quantization bit set to 4
---------- Embedding Table 14, quantization used, n = 14992, m = 16, quantization bit set to 4
---------- Embedding Table 15, quantization used, n = 5461306, m = 16, quantization bit set to 4
---------- Embedding Table 16, quantization used, n = 10, m = 16, quantization bit set to 4
---------- Embedding Table 17, quantization used, n = 5652, m = 16, quantization bit set to 4
---------- Embedding Table 18, quantization used, n = 2173, m = 16, quantization bit set to 4
---------- Embedding Table 19, quantization used, n = 4, m = 16, quantization bit set to 4
---------- Embedding Table 20, quantization used, n = 7046547, m = 16, quantization bit set to 4
---------- Embedding Table 21, quantization used, n = 18, m = 16, quantization bit set to 4
---------- Embedding Table 22, quantization used, n = 15, m = 16, quantization bit set to 4
---------- Embedding Table 23, quantization used, n = 286181, m = 16, quantization bit set to 4
---------- Embedding Table 24, quantization used, n = 105, m = 16, quantization bit set to 4
---------- Embedding Table 25, quantization used, n = 142572, m = 16, quantization bit set to 4
log path is written: /rscratch/data/dlrm_criteo/
optimizer selected is  sgd
Finished training it 1024/76743 of epoch 0, 80.97 ms/it, loss 19.114345
Finished training it 2048/76743 of epoch 0, 73.51 ms/it, loss 24.236587
Finished training it 2048/76743 of epoch 0, 73.62 ms/it, loss 24.167861
Finished training it 2048/76743 of epoch 0, 73.71 ms/it, loss 24.161479
Finished training it 2048/76743 of epoch 0, 73.73 ms/it, loss 24.224348
Finished training it 3072/76743 of epoch 0, 73.61 ms/it, loss 24.544390
Finished training it 3072/76743 of epoch 0, 73.59 ms/it, loss 24.128246
Finished training it 3072/76743 of epoch 0, 73.59 ms/it, loss 24.263063
Finished training it 3072/76743 of epoch 0, 73.37 ms/it, loss 24.456197
Finished training it 4096/76743 of epoch 0, 73.83 ms/it, loss 24.329958
Finished training it 4096/76743 of epoch 0, 74.01 ms/it, loss 24.421389
Finished training it 4096/76743 of epoch 0, 73.97 ms/it, loss 24.430781
Finished training it 4096/76743 of epoch 0, 73.94 ms/it, loss 24.184746
Finished training it 5120/76743 of epoch 0, 73.20 ms/it, loss 24.440001
Finished training it 5120/76743 of epoch 0, 73.26 ms/it, loss 24.158766
Finished training it 5120/76743 of epoch 0, 73.31 ms/it, loss 24.390902
Finished training it 5120/76743 of epoch 0, 73.31 ms/it, loss 24.453278
Finished training it 6144/76743 of epoch 0, 73.15 ms/it, loss 24.139084
Finished training it 6144/76743 of epoch 0, 73.12 ms/it, loss 24.149218
Finished training it 6144/76743 of epoch 0, 73.31 ms/it, loss 24.371934
Finished training it 6144/76743 of epoch 0, 73.27 ms/it, loss 24.359928
Finished training it 7168/76743 of epoch 0, 73.49 ms/it, loss 24.213713
Finished training it 7168/76743 of epoch 0, 73.32 ms/it, loss 24.317252
Finished training it 7168/76743 of epoch 0, 73.35 ms/it, loss 24.289040
Finished training it 7168/76743 of epoch 0, 73.36 ms/it, loss 24.207261
Finished training it 8192/76743 of epoch 0, 73.61 ms/it, loss 24.174687
Finished training it 8192/76743 of epoch 0, 73.56 ms/it, loss 24.071066
Finished training it 8192/76743 of epoch 0, 73.47 ms/it, loss 24.358855
Finished training it 8192/76743 of epoch 0, 73.41 ms/it, loss 24.113641
Finished training it 9216/76743 of epoch 0, 72.96 ms/it, loss 24.313274
Finished training it 9216/76743 of epoch 0, 72.92 ms/it, loss 24.297129
Finished training it 9216/76743 of epoch 0, 72.92 ms/it, loss 24.461437
Finished training it 9216/76743 of epoch 0, 72.89 ms/it, loss 24.309817
Finished training it 10240/76743 of epoch 0, 72.61 ms/it, loss 24.231670
Finished training it 10240/76743 of epoch 0, 72.65 ms/it, loss 24.335599
Finished training it 10240/76743 of epoch 0, 72.68 ms/it, loss 24.285060
Finished training it 10240/76743 of epoch 0, 72.50 ms/it, loss 24.353952
Testing at - 10240/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 74.56 ms/it, loss 24.323598
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 0, 74.35 ms/it, loss 24.357442
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 74.49 ms/it, loss 24.467772
Testing at - 10240/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 0, 74.44 ms/it, loss 24.220365
Finished training it 12288/76743 of epoch 0, 73.92 ms/it, loss 24.351425
Finished training it 12288/76743 of epoch 0, 74.09 ms/it, loss 24.353618
Finished training it 12288/76743 of epoch 0, 74.00 ms/it, loss 24.261673
Finished training it 12288/76743 of epoch 0, 73.97 ms/it, loss 24.309236
Finished training it 13312/76743 of epoch 0, 83.89 ms/it, loss 24.489936
Finished training it 13312/76743 of epoch 0, 83.91 ms/it, loss 24.256704
Finished training it 13312/76743 of epoch 0, 84.36 ms/it, loss 24.290612
Finished training it 13312/76743 of epoch 0, 83.96 ms/it, loss 24.155407
Finished training it 14336/76743 of epoch 0, 74.47 ms/it, loss 24.171313
Finished training it 14336/76743 of epoch 0, 74.46 ms/it, loss 24.256709
Finished training it 14336/76743 of epoch 0, 74.41 ms/it, loss 24.472743
Finished training it 14336/76743 of epoch 0, 74.68 ms/it, loss 24.350832
Finished training it 15360/76743 of epoch 0, 80.53 ms/it, loss 24.228047
Finished training it 15360/76743 of epoch 0, 80.37 ms/it, loss 24.185034
Finished training it 15360/76743 of epoch 0, 80.45 ms/it, loss 24.260846
Finished training it 15360/76743 of epoch 0, 80.34 ms/it, loss 24.181194
Finished training it 16384/76743 of epoch 0, 74.30 ms/it, loss 24.397548
Finished training it 16384/76743 of epoch 0, 74.07 ms/it, loss 24.120525
Finished training it 16384/76743 of epoch 0, 74.06 ms/it, loss 24.223141
Finished training it 16384/76743 of epoch 0, 74.05 ms/it, loss 24.328512
Finished training it 17408/76743 of epoch 0, 74.51 ms/it, loss 24.278783
Finished training it 17408/76743 of epoch 0, 74.60 ms/it, loss 24.088693
Finished training it 17408/76743 of epoch 0, 74.31 ms/it, loss 24.039388
Finished training it 17408/76743 of epoch 0, 74.60 ms/it, loss 24.131700
Finished training it 18432/76743 of epoch 0, 74.16 ms/it, loss 24.187812
Finished training it 18432/76743 of epoch 0, 74.30 ms/it, loss 24.424964
Finished training it 18432/76743 of epoch 0, 74.08 ms/it, loss 24.319557
Finished training it 18432/76743 of epoch 0, 74.11 ms/it, loss 24.174317
Finished training it 19456/76743 of epoch 0, 74.05 ms/it, loss 24.529609
Finished training it 19456/76743 of epoch 0, 74.22 ms/it, loss 24.153289
Finished training it 19456/76743 of epoch 0, 74.01 ms/it, loss 24.204686
Finished training it 19456/76743 of epoch 0, 74.04 ms/it, loss 24.326517
Finished training it 20480/76743 of epoch 0, 74.54 ms/it, loss 24.159413
Finished training it 20480/76743 of epoch 0, 74.73 ms/it, loss 24.093179
Finished training it 20480/76743 of epoch 0, 74.75 ms/it, loss 24.298019
Finished training it 20480/76743 of epoch 0, 74.71 ms/it, loss 24.375622
Testing at - 20480/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 0, 74.06 ms/it, loss 24.054095
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 74.33 ms/it, loss 24.272153
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 74.41 ms/it, loss 24.281373
Testing at - 20480/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 0, 74.30 ms/it, loss 24.195706
Finished training it 22528/76743 of epoch 0, 74.46 ms/it, loss 24.229826
Finished training it 22528/76743 of epoch 0, 74.41 ms/it, loss 24.206991
Finished training it 22528/76743 of epoch 0, 74.40 ms/it, loss 24.471648
Finished training it 22528/76743 of epoch 0, 74.23 ms/it, loss 24.162207
Finished training it 23552/76743 of epoch 0, 74.69 ms/it, loss 24.619583
Finished training it 23552/76743 of epoch 0, 74.50 ms/it, loss 24.327694
Finished training it 23552/76743 of epoch 0, 74.65 ms/it, loss 24.225228
Finished training it 23552/76743 of epoch 0, 74.50 ms/it, loss 24.273973
Finished training it 24576/76743 of epoch 0, 80.41 ms/it, loss 24.634974
Finished training it 24576/76743 of epoch 0, 80.15 ms/it, loss 24.274088
Finished training it 24576/76743 of epoch 0, 80.08 ms/it, loss 24.265896
Finished training it 24576/76743 of epoch 0, 80.21 ms/it, loss 24.280983
Finished training it 25600/76743 of epoch 0, 84.31 ms/it, loss 24.237091
Finished training it 25600/76743 of epoch 0, 84.08 ms/it, loss 24.526065
Finished training it 25600/76743 of epoch 0, 84.57 ms/it, loss 24.237933
Finished training it 25600/76743 of epoch 0, 84.31 ms/it, loss 24.171101
Finished training it 26624/76743 of epoch 0, 74.49 ms/it, loss 24.277781
Finished training it 26624/76743 of epoch 0, 74.51 ms/it, loss 24.238076
Finished training it 26624/76743 of epoch 0, 74.65 ms/it, loss 24.218598
Finished training it 26624/76743 of epoch 0, 74.30 ms/it, loss 24.255146
Finished training it 27648/76743 of epoch 0, 74.83 ms/it, loss 24.202743
Finished training it 27648/76743 of epoch 0, 74.78 ms/it, loss 24.396507
Finished training it 27648/76743 of epoch 0, 74.69 ms/it, loss 24.321370
Finished training it 27648/76743 of epoch 0, 74.59 ms/it, loss 24.134486
Finished training it 28672/76743 of epoch 0, 74.34 ms/it, loss 24.206901
Finished training it 28672/76743 of epoch 0, 74.42 ms/it, loss 24.109023
Finished training it 28672/76743 of epoch 0, 74.24 ms/it, loss 24.218743
Finished training it 28672/76743 of epoch 0, 74.35 ms/it, loss 24.217211
Finished training it 29696/76743 of epoch 0, 74.43 ms/it, loss 24.305786
Finished training it 29696/76743 of epoch 0, 74.46 ms/it, loss 24.398887
Finished training it 29696/76743 of epoch 0, 74.61 ms/it, loss 24.361285
Finished training it 29696/76743 of epoch 0, 74.46 ms/it, loss 24.321783
Finished training it 30720/76743 of epoch 0, 75.25 ms/it, loss 24.071940
Finished training it 30720/76743 of epoch 0, 75.16 ms/it, loss 23.992839
Finished training it 30720/76743 of epoch 0, 75.30 ms/it, loss 24.273695
Finished training it 30720/76743 of epoch 0, 75.32 ms/it, loss 24.240841
Testing at - 30720/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 0, 75.12 ms/it, loss 24.127485
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 74.87 ms/it, loss 24.106060
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 75.06 ms/it, loss 24.271353
Testing at - 30720/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 0, 75.06 ms/it, loss 24.311424
Finished training it 32768/76743 of epoch 0, 74.46 ms/it, loss 24.230309
Finished training it 32768/76743 of epoch 0, 74.58 ms/it, loss 24.381620
Finished training it 32768/76743 of epoch 0, 74.51 ms/it, loss 24.280928
Finished training it 32768/76743 of epoch 0, 74.44 ms/it, loss 24.389219
Finished training it 33792/76743 of epoch 0, 74.76 ms/it, loss 24.526224
Finished training it 33792/76743 of epoch 0, 74.78 ms/it, loss 24.387993
Finished training it 33792/76743 of epoch 0, 74.58 ms/it, loss 24.310300
Finished training it 33792/76743 of epoch 0, 74.75 ms/it, loss 24.495869
Finished training it 34816/76743 of epoch 0, 74.19 ms/it, loss 24.357910
Finished training it 34816/76743 of epoch 0, 74.18 ms/it, loss 24.205009
Finished training it 34816/76743 of epoch 0, 74.21 ms/it, loss 24.505708
Finished training it 34816/76743 of epoch 0, 74.31 ms/it, loss 24.230187
Finished training it 35840/76743 of epoch 0, 74.50 ms/it, loss 24.209030
Finished training it 35840/76743 of epoch 0, 74.35 ms/it, loss 24.532543
Finished training it 35840/76743 of epoch 0, 74.65 ms/it, loss 24.156129
Finished training it 35840/76743 of epoch 0, 74.50 ms/it, loss 24.143403
Finished training it 36864/76743 of epoch 0, 80.09 ms/it, loss 24.338342
Finished training it 36864/76743 of epoch 0, 80.18 ms/it, loss 24.432832
Finished training it 36864/76743 of epoch 0, 79.95 ms/it, loss 24.260090
Finished training it 36864/76743 of epoch 0, 79.94 ms/it, loss 24.182978
Finished training it 37888/76743 of epoch 0, 74.36 ms/it, loss 24.348713
Finished training it 37888/76743 of epoch 0, 74.32 ms/it, loss 24.341406
Finished training it 37888/76743 of epoch 0, 74.24 ms/it, loss 24.226393
Finished training it 37888/76743 of epoch 0, 74.42 ms/it, loss 24.308601
Finished training it 38912/76743 of epoch 0, 74.77 ms/it, loss 24.178926
Finished training it 38912/76743 of epoch 0, 74.66 ms/it, loss 24.266204
Finished training it 38912/76743 of epoch 0, 74.81 ms/it, loss 24.201709
Finished training it 38912/76743 of epoch 0, 74.88 ms/it, loss 24.202864
Finished training it 39936/76743 of epoch 0, 74.35 ms/it, loss 24.322397
Finished training it 39936/76743 of epoch 0, 74.22 ms/it, loss 24.202944
Finished training it 39936/76743 of epoch 0, 74.29 ms/it, loss 24.383961
Finished training it 39936/76743 of epoch 0, 74.26 ms/it, loss 24.218407
Finished training it 40960/76743 of epoch 0, 75.02 ms/it, loss 24.190751
Finished training it 40960/76743 of epoch 0, 74.86 ms/it, loss 24.330869
Finished training it 40960/76743 of epoch 0, 74.92 ms/it, loss 24.309766
Finished training it 40960/76743 of epoch 0, 74.81 ms/it, loss 24.074388
Testing at - 40960/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 0, 74.48 ms/it, loss 24.197903
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 74.50 ms/it, loss 24.260138
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 74.54 ms/it, loss 24.277441
Testing at - 40960/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 0, 74.56 ms/it, loss 24.497215
Finished training it 43008/76743 of epoch 0, 74.58 ms/it, loss 24.144155
Finished training it 43008/76743 of epoch 0, 74.38 ms/it, loss 24.292737
Finished training it 43008/76743 of epoch 0, 74.65 ms/it, loss 24.150188
Finished training it 43008/76743 of epoch 0, 74.56 ms/it, loss 24.366356
Finished training it 44032/76743 of epoch 0, 79.31 ms/it, loss 24.348882
Finished training it 44032/76743 of epoch 0, 79.34 ms/it, loss 24.238102
Finished training it 44032/76743 of epoch 0, 79.34 ms/it, loss 24.238707
Finished training it 44032/76743 of epoch 0, 79.23 ms/it, loss 24.110822
Finished training it 45056/76743 of epoch 0, 74.53 ms/it, loss 24.268460
Finished training it 45056/76743 of epoch 0, 74.46 ms/it, loss 24.241880
Finished training it 45056/76743 of epoch 0, 74.39 ms/it, loss 24.370685
Finished training it 45056/76743 of epoch 0, 74.47 ms/it, loss 24.360514
Finished training it 46080/76743 of epoch 0, 74.62 ms/it, loss 24.285685
Finished training it 46080/76743 of epoch 0, 74.56 ms/it, loss 24.317123
Finished training it 46080/76743 of epoch 0, 74.66 ms/it, loss 24.242425
Finished training it 46080/76743 of epoch 0, 74.27 ms/it, loss 24.159737
Finished training it 47104/76743 of epoch 0, 74.84 ms/it, loss 24.259043
Finished training it 47104/76743 of epoch 0, 74.85 ms/it, loss 24.440559
Finished training it 47104/76743 of epoch 0, 74.65 ms/it, loss 24.229079
Finished training it 47104/76743 of epoch 0, 74.74 ms/it, loss 24.345867
Finished training it 48128/76743 of epoch 0, 74.20 ms/it, loss 24.410569
Finished training it 48128/76743 of epoch 0, 74.17 ms/it, loss 24.383134
Finished training it 48128/76743 of epoch 0, 74.22 ms/it, loss 24.259750
Finished training it 48128/76743 of epoch 0, 74.11 ms/it, loss 24.115572
Finished training it 49152/76743 of epoch 0, 74.67 ms/it, loss 24.251062
Finished training it 49152/76743 of epoch 0, 74.78 ms/it, loss 24.221339
Finished training it 49152/76743 of epoch 0, 74.66 ms/it, loss 24.333336
Finished training it 49152/76743 of epoch 0, 74.80 ms/it, loss 24.453304
Finished training it 50176/76743 of epoch 0, 74.73 ms/it, loss 24.396471
Finished training it 50176/76743 of epoch 0, 74.75 ms/it, loss 24.378413
Finished training it 50176/76743 of epoch 0, 74.77 ms/it, loss 24.257986
Finished training it 50176/76743 of epoch 0, 74.68 ms/it, loss 24.517205
Finished training it 51200/76743 of epoch 0, 74.69 ms/it, loss 24.262490
Finished training it 51200/76743 of epoch 0, 74.61 ms/it, loss 24.150465
Finished training it 51200/76743 of epoch 0, 74.81 ms/it, loss 24.279513
Finished training it 51200/76743 of epoch 0, 74.73 ms/it, loss 24.439163
Testing at - 51200/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 74.46 ms/it, loss 24.412005
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 0, 74.28 ms/it, loss 24.264502
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 74.53 ms/it, loss 24.125841
Testing at - 51200/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 0, 74.33 ms/it, loss 24.370621
Finished training it 53248/76743 of epoch 0, 74.41 ms/it, loss 24.282464
Finished training it 53248/76743 of epoch 0, 74.55 ms/it, loss 24.294666
Finished training it 53248/76743 of epoch 0, 74.35 ms/it, loss 24.241198
Finished training it 53248/76743 of epoch 0, 74.38 ms/it, loss 24.230039
Finished training it 54272/76743 of epoch 0, 85.19 ms/it, loss 24.279751
Finished training it 54272/76743 of epoch 0, 85.11 ms/it, loss 24.472446
Finished training it 54272/76743 of epoch 0, 85.36 ms/it, loss 24.105594
Finished training it 54272/76743 of epoch 0, 84.83 ms/it, loss 24.405288
Finished training it 55296/76743 of epoch 0, 74.72 ms/it, loss 24.274940
Finished training it 55296/76743 of epoch 0, 74.76 ms/it, loss 24.331118
Finished training it 55296/76743 of epoch 0, 74.59 ms/it, loss 24.187426
Finished training it 55296/76743 of epoch 0, 74.73 ms/it, loss 24.082505
Finished training it 56320/76743 of epoch 0, 79.36 ms/it, loss 24.431147
Finished training it 56320/76743 of epoch 0, 79.34 ms/it, loss 24.277372
Finished training it 56320/76743 of epoch 0, 79.39 ms/it, loss 24.281176
Finished training it 56320/76743 of epoch 0, 79.27 ms/it, loss 24.188760
Finished training it 57344/76743 of epoch 0, 74.33 ms/it, loss 24.292324
Finished training it 57344/76743 of epoch 0, 74.41 ms/it, loss 24.002506
Finished training it 57344/76743 of epoch 0, 74.46 ms/it, loss 24.248720
Finished training it 57344/76743 of epoch 0, 74.40 ms/it, loss 24.233338
Finished training it 58368/76743 of epoch 0, 74.53 ms/it, loss 24.222974
Finished training it 58368/76743 of epoch 0, 74.41 ms/it, loss 24.437059
Finished training it 58368/76743 of epoch 0, 74.53 ms/it, loss 24.126666
Finished training it 58368/76743 of epoch 0, 74.55 ms/it, loss 24.147354
Finished training it 59392/76743 of epoch 0, 74.02 ms/it, loss 24.225540
Finished training it 59392/76743 of epoch 0, 73.94 ms/it, loss 24.164856
Finished training it 59392/76743 of epoch 0, 73.96 ms/it, loss 24.239263
Finished training it 59392/76743 of epoch 0, 74.14 ms/it, loss 24.268916
Finished training it 60416/76743 of epoch 0, 74.80 ms/it, loss 24.389684
Finished training it 60416/76743 of epoch 0, 74.68 ms/it, loss 24.051727
Finished training it 60416/76743 of epoch 0, 74.62 ms/it, loss 24.223700
Finished training it 60416/76743 of epoch 0, 74.61 ms/it, loss 23.950458
Finished training it 61440/76743 of epoch 0, 74.24 ms/it, loss 24.362169
Finished training it 61440/76743 of epoch 0, 74.47 ms/it, loss 24.581363
Finished training it 61440/76743 of epoch 0, 74.35 ms/it, loss 24.082128
Finished training it 61440/76743 of epoch 0, 74.41 ms/it, loss 24.062505
Testing at - 61440/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 74.68 ms/it, loss 24.405997
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 74.79 ms/it, loss 24.341869
Testing at - 61440/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 0, 74.65 ms/it, loss 24.458104
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 0, 74.64 ms/it, loss 24.378399
Finished training it 63488/76743 of epoch 0, 74.70 ms/it, loss 24.358593
Finished training it 63488/76743 of epoch 0, 74.63 ms/it, loss 24.421784
Finished training it 63488/76743 of epoch 0, 74.71 ms/it, loss 24.395617
Finished training it 63488/76743 of epoch 0, 74.78 ms/it, loss 24.370193
Finished training it 64512/76743 of epoch 0, 74.08 ms/it, loss 24.321469
Finished training it 64512/76743 of epoch 0, 74.21 ms/it, loss 24.217540
Finished training it 64512/76743 of epoch 0, 74.18 ms/it, loss 24.247719
Finished training it 64512/76743 of epoch 0, 74.09 ms/it, loss 24.231569
Finished training it 65536/76743 of epoch 0, 79.18 ms/it, loss 24.347982
Finished training it 65536/76743 of epoch 0, 79.72 ms/it, loss 24.242150
Finished training it 65536/76743 of epoch 0, 79.58 ms/it, loss 24.178863
Finished training it 65536/76743 of epoch 0, 79.55 ms/it, loss 24.190423
Finished training it 66560/76743 of epoch 0, 79.92 ms/it, loss 24.372258
Finished training it 66560/76743 of epoch 0, 79.77 ms/it, loss 24.294210
Finished training it 66560/76743 of epoch 0, 79.98 ms/it, loss 24.391996
Finished training it 66560/76743 of epoch 0, 79.85 ms/it, loss 24.408196
Finished training it 67584/76743 of epoch 0, 75.01 ms/it, loss 24.151942
Finished training it 67584/76743 of epoch 0, 75.03 ms/it, loss 24.339801
Finished training it 67584/76743 of epoch 0, 74.84 ms/it, loss 24.400630
Finished training it 67584/76743 of epoch 0, 75.13 ms/it, loss 24.346636
Finished training it 68608/76743 of epoch 0, 74.88 ms/it, loss 24.249631
Finished training it 68608/76743 of epoch 0, 75.01 ms/it, loss 24.419823
Finished training it 68608/76743 of epoch 0, 74.75 ms/it, loss 24.335961
Finished training it 68608/76743 of epoch 0, 74.92 ms/it, loss 24.573837
Finished training it 69632/76743 of epoch 0, 74.60 ms/it, loss 24.291498
Finished training it 69632/76743 of epoch 0, 74.69 ms/it, loss 24.033258
Finished training it 69632/76743 of epoch 0, 74.61 ms/it, loss 24.135545
Finished training it 69632/76743 of epoch 0, 74.59 ms/it, loss 24.018729
Finished training it 70656/76743 of epoch 0, 74.24 ms/it, loss 24.481618
Finished training it 70656/76743 of epoch 0, 74.30 ms/it, loss 24.339156
Finished training it 70656/76743 of epoch 0, 74.23 ms/it, loss 24.428839
Finished training it 70656/76743 of epoch 0, 74.43 ms/it, loss 24.140912
Finished training it 71680/76743 of epoch 0, 74.58 ms/it, loss 24.135777
Finished training it 71680/76743 of epoch 0, 74.62 ms/it, loss 24.364486
Finished training it 71680/76743 of epoch 0, 74.62 ms/it, loss 24.120990
Finished training it 71680/76743 of epoch 0, 74.68 ms/it, loss 24.354201
Testing at - 71680/76743 of epoch 0,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 74.39 ms/it, loss 24.445208
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 74.48 ms/it, loss 24.384421
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 0, 74.27 ms/it, loss 24.408578
Testing at - 71680/76743 of epoch 0,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 0, 74.41 ms/it, loss 23.998104
Finished training it 73728/76743 of epoch 0, 74.82 ms/it, loss 24.391226
Finished training it 73728/76743 of epoch 0, 74.91 ms/it, loss 24.434798
Finished training it 73728/76743 of epoch 0, 74.88 ms/it, loss 24.178799
Finished training it 73728/76743 of epoch 0, 74.91 ms/it, loss 24.247046
Finished training it 74752/76743 of epoch 0, 74.73 ms/it, loss 24.560843
Finished training it 74752/76743 of epoch 0, 74.48 ms/it, loss 24.429993
Finished training it 74752/76743 of epoch 0, 74.43 ms/it, loss 24.089886
Finished training it 74752/76743 of epoch 0, 74.67 ms/it, loss 24.213227
Finished training it 75776/76743 of epoch 0, 74.01 ms/it, loss 24.160827
Finished training it 75776/76743 of epoch 0, 73.95 ms/it, loss 24.143740
Finished training it 75776/76743 of epoch 0, 73.91 ms/it, loss 24.315757
Finished training it 75776/76743 of epoch 0, 73.91 ms/it, loss 24.441027
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 80.83 ms/it, loss 24.160130
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 80.87 ms/it, loss 24.369669
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 80.78 ms/it, loss 24.268492
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 1, 80.77 ms/it, loss 24.168731
Finished training it 2048/76743 of epoch 1, 74.55 ms/it, loss 24.236587
Finished training it 2048/76743 of epoch 1, 74.35 ms/it, loss 24.167861
Finished training it 2048/76743 of epoch 1, 74.53 ms/it, loss 24.224348
Finished training it 2048/76743 of epoch 1, 74.65 ms/it, loss 24.161479
Finished training it 3072/76743 of epoch 1, 74.73 ms/it, loss 24.263063
Finished training it 3072/76743 of epoch 1, 74.84 ms/it, loss 24.544390
Finished training it 3072/76743 of epoch 1, 74.89 ms/it, loss 24.456197
Finished training it 3072/76743 of epoch 1, 74.95 ms/it, loss 24.128246
Finished training it 4096/76743 of epoch 1, 74.85 ms/it, loss 24.184746
Finished training it 4096/76743 of epoch 1, 74.86 ms/it, loss 24.430781
Finished training it 4096/76743 of epoch 1, 74.97 ms/it, loss 24.421389
Finished training it 4096/76743 of epoch 1, 74.68 ms/it, loss 24.329958
Finished training it 5120/76743 of epoch 1, 74.37 ms/it, loss 24.453278
Finished training it 5120/76743 of epoch 1, 74.50 ms/it, loss 24.158766
Finished training it 5120/76743 of epoch 1, 74.45 ms/it, loss 24.390902
Finished training it 5120/76743 of epoch 1, 74.22 ms/it, loss 24.440001
Finished training it 6144/76743 of epoch 1, 74.88 ms/it, loss 24.371934
Finished training it 6144/76743 of epoch 1, 74.73 ms/it, loss 24.359928
Finished training it 6144/76743 of epoch 1, 74.68 ms/it, loss 24.149218
Finished training it 6144/76743 of epoch 1, 74.81 ms/it, loss 24.139084
Finished training it 7168/76743 of epoch 1, 74.69 ms/it, loss 24.317252
Finished training it 7168/76743 of epoch 1, 74.73 ms/it, loss 24.289040
Finished training it 7168/76743 of epoch 1, 74.64 ms/it, loss 24.207261
Finished training it 7168/76743 of epoch 1, 74.65 ms/it, loss 24.213713
Finished training it 8192/76743 of epoch 1, 74.35 ms/it, loss 24.071066
Finished training it 8192/76743 of epoch 1, 74.45 ms/it, loss 24.174687
Finished training it 8192/76743 of epoch 1, 74.28 ms/it, loss 24.358855
Finished training it 8192/76743 of epoch 1, 74.38 ms/it, loss 24.113641
Finished training it 9216/76743 of epoch 1, 74.26 ms/it, loss 24.297129
Finished training it 9216/76743 of epoch 1, 74.25 ms/it, loss 24.309817
Finished training it 9216/76743 of epoch 1, 74.29 ms/it, loss 24.461437
Finished training it 9216/76743 of epoch 1, 74.39 ms/it, loss 24.313274
Finished training it 10240/76743 of epoch 1, 74.71 ms/it, loss 24.285060
Finished training it 10240/76743 of epoch 1, 74.67 ms/it, loss 24.353952
Finished training it 10240/76743 of epoch 1, 74.51 ms/it, loss 24.231670
Finished training it 10240/76743 of epoch 1, 74.66 ms/it, loss 24.335599
Testing at - 10240/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 74.53 ms/it, loss 24.467772
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 74.51 ms/it, loss 24.220365
Testing at - 10240/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 1, 74.57 ms/it, loss 24.323598
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 1, 74.29 ms/it, loss 24.357442
Finished training it 12288/76743 of epoch 1, 74.86 ms/it, loss 24.353618
Finished training it 12288/76743 of epoch 1, 74.61 ms/it, loss 24.351425
Finished training it 12288/76743 of epoch 1, 74.76 ms/it, loss 24.309236
Finished training it 12288/76743 of epoch 1, 74.85 ms/it, loss 24.261673
Finished training it 13312/76743 of epoch 1, 74.70 ms/it, loss 24.489936
Finished training it 13312/76743 of epoch 1, 74.70 ms/it, loss 24.256704
Finished training it 13312/76743 of epoch 1, 74.75 ms/it, loss 24.290612
Finished training it 13312/76743 of epoch 1, 74.78 ms/it, loss 24.155407
Finished training it 14336/76743 of epoch 1, 91.02 ms/it, loss 24.472743
Finished training it 14336/76743 of epoch 1, 91.04 ms/it, loss 24.171313
Finished training it 14336/76743 of epoch 1, 91.04 ms/it, loss 24.350832
Finished training it 14336/76743 of epoch 1, 90.90 ms/it, loss 24.256709
Finished training it 15360/76743 of epoch 1, 74.87 ms/it, loss 24.260846
Finished training it 15360/76743 of epoch 1, 74.78 ms/it, loss 24.185034
Finished training it 15360/76743 of epoch 1, 74.80 ms/it, loss 24.181194
Finished training it 15360/76743 of epoch 1, 74.96 ms/it, loss 24.228047
Finished training it 16384/76743 of epoch 1, 74.40 ms/it, loss 24.120525
Finished training it 16384/76743 of epoch 1, 74.27 ms/it, loss 24.223141
Finished training it 16384/76743 of epoch 1, 74.36 ms/it, loss 24.328512
Finished training it 16384/76743 of epoch 1, 74.42 ms/it, loss 24.397548
Finished training it 17408/76743 of epoch 1, 74.35 ms/it, loss 24.278783
Finished training it 17408/76743 of epoch 1, 74.40 ms/it, loss 24.039388
Finished training it 17408/76743 of epoch 1, 74.48 ms/it, loss 24.131700
Finished training it 17408/76743 of epoch 1, 74.55 ms/it, loss 24.088693
Finished training it 18432/76743 of epoch 1, 74.40 ms/it, loss 24.187812
Finished training it 18432/76743 of epoch 1, 74.53 ms/it, loss 24.319557
Finished training it 18432/76743 of epoch 1, 74.58 ms/it, loss 24.174317
Finished training it 18432/76743 of epoch 1, 74.57 ms/it, loss 24.424964
Finished training it 19456/76743 of epoch 1, 74.73 ms/it, loss 24.204686
Finished training it 19456/76743 of epoch 1, 74.75 ms/it, loss 24.153289
Finished training it 19456/76743 of epoch 1, 74.73 ms/it, loss 24.529609
Finished training it 19456/76743 of epoch 1, 74.65 ms/it, loss 24.326517
Finished training it 20480/76743 of epoch 1, 74.71 ms/it, loss 24.093179
Finished training it 20480/76743 of epoch 1, 74.72 ms/it, loss 24.159413
Finished training it 20480/76743 of epoch 1, 74.73 ms/it, loss 24.298019
Finished training it 20480/76743 of epoch 1, 74.83 ms/it, loss 24.375622
Testing at - 20480/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 73.87 ms/it, loss 24.272153
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 1, 73.90 ms/it, loss 24.054095
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 73.90 ms/it, loss 24.195706
Testing at - 20480/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 1, 73.97 ms/it, loss 24.281373
Finished training it 22528/76743 of epoch 1, 74.45 ms/it, loss 24.162207
Finished training it 22528/76743 of epoch 1, 74.35 ms/it, loss 24.206991
Finished training it 22528/76743 of epoch 1, 74.44 ms/it, loss 24.471648
Finished training it 22528/76743 of epoch 1, 74.55 ms/it, loss 24.229826
Finished training it 23552/76743 of epoch 1, 75.07 ms/it, loss 24.225228
Finished training it 23552/76743 of epoch 1, 75.04 ms/it, loss 24.619583
Finished training it 23552/76743 of epoch 1, 74.94 ms/it, loss 24.327694
Finished training it 23552/76743 of epoch 1, 74.82 ms/it, loss 24.273973
Finished training it 24576/76743 of epoch 1, 74.65 ms/it, loss 24.280983
Finished training it 24576/76743 of epoch 1, 74.51 ms/it, loss 24.634974
Finished training it 24576/76743 of epoch 1, 74.68 ms/it, loss 24.274088
Finished training it 24576/76743 of epoch 1, 74.60 ms/it, loss 24.265896
Finished training it 25600/76743 of epoch 1, 74.35 ms/it, loss 24.237091
Finished training it 25600/76743 of epoch 1, 74.13 ms/it, loss 24.526065
Finished training it 25600/76743 of epoch 1, 74.33 ms/it, loss 24.171101
Finished training it 25600/76743 of epoch 1, 74.48 ms/it, loss 24.237933
Finished training it 26624/76743 of epoch 1, 75.49 ms/it, loss 24.277781
Finished training it 26624/76743 of epoch 1, 75.44 ms/it, loss 24.218598
Finished training it 26624/76743 of epoch 1, 75.45 ms/it, loss 24.238076
Finished training it 26624/76743 of epoch 1, 75.41 ms/it, loss 24.255146
Finished training it 27648/76743 of epoch 1, 75.18 ms/it, loss 24.134486
Finished training it 27648/76743 of epoch 1, 75.07 ms/it, loss 24.321370
Finished training it 27648/76743 of epoch 1, 75.16 ms/it, loss 24.396507
Finished training it 27648/76743 of epoch 1, 75.21 ms/it, loss 24.202743
Finished training it 28672/76743 of epoch 1, 74.70 ms/it, loss 24.206901
Finished training it 28672/76743 of epoch 1, 74.76 ms/it, loss 24.218743
Finished training it 28672/76743 of epoch 1, 74.92 ms/it, loss 24.109023
Finished training it 28672/76743 of epoch 1, 74.76 ms/it, loss 24.217211
Finished training it 29696/76743 of epoch 1, 74.58 ms/it, loss 24.305786
Finished training it 29696/76743 of epoch 1, 74.64 ms/it, loss 24.398887
Finished training it 29696/76743 of epoch 1, 74.52 ms/it, loss 24.321783
Finished training it 29696/76743 of epoch 1, 74.71 ms/it, loss 24.361285
Finished training it 30720/76743 of epoch 1, 74.82 ms/it, loss 24.071940
Finished training it 30720/76743 of epoch 1, 75.03 ms/it, loss 24.240841
Finished training it 30720/76743 of epoch 1, 74.90 ms/it, loss 23.992839
Finished training it 30720/76743 of epoch 1, 75.07 ms/it, loss 24.273695
Testing at - 30720/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 74.86 ms/it, loss 24.311424
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 1, 74.56 ms/it, loss 24.127485
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 74.83 ms/it, loss 24.106060
Testing at - 30720/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 1, 74.79 ms/it, loss 24.271353
Finished training it 32768/76743 of epoch 1, 74.72 ms/it, loss 24.280928
Finished training it 32768/76743 of epoch 1, 74.85 ms/it, loss 24.381620
Finished training it 32768/76743 of epoch 1, 74.65 ms/it, loss 24.389219
Finished training it 32768/76743 of epoch 1, 74.70 ms/it, loss 24.230309
Finished training it 33792/76743 of epoch 1, 89.43 ms/it, loss 24.526224
Finished training it 33792/76743 of epoch 1, 89.48 ms/it, loss 24.310300
Finished training it 33792/76743 of epoch 1, 89.60 ms/it, loss 24.495869
Finished training it 33792/76743 of epoch 1, 89.56 ms/it, loss 24.387993
Finished training it 34816/76743 of epoch 1, 74.54 ms/it, loss 24.205009
Finished training it 34816/76743 of epoch 1, 74.52 ms/it, loss 24.505708
Finished training it 34816/76743 of epoch 1, 74.45 ms/it, loss 24.357910
Finished training it 34816/76743 of epoch 1, 74.69 ms/it, loss 24.230187
Finished training it 35840/76743 of epoch 1, 74.88 ms/it, loss 24.532543
Finished training it 35840/76743 of epoch 1, 75.03 ms/it, loss 24.143403
Finished training it 35840/76743 of epoch 1, 75.09 ms/it, loss 24.156129
Finished training it 35840/76743 of epoch 1, 74.73 ms/it, loss 24.209030
Finished training it 36864/76743 of epoch 1, 74.80 ms/it, loss 24.260090
Finished training it 36864/76743 of epoch 1, 74.85 ms/it, loss 24.338342
Finished training it 36864/76743 of epoch 1, 74.80 ms/it, loss 24.182978
Finished training it 36864/76743 of epoch 1, 74.84 ms/it, loss 24.432832
Finished training it 37888/76743 of epoch 1, 74.81 ms/it, loss 24.226393
Finished training it 37888/76743 of epoch 1, 74.84 ms/it, loss 24.308601
Finished training it 37888/76743 of epoch 1, 74.62 ms/it, loss 24.348713
Finished training it 37888/76743 of epoch 1, 74.80 ms/it, loss 24.341406
Finished training it 38912/76743 of epoch 1, 74.99 ms/it, loss 24.202864
Finished training it 38912/76743 of epoch 1, 75.01 ms/it, loss 24.201709
Finished training it 38912/76743 of epoch 1, 74.78 ms/it, loss 24.178926
Finished training it 38912/76743 of epoch 1, 74.96 ms/it, loss 24.266204
Finished training it 39936/76743 of epoch 1, 74.53 ms/it, loss 24.218407
Finished training it 39936/76743 of epoch 1, 74.76 ms/it, loss 24.322397
Finished training it 39936/76743 of epoch 1, 74.72 ms/it, loss 24.383961
Finished training it 39936/76743 of epoch 1, 74.62 ms/it, loss 24.202944
Finished training it 40960/76743 of epoch 1, 74.11 ms/it, loss 24.330869
Finished training it 40960/76743 of epoch 1, 74.25 ms/it, loss 24.309766
Finished training it 40960/76743 of epoch 1, 74.16 ms/it, loss 24.074388
Finished training it 40960/76743 of epoch 1, 74.33 ms/it, loss 24.190751
Testing at - 40960/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 74.58 ms/it, loss 24.277441
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 1, 74.73 ms/it, loss 24.197903
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 74.72 ms/it, loss 24.497215
Testing at - 40960/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 1, 74.70 ms/it, loss 24.260138
Finished training it 43008/76743 of epoch 1, 74.73 ms/it, loss 24.150188
Finished training it 43008/76743 of epoch 1, 74.55 ms/it, loss 24.366356
Finished training it 43008/76743 of epoch 1, 74.58 ms/it, loss 24.292737
Finished training it 43008/76743 of epoch 1, 74.74 ms/it, loss 24.144155
Finished training it 44032/76743 of epoch 1, 74.93 ms/it, loss 24.110822
Finished training it 44032/76743 of epoch 1, 74.87 ms/it, loss 24.348882
Finished training it 44032/76743 of epoch 1, 75.01 ms/it, loss 24.238102
Finished training it 44032/76743 of epoch 1, 74.93 ms/it, loss 24.238707
Finished training it 45056/76743 of epoch 1, 80.15 ms/it, loss 24.268460
Finished training it 45056/76743 of epoch 1, 80.22 ms/it, loss 24.241880
Finished training it 45056/76743 of epoch 1, 80.06 ms/it, loss 24.370685
Finished training it 45056/76743 of epoch 1, 80.13 ms/it, loss 24.360514
Finished training it 46080/76743 of epoch 1, 74.59 ms/it, loss 24.242425
Finished training it 46080/76743 of epoch 1, 74.59 ms/it, loss 24.159737
Finished training it 46080/76743 of epoch 1, 74.60 ms/it, loss 24.317123
Finished training it 46080/76743 of epoch 1, 74.55 ms/it, loss 24.285685
Finished training it 47104/76743 of epoch 1, 74.94 ms/it, loss 24.440559
Finished training it 47104/76743 of epoch 1, 74.82 ms/it, loss 24.345867
Finished training it 47104/76743 of epoch 1, 74.87 ms/it, loss 24.259043
Finished training it 47104/76743 of epoch 1, 74.73 ms/it, loss 24.229079
Finished training it 48128/76743 of epoch 1, 74.34 ms/it, loss 24.259750
Finished training it 48128/76743 of epoch 1, 74.50 ms/it, loss 24.410569
Finished training it 48128/76743 of epoch 1, 74.49 ms/it, loss 24.115572
Finished training it 48128/76743 of epoch 1, 74.56 ms/it, loss 24.383134
Finished training it 49152/76743 of epoch 1, 74.43 ms/it, loss 24.453304
Finished training it 49152/76743 of epoch 1, 74.40 ms/it, loss 24.333336
Finished training it 49152/76743 of epoch 1, 74.37 ms/it, loss 24.251062
Finished training it 49152/76743 of epoch 1, 74.47 ms/it, loss 24.221339
Finished training it 50176/76743 of epoch 1, 75.12 ms/it, loss 24.378413
Finished training it 50176/76743 of epoch 1, 75.15 ms/it, loss 24.257986
Finished training it 50176/76743 of epoch 1, 75.16 ms/it, loss 24.517205
Finished training it 50176/76743 of epoch 1, 74.91 ms/it, loss 24.396471
Finished training it 51200/76743 of epoch 1, 74.44 ms/it, loss 24.439163
Finished training it 51200/76743 of epoch 1, 74.27 ms/it, loss 24.150465
Finished training it 51200/76743 of epoch 1, 74.53 ms/it, loss 24.262490
Finished training it 51200/76743 of epoch 1, 74.53 ms/it, loss 24.279513
Testing at - 51200/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 74.91 ms/it, loss 24.370621
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 75.11 ms/it, loss 24.125841
Testing at - 51200/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 1, 75.18 ms/it, loss 24.412005
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 1, 74.95 ms/it, loss 24.264502
Finished training it 53248/76743 of epoch 1, 75.23 ms/it, loss 24.294666
Finished training it 53248/76743 of epoch 1, 75.05 ms/it, loss 24.230039
Finished training it 53248/76743 of epoch 1, 75.18 ms/it, loss 24.282464
Finished training it 53248/76743 of epoch 1, 74.94 ms/it, loss 24.241198
Finished training it 54272/76743 of epoch 1, 80.89 ms/it, loss 24.279751
Finished training it 54272/76743 of epoch 1, 81.07 ms/it, loss 24.472446
Finished training it 54272/76743 of epoch 1, 80.64 ms/it, loss 24.405288
Finished training it 54272/76743 of epoch 1, 81.02 ms/it, loss 24.105594
Finished training it 55296/76743 of epoch 1, 74.60 ms/it, loss 24.082505
Finished training it 55296/76743 of epoch 1, 74.78 ms/it, loss 24.274940
Finished training it 55296/76743 of epoch 1, 74.79 ms/it, loss 24.331118
Finished training it 55296/76743 of epoch 1, 74.59 ms/it, loss 24.187426
Finished training it 56320/76743 of epoch 1, 74.96 ms/it, loss 24.281176
Finished training it 56320/76743 of epoch 1, 75.05 ms/it, loss 24.431147
Finished training it 56320/76743 of epoch 1, 74.96 ms/it, loss 24.277372
Finished training it 56320/76743 of epoch 1, 74.90 ms/it, loss 24.188760
Finished training it 57344/76743 of epoch 1, 74.72 ms/it, loss 24.248720
Finished training it 57344/76743 of epoch 1, 74.68 ms/it, loss 24.233338
Finished training it 57344/76743 of epoch 1, 74.62 ms/it, loss 24.292324
Finished training it 57344/76743 of epoch 1, 74.56 ms/it, loss 24.002506
Finished training it 58368/76743 of epoch 1, 74.76 ms/it, loss 24.126666
Finished training it 58368/76743 of epoch 1, 74.90 ms/it, loss 24.222974
Finished training it 58368/76743 of epoch 1, 74.98 ms/it, loss 24.147354
Finished training it 58368/76743 of epoch 1, 74.83 ms/it, loss 24.437059
Finished training it 59392/76743 of epoch 1, 74.80 ms/it, loss 24.164856
Finished training it 59392/76743 of epoch 1, 75.07 ms/it, loss 24.268916
Finished training it 59392/76743 of epoch 1, 74.84 ms/it, loss 24.239263
Finished training it 59392/76743 of epoch 1, 74.95 ms/it, loss 24.225540
Finished training it 60416/76743 of epoch 1, 74.76 ms/it, loss 24.051727
Finished training it 60416/76743 of epoch 1, 74.88 ms/it, loss 24.389684
Finished training it 60416/76743 of epoch 1, 74.61 ms/it, loss 23.950458
Finished training it 60416/76743 of epoch 1, 74.77 ms/it, loss 24.223700
Finished training it 61440/76743 of epoch 1, 74.76 ms/it, loss 24.581363
Finished training it 61440/76743 of epoch 1, 74.73 ms/it, loss 24.062505
Finished training it 61440/76743 of epoch 1, 74.59 ms/it, loss 24.082128
Finished training it 61440/76743 of epoch 1, 74.63 ms/it, loss 24.362169
Testing at - 61440/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 74.39 ms/it, loss 24.458104
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 1, 74.24 ms/it, loss 24.378399
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 74.36 ms/it, loss 24.405997
Testing at - 61440/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 1, 74.36 ms/it, loss 24.341869
Finished training it 63488/76743 of epoch 1, 74.74 ms/it, loss 24.421784
Finished training it 63488/76743 of epoch 1, 74.90 ms/it, loss 24.370193
Finished training it 63488/76743 of epoch 1, 74.91 ms/it, loss 24.358593
Finished training it 63488/76743 of epoch 1, 74.93 ms/it, loss 24.395617
Finished training it 64512/76743 of epoch 1, 79.97 ms/it, loss 24.321469
Finished training it 64512/76743 of epoch 1, 80.06 ms/it, loss 24.217540
Finished training it 64512/76743 of epoch 1, 80.08 ms/it, loss 24.247719
Finished training it 64512/76743 of epoch 1, 80.22 ms/it, loss 24.231569
Finished training it 65536/76743 of epoch 1, 75.29 ms/it, loss 24.178863
Finished training it 65536/76743 of epoch 1, 75.23 ms/it, loss 24.190423
Finished training it 65536/76743 of epoch 1, 75.37 ms/it, loss 24.242150
Finished training it 65536/76743 of epoch 1, 75.32 ms/it, loss 24.347982
Finished training it 66560/76743 of epoch 1, 74.62 ms/it, loss 24.372258
Finished training it 66560/76743 of epoch 1, 74.57 ms/it, loss 24.408196
Finished training it 66560/76743 of epoch 1, 74.49 ms/it, loss 24.294210
Finished training it 66560/76743 of epoch 1, 74.51 ms/it, loss 24.391996
Finished training it 67584/76743 of epoch 1, 74.15 ms/it, loss 24.400630
Finished training it 67584/76743 of epoch 1, 74.09 ms/it, loss 24.151942
Finished training it 67584/76743 of epoch 1, 74.16 ms/it, loss 24.339801
Finished training it 67584/76743 of epoch 1, 74.20 ms/it, loss 24.346636
Finished training it 68608/76743 of epoch 1, 74.89 ms/it, loss 24.419823
Finished training it 68608/76743 of epoch 1, 74.87 ms/it, loss 24.249631
Finished training it 68608/76743 of epoch 1, 74.88 ms/it, loss 24.335961
Finished training it 68608/76743 of epoch 1, 74.69 ms/it, loss 24.573837
Finished training it 69632/76743 of epoch 1, 74.53 ms/it, loss 24.033258
Finished training it 69632/76743 of epoch 1, 74.43 ms/it, loss 24.291498
Finished training it 69632/76743 of epoch 1, 74.41 ms/it, loss 24.018729
Finished training it 69632/76743 of epoch 1, 74.49 ms/it, loss 24.135545
Finished training it 70656/76743 of epoch 1, 74.73 ms/it, loss 24.140912
Finished training it 70656/76743 of epoch 1, 74.77 ms/it, loss 24.339156
Finished training it 70656/76743 of epoch 1, 74.63 ms/it, loss 24.481618
Finished training it 70656/76743 of epoch 1, 74.68 ms/it, loss 24.428839
Finished training it 71680/76743 of epoch 1, 74.73 ms/it, loss 24.120990
Finished training it 71680/76743 of epoch 1, 74.64 ms/it, loss 24.364486
Finished training it 71680/76743 of epoch 1, 74.61 ms/it, loss 24.135777
Finished training it 71680/76743 of epoch 1, 74.80 ms/it, loss 24.354201
Testing at - 71680/76743 of epoch 1,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 1, 74.32 ms/it, loss 24.408578
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 74.43 ms/it, loss 23.998104
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 74.55 ms/it, loss 24.384421
Testing at - 71680/76743 of epoch 1,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 1, 74.43 ms/it, loss 24.445208
Finished training it 73728/76743 of epoch 1, 74.65 ms/it, loss 24.391226
Finished training it 73728/76743 of epoch 1, 74.48 ms/it, loss 24.434798
Finished training it 73728/76743 of epoch 1, 74.72 ms/it, loss 24.247046
Finished training it 73728/76743 of epoch 1, 74.69 ms/it, loss 24.178799
Finished training it 74752/76743 of epoch 1, 79.92 ms/it, loss 24.560843
Finished training it 74752/76743 of epoch 1, 79.65 ms/it, loss 24.213227
Finished training it 74752/76743 of epoch 1, 79.73 ms/it, loss 24.429993
Finished training it 74752/76743 of epoch 1, 79.38 ms/it, loss 24.089886
Finished training it 75776/76743 of epoch 1, 75.68 ms/it, loss 24.160827
Finished training it 75776/76743 of epoch 1, 75.65 ms/it, loss 24.143740
Finished training it 75776/76743 of epoch 1, 75.65 ms/it, loss 24.441027
Finished training it 75776/76743 of epoch 1, 75.43 ms/it, loss 24.315757
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 77.45 ms/it, loss 24.160130
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 77.52 ms/it, loss 24.369669
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 77.46 ms/it, loss 24.268492
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 2, 77.28 ms/it, loss 24.168731
Finished training it 2048/76743 of epoch 2, 74.17 ms/it, loss 24.167861
Finished training it 2048/76743 of epoch 2, 74.24 ms/it, loss 24.161479
Finished training it 2048/76743 of epoch 2, 74.01 ms/it, loss 24.236587
Finished training it 2048/76743 of epoch 2, 74.15 ms/it, loss 24.224348
Finished training it 3072/76743 of epoch 2, 74.50 ms/it, loss 24.544390
Finished training it 3072/76743 of epoch 2, 74.35 ms/it, loss 24.456197
Finished training it 3072/76743 of epoch 2, 74.56 ms/it, loss 24.128246
Finished training it 3072/76743 of epoch 2, 74.45 ms/it, loss 24.263063
Finished training it 4096/76743 of epoch 2, 75.25 ms/it, loss 24.421389
Finished training it 4096/76743 of epoch 2, 75.26 ms/it, loss 24.430781
Finished training it 4096/76743 of epoch 2, 75.05 ms/it, loss 24.329958
Finished training it 4096/76743 of epoch 2, 75.15 ms/it, loss 24.184746
Finished training it 5120/76743 of epoch 2, 74.84 ms/it, loss 24.390902
Finished training it 5120/76743 of epoch 2, 74.88 ms/it, loss 24.158766
Finished training it 5120/76743 of epoch 2, 74.64 ms/it, loss 24.453278
Finished training it 5120/76743 of epoch 2, 74.64 ms/it, loss 24.440001
Finished training it 6144/76743 of epoch 2, 74.64 ms/it, loss 24.139084
Finished training it 6144/76743 of epoch 2, 74.59 ms/it, loss 24.359928
Finished training it 6144/76743 of epoch 2, 74.66 ms/it, loss 24.371934
Finished training it 6144/76743 of epoch 2, 74.46 ms/it, loss 24.149218
Finished training it 7168/76743 of epoch 2, 74.73 ms/it, loss 24.213713
Finished training it 7168/76743 of epoch 2, 74.83 ms/it, loss 24.317252
Finished training it 7168/76743 of epoch 2, 74.78 ms/it, loss 24.207261
Finished training it 7168/76743 of epoch 2, 74.88 ms/it, loss 24.289040
Finished training it 8192/76743 of epoch 2, 74.58 ms/it, loss 24.071066
Finished training it 8192/76743 of epoch 2, 74.60 ms/it, loss 24.113641
Finished training it 8192/76743 of epoch 2, 74.65 ms/it, loss 24.174687
Finished training it 8192/76743 of epoch 2, 74.48 ms/it, loss 24.358855
Finished training it 9216/76743 of epoch 2, 74.54 ms/it, loss 24.461437
Finished training it 9216/76743 of epoch 2, 74.50 ms/it, loss 24.297129
Finished training it 9216/76743 of epoch 2, 74.53 ms/it, loss 24.313274
Finished training it 9216/76743 of epoch 2, 74.45 ms/it, loss 24.309817
Finished training it 10240/76743 of epoch 2, 74.54 ms/it, loss 24.353952
Finished training it 10240/76743 of epoch 2, 74.68 ms/it, loss 24.335599
Finished training it 10240/76743 of epoch 2, 74.59 ms/it, loss 24.231670
Finished training it 10240/76743 of epoch 2, 74.72 ms/it, loss 24.285060
Testing at - 10240/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 2, 74.35 ms/it, loss 24.357442
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 74.35 ms/it, loss 24.220365
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 74.61 ms/it, loss 24.323598
Testing at - 10240/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 2, 74.56 ms/it, loss 24.467772
Finished training it 12288/76743 of epoch 2, 74.86 ms/it, loss 24.261673
Finished training it 12288/76743 of epoch 2, 74.92 ms/it, loss 24.353618
Finished training it 12288/76743 of epoch 2, 74.78 ms/it, loss 24.309236
Finished training it 12288/76743 of epoch 2, 74.73 ms/it, loss 24.351425
Finished training it 13312/76743 of epoch 2, 74.86 ms/it, loss 24.256704
Finished training it 13312/76743 of epoch 2, 74.92 ms/it, loss 24.290612
Finished training it 13312/76743 of epoch 2, 74.83 ms/it, loss 24.155407
Finished training it 13312/76743 of epoch 2, 74.73 ms/it, loss 24.489936
Finished training it 14336/76743 of epoch 2, 89.89 ms/it, loss 24.256709
Finished training it 14336/76743 of epoch 2, 89.94 ms/it, loss 24.350832
Finished training it 14336/76743 of epoch 2, 89.90 ms/it, loss 24.171313
Finished training it 14336/76743 of epoch 2, 90.13 ms/it, loss 24.472743
Finished training it 15360/76743 of epoch 2, 74.89 ms/it, loss 24.181194
Finished training it 15360/76743 of epoch 2, 74.88 ms/it, loss 24.185034
Finished training it 15360/76743 of epoch 2, 75.10 ms/it, loss 24.260846
Finished training it 15360/76743 of epoch 2, 75.10 ms/it, loss 24.228047
Finished training it 16384/76743 of epoch 2, 74.75 ms/it, loss 24.397548
Finished training it 16384/76743 of epoch 2, 74.69 ms/it, loss 24.120525
Finished training it 16384/76743 of epoch 2, 74.55 ms/it, loss 24.328512
Finished training it 16384/76743 of epoch 2, 74.61 ms/it, loss 24.223141
Finished training it 17408/76743 of epoch 2, 74.45 ms/it, loss 24.131700
Finished training it 17408/76743 of epoch 2, 74.39 ms/it, loss 24.278783
Finished training it 17408/76743 of epoch 2, 74.47 ms/it, loss 24.088693
Finished training it 17408/76743 of epoch 2, 74.31 ms/it, loss 24.039388
Finished training it 18432/76743 of epoch 2, 74.58 ms/it, loss 24.319557
Finished training it 18432/76743 of epoch 2, 74.83 ms/it, loss 24.424964
Finished training it 18432/76743 of epoch 2, 74.83 ms/it, loss 24.174317
Finished training it 18432/76743 of epoch 2, 74.73 ms/it, loss 24.187812
Finished training it 19456/76743 of epoch 2, 74.79 ms/it, loss 24.326517
Finished training it 19456/76743 of epoch 2, 74.97 ms/it, loss 24.153289
Finished training it 19456/76743 of epoch 2, 74.91 ms/it, loss 24.529609
Finished training it 19456/76743 of epoch 2, 74.78 ms/it, loss 24.204686
Finished training it 20480/76743 of epoch 2, 74.72 ms/it, loss 24.159413
Finished training it 20480/76743 of epoch 2, 74.98 ms/it, loss 24.298019
Finished training it 20480/76743 of epoch 2, 74.77 ms/it, loss 24.093179
Finished training it 20480/76743 of epoch 2, 74.99 ms/it, loss 24.375622
Testing at - 20480/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 74.48 ms/it, loss 24.272153
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 74.63 ms/it, loss 24.281373
Testing at - 20480/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 2, 74.62 ms/it, loss 24.195706
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 2, 74.49 ms/it, loss 24.054095
Finished training it 22528/76743 of epoch 2, 74.37 ms/it, loss 24.162207
Finished training it 22528/76743 of epoch 2, 74.51 ms/it, loss 24.471648
Finished training it 22528/76743 of epoch 2, 74.55 ms/it, loss 24.229826
Finished training it 22528/76743 of epoch 2, 74.44 ms/it, loss 24.206991
Finished training it 23552/76743 of epoch 2, 74.65 ms/it, loss 24.273973
Finished training it 23552/76743 of epoch 2, 74.68 ms/it, loss 24.327694
Finished training it 23552/76743 of epoch 2, 74.90 ms/it, loss 24.225228
Finished training it 23552/76743 of epoch 2, 74.82 ms/it, loss 24.619583
Finished training it 24576/76743 of epoch 2, 74.61 ms/it, loss 24.265896
Finished training it 24576/76743 of epoch 2, 74.71 ms/it, loss 24.634974
Finished training it 24576/76743 of epoch 2, 74.75 ms/it, loss 24.280983
Finished training it 24576/76743 of epoch 2, 74.86 ms/it, loss 24.274088
Finished training it 25600/76743 of epoch 2, 75.22 ms/it, loss 24.526065
Finished training it 25600/76743 of epoch 2, 75.48 ms/it, loss 24.237091
Finished training it 25600/76743 of epoch 2, 75.59 ms/it, loss 24.237933
Finished training it 25600/76743 of epoch 2, 75.30 ms/it, loss 24.171101
Finished training it 26624/76743 of epoch 2, 74.85 ms/it, loss 24.218598
Finished training it 26624/76743 of epoch 2, 74.73 ms/it, loss 24.238076
Finished training it 26624/76743 of epoch 2, 74.56 ms/it, loss 24.277781
Finished training it 26624/76743 of epoch 2, 74.55 ms/it, loss 24.255146
Finished training it 27648/76743 of epoch 2, 74.53 ms/it, loss 24.321370
Finished training it 27648/76743 of epoch 2, 74.68 ms/it, loss 24.134486
Finished training it 27648/76743 of epoch 2, 74.65 ms/it, loss 24.396507
Finished training it 27648/76743 of epoch 2, 74.70 ms/it, loss 24.202743
Finished training it 28672/76743 of epoch 2, 74.73 ms/it, loss 24.217211
Finished training it 28672/76743 of epoch 2, 74.79 ms/it, loss 24.109023
Finished training it 28672/76743 of epoch 2, 74.55 ms/it, loss 24.206901
Finished training it 28672/76743 of epoch 2, 74.67 ms/it, loss 24.218743
Finished training it 29696/76743 of epoch 2, 74.73 ms/it, loss 24.398887
Finished training it 29696/76743 of epoch 2, 74.67 ms/it, loss 24.321783
Finished training it 29696/76743 of epoch 2, 74.48 ms/it, loss 24.305786
Finished training it 29696/76743 of epoch 2, 74.81 ms/it, loss 24.361285
Finished training it 30720/76743 of epoch 2, 74.71 ms/it, loss 24.071940
Finished training it 30720/76743 of epoch 2, 74.81 ms/it, loss 24.240841
Finished training it 30720/76743 of epoch 2, 74.84 ms/it, loss 24.273695
Finished training it 30720/76743 of epoch 2, 74.73 ms/it, loss 23.992839
Testing at - 30720/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 75.18 ms/it, loss 24.311424
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 2, 75.09 ms/it, loss 24.127485
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 75.21 ms/it, loss 24.271353
Testing at - 30720/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 2, 75.12 ms/it, loss 24.106060
Finished training it 32768/76743 of epoch 2, 75.58 ms/it, loss 24.381620
Finished training it 32768/76743 of epoch 2, 75.65 ms/it, loss 24.280928
Finished training it 32768/76743 of epoch 2, 75.48 ms/it, loss 24.230309
Finished training it 32768/76743 of epoch 2, 75.47 ms/it, loss 24.389219
Finished training it 33792/76743 of epoch 2, 91.20 ms/it, loss 24.526224
Finished training it 33792/76743 of epoch 2, 91.17 ms/it, loss 24.387993
Finished training it 33792/76743 of epoch 2, 91.20 ms/it, loss 24.310300
Finished training it 33792/76743 of epoch 2, 91.26 ms/it, loss 24.495869
Finished training it 34816/76743 of epoch 2, 74.79 ms/it, loss 24.357910
Finished training it 34816/76743 of epoch 2, 74.82 ms/it, loss 24.505708
Finished training it 34816/76743 of epoch 2, 74.91 ms/it, loss 24.230187
Finished training it 34816/76743 of epoch 2, 74.68 ms/it, loss 24.205009
Finished training it 35840/76743 of epoch 2, 75.17 ms/it, loss 24.143403
Finished training it 35840/76743 of epoch 2, 74.88 ms/it, loss 24.532543
Finished training it 35840/76743 of epoch 2, 74.99 ms/it, loss 24.209030
Finished training it 35840/76743 of epoch 2, 75.19 ms/it, loss 24.156129
Finished training it 36864/76743 of epoch 2, 74.94 ms/it, loss 24.432832
Finished training it 36864/76743 of epoch 2, 74.82 ms/it, loss 24.260090
Finished training it 36864/76743 of epoch 2, 75.02 ms/it, loss 24.338342
Finished training it 36864/76743 of epoch 2, 74.82 ms/it, loss 24.182978
Finished training it 37888/76743 of epoch 2, 74.70 ms/it, loss 24.308601
Finished training it 37888/76743 of epoch 2, 74.57 ms/it, loss 24.226393
Finished training it 37888/76743 of epoch 2, 74.53 ms/it, loss 24.341406
Finished training it 37888/76743 of epoch 2, 74.46 ms/it, loss 24.348713
Finished training it 38912/76743 of epoch 2, 74.74 ms/it, loss 24.202864
Finished training it 38912/76743 of epoch 2, 74.70 ms/it, loss 24.201709
Finished training it 38912/76743 of epoch 2, 74.60 ms/it, loss 24.266204
Finished training it 38912/76743 of epoch 2, 74.61 ms/it, loss 24.178926
Finished training it 39936/76743 of epoch 2, 74.49 ms/it, loss 24.383961
Finished training it 39936/76743 of epoch 2, 74.59 ms/it, loss 24.322397
Finished training it 39936/76743 of epoch 2, 74.36 ms/it, loss 24.218407
Finished training it 39936/76743 of epoch 2, 74.43 ms/it, loss 24.202944
Finished training it 40960/76743 of epoch 2, 74.49 ms/it, loss 24.330869
Finished training it 40960/76743 of epoch 2, 74.57 ms/it, loss 24.190751
Finished training it 40960/76743 of epoch 2, 74.53 ms/it, loss 24.309766
Finished training it 40960/76743 of epoch 2, 74.46 ms/it, loss 24.074388
Testing at - 40960/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 41984/76743 of epoch 2, 74.41 ms/it, loss 24.197903
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 74.56 ms/it, loss 24.497215
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 74.32 ms/it, loss 24.277441
Testing at - 40960/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 41984/76743 of epoch 2, 74.44 ms/it, loss 24.260138
Finished training it 43008/76743 of epoch 2, 74.52 ms/it, loss 24.366356
Finished training it 43008/76743 of epoch 2, 74.45 ms/it, loss 24.292737
Finished training it 43008/76743 of epoch 2, 74.61 ms/it, loss 24.144155
Finished training it 43008/76743 of epoch 2, 74.70 ms/it, loss 24.150188
Finished training it 44032/76743 of epoch 2, 74.79 ms/it, loss 24.348882
Finished training it 44032/76743 of epoch 2, 74.83 ms/it, loss 24.238707
Finished training it 44032/76743 of epoch 2, 74.75 ms/it, loss 24.110822
Finished training it 44032/76743 of epoch 2, 74.89 ms/it, loss 24.238102
Finished training it 45056/76743 of epoch 2, 80.00 ms/it, loss 24.241880
Finished training it 45056/76743 of epoch 2, 79.98 ms/it, loss 24.268460
Finished training it 45056/76743 of epoch 2, 79.91 ms/it, loss 24.370685
Finished training it 45056/76743 of epoch 2, 80.01 ms/it, loss 24.360514
Finished training it 46080/76743 of epoch 2, 74.73 ms/it, loss 24.242425
Finished training it 46080/76743 of epoch 2, 74.69 ms/it, loss 24.285685
Finished training it 46080/76743 of epoch 2, 74.61 ms/it, loss 24.317123
Finished training it 46080/76743 of epoch 2, 74.50 ms/it, loss 24.159737
Finished training it 47104/76743 of epoch 2, 74.25 ms/it, loss 24.229079
Finished training it 47104/76743 of epoch 2, 74.43 ms/it, loss 24.345867
Finished training it 47104/76743 of epoch 2, 74.48 ms/it, loss 24.440559
Finished training it 47104/76743 of epoch 2, 74.47 ms/it, loss 24.259043
Finished training it 48128/76743 of epoch 2, 74.93 ms/it, loss 24.383134
Finished training it 48128/76743 of epoch 2, 74.90 ms/it, loss 24.410569
Finished training it 48128/76743 of epoch 2, 74.81 ms/it, loss 24.259750
Finished training it 48128/76743 of epoch 2, 74.81 ms/it, loss 24.115572
Finished training it 49152/76743 of epoch 2, 74.99 ms/it, loss 24.333336
Finished training it 49152/76743 of epoch 2, 75.12 ms/it, loss 24.221339
Finished training it 49152/76743 of epoch 2, 74.87 ms/it, loss 24.251062
Finished training it 49152/76743 of epoch 2, 75.23 ms/it, loss 24.453304
Finished training it 50176/76743 of epoch 2, 74.96 ms/it, loss 24.396471
Finished training it 50176/76743 of epoch 2, 75.15 ms/it, loss 24.257986
Finished training it 50176/76743 of epoch 2, 75.08 ms/it, loss 24.517205
Finished training it 50176/76743 of epoch 2, 75.15 ms/it, loss 24.378413
Finished training it 51200/76743 of epoch 2, 74.32 ms/it, loss 24.150465
Finished training it 51200/76743 of epoch 2, 74.46 ms/it, loss 24.439163
Finished training it 51200/76743 of epoch 2, 74.50 ms/it, loss 24.262490
Finished training it 51200/76743 of epoch 2, 74.59 ms/it, loss 24.279513
Testing at - 51200/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 52224/76743 of epoch 2, 74.58 ms/it, loss 24.264502
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 74.75 ms/it, loss 24.125841
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 74.69 ms/it, loss 24.412005
Testing at - 51200/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 52224/76743 of epoch 2, 74.47 ms/it, loss 24.370621
Finished training it 53248/76743 of epoch 2, 75.00 ms/it, loss 24.230039
Finished training it 53248/76743 of epoch 2, 75.18 ms/it, loss 24.282464
Finished training it 53248/76743 of epoch 2, 75.23 ms/it, loss 24.294666
Finished training it 53248/76743 of epoch 2, 75.08 ms/it, loss 24.241198
Finished training it 54272/76743 of epoch 2, 80.02 ms/it, loss 24.405288
Finished training it 54272/76743 of epoch 2, 80.08 ms/it, loss 24.105594
Finished training it 54272/76743 of epoch 2, 80.32 ms/it, loss 24.279751
Finished training it 54272/76743 of epoch 2, 80.47 ms/it, loss 24.472446
Finished training it 55296/76743 of epoch 2, 74.83 ms/it, loss 24.274940
Finished training it 55296/76743 of epoch 2, 74.74 ms/it, loss 24.187426
Finished training it 55296/76743 of epoch 2, 74.60 ms/it, loss 24.082505
Finished training it 55296/76743 of epoch 2, 74.81 ms/it, loss 24.331118
Finished training it 56320/76743 of epoch 2, 74.56 ms/it, loss 24.277372
Finished training it 56320/76743 of epoch 2, 74.33 ms/it, loss 24.188760
Finished training it 56320/76743 of epoch 2, 74.51 ms/it, loss 24.431147
Finished training it 56320/76743 of epoch 2, 74.35 ms/it, loss 24.281176
Finished training it 57344/76743 of epoch 2, 74.50 ms/it, loss 24.233338
Finished training it 57344/76743 of epoch 2, 74.54 ms/it, loss 24.248720
Finished training it 57344/76743 of epoch 2, 74.46 ms/it, loss 24.002506
Finished training it 57344/76743 of epoch 2, 74.29 ms/it, loss 24.292324
Finished training it 58368/76743 of epoch 2, 74.35 ms/it, loss 24.126666
Finished training it 58368/76743 of epoch 2, 74.52 ms/it, loss 24.147354
Finished training it 58368/76743 of epoch 2, 74.43 ms/it, loss 24.222974
Finished training it 58368/76743 of epoch 2, 74.36 ms/it, loss 24.437059
Finished training it 59392/76743 of epoch 2, 74.70 ms/it, loss 24.164856
Finished training it 59392/76743 of epoch 2, 74.96 ms/it, loss 24.268916
Finished training it 59392/76743 of epoch 2, 74.86 ms/it, loss 24.239263
Finished training it 59392/76743 of epoch 2, 74.91 ms/it, loss 24.225540
Finished training it 60416/76743 of epoch 2, 74.83 ms/it, loss 24.051727
Finished training it 60416/76743 of epoch 2, 74.70 ms/it, loss 24.223700
Finished training it 60416/76743 of epoch 2, 74.79 ms/it, loss 24.389684
Finished training it 60416/76743 of epoch 2, 74.69 ms/it, loss 23.950458
Finished training it 61440/76743 of epoch 2, 75.20 ms/it, loss 24.062505
Finished training it 61440/76743 of epoch 2, 75.16 ms/it, loss 24.082128
Finished training it 61440/76743 of epoch 2, 75.05 ms/it, loss 24.362169
Finished training it 61440/76743 of epoch 2, 75.19 ms/it, loss 24.581363
Testing at - 61440/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 62464/76743 of epoch 2, 74.27 ms/it, loss 24.378399
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 74.49 ms/it, loss 24.341869
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 74.40 ms/it, loss 24.405997
Testing at - 61440/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 62464/76743 of epoch 2, 74.21 ms/it, loss 24.458104
Finished training it 63488/76743 of epoch 2, 74.82 ms/it, loss 24.370193
Finished training it 63488/76743 of epoch 2, 74.62 ms/it, loss 24.395617
Finished training it 63488/76743 of epoch 2, 74.70 ms/it, loss 24.421784
Finished training it 63488/76743 of epoch 2, 74.78 ms/it, loss 24.358593
Finished training it 64512/76743 of epoch 2, 80.74 ms/it, loss 24.231569
Finished training it 64512/76743 of epoch 2, 80.50 ms/it, loss 24.247719
Finished training it 64512/76743 of epoch 2, 80.53 ms/it, loss 24.217540
Finished training it 64512/76743 of epoch 2, 80.44 ms/it, loss 24.321469
Finished training it 65536/76743 of epoch 2, 85.43 ms/it, loss 24.190423
Finished training it 65536/76743 of epoch 2, 85.14 ms/it, loss 24.347982
Finished training it 65536/76743 of epoch 2, 85.13 ms/it, loss 24.178863
Finished training it 65536/76743 of epoch 2, 85.49 ms/it, loss 24.242150
Finished training it 66560/76743 of epoch 2, 74.59 ms/it, loss 24.408196
Finished training it 66560/76743 of epoch 2, 74.57 ms/it, loss 24.391996
Finished training it 66560/76743 of epoch 2, 74.52 ms/it, loss 24.294210
Finished training it 66560/76743 of epoch 2, 74.66 ms/it, loss 24.372258
Finished training it 67584/76743 of epoch 2, 75.16 ms/it, loss 24.339801
Finished training it 67584/76743 of epoch 2, 75.20 ms/it, loss 24.346636
Finished training it 67584/76743 of epoch 2, 75.01 ms/it, loss 24.400630
Finished training it 67584/76743 of epoch 2, 75.08 ms/it, loss 24.151942
Finished training it 68608/76743 of epoch 2, 74.33 ms/it, loss 24.249631
Finished training it 68608/76743 of epoch 2, 74.24 ms/it, loss 24.335961
Finished training it 68608/76743 of epoch 2, 74.22 ms/it, loss 24.573837
Finished training it 68608/76743 of epoch 2, 74.42 ms/it, loss 24.419823
Finished training it 69632/76743 of epoch 2, 74.29 ms/it, loss 24.291498
Finished training it 69632/76743 of epoch 2, 74.47 ms/it, loss 24.018729
Finished training it 69632/76743 of epoch 2, 74.51 ms/it, loss 24.135545
Finished training it 69632/76743 of epoch 2, 74.56 ms/it, loss 24.033258
Finished training it 70656/76743 of epoch 2, 74.49 ms/it, loss 24.140912
Finished training it 70656/76743 of epoch 2, 74.22 ms/it, loss 24.481618
Finished training it 70656/76743 of epoch 2, 74.39 ms/it, loss 24.339156
Finished training it 70656/76743 of epoch 2, 74.40 ms/it, loss 24.428839
Finished training it 71680/76743 of epoch 2, 74.93 ms/it, loss 24.120990
Finished training it 71680/76743 of epoch 2, 74.78 ms/it, loss 24.135777
Finished training it 71680/76743 of epoch 2, 74.92 ms/it, loss 24.354201
Finished training it 71680/76743 of epoch 2, 74.73 ms/it, loss 24.364486
Testing at - 71680/76743 of epoch 2,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 72704/76743 of epoch 2, 74.29 ms/it, loss 24.408578
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 74.26 ms/it, loss 23.998104
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 74.35 ms/it, loss 24.445208
Testing at - 71680/76743 of epoch 2,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 72704/76743 of epoch 2, 74.39 ms/it, loss 24.384421
Finished training it 73728/76743 of epoch 2, 75.00 ms/it, loss 24.247046
Finished training it 73728/76743 of epoch 2, 74.85 ms/it, loss 24.434798
Finished training it 73728/76743 of epoch 2, 74.99 ms/it, loss 24.178799
Finished training it 73728/76743 of epoch 2, 74.91 ms/it, loss 24.391226
Finished training it 74752/76743 of epoch 2, 74.67 ms/it, loss 24.089886
Finished training it 74752/76743 of epoch 2, 74.77 ms/it, loss 24.429993
Finished training it 74752/76743 of epoch 2, 74.93 ms/it, loss 24.213227
Finished training it 74752/76743 of epoch 2, 74.87 ms/it, loss 24.560843
Finished training it 75776/76743 of epoch 2, 74.57 ms/it, loss 24.441027
Finished training it 75776/76743 of epoch 2, 74.70 ms/it, loss 24.143740
Finished training it 75776/76743 of epoch 2, 74.71 ms/it, loss 24.160827
Finished training it 75776/76743 of epoch 2, 74.57 ms/it, loss 24.315757
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 78.72 ms/it, loss 24.160130
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 78.57 ms/it, loss 24.268492
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 78.71 ms/it, loss 24.369669
Warning: Skipping the batch 76742 with size 14
Finished training it 1024/76743 of epoch 3, 78.53 ms/it, loss 24.168731
Finished training it 2048/76743 of epoch 3, 75.11 ms/it, loss 24.236587
Finished training it 2048/76743 of epoch 3, 75.20 ms/it, loss 24.161479
Finished training it 2048/76743 of epoch 3, 75.24 ms/it, loss 24.224348
Finished training it 2048/76743 of epoch 3, 75.06 ms/it, loss 24.167861
Finished training it 3072/76743 of epoch 3, 74.99 ms/it, loss 24.544390
Finished training it 3072/76743 of epoch 3, 74.90 ms/it, loss 24.456197
Finished training it 3072/76743 of epoch 3, 75.07 ms/it, loss 24.128246
Finished training it 3072/76743 of epoch 3, 74.86 ms/it, loss 24.263063
Finished training it 4096/76743 of epoch 3, 74.80 ms/it, loss 24.184746
Finished training it 4096/76743 of epoch 3, 74.95 ms/it, loss 24.329958
Finished training it 4096/76743 of epoch 3, 75.09 ms/it, loss 24.430781
Finished training it 4096/76743 of epoch 3, 75.08 ms/it, loss 24.421389
Finished training it 5120/76743 of epoch 3, 75.06 ms/it, loss 24.390902
Finished training it 5120/76743 of epoch 3, 74.91 ms/it, loss 24.440001
Finished training it 5120/76743 of epoch 3, 75.04 ms/it, loss 24.158766
Finished training it 5120/76743 of epoch 3, 74.81 ms/it, loss 24.453278
Finished training it 6144/76743 of epoch 3, 74.78 ms/it, loss 24.371934
Finished training it 6144/76743 of epoch 3, 74.64 ms/it, loss 24.359928
Finished training it 6144/76743 of epoch 3, 74.54 ms/it, loss 24.149218
Finished training it 6144/76743 of epoch 3, 74.70 ms/it, loss 24.139084
Finished training it 7168/76743 of epoch 3, 74.47 ms/it, loss 24.207261
Finished training it 7168/76743 of epoch 3, 74.52 ms/it, loss 24.317252
Finished training it 7168/76743 of epoch 3, 74.46 ms/it, loss 24.213713
Finished training it 7168/76743 of epoch 3, 74.61 ms/it, loss 24.289040
Finished training it 8192/76743 of epoch 3, 74.85 ms/it, loss 24.113641
Finished training it 8192/76743 of epoch 3, 74.72 ms/it, loss 24.071066
Finished training it 8192/76743 of epoch 3, 74.87 ms/it, loss 24.174687
Finished training it 8192/76743 of epoch 3, 74.80 ms/it, loss 24.358855
Finished training it 9216/76743 of epoch 3, 74.50 ms/it, loss 24.309817
Finished training it 9216/76743 of epoch 3, 74.51 ms/it, loss 24.461437
Finished training it 9216/76743 of epoch 3, 74.49 ms/it, loss 24.297129
Finished training it 9216/76743 of epoch 3, 74.62 ms/it, loss 24.313274
Finished training it 10240/76743 of epoch 3, 74.53 ms/it, loss 24.335599
Finished training it 10240/76743 of epoch 3, 74.34 ms/it, loss 24.353952
Finished training it 10240/76743 of epoch 3, 74.38 ms/it, loss 24.231670
Finished training it 10240/76743 of epoch 3, 74.68 ms/it, loss 24.285060
Testing at - 10240/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 11264/76743 of epoch 3, 74.98 ms/it, loss 24.357442
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 74.97 ms/it, loss 24.323598
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 75.02 ms/it, loss 24.467772
Testing at - 10240/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 11264/76743 of epoch 3, 74.83 ms/it, loss 24.220365
Finished training it 12288/76743 of epoch 3, 75.09 ms/it, loss 24.351425
Finished training it 12288/76743 of epoch 3, 75.19 ms/it, loss 24.261673
Finished training it 12288/76743 of epoch 3, 75.04 ms/it, loss 24.309236
Finished training it 12288/76743 of epoch 3, 75.25 ms/it, loss 24.353618
Finished training it 13312/76743 of epoch 3, 74.55 ms/it, loss 24.256704
Finished training it 13312/76743 of epoch 3, 74.58 ms/it, loss 24.489936
Finished training it 13312/76743 of epoch 3, 74.71 ms/it, loss 24.290612
Finished training it 13312/76743 of epoch 3, 74.62 ms/it, loss 24.155407
Finished training it 14336/76743 of epoch 3, 89.98 ms/it, loss 24.171313
Finished training it 14336/76743 of epoch 3, 89.88 ms/it, loss 24.256709
Finished training it 14336/76743 of epoch 3, 90.26 ms/it, loss 24.350832
Finished training it 14336/76743 of epoch 3, 89.86 ms/it, loss 24.472743
Finished training it 15360/76743 of epoch 3, 74.62 ms/it, loss 24.260846
Finished training it 15360/76743 of epoch 3, 74.65 ms/it, loss 24.228047
Finished training it 15360/76743 of epoch 3, 74.46 ms/it, loss 24.181194
Finished training it 15360/76743 of epoch 3, 74.46 ms/it, loss 24.185034
Finished training it 16384/76743 of epoch 3, 75.04 ms/it, loss 24.120525
Finished training it 16384/76743 of epoch 3, 74.90 ms/it, loss 24.328512
Finished training it 16384/76743 of epoch 3, 75.06 ms/it, loss 24.397548
Finished training it 16384/76743 of epoch 3, 74.86 ms/it, loss 24.223141
Finished training it 17408/76743 of epoch 3, 75.15 ms/it, loss 24.131700
Finished training it 17408/76743 of epoch 3, 75.12 ms/it, loss 24.088693
Finished training it 17408/76743 of epoch 3, 75.03 ms/it, loss 24.039388
Finished training it 17408/76743 of epoch 3, 75.02 ms/it, loss 24.278783
Finished training it 18432/76743 of epoch 3, 74.87 ms/it, loss 24.319557
Finished training it 18432/76743 of epoch 3, 75.00 ms/it, loss 24.174317
Finished training it 18432/76743 of epoch 3, 74.99 ms/it, loss 24.424964
Finished training it 18432/76743 of epoch 3, 74.99 ms/it, loss 24.187812
Finished training it 19456/76743 of epoch 3, 75.24 ms/it, loss 24.153289
Finished training it 19456/76743 of epoch 3, 75.19 ms/it, loss 24.529609
Finished training it 19456/76743 of epoch 3, 75.07 ms/it, loss 24.326517
Finished training it 19456/76743 of epoch 3, 74.96 ms/it, loss 24.204686
Finished training it 20480/76743 of epoch 3, 74.78 ms/it, loss 24.093179
Finished training it 20480/76743 of epoch 3, 74.96 ms/it, loss 24.298019
Finished training it 20480/76743 of epoch 3, 74.85 ms/it, loss 24.159413
Finished training it 20480/76743 of epoch 3, 74.98 ms/it, loss 24.375622
Testing at - 20480/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 74.82 ms/it, loss 24.195706
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one0.0.pt
Finished training it 21504/76743 of epoch 3, 74.76 ms/it, loss 24.054095
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 74.63 ms/it, loss 24.272153
Testing at - 20480/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 21504/76743 of epoch 3, 74.85 ms/it, loss 24.281373
Finished training it 22528/76743 of epoch 3, 74.56 ms/it, loss 24.229826
Finished training it 22528/76743 of epoch 3, 74.42 ms/it, loss 24.206991
Finished training it 22528/76743 of epoch 3, 74.54 ms/it, loss 24.471648
Finished training it 22528/76743 of epoch 3, 74.37 ms/it, loss 24.162207
Finished training it 23552/76743 of epoch 3, 74.58 ms/it, loss 24.273973
Finished training it 23552/76743 of epoch 3, 74.77 ms/it, loss 24.619583
Finished training it 23552/76743 of epoch 3, 74.85 ms/it, loss 24.225228
Finished training it 23552/76743 of epoch 3, 74.66 ms/it, loss 24.327694
Finished training it 24576/76743 of epoch 3, 74.86 ms/it, loss 24.280983
Finished training it 24576/76743 of epoch 3, 74.68 ms/it, loss 24.634974
Finished training it 24576/76743 of epoch 3, 74.68 ms/it, loss 24.265896
Finished training it 24576/76743 of epoch 3, 74.92 ms/it, loss 24.274088
Finished training it 25600/76743 of epoch 3, 74.70 ms/it, loss 24.526065
Finished training it 25600/76743 of epoch 3, 74.73 ms/it, loss 24.237091
Finished training it 25600/76743 of epoch 3, 74.53 ms/it, loss 24.171101
Finished training it 25600/76743 of epoch 3, 74.85 ms/it, loss 24.237933
Finished training it 26624/76743 of epoch 3, 74.62 ms/it, loss 24.255146
Finished training it 26624/76743 of epoch 3, 74.70 ms/it, loss 24.218598
Finished training it 26624/76743 of epoch 3, 74.61 ms/it, loss 24.238076
Finished training it 26624/76743 of epoch 3, 74.55 ms/it, loss 24.277781
Finished training it 27648/76743 of epoch 3, 74.28 ms/it, loss 24.321370
Finished training it 27648/76743 of epoch 3, 74.36 ms/it, loss 24.202743
Finished training it 27648/76743 of epoch 3, 74.24 ms/it, loss 24.134486
Finished training it 27648/76743 of epoch 3, 74.40 ms/it, loss 24.396507
Finished training it 28672/76743 of epoch 3, 75.38 ms/it, loss 24.206901
Finished training it 28672/76743 of epoch 3, 75.40 ms/it, loss 24.218743
Finished training it 28672/76743 of epoch 3, 75.40 ms/it, loss 24.217211
Finished training it 28672/76743 of epoch 3, 75.51 ms/it, loss 24.109023
Finished training it 29696/76743 of epoch 3, 74.41 ms/it, loss 24.305786
Finished training it 29696/76743 of epoch 3, 74.21 ms/it, loss 24.321783
Finished training it 29696/76743 of epoch 3, 74.55 ms/it, loss 24.361285
Finished training it 29696/76743 of epoch 3, 74.49 ms/it, loss 24.398887
Finished training it 30720/76743 of epoch 3, 75.36 ms/it, loss 24.240841
Finished training it 30720/76743 of epoch 3, 75.28 ms/it, loss 24.071940
Finished training it 30720/76743 of epoch 3, 75.48 ms/it, loss 24.273695
Finished training it 30720/76743 of epoch 3, 75.40 ms/it, loss 23.992839
Testing at - 30720/76743 of epoch 3,
steps testing: 0.0steps testing: 0.007818302646495447steps testing: 0.015636605292990893steps testing: 0.023454907939486336steps testing: 0.03127321058598179steps testing: 0.039091513232477226steps testing: 0.04690981587897267steps testing: 0.05472811852546812steps testing: 0.06254642117196357steps testing: 0.07036472381845901steps testing: 0.07818302646495445steps testing: 0.0860013291114499steps testing: 0.09381963175794535steps testing: 0.1016379344044408steps testing: 0.10945623705093624steps testing: 0.11727453969743169steps testing: 0.12509284234392715steps testing: 0.13291114499042259steps testing: 0.14072944763691803steps testing: 0.14854775028341347steps testing: 0.1563660529299089steps testing: 0.16418435557640437steps testing: 0.1720026582228998steps testing: 0.17982096086939525steps testing: 0.1876392635158907steps testing: 0.19545756616238616steps testing: 0.2032758688088816steps testing: 0.21109417145537704steps testing: 0.21891247410187248steps testing: 0.22673077674836792steps testing: 0.23454907939486339steps testing: 0.24236738204135883steps testing: 0.2501856846878543steps testing: 0.2580039873343497steps testing: 0.26582228998084517steps testing: 0.2736405926273406steps testing: 0.28145889527383605steps testing: 0.2892771979203315steps testing: 0.29709550056682693steps testing: 0.3049138032133224steps testing: 0.3127321058598178steps testing: 0.3205504085063133steps testing: 0.32836871115280875steps testing: 0.33618701379930416steps testing: 0.3440053164457996steps testing: 0.35182361909229504steps testing: 0.3596419217387905steps testing: 0.36746022438528597steps testing: 0.3752785270317814steps testing: 0.38309682967827685steps testing: 0.3909151323247723steps testing: 0.39873343497126773steps testing: 0.4065517376177632steps testing: 0.4143700402642586steps testing: 0.4221883429107541steps testing: 0.43000664555724954steps testing: 0.43782494820374496steps testing: 0.4456432508502404steps testing: 0.45346155349673584steps testing: 0.4612798561432313steps testing: 0.46909815878972677steps testing: 0.4769164614362222steps testing: 0.48473476408271765steps testing: 0.49255306672921306steps testing: 0.5003713693757086steps testing: 0.508189672022204steps testing: 0.5160079746686994steps testing: 0.5238262773151948steps testing: 0.5316445799616903steps testing: 0.5394628826081858steps testing: 0.5472811852546812steps testing: 0.5550994879011767steps testing: 0.5629177905476721steps testing: 0.5707360931941675steps testing: 0.578554395840663steps testing: 0.5863726984871585steps testing: 0.5941910011336539steps testing: 0.6020093037801493steps testing: 0.6098276064266448steps testing: 0.6176459090731402steps testing: 0.6254642117196356steps testing: 0.6332825143661311steps testing: 0.6411008170126266steps testing: 0.648919119659122steps testing: 0.6567374223056175steps testing: 0.6645557249521129steps testing: 0.6723740275986083steps testing: 0.6801923302451038steps testing: 0.6880106328915992steps testing: 0.6958289355380947steps testing: 0.7036472381845901steps testing: 0.7114655408310856steps testing: 0.719283843477581steps testing: 0.7271021461240764steps testing: 0.7349204487705719steps testing: 0.7427387514170674steps testing: 0.7505570540635628steps testing: 0.7583753567100583steps testing: 0.7661936593565537steps testing: 0.7740119620030491steps testing: 0.7818302646495446steps testing: 0.78964856729604steps testing: 0.7974668699425355steps testing: 0.8052851725890309steps testing: 0.8131034752355264steps testing: 0.8209217778820218steps testing: 0.8287400805285172steps testing: 0.8365583831750127steps testing: 0.8443766858215082steps testing: 0.8521949884680036steps testing: 0.8600132911144991steps testing: 0.8678315937609945steps testing: 0.8756498964074899steps testing: 0.8834681990539853steps testing: 0.8912865017004808steps testing: 0.8991048043469763steps testing: 0.9069231069934717steps testing: 0.9147414096399672steps testing: 0.9225597122864626steps testing: 0.930378014932958steps testing: 0.9381963175794535steps testing: 0.946014620225949steps testing: 0.9538329228724444steps testing: 0.9616512255189399steps testing: 0.9694695281654353steps testing: 0.9772878308119307steps testing: 0.9851061334584261steps testing: 0.9929244361049216Warning: Skipping the batch 25580 with size 90
rank: 0 test_accu: 2418454.0
get out
0 has test check 2418454.0 and sample count 3274240
 accuracy 73.863 %, best 73.863 %, roc auc score 0.4914, best 0.4914
Saving model to /rscratch/data/dlrm_criteo/save_model_after_training_one1.0.pt
Finished training it 31744/76743 of epoch 3, 74.46 ms/it, loss 24.127485
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 1 test_accu: 2418454.0
get out
1 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 74.66 ms/it, loss 24.271353
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 2 test_accu: 2418454.0
get out
2 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 74.71 ms/it, loss 24.311424
Testing at - 30720/76743 of epoch 3,
Warning: Skipping the batch 25580 with size 90
rank: 3 test_accu: 2418454.0
get out
3 has test check 2418454.0 and sample count 3274240
Finished training it 31744/76743 of epoch 3, 74.64 ms/it, loss 24.106060
Finished training it 32768/76743 of epoch 3, 74.47 ms/it, loss 24.280928
Finished training it 32768/76743 of epoch 3, 74.12 ms/it, loss 24.389219
Finished training it 32768/76743 of epoch 3, 74.50 ms/it, loss 24.381620
Finished training it 32768/76743 of epoch 3, 74.40 ms/it, loss 24.230309
Finished training it 33792/76743 of epoch 3, 91.06 ms/it, loss 24.387993
Finished training it 33792/76743 of epoch 3, 91.38 ms/it, loss 24.310300
Finished training it 33792/76743 of epoch 3, 91.08 ms/it, loss 24.495869
Finished training it 33792/76743 of epoch 3, 91.11 ms/it, loss 24.526224
Finished training it 34816/76743 of epoch 3, 74.87 ms/it, loss 24.505708
Finished training it 34816/76743 of epoch 3, 74.81 ms/it, loss 24.230187
Finished training it 34816/76743 of epoch 3, 74.83 ms/it, loss 24.205009
Finished training it 34816/76743 of epoch 3, 74.83 ms/it, loss 24.357910
Finished training it 35840/76743 of epoch 3, 75.01 ms/it, loss 24.143403
Finished training it 35840/76743 of epoch 3, 74.99 ms/it, loss 24.156129
Finished training it 35840/76743 of epoch 3, 74.98 ms/it, loss 24.209030
Finished training it 35840/76743 of epoch 3, 74.85 ms/it, loss 24.532543
Finished training it 36864/76743 of epoch 3, 75.23 ms/it, loss 24.338342
Finished training it 36864/76743 of epoch 3, 75.24 ms/it, loss 24.432832
Finished training it 36864/76743 of epoch 3, 75.23 ms/it, loss 24.182978
Finished training it 36864/76743 of epoch 3, 75.11 ms/it, loss 24.260090
Finished training it 37888/76743 of epoch 3, 74.86 ms/it, loss 24.348713
Finished training it 37888/76743 of epoch 3, 74.73 ms/it, loss 24.226393
Finished training it 37888/76743 of epoch 3, 74.85 ms/it, loss 24.341406
Finished training it 37888/76743 of epoch 3, 74.81 ms/it, loss 24.308601
Finished training it 38912/76743 of epoch 3, 74.64 ms/it, loss 24.202864
Finished training it 38912/76743 of epoch 3, 74.68 ms/it, loss 24.201709
Finished training it 38912/76743 of epoch 3, 74.53 ms/it, loss 24.178926
Finished training it 38912/76743 of epoch 3, 74.51 ms/it, loss 24.266204
Finished training it 39936/76743 of epoch 3, 74.91 ms/it, loss 24.202944
Finished training it 39936/76743 of epoch 3, 74.99 ms/it, loss 24.218407
Finished training it 39936/76743 of epoch 3, 75.10 ms/it, loss 24.383961
Finished training it 39936/76743 of epoch 3, 75.08 ms/it, loss 24.322397
